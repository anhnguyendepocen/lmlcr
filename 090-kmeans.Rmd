<!--
kate: indent-width 4; word-wrap-column 74; default-dictionary en_AU
Copyright (C) 2020, Marek Gagolewski, https://www.gagolewski.com
This material is licensed under the Creative Commons BY-NC-ND 4.0 License.
-->

# Clustering with K-Means {#chap:kmeans}


{ LATEX \color{gray} }

**TODO** In this chapter, we will:

* ...

* ...


{ LATEX \normalcolor }







<!-- TODO: citations


centroid linkage

k-nearest centroids

exercise: Ward linkage




Note that we have studied "crisp" (disjoint) partitions only.



-->




## Within-Cluster Sum of Squares


Prove that it holds:
\[
\begin{align}
\mathrm{Var}(x_1,\dots,x_n)
=&\frac{1}{n} \sum_{i=1}^{n} \left(x_{i}-\bar{x}\right)^{2} \\
=&\frac{1}{n} \sum_{i=1}^{n} \left(x_{i}-\frac{1}{n}\sum_{j=1}^{n} x_j\right)^{2} \\
=&\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}-\left(\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}\right)^{2} \\
=&\frac{1}{2 n^{2}} \sum_{i=1}^{n}\sum_{j=1}^{n} (x_{i}-x_{j})^{2} \\
=&\frac{1}{n^{2}} \sum_{i=1}^{n-1}\sum_{j=i+1}^{n} (x_{i}-x_{j})^{2} \\
\end{align}
\]
In other words, the (biased) sample variance (average
squared distance to the mean, $\bar{x}$)
is equal to
average of squared values minus square of the average
which is equal to half of average distance between each pair of points
(or just the average distance between each unique pair).
Note that the former requires $\sim 2n$ arithmetic operations,
while the latter $\sim n^2$

```{r echo=FALSE}
# x <- rnorm(10)
# mean((x-mean(x))^2)
# mean(outer(x, x, function(a, b) (a-b)^2))/2
```



within-cluster sum of squares:
\[
\begin{align}
f(C_1,\dots,C_K)
=& \sum_{k=1}^K \sum_{x_i\in C_K} \| x_i-\mu_k \|^2  \\
=& \sum_{k=1}^K \sum_{x_i\in C_K} \sum_{u=1}^d \left(x_{i,u}-\mu_{k,u}\right)^2  \\
=& \sum_{k=1}^K  \sum_{u=1}^d \sum_{x_i\in C_K} \left(x_{i,u}-\frac{1}{|C_k|} \sum_{x_j\in C_k} x_{j,u}\right)^2  \\
=& \sum_{k=1}^K \frac{2}{|C_i|} \sum_{x_i\in C_K} \sum_{x_j\in C_K}  \sum_{u=1}^d
 \left(
    x_{i,u}-x_{j,u}
 \right)^2 \\
=& \sum_{k=1}^K \frac{2}{|C_i|} \sum_{x_i\in C_K} \sum_{x_j\in C_K} \| x_i-x_j \|^2
\end{align}
\]


Ward (1963) - agglomerative strategy

Edwards and Cavalli-Sforza (1965) - (greedy?=99% sure) divisive strategy
A. W. F. Edwards and L. L. Cavalli-Sforza, A Method for Cluster Analysis, Biometrics Vol. 21, No. 2 (Jun., 1965), pp. 362-375 DOI: 10.2307/2528096

Calinski and Harabasz - exhaustive divisive over MST - each (n - 1 \choose k-1) possible splits of an MST

??? - agglomerative over MST

??? - greedy divisive over MST

k-means - fixed-point iteration algorithm




TODO?? relation to Linear discriminant analysis and PCA??






## K-means Clustering


### Example in R

..hierarchical clustering is nice, because it outputs a whole
hierarchy of nested partitions and it works in arbitrary spaces
equipped with a distance, but most algorithms are slow for large datasets


Let's begin our clustering adventure
by applying the $K$-means clustering method to find $K=3$ groups
in the famous Fisher's `iris` data set (variables `Sepal.Width`
and `Petal.Length` variables only):

```{r kmeans1b,echo=-1}
set.seed(123)
X <- as.matrix(iris[,c(3,2)])
# never forget to set nstart>>1!
km <- kmeans(X, centers=3, nstart=10)
km$cluster # labels assigned to each of 150 points:
```


Remark.

: Later we'll see that `nstart` is responsible for random restarting the
(local) optimisation procedure, just as we did in the previous chapter.





Let's draw a scatter plot that depicts the detected clusters:

```{r kmeans12, fig.cap="3-means clustering on a projection of the Iris dataset"}
plot(X, col=km$cluster)
```

The colours in Figure \@ref(fig:kmeans12) indicate the detected clusters.
The left group is clearly well-separated from the other two.


What can we do with this information? Well, if we were experts on plants
(in the 1930s), that'd definitely be something ground-breaking.
Figure \@ref(fig:kmeans123) is a version of the aforementioned scatter plot
now with the true iris species added.

```{r kmeans123, fig.cap="3-means clustering (colours) vs true Iris species (shapes)"}
plot(X, col=km$cluster, pch=as.numeric(iris$Species))
```


Here is a contingency table for detected clusters vs. true iris species:

```{r kmeans1234}
(C <- table(km$cluster, iris$Species))
```

It turns out that the discovered partition matches the original iris
species very well. We have just made a "discovery" in the field of
botany (actually some research fields classify their objects of study
into families, genres etc. by means of such tools).

Were the actual Iris species what we had hoped to match?
Was that our aim? Well, surely we have had begun our journey with
"clear minds" (yet with hungry eyes). Note that the true class labels
were not used during the clustering procedure -- we're dealing with
an unsupervised learning problem here. The result turned useful,
it's a win.




Remark.

: (\*) There are several indices that assess the
similarity of two partitions, for example the Adjusted Rand Index (ARI)
the Normalised Mutual Information Score (NMI)
or set matching-based measures,
see, e.g., [@comparing_paritions], [@external_cluster_validity].

<!-- genieclust.compare_partitions ? -->



<!-- OK, this is not the best measure:

sum(apply(C, 1, max))/sum(C) # "accuracy"

genieclust.compare_partitions ?

-->






### Problem Statement

The aim of *$K$-means clustering* is to find $K$ "good" cluster centres
$\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}$.

Then, a point $\mathbf{x}_{i,\cdot}$ will be assigned to the
cluster represented by the closest centre. Here, by *closest*
we mean the *squared* Euclidean distance.

More formally,
assuming all the points are in a $p$-dimensional space, $\mathbb{R}^p$,
we define the distance between the $i$-th point and the $k$-th
centre as:

\[
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}) = \| \mathbf{x}_{i,\cdot} - \boldsymbol\mu_{k,\cdot} \|^2 =  \sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\]

Then the $i$-th point's cluster is determined by:

\[
\mathrm{C}(i) = \mathrm{arg}\min_{k=1,\dots,K}
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}),
\]

where, as usual, $\mathrm{arg}\min$ (argument minimum) is the index
$k$ that minimises the given expression.







In the previous example, the three identified cluster centres in $\mathbb{R}^2$
are given by (see Figure \@ref(fig:kmeans_problem1) for illustration):

```{r kmeans_problem1,fig.cap="Cluster centres (blue dots) identified by the 3-means algorithm"}
km$centers
plot(X, col=km$cluster, asp=1) # asp=1 gives the same scale on both axes
points(km$centers, cex=2, col=4, pch=16)
```





Figure \@ref(fig:kmeans_problem2) depicts the partition
of the whole $\mathbb{R}^2$ space
into clusters based on the closeness to the three cluster centres.

```{r kmeans_problem2,echo=FALSE,fig.cap="The division of the whole space into three sets based on the proximity to cluster centres (a so-called Voronoi diagram)"}
x <- seq(0, 8, length.out=250)
y <- seq(1.0, 5.1, length.out=250)
z <- matrix(NA_real_, length(x), length(y))
for (i in 1:length(x))
    for (j in 1:length(y))
        z[i,j] <- which.min(colSums(((t(km$centers)-c(x[i],y[j]))^2)))
image(x, y, z, ann=FALSE, asp=1,
    axes=FALSE, col=c("#00000044", "#ff000044", "#00ff0044"))
points(X, col=km$cluster)
points(km$centers, cex=2, col=4, pch=16)
```




To compute the distances between all the points and the cluster centres,
we may call `pdist::pdist()`:

```{r kmeans_problem3}
library("pdist")
D <- as.matrix(pdist(X, km$centers))^2
head(D)
```

where `D[i,k]` gives the squared Euclidean distance between
$\mathbf{x}_{i,\cdot}$ and $\boldsymbol\mu_{k,\cdot}$.


The cluster memberships the ($\mathrm{arg}\min$s)
can now be determined by:

```{r}
(idx <- apply(D, 1, which.min)) # for every row of D...
all(km$cluster == idx) # sanity check
```






### Algorithms for the K-means Problem


All good, but how do we find "good" cluster centres?
Good, better, best... yet again we are in a need for a goodness-of-fit metric.
In the $K$-means clustering, we determine
$\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}$
that minimise the total within-cluster distances (distances from each point
to each own cluster centre):

\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{C(i),\cdot}),
\]

Note that the $\boldsymbol\mu$s are also "hidden" inside the point-to-cluster
belongingness mapping, $\mathrm{C}$.
Expanding the above yields:

\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{k=1,\dots,K}
\sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\right).
\]

Unfortunately, the $\min$ operator in the objective function
makes this optimisation problem not tractable
with the methods discussed in the previous chapter.





The above problem is *hard* to solve (\* more precisely,
it is an NP-hard problem).
Therefore, in practice we use various heuristics to solve it.
The `kmeans()` function itself implements 3 of them:
the Hartigan-Wong, Lloyd (a.k.a. Lloyd-Forgy) and MacQueen algorithms.



Remark.

: (\*) Technically, there is no such thing as "*the* K-means algorithm" --
all the aforementioned methods are particular heuristic
approaches to solving the K-means
clustering problem formalised as the above optimisation task.
By setting `nstart = 10` above, we ask the (Hartigan-Wong, which is
the default one in `kmeans()`) algorithm
to find 10 solution candidates obtained by considering different random
initial clusterings and choose the best one (with respect to the sum
of within-cluster distances) amongst them. This does not guarantee
finding the optimal solution, especially for very unbalanced datasets,
but increases the likelihood of such.



Remark.

:   The *squared* Euclidean distance was of course chosen to make computations
    easier. It turns out that for any given subset of input points
    $\mathbf{x}_{i_1,\cdot},\dots,\mathbf{x}_{i_m,\cdot}$,
    the point $\boldsymbol\mu_{k,\cdot}$ that minimises the total  distances
    to all of them, i.e.,

    \[
        \min_{\boldsymbol\mu_{k,\cdot}\in \mathbb{R}^p}
        \sum_{\ell=1}^m \left(
        \sum_{j=1}^p \left(x_{i_\ell,j}-\mu_{k,j}\right)^2
        \right),
    \]

    is exactly these points' *centroid* -- which is given by
    the componentwise arithmetic means of their coordinates.

    For example:

    ```{r kmeans_problem4}
    colMeans(X[km$cluster == 1,]) # centroid of the points in the 1st cluster
    km$centers[1,] # the centre of the 1st cluster
    ```

. . .


TODO: see Lloyd's (and Forgy's) original papers to verify these method defs



Among the various heuristics to solve the K-means problem,
Lloyd's algorithm (1957) is perhaps the simplest.
This is probably the reason why it is sometimes referred
to as "the" K-means algorithm:

1. Start with random cluster centres $\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}$.

2. For each point $\mathbf{x}_{i,\cdot}$, determine its closest centre $C(i)\in\{1,\dots,K\}$:

    \[
    \mathrm{C}(i) = \mathrm{arg}\min_{k=1,\dots,K}
    d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}).
    \]

3. For each cluster $k\in\{1,\dots,K\}$, compute the new cluster centre $\boldsymbol\mu_{k,\cdot}$ as the centroid of
all the point indices $i$ such that $C(i)=k$.

4. If the cluster centres changed since the last iteration, go to step 2,
otherwise stop and return the result.


. . .


(\*) Here's an example implementation.
As the initial cluster centres, let's pick some "noisy" versions
of $K$ randomly chosen points in $\mathbf{X}$.

```{r kmeanimpl1}
set.seed(12345)
K <- 3

# Random initial cluster centres:
M <- jitter(X[sample(1:nrow(X), K),])
M
```


In what follows, we will be maintaining a matrix
such that `D[i,k]` is the distance between the $i$-th point
and the $k$-th centre and a vector such that `idx[i]`
denotes the index of the cluster centre closest to the `i`-th point.

```{r kmeanimpl2}
D <- as.matrix(pdist(X, M))^2
idx <- apply(D, 1, which.min)
```





```{r kmeanimpl3}
repeat {
    # Determine the new cluster centres:
    M <- t(sapply(1:K, function(k) {
        # the centroid of all points in the k-th cluster:
        colMeans(X[idx==k,])
    }))

    # Store the previous cluster belongingness info:
    old_idx <- idx

    # Recompute D and idx:
    D <- as.matrix(pdist(X, M))^2
    idx <- apply(D, 1, which.min)

    # Check if converged already:
    if (all(idx == old_idx)) break
}
```


Let's compare the obtained cluster centres with the ones returned
by `kmeans()`:

```{r kmeanimpl4}
M # our result
km$center # result of kmeans()
```

These two represent exactly the same 3-partitions
(note that the actual labels (the order of centres) are not important).


The value of the objective function (total within-cluster distances)
at the identified candidate solution
is equal to:

```{r kmeansimpl5}
sum(D[cbind(1:nrow(X),idx)]) # indexing with a 2-column matrix!
km$tot.withinss # as reported by kmeans()
```

We would need it if we were to implement the `nstart` functionality,
which is left as an:

{ BEGIN exercise }
(\*) Wrap the implementation of the Lloyd algorithm into a standalone R function,
with a similar look-and-feel as the original `kmeans()`.
{ END exercise }





```{r kmeanimpl_plot,echo=FALSE,warning=FALSE,fig.cap="The arrows denote the cluster centres in each iteration of the Lloyd algorithm"}
set.seed(12345)
K <- 3
M <- jitter(X[sample(1:nrow(X), K),])
plot(X, asp=1, col="#00000044")
D <- as.matrix(pdist(X, M))^2
idx <- apply(D, 1, which.min)
it <- 1
repeat {
    # Determine the new cluster centres:
    old_M <- M
    M <- t(sapply(1:K, function(k) {
        # the centroid of all points in the k-th cluster:
        colMeans(X[idx==k,])
    }))
    suppressWarnings(arrows(old_M[,1], old_M[,2], M[,1], M[,2], col=c(1,2,3), length=0.1, lwd=2))

    # Store the previous cluster belongingness info:
    old_idx <- idx

    # Recompute D and idx:
    D <- as.matrix(pdist(X, M))^2
    idx <- apply(D, 1, which.min)

    # Check if converged already:
    if (all(idx == old_idx)) break
    it <- it+1
}
```

On a side note, our algorithm needed ```r {it}```
iterations to identify the (locally optimal) cluster centres.
Figure \@ref(fig:kmeanimpl_plot) depicts its quest for the clustering grail.


<!--

# TODO: k-medoids???

## TODO




TODO maybe k-medoids, nice for optimisation
plus they allow for different metrics

alternatively, DBSCAN or PCA or MDS or ...



-->





### K-means Revisited





In **K-means clustering** we are minimising the squared Euclidean distance
to each point's cluster centre:
\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{k=1,\dots,K}
\sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\right).
\]


This is an (NP-)hard problem!
There is no efficient exact algorithm.

We need approximations. In the last chapter, we have
discussed the iterative Lloyd's algorithm (1957),
which is amongst a few procedures implemented in the `kmeans()` function.

Lloyd, Stuart P. (1957). "Least square quantization in PCM".
Bell Telephone Laboratories Paper.
Published much later:
Lloyd, Stuart P. (1982). "Least squares quantization in PCM" (PDF).
IEEE Transactions on Information Theory. 28 (2): 129–137.
doi:10.1109/TIT.1982.1056489.

<!-- a simple fixed-point iteration that loops back and forth between the two above update equations until a change in ... becomes negligible.  -->

<!-- Lloyd is  an expectation–maximization (EM)-type algorithm ??? relate/mention Gaussian mixtures ?? -->



MacQueen, J. B. (1967). Some Methods for classification and Analysis
of Multivariate Observations. Proceedings of 5th Berkeley Symposium
on Mathematical Statistics and Probability. 1. University of California Press. pp. 281–297. --
studies the "k-means process" (and its asymptotic behaviour)
*Stated informally, the k-means procedure consists of simply starting with k
groups each of which consists of a single random point,
and thereafter adding each new point to the group whose
mean the new point is nearest. After a point is added to a group,
the mean of that group is adjusted in order to take account of the new
point. Thus at each stage the k-means are, in fact, the means of the groups
they represent (hence the term k-means).* (p.283)

Hartigan-Wong == ??





To recall, Lloyd's algorithm (1957) is sometimes referred to as "the" K-means algorithm:

1. Start with random cluster centres $\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}$.

2. For each point $\mathbf{x}_{i,\cdot}$, determine its closest centre $C(i)\in\{1,\dots,K\}$.

3. For each cluster $k\in\{1,\dots,K\}$, compute the new cluster centre $\boldsymbol\mu_{k,\cdot}$ as the componentwise arithmetic mean
of the coordinates of all the point indices $i$ such that $C(i)=k$.

4. If the cluster centres changed since last iteration, go to step 2, otherwise stop and return the result.

\ \

As the procedure might get stuck in a local minimum,
a few restarts are recommended (as usual).

Hence, we are used to calling:

```{r,eval=FALSE}
kmeans(X, centers=k, nstart=10)
```


### optim() vs. kmeans()




Let us compare how a general-purpose optimiser such as the BFGS algorithm
implemented in `optim()` compares with a customised, problem-specific solver.

We will need some benchmark data.

```{r,gendata,cache=TRUE}
gen_cluster <- function(n, p, m, s) {
    vectors <- matrix(rnorm(n*p), nrow=n, ncol=p)
    unit_vectors <- vectors/sqrt(rowSums(vectors^2))
    unit_vectors*rnorm(n, 0, s)+rep(m, each=n)
}
```

The above function generates $n$ points in $\mathbb{R}^p$
from a distribution centred at $\mathbf{m}\in\mathbb{R}^p$,
spread randomly in every possible direction with scale factor $s$.




Two example clusters in $\mathbb{R}^2$:

```{r,dependson='gendata',gendata_example,cache=TRUE,echo=-1}
set.seed(1243)
# plot the "black" cluster
plot(gen_cluster(500, 2, c(0, 0), 1), col="#00000022", pch=16,
    xlim=c(-3, 4), ylim=c(-3, 4), asp=1, ann=FALSE)
# plot the "red" cluster
points(gen_cluster(250, 2, c(1.5, 1), 0.5), col="#ff000022", pch=16)
```




Let's generate the benchmark dataset $\mathbf{X}$
that consists of three clusters in a high-dimensional space.

```{r,dependson='gendata',gendata2,cache=TRUE}
set.seed(123)
p  <- 32
Ns <- c(50, 100, 20)
Ms <- c(0, 1, 2)
s  <- 1.5*p
K  <- length(Ns)

X <- lapply(1:K, function(k)
    gen_cluster(Ns[k], p, rep(Ms[k], p), s))
X <- do.call(rbind, X) # rbind(X[[1]], X[[2]], X[[3]])
```



<!--
X <- as.matrix(read.csv("datasets/sipu_unbalance.csv",
    header=FALSE, sep=" ", comment.char="#"))
X <- X/10000-30 # a more user-friendly scale
K <- 8
p <- 2

library("FNN")
get_fitness <- function(mu, X) {
    # For each point in X,
    # get the index of the closest point in mu:
    memb <- FNN::get.knnx(mu, X, 1)$nn.index

    # compute the sum of squared distances
    # between each point and its closes cluster centre:
    sum((X-mu[memb,])^2)
}

Storn R, Price K (1997). “Differential Evolution – A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces.”Journal of Global Optimization,11(4), 341–359.

Mullen K, Ardia D, Gil D, Windover D, Cline J (2011). “DEoptim:  An R Package for GlobalOptimization by Differential Evolution.”Journal of Statistical Software,40(6), 1–26. URLhttp://www.jstatsoft.org/v40/i06/

library("DEoptim")
obj <- function(mu) {
            get_fitness(matrix(mu, nrow=K), X)
        }
res <- DEoptim(fn=obj,
        lower=rep(apply(X, 2, min), each=K),
        upper=rep(apply(X, 2, max), each=K)
        #control=list(itermax=1000)
        )

mu_res <- matrix(res$optim$bestmem, nrow=K)
plot(X)
points(mu_res, col=2,cex=3)
get_fitness(mu_res, X)
km <- kmeans(X, mu_res)
get_fitness(km$centers, X)




res <- optim(X[sample(nrow(X), K),],
    fn=obj,
        #lower=rep(apply(X, 2, min), each=K),
        #upper=rep(apply(X, 2, max), each=K),
        method="SANN",
        control = list(maxit = 20000)
        )

mu_res <- matrix(res$par, nrow=K)
plot(X)
points(mu_res, col=2,cex=3)
get_fitness(mu_res, X)
km <- kmeans(X, mu_res)
get_fitness(km$centers, X)

Zambrano-Bigiarini, M., Rojas, R. (2013). “A model-independent Particle Swarm Optimisation software for model calibration.” Environmental Modelling & Software, 43, 5-25. doi: 10.1016/j.envsoft.2013.01.004, http://dx.doi.org/10.1016/j.envsoft.2013.01.004.

library("hydroPSO")
res <- hydroPSO(fn=obj,
        lower=rep(apply(X, 2, min), each=K),
        upper=rep(apply(X, 2, max), each=K)
        #control=list(itermax=1000)
        )

mu_res <- matrix(res$par, nrow=K)
plot(X)
points(mu_res, col=2,cex=3)
get_fitness(mu_res, X)
km <- kmeans(X, mu_res)
get_fitness(km$centers, X)




cntr <- matrix(ncol=2, byrow=TRUE, c( # initial guess
   -15,   5,
   -12,   10,
   -10,   5,
    15,   0,
    15,   10,
    20,   5,
    25,   0,
    25,   10))
km <- kmeans(X, cntr)
get_fitness(km$centers, X)

km <- kmeans(X, K, nstart=10)
get_fitness(km$centers, X)

-->





The objective function for the K-means clustering problem:

```{r,dependson='gendata2',gendata3,cache=TRUE}
library("FNN")
get_fitness <- function(mu, X) {
    # For each point in X,
    # get the index of the closest point in mu:
    memb <- FNN::get.knnx(mu, X, 1)$nn.index

    # compute the sum of squared distances
    # between each point and its closes cluster centre:
    sum((X-mu[memb,])^2)
}
```








Setting up the solvers:

```{r,dependson='gendata3',gendata3b,cache=TRUE}
min_HartiganWong <- function(mu0, X)
    get_fitness(
        # algorithm="Hartigan-Wong"
        kmeans(X, mu0, iter.max=100)$centers,
    X)
min_Lloyd <- function(mu0, X)
    get_fitness(
        kmeans(X, mu0, iter.max=100, algorithm="Lloyd")$centers,
    X)
min_optim <- function(mu0, X)
    optim(mu0,
        function(mu, X) {
            get_fitness(matrix(mu, nrow=nrow(mu0)), X)
        }, X=X, method="BFGS", control=list(reltol=1e-16)
    )$val
```




Running the simulation:

```{r,dependson='gendata3b',gendata4,cache=TRUE,warning=FALSE}
nstart <- 100
set.seed(123)
res <- replicate(nstart, {
  mu0 <- X[sample(nrow(X), K),]
    c(
        HartiganWong=min_HartiganWong(mu0, X),
        Lloyd=min_Lloyd(mu0, X),
        optim=min_optim(mu0, X)
    )
})
```






Notice a considerable variability of the
objective function at the local minima found:

```{r gendata5,dependson='gendata4',cache=TRUE}
boxplot(as.data.frame(t(res)), horizontal=TRUE, col="white")
```






```{r,dependson='gendata5',gendata6,cache=TRUE}
print(apply(res, 1, function(x)
    c(summary(x), sd=sd(x))
))
```

Of course, we are interested in the smallest value of the objective,
because we're trying to pinpoint the global minimum.


```{r,dependson='gendata5',gendata6c,cache=TRUE}
print(apply(res, 1, min))
```





The Hartigan-Wong algorithm (the default one in `kmeans()`)
is the most reliable one of the three:

* it gives the best solution (low bias)
* the solutions have the lowest degree of variability (low variance)
* it is the fastest:





```{r,dependson='gendata3b',gendata4b,cache=TRUE}
library("microbenchmark")
set.seed(123)
mu0 <- X[sample(nrow(X), K),]
summary(microbenchmark(
    HartiganWong=min_HartiganWong(mu0, X),
    Lloyd=min_Lloyd(mu0, X),
    optim=min_optim(mu0, X),
    times=10
), unit="relative")
```





```{r,dependson='gendata5',gendata6b,cache=TRUE}
print(min(res))
```


Is it the global minimum?

> We don't know, we just didn't happen to find anything better (yet).

Did we put enough effort to find it?

> Well, maybe. We can try more random restarts:

```{r,dependson='gendata6',gendata7,cache=TRUE}
res_tried_very_hard <- kmeans(X, K, nstart=100000, iter.max=10000)$centers
print(get_fitness(res_tried_very_hard, X))
```

Is it good enough?

> It depends what we'd like to do with this. Does it make your boss happy?
Does it generate revenue? Does it help solve any other problem?
Is it useful anyhow?
Are you really looking for the global minimum?




## Exercises


### Clustering of the World Factbook


Let's perform a cluster analysis of countries
based on the information contained in the World Factbook dataset:

```{r clustering_factbook1}
factbook <- read.csv("datasets/world_factbook_2020.csv",
    comment.char="#")
```


{ BEGIN exercise }
Remove all the columns that consist of more than 40 missing values.
Then remove all the rows with at least 1 missing value.
{ END exercise }

{ BEGIN solution }

To remove appropriate columns, we must first count the number
of `NA`s in them.

```{r clustering_factbook2a}
count_na_in_columns <- sapply(factbook, function(x) sum(is.na(x)))
factbook <- factbook[count_na_in_columns <= 40] # column removal
```

Getting rid of the rows plagued by missing values is as simple as calling
the `na.omit()` function:

```{r clustering_factbook2b}
factbook <- na.omit(factbook) # row removal
dim(factbook) # how many rows and cols remained
```

Missing value removal is necessary for metric-based
clustering methods, especially K-means. Otherwise, some of the computed distances
would be not available.

{ END solution }



{ BEGIN exercise }
Standardise all the numeric columns.
{ END exercise }

{ BEGIN solution }

Distance-based
methods are very sensitive to the order of magnitude of
the variables, and our dataset is a mess with regards to this
(population, GDP, birth rate, oil production etc.) -- standardisation
of variables is definitely a good idea:

```{r clustering_factbook2}
for (i in 2:ncol(factbook)) # skip `country`
    factbook[[i]] <- (factbook[[i]]-mean(factbook[[i]]))/
                        sd(factbook[[i]])
```

Recall that Z-scores (values of the standardised variables)
have a very intuitive interpretation: $0$ is the value equal to the column
mean, $1$ is one standard deviation above the mean, $-2$ is two standard deviations
below the mean etc.

{ END solution }


{ BEGIN exercise }
Apply the 2-means algorithm, i.e., K-means with $K=2$.
Analyse the results.
{ END exercise }

{ BEGIN solution }

Calling `kmeans()`:

```{r clustering_factbook3,echo=-1}
set.seed(123)
km <- kmeans(factbook[-1], 2, nstart=10)
```

Let's split the country list w.r.t. the obtained cluster labels.
It turns out that the obtained partition is heavily imbalanced, so we'll
print only the contents of the first group:

```{r clustering_factbook4}
km_countries <- split(factbook[[1]], km$cluster)
km_countries[[1]]
```


With regards to which criteria has the K-means algorithm distinguished
the countries? Let's inspect the cluster centres to check the average
Z-scores of all the countries in each cluster:


```{r clustering_factbook5}
t(km$centers) # transposed for readability
```

Countries in Cluster 2 are... average (Z-scores $\simeq 0$).
On the other hand, the three countries in Cluster 1
dominate the others w.r.t. area, population, GDP PPP, labour force etc.


{ END solution }






{ BEGIN exercise }
Apply the complete linkage agglomerative hierarchical clustering algorithm.
{ END exercise }

{ BEGIN solution }

Recall that the complete linkage-based method is implemented in the
`hclust()` function:

```{r clustering_factbook6}
d <- dist(factbook[-1]) # skip `country`
h <- hclust(d, method="complete")
```


A "nice" number of clusters to divide our dataset into
can be read from the dendrogram, see Figure \@ref(fig:clustering_factbook7).


```{r clustering_factbook7,fig.cap="Cluster dendrogram for the World Factbook dataset -- Complete linkage"}
plot(h, labels=FALSE, ann=FALSE); box()
```

It seems that a 9-partition might reveal something interesting,
because it will distinguish two larger country groups.
However, there will be many singletons if we do so either way.

```{r clustering_factbook8}
y <- cutree(h, 9)
h_countries <- split(factbook[[1]], y)
sapply(h_countries, length) # number of elements in each cluster
```

Most likely this is not an interesting partitioning of this dataset,
therefore we'll not be exploring it any further.


{ END solution }



{ BEGIN exercise }
Apply the Genie  clustering algorithm (package `genieclust`).
{ END exercise }

{ BEGIN solution }

The Genie algorithm [@genie] is a hierarchical clustering algorithm
implemented in R package `genieclust`.
Its interface is compatible with `hclust()`.

```{r clustering_factbook9}
library("genieclust")
d <- dist(factbook[-1])
g <- gclust(d)
```

The cluster dendrogram in Figure \@ref(fig:clustering_factbook10)
reveals  3 evident clusters.

```{r clustering_factbook10,fig.cap="Cluster dendrogram for the World Factbook dataset -- Genie algorithm"}
plot(g, labels=FALSE, ann=FALSE); box()
```

Let's determine the 3-partition of the data set.

```{r clustering_factbook11a}
y <- cutree(g, 3)
```

Here are few countries in each cluster:

```{r clustering_factbook11b,echo=-1}
set.seed(123)
y <- cutree(g, 3)
sapply(split(factbook$countr, y), sample, 6)
```


We can draw the countries in each cluster
on a map by using the `rworldmap` package (see its documentation for more details),
see Figure \@ref(fig:clustering_factbook12).

```{r clustering_factbook12,fig.cap="3 clusters discovered by the Genie algorithm",cache=TRUE,message=FALSE,warnings=FALSE,echo=-1}
par(ann=FALSE)
library("rworldmap")
mapdata <- data.frame(country=factbook$country, cluster=y)
# 3 country names must be adjusted to get a match
mapdata$country[mapdata$country == "Czechia"] <- "Czech Republic"
mapdata$country[mapdata$country == "Eswatini"] <- "Swaziland"
mapdata$country[mapdata$country == "Cabo Verde"] <- "Cape Verde"
mapdata <- joinCountryData2Map(mapdata, joinCode="NAME",
  nameJoinColumn="country")
par(mar=c(0,0,0,0))
mapCountryData(mapdata, nameColumnToPlot="cluster",
    catMethod="categorical", missingCountryCol="gray",
    colourPalette=palette()[2:4],
    mapTitle="", addLegend=TRUE)
```

Here are the average Z-scores in each cluster:

```{r clustering_factbook13}
round(sapply(split(factbook[-1], y), colMeans), 3)
```

That is really interesting! The interpretation of the above is left
to the reader.

<!--
# factbook$country[!(factbook$country %in% mapdata$country)]
-->



{ END solution }








### Unbalance Dataset -- K-Means Needs Multiple Starts


Let us consider a benchmark (artificial) dataset
proposed in [@external_cluster_validity]:

```{r sipu_unbalance1}
unbalance <- as.matrix(read.csv("datasets/sipu_unbalance.csv",
    header=FALSE, sep=" ", comment.char="#"))
unbalance <- unbalance/10000-30 # a more user-friendly scale
```

According to its authors, this dataset is comprised of 8 clusters:
there are 3 groups on the lefthand side (2000 points each)
and 5 on the right side (100 each).


```{r sipu_unbalance2,fig.cap="`sipu_unbalance` dataset"}
plot(unbalance, asp=1)
```


{ BEGIN exercise }
Apply the K-means algorithm with $K=8$.
{ END exercise }

{ BEGIN solution }

Of course, here by "the" K-means we mean the default method
available in the `kmeans()` function.
The clustering results are depicted in Figure \@ref(fig:sipu_unbalance3a).

```{r sipu_unbalance3a,echo=-1,fig.cap="Results of K-means on the `sipu_unbalance` dataset",cache=TRUE}
set.seed(123)
km <- kmeans(unbalance, 8, nstart=10)
plot(unbalance, asp=1, col=km$cluster)
```

This is far from what we expected.
The total within-cluster distances are equal to:

```{r sipu_unbalance4a,cache=TRUE}
km$tot.withinss
```


Increasing the number of restarts even further improves the solution,
but the local minimum is still far from the global one,
compare Figure \@ref(fig:sipu_unbalance3b).

```{r sipu_unbalance3b,echo=-1,fig.cap="Results of K-means on the `sipu_unbalance` dataset -- many more restarts",cache=TRUE}
set.seed(1234)
km <- suppressWarnings(kmeans(unbalance, 8, nstart=1000, iter.max=1000))
plot(unbalance, asp=1, col=km$cluster)
```

```{r sipu_unbalance4b,cache=TRUE}
km$tot.withinss
```


{ END solution }




{ BEGIN exercise }
Apply the K-means algorithm starting from a "good" initial guess
on the true cluster centres.
{ END exercise }

{ BEGIN solution }

Clustering is -- in its essence -- an unsupervised learning method,
so what we're going to do now could be called, let's be blunt about it, cheating.
Luckily, we have an oracle at our disposal -- it has provided us
with the following educated guesses (by looking at the scatter plot)
about the localisation of the cluster centres:

```{r sipu_unbalance5,cache=TRUE}
cntr <- matrix(ncol=2, byrow=TRUE, c(
   -15,   5,
   -12,   10,
   -10,   5,
    15,   0,
    15,   10,
    20,   5,
    25,   0,
    25,   10))
```

Running `kmeans()` yields the clustering depicted in Figure \@ref(fig:sipu_unbalance6).

```{r sipu_unbalance6,echo=-1,fig.cap="Results of K-means on the `sipu_unbalance` dataset -- an educated guess on the cluster centres' locations",cache=TRUE}
set.seed(123)
km <- kmeans(unbalance, cntr)
plot(unbalance, asp=1, col=km$cluster)
```

The  total within-cluster distances are now equal to:

```{r sipu_unbalance7,cache=TRUE}
km$tot.withinss
```

This is finally the globally optimal solution to the K-means problem
we were asked to solve. Recall that the algorithms implemented in the `kmeans()`
function are just fast heuristics that are supposed to find
local optima of the K-means objective function, which is given
by the within-cluster sum of squared Euclidean distances.



<!--
http://cs.joensuu.fi/sipu/datasets/:

cntr <- matrix(ncol=2, byrow=TRUE, c(
    209948,   349963,
    539379,   299653,
    440134,   400135,
    440754,   298283,
    491036,   349798,
    150007,   350104,
    538884,   400947,
    179955,   380008))
-->


{ END solution }






### Clustering of Typical 2D Benchmark Datasets

Let us consider a few clustering benchmark datasets
available at https://github.com/gagolews/clustering_benchmarks_v1
and http://cs.joensuu.fi/sipu/datasets/.
Here is a list of file names together with the
corresponding numbers of clusters (as given by datasets' authors):

```{r clustering_benchmarks1,cache=TRUE}
files <- c("datasets/wut_isolation.csv",
           "datasets/wut_mk2.csv",
           "datasets/wut_z3.csv",
           "datasets/sipu_aggregation.csv",
           "datasets/sipu_pathbased.csv",
           "datasets/sipu_unbalance.csv")
Ks <- c(3, 2, 4, 7, 3, 8)
```

All the datasets are two-dimensional, hence we'll be able to visualise
the obtained results and assess the sensibility of the obtained clusterings.

{ BEGIN exercise }
Apply the K-means, the single, average and complete linkage
and the Genie algorithm (from package `genieclust`)
on the aforementioned datasets and discuss the results.
{ END exercise }

{ BEGIN solution }

Apart from a call to the Genie algorithm with the default parameters,
we will also look at the results it generates when we set the
`gini_threshold` parameter to 0.5 (default is 0.3; smaller thresholds
lead to clusters of more balanced sizes as measured by the Gini index).

The following function is our workhorse that will perform all the computations
and will draw all the figures for a single dataset:

```{r clustering_benchmarks2,dependson="clustering_benchmarks1",cache=TRUE}
clusterise <- function(file, K) {
    X <- read.csv(file,
        header=FALSE, sep=" ", comment.char="#")
    d <- dist(X)
    par(mfrow=c(2, 3))
    par(mar=c(0.5, 0.5, 2, 0.5))

    y <- kmeans(X, K, nstart=10)$cluster
    plot(X, asp=1, col=y, ann=FALSE, axes=FALSE)
    mtext("K-means", line=0.5)

    y <- cutree(hclust(d, "complete"), K)
    plot(X, asp=1, col=y, ann=FALSE, axes=FALSE)
    mtext("Complete Linkage", line=0.5)

    y <- cutree(hclust(d, "average"), K)
    plot(X, asp=1, col=y, ann=FALSE, axes=FALSE)
    mtext("Average Linkage", line=0.5)

    y <- cutree(hclust(d, "single"), K)
    plot(X, asp=1, col=y, ann=FALSE, axes=FALSE)
    mtext("Single Linkage", line=0.5)

    y <- genie(d, K) # gini_threshold=0.3
    plot(X, asp=1, col=y, ann=FALSE, axes=FALSE)
    mtext("Genie (default)", line=0.5)

    y <- genie(d, K, gini_threshold=0.5)
    plot(X, asp=1, col=y, ann=FALSE, axes=FALSE)
    mtext("Genie (g=0.5)", line=0.5)
}
```


Applying the above as `clusterise(files[i], Ks[i])` yields
Figures \@ref(fig:clustering_benchmarks_plot1)-\@ref(fig:clustering_benchmarks_plot6).

```{r clustering_benchmarks_plot1,dependson="clustering_benchmarks2",cache=TRUE,echo=FALSE,fig.height=4,fig.cap="Clustering of the `wut_isolation` dataset"}
suppressWarnings(clusterise(files[1], Ks[1]))
```

```{r clustering_benchmarks_plot2,dependson="clustering_benchmarks2",cache=TRUE,echo=FALSE,fig.height=4,fig.cap="Clustering of the `wut_mk2` dataset"}
suppressWarnings(clusterise(files[2], Ks[2]))
```

```{r clustering_benchmarks_plot3,dependson="clustering_benchmarks2",cache=TRUE,echo=FALSE,fig.height=4,fig.cap="Clustering of the `wut_z3` dataset"}
suppressWarnings(clusterise(files[3], Ks[3]))
```

```{r clustering_benchmarks_plot4,dependson="clustering_benchmarks2",cache=TRUE,echo=FALSE,fig.height=4,fig.cap="Clustering of the `sipu_aggregation` dataset"}
suppressWarnings(clusterise(files[4], Ks[4]))
```

```{r clustering_benchmarks_plot5,dependson="clustering_benchmarks2",cache=TRUE,echo=FALSE,fig.height=4,fig.cap="Clustering of the `sipu_pathbased` dataset"}
suppressWarnings(clusterise(files[5], Ks[5]))
```

```{r clustering_benchmarks_plot6,dependson="clustering_benchmarks2",cache=TRUE,echo=FALSE,fig.height=4,fig.cap="Clustering of the `sipu_unbalance` dataset"}
suppressWarnings(clusterise(files[6], Ks[6]))
```

Note that, by definition, K-means is only able to detect clusters
of convex shapes. The Genie algorithm, on the other hand, might
fail to detect clusters of very small sizes amongst the more populous ones.
Single linkage is very sensitive to outliers in data -- it often outputs
clusters of cardinality 1.


{ END solution }





### Wine Quality -- `volatile.acidity` and `sulphates`

Let's consider the Wine Quality dataset:

```{r task_clustering1_1}
wine_quality <- read.csv("datasets/wine_quality_all.csv",
    comment.char="#")
```

{ BEGIN exercise }
Apply the 2-means clustering algorithm (i.e., K-means with $K=2$)
on a subset of `wine_quality` consisting only of
the `volatile.acidity` and `sulphates` columns.
{ END exercise }

{ BEGIN exercise }
Print the contingency table (see the `table()` function)
for the discovered clusters vs. the wine colour (see the `color`
column in `wine_quality`).
{ END exercise }


{ BEGIN exercise }
Draw two scatter plots of `volatile.acidity` vs. `sulphates`:

* one where points' colours correspond to the discovered cluster labels
    (1st cluster = black symbols, 2nd cluster = red symbols),
* the other one such that points' colours correspond to the true wine colours
    (white wines = black symbols, red wines = red symbols).

Is there are a good match between the discovered clusters
and the true wine colours? Discuss.
{ END exercise }

<!--
(n <- nrow(wines)) # number of samples
for (i in 1:11)
    wines[[i]] <- (wines[[i]]-mean(wines[[i]]))/sd(wines[[i]])
names(wines)

ari_best <- 0
for (i in 1:10) {
    for (j in (i+1):11) {
        X <- as.matrix(wines[c(i,j)])
        y_pred <- kmeans(X, 2, nstart=10)$cluster
        y_true <- as.integer(factor(wines[["color"]]))
        #plot(X, pch=y_pred, col=y_true, asp=1)
        #
        ari <- mclust::adjustedRandIndex(y_pred, y_true)
        if (ari > ari_best[1])
            ari_best <- c(ari, i, j)
    }
}

print(ari_best)
X <- as.matrix(wines[ari_best[-1]])
y_pred <- kmeans(X, 2, nstart=10)$cluster
par(mfrow=c(1,2))
plot(X, pch=y_pred, col=y_true, asp=1)
plot(X, col=y_pred, pch=y_true, asp=1)
table(y_pred, y_true)
-->




### Wine Quality -- `chlorides` and `total.sulfur.dioxide`

The Wine Quality dataset again:

```{r task_clustering2_1}
wine_quality <- read.csv("datasets/wine_quality_all.csv",
    comment.char="#")
```


{ BEGIN exercise }
Create a matrix `X` by extracting the `chlorides`
and `total.sulfur.dioxide` columns from `wines`.
{ END exercise }


{ BEGIN exercise }
Standardise both columns in `X` (i.e., from each column,
subtract its mean and then divide by its standard deviation).
{ END exercise }


{ BEGIN exercise }
Apply the 2-means clustering algorithm (i.e., K-means with $K=2$) on `X`.
Store the obtained cluster labels in a vector named `y_kmeans`.
{ END exercise }


{ BEGIN exercise }
Apply the average linkage hierarchical clustering method.
Cut the obtained hierarchy (see the `cutree()` function)
so as to obtain 2 clusters and store the results in a vector named `y_average`.
{ END exercise }


{ BEGIN exercise }
Apply the single linkage hierarchical clustering method.
Cut the obtained hierarchy into two groups and store the results in
a vector named `y_single`.
{ END exercise }


{ BEGIN exercise }
Apply the complete linkage hierarchical clustering method.
Cut the obtained hierarchy (see the `cutree()` function)
into two groups and store the results in a vector named `y_complete`.
{ END exercise }


{ BEGIN exercise }
Print the 4 contingency tables (see the `table()` function)
for each of the 4 discovered partitions (`y_kmeans`, ..., `y_complete`)
vs. the wine colour (see the `color` column in `wines`).
Is there are a good match between the discovered clusters
and the true wine colours? Discuss.
{ END exercise }






## Outro

### Remarks



In K-means, we need to specify the number of clusters, $K$, in advance.
What if we don't have any idea how to choose this parameter (which is often
the case)?

Also, the problem with K-means is that there is no guarantee that a
$K$-partition is any "similar" to the $K'$-one for $K\neq K'$,
see Figure \@ref(fig:kmeans_different_K).



```{r kmeans1,echo=-1}
set.seed(123)
X <- as.matrix(iris[,c(3,2)])
# never forget to set nstart>>1!
km <- kmeans(X, centers=3, nstart=10)
km$cluster # labels assigned to each of 150 points:
```



```{r kmeans_different_K,fig.cap="3-means (colours) vs. 4-means (symbols) on example data; the \"circle\" cluster cannot decide if it likes the green or the black one more"}
km1 <- kmeans(X, 3, nstart=10)
km2 <- kmeans(X, 4, nstart=10)
plot(X, col=km1$cluster, pch=km2$cluster, asp=1)
```




Unsupervised learning is often performed during the data pre-processing
and exploration stage.
Assessing the quality of clustering is particularly challenging as,
unlike in a supervised setting,
we have no access to "ground truth" information.



In practice, we often apply different clustering algorithms
and just see where they lead us. There's no teacher that would
tell us what we should do, so whatever we do is awesome, right?
Well, not precisely. Most frequently, you, my dear reader, will work
for some party that's genuinely
interested in your explaining why did you spent the last month coming up
with nothing useful at all. Thus, the main body of work related to proving
the use-ful*l*/less-ness will be on you.


Clustering methods can aid us in supervised tasks
-- instead of fitting a single "large model", it might be
useful to fit separate models to each cluster.


. . .

To sum up, the aim of K-means is to find
$K$ clusters based on the notion of the points' closeness
to the cluster centres. Remember that $K$ must be set in advance.
By definition (\* via its relation to Voronoi diagrams),
all clusters will be of convex shapes.

However, we may try applying $K'$-means for $K' \gg  K$
to obtain a "fine grained" compressed representation of data and then
combine the (sub)clusters into more meaningful groups
using other methods (such as the hierarchical ones).

Iterative K-means algorithms are very fast (e.g., a mini-batch
version of the algorithm can be implement to speed up the optimisation
process) even for large data sets,
but they may fail to find a desirable solution, especially if clusters
are unbalanced.



. . .

Hierarchical methods, on the other hand, output
a whole family of mutually nested partitions, which may provide us
with insight into the underlying structure of data data.
Unfortunately, there is no easy way to assign new points
to existing clusters; yet, you can always build a classifier
(e.g., a decision tree or a neural network) that learns
the discovered labels.

A linkage scheme must be chosen with care, for instance, single linkage
can be sensitive to outliers. However, it is generally the fastest.
The methods implemented in `hclust()` are generally slow; they  have
time complexity  between $O(n^2)$ and $O(n^3)$.


Remark.

: Note that the `fastcluster` package provides a more efficient
and memory-saving
implementation of some methods available via a call to `hclust()`.
See also the `genieclust` package for a super-robust version
of the single linkage
algorithm based on the datasets's Euclidean minimum spanning tree,
which can be computed quite quickly.

```{r eval=FALSE,echo=FALSE}
library("fastcluster")
library("genieclust")
```




Finally, note that all the discussed clustering methods are based on the
notion of pairwise distances. These of course tend
to behave weirdly in high-dimensional
spaces ("the curse of dimensionality"). Moreover, some hardcore
feature engineering might be needed to obtain meaningful results.






{ LATEX \color{gray} }

. . .

**TODO** .....


. . .

{ LATEX \normalcolor }





Recommended further reading: [@islr: Section 10.3]

Other: [@esl: Section 14.3]



Additionally, check out other noteworthy clustering approaches:

- Genie (see R package `genieclust`) [@genie]
- ITM [@itm]
- DBSCAN, HDBSCAN* [@pre_dbscan; @dbscan; @hdbscan]
- K-medoids, K-medians
- Fuzzy C-means (a.k.a. weighted K-means) [@cmeans]
- Spectral clustering; e.g., [@spectral_nips]
- BIRCH [@birch]



Next chapter...

