<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Agglomerative Hierarchical Clustering | Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Agglomerative Hierarchical Clustering | Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Agglomerative Hierarchical Clustering | Machine Learning Classics with R" />
  
  <meta name="twitter:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  

<meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap-introduction.html"/>
<link rel="next" href="chap-knn.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

div.remark {
    border-left: 2px solid gray;
    padding-left: 1em;
}

div.definition {
    font-style: italic;
    border-left: 2px solid #550000;
    padding-left: 1em;
}

</style>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="chap-introduction.html"><a href="chap-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="chap-introduction.html"><a href="chap-introduction.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a><ul>
<li class="chapter" data-level="1.1.1" data-path="chap-introduction.html"><a href="chap-introduction.html#data-sources"><i class="fa fa-check"></i><b>1.1.1</b> Data Sources</a></li>
<li class="chapter" data-level="1.1.2" data-path="chap-introduction.html"><a href="chap-introduction.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chap-introduction.html"><a href="chap-introduction.html#input-data-x"><i class="fa fa-check"></i><b>1.2</b> Input Data, <strong>X</strong></a><ul>
<li class="chapter" data-level="1.2.1" data-path="chap-introduction.html"><a href="chap-introduction.html#abstract-formalism"><i class="fa fa-check"></i><b>1.2.1</b> Abstract Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="chap-introduction.html"><a href="chap-introduction.html#concrete-example"><i class="fa fa-check"></i><b>1.2.2</b> Concrete Example</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="chap-introduction.html"><a href="chap-introduction.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.3</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="1.3.1" data-path="chap-introduction.html"><a href="chap-introduction.html#dimensionality-reduction"><i class="fa fa-check"></i><b>1.3.1</b> Dimensionality Reduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="chap-introduction.html"><a href="chap-introduction.html#anomaly-detection"><i class="fa fa-check"></i><b>1.3.2</b> Anomaly Detection</a></li>
<li class="chapter" data-level="1.3.3" data-path="chap-introduction.html"><a href="chap-introduction.html#clustering"><i class="fa fa-check"></i><b>1.3.3</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="chap-introduction.html"><a href="chap-introduction.html#supervised-learning"><i class="fa fa-check"></i><b>1.4</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.4.1" data-path="chap-introduction.html"><a href="chap-introduction.html#desired-outputs-y"><i class="fa fa-check"></i><b>1.4.1</b> Desired Outputs, <strong>y</strong></a></li>
<li class="chapter" data-level="1.4.2" data-path="chap-introduction.html"><a href="chap-introduction.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.4.2</b> Types of Supervised Learning Problems</a></li>
<li class="chapter" data-level="1.4.3" data-path="chap-introduction.html"><a href="chap-introduction.html#one-dataset-many-problems"><i class="fa fa-check"></i><b>1.4.3</b> One Dataset – Many Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-hclust.html"><a href="chap-hclust.html"><i class="fa fa-check"></i><b>2</b> Agglomerative Hierarchical Clustering</a><ul>
<li class="chapter" data-level="2.1" data-path="chap-hclust.html"><a href="chap-hclust.html#dataset-partitions"><i class="fa fa-check"></i><b>2.1</b> Dataset Partitions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="chap-hclust.html"><a href="chap-hclust.html#label-vectors"><i class="fa fa-check"></i><b>2.1.1</b> Label Vectors</a></li>
<li class="chapter" data-level="2.1.2" data-path="chap-hclust.html"><a href="chap-hclust.html#k-partitions"><i class="fa fa-check"></i><b>2.1.2</b> K-Partitions</a></li>
<li class="chapter" data-level="2.1.3" data-path="chap-hclust.html"><a href="chap-hclust.html#interesting-partitions"><i class="fa fa-check"></i><b>2.1.3</b> “Interesting” Partitions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chap-hclust.html"><a href="chap-hclust.html#sec:euclidean"><i class="fa fa-check"></i><b>2.2</b> Euclidean Distance</a></li>
<li class="chapter" data-level="2.3" data-path="chap-hclust.html"><a href="chap-hclust.html#hierarchical-clustering-at-a-glance"><i class="fa fa-check"></i><b>2.3</b> Hierarchical Clustering at a Glance</a></li>
<li class="chapter" data-level="2.4" data-path="chap-hclust.html"><a href="chap-hclust.html#cluster-dendrograms"><i class="fa fa-check"></i><b>2.4</b> Cluster Dendrograms</a></li>
<li class="chapter" data-level="2.5" data-path="chap-hclust.html"><a href="chap-hclust.html#linkage-functions"><i class="fa fa-check"></i><b>2.5</b> Linkage Functions</a></li>
<li class="chapter" data-level="2.6" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises-in-r"><i class="fa fa-check"></i><b>2.6</b> Exercises in R</a></li>
<li class="chapter" data-level="2.7" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>2.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-knn.html"><a href="chap-knn.html"><i class="fa fa-check"></i><b>3</b> Classification and Regression with K-Nearest Neighbours</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="chap-knn.html"><a href="chap-knn.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="chap-knn.html"><a href="chap-knn.html#data"><i class="fa fa-check"></i><b>3.1.2</b> Data</a></li>
<li class="chapter" data-level="3.1.3" data-path="chap-knn.html"><a href="chap-knn.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.3</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.4" data-path="chap-knn.html"><a href="chap-knn.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.4</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="chap-knn.html"><a href="chap-knn.html#k-nearest-neighbour-classifier"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a><ul>
<li class="chapter" data-level="3.2.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction-1"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="chap-knn.html"><a href="chap-knn.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="chap-knn.html"><a href="chap-knn.html#feature-engineering"><i class="fa fa-check"></i><b>3.2.3</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="chap-knn.html"><a href="chap-knn.html#model-assessment-and-selection"><i class="fa fa-check"></i><b>3.3</b> Model Assessment and Selection</a><ul>
<li class="chapter" data-level="3.3.1" data-path="chap-knn.html"><a href="chap-knn.html#performance-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="chap-knn.html"><a href="chap-knn.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>3.3.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="3.3.3" data-path="chap-knn.html"><a href="chap-knn.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>3.3.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="chap-knn.html"><a href="chap-knn.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.4</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="chap-knn.html"><a href="chap-knn.html#factor-data-type"><i class="fa fa-check"></i><b>3.4.1</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.4.2" data-path="chap-knn.html"><a href="chap-knn.html#main-routine"><i class="fa fa-check"></i><b>3.4.2</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.4.3" data-path="chap-knn.html"><a href="chap-knn.html#mode"><i class="fa fa-check"></i><b>3.4.3</b> Mode</a></li>
<li class="chapter" data-level="3.4.4" data-path="chap-knn.html"><a href="chap-knn.html#nn-search-routines"><i class="fa fa-check"></i><b>3.4.4</b> NN Search Routines (*)</a></li>
<li class="chapter" data-level="3.4.5" data-path="chap-knn.html"><a href="chap-knn.html#different-metrics"><i class="fa fa-check"></i><b>3.4.5</b> Different Metrics (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="chap-knn.html"><a href="chap-knn.html#exercises"><i class="fa fa-check"></i><b>3.5</b> Exercises</a><ul>
<li class="chapter" data-level="3.5.1" data-path="chap-knn.html"><a href="chap-knn.html#wine-quality-best-k-nn-parameters-via-cross-validation"><i class="fa fa-check"></i><b>3.5.1</b> Wine Quality – Best K-NN Parameters via Cross-Validation (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="chap-knn.html"><a href="chap-knn.html#outro"><i class="fa fa-check"></i><b>3.6</b> Outro</a><ul>
<li class="chapter" data-level="3.6.1" data-path="chap-knn.html"><a href="chap-knn.html#remarks-1"><i class="fa fa-check"></i><b>3.6.1</b> Remarks</a></li>
<li class="chapter" data-level="3.6.2" data-path="chap-knn.html"><a href="chap-knn.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.6.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.6.3" data-path="chap-knn.html"><a href="chap-knn.html#further-reading"><i class="fa fa-check"></i><b>3.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-trees.html"><a href="chap-trees.html"><i class="fa fa-check"></i><b>4</b> Decision and Regression Trees</a><ul>
<li class="chapter" data-level="4.1" data-path="chap-trees.html"><a href="chap-trees.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="chap-trees.html"><a href="chap-trees.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="chap-trees.html"><a href="chap-trees.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="chap-trees.html"><a href="chap-trees.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision Trees</a><ul>
<li class="chapter" data-level="4.2.1" data-path="chap-trees.html"><a href="chap-trees.html#introduction-3"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="chap-trees.html"><a href="chap-trees.html#example-in-r-1"><i class="fa fa-check"></i><b>4.2.2</b> Example in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="chap-trees.html"><a href="chap-trees.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="chap-trees.html"><a href="chap-trees.html#exercises-1"><i class="fa fa-check"></i><b>4.3</b> Exercises</a><ul>
<li class="chapter" data-level="4.3.1" data-path="chap-trees.html"><a href="chap-trees.html#edstats-where-girls-are-better-at-maths-than-boys"><i class="fa fa-check"></i><b>4.3.1</b> EdStats – Where Girls Are Better at Maths Than Boys?</a></li>
<li class="chapter" data-level="4.3.2" data-path="chap-trees.html"><a href="chap-trees.html#edstats-and-world-factbook-joining-forces"><i class="fa fa-check"></i><b>4.3.2</b> EdStats and World Factbook – Joining Forces</a></li>
<li class="chapter" data-level="4.3.3" data-path="chap-trees.html"><a href="chap-trees.html#wine-quality-random-forest-and-xgboost"><i class="fa fa-check"></i><b>4.3.3</b> Wine Quality – Random Forest and XGBoost (*)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chap-trees.html"><a href="chap-trees.html#outro-1"><i class="fa fa-check"></i><b>4.4</b> Outro</a><ul>
<li class="chapter" data-level="4.4.1" data-path="chap-trees.html"><a href="chap-trees.html#remarks-2"><i class="fa fa-check"></i><b>4.4.1</b> Remarks</a></li>
<li class="chapter" data-level="4.4.2" data-path="chap-trees.html"><a href="chap-trees.html#further-reading-1"><i class="fa fa-check"></i><b>4.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html"><i class="fa fa-check"></i><b>5</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#simple-regression"><i class="fa fa-check"></i><b>5.1</b> Simple Regression</a><ul>
<li class="chapter" data-level="5.1.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#introduction-4"><i class="fa fa-check"></i><b>5.1.1</b> Introduction</a></li>
<li class="chapter" data-level="5.1.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#search-space-and-objective"><i class="fa fa-check"></i><b>5.1.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#introduction-5"><i class="fa fa-check"></i><b>5.2.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#solution-in-r"><i class="fa fa-check"></i><b>5.2.2</b> Solution in R</a></li>
<li class="chapter" data-level="5.2.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#analytic-solution"><i class="fa fa-check"></i><b>5.2.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="5.2.4" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>5.2.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#exercises-2"><i class="fa fa-check"></i><b>5.3</b> Exercises</a><ul>
<li class="chapter" data-level="5.3.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>5.3.1</b> The Anscombe Quartet</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#median-house-value-in-boston"><i class="fa fa-check"></i><b>5.3.2</b> Median House Value in Boston</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#outro-2"><i class="fa fa-check"></i><b>5.4</b> Outro</a><ul>
<li class="chapter" data-level="5.4.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#remarks-3"><i class="fa fa-check"></i><b>5.4.1</b> Remarks</a></li>
<li class="chapter" data-level="5.4.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#further-reading-2"><i class="fa fa-check"></i><b>5.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html"><i class="fa fa-check"></i><b>6</b> Multiple Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#introduction-6"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#formalism"><i class="fa fa-check"></i><b>6.1.1</b> Formalism</a></li>
<li class="chapter" data-level="6.1.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>6.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#multiple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#problem-formulation"><i class="fa fa-check"></i><b>6.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="6.2.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>6.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#finding-the-best-model"><i class="fa fa-check"></i><b>6.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="6.3.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#model-diagnostics"><i class="fa fa-check"></i><b>6.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="6.3.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#variable-selection"><i class="fa fa-check"></i><b>6.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="6.3.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#variable-transformation"><i class="fa fa-check"></i><b>6.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="6.3.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>6.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#exercises-3"><i class="fa fa-check"></i><b>6.4</b> Exercises</a><ul>
<li class="chapter" data-level="6.4.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>6.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="6.4.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>6.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="6.4.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>6.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="6.4.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>6.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="6.4.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>6.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
<li class="chapter" data-level="6.4.6" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#median-house-value-in-boston-continued"><i class="fa fa-check"></i><b>6.4.6</b> Median House Value in Boston (Continued)</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#outro-3"><i class="fa fa-check"></i><b>6.5</b> Outro</a><ul>
<li class="chapter" data-level="6.5.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#remarks-4"><i class="fa fa-check"></i><b>6.5.1</b> Remarks</a></li>
<li class="chapter" data-level="6.5.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#other-methods-for-regression"><i class="fa fa-check"></i><b>6.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="6.5.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>6.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="6.5.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>6.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="6.5.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>6.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="6.5.6" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#further-reading-3"><i class="fa fa-check"></i><b>6.5.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-logistic.html"><a href="chap-logistic.html"><i class="fa fa-check"></i><b>7</b> Classification with Linear Models</a><ul>
<li class="chapter" data-level="7.1" data-path="chap-logistic.html"><a href="chap-logistic.html#introduction-7"><i class="fa fa-check"></i><b>7.1</b> Introduction</a><ul>
<li class="chapter" data-level="7.1.1" data-path="chap-logistic.html"><a href="chap-logistic.html#classification-task-2"><i class="fa fa-check"></i><b>7.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="7.1.2" data-path="chap-logistic.html"><a href="chap-logistic.html#data-2"><i class="fa fa-check"></i><b>7.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chap-logistic.html"><a href="chap-logistic.html#binary-logistic-regression"><i class="fa fa-check"></i><b>7.2</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="chap-logistic.html"><a href="chap-logistic.html#motivation"><i class="fa fa-check"></i><b>7.2.1</b> Motivation</a></li>
<li class="chapter" data-level="7.2.2" data-path="chap-logistic.html"><a href="chap-logistic.html#logistic-model"><i class="fa fa-check"></i><b>7.2.2</b> Logistic Model</a></li>
<li class="chapter" data-level="7.2.3" data-path="chap-logistic.html"><a href="chap-logistic.html#example-in-r-2"><i class="fa fa-check"></i><b>7.2.3</b> Example in R</a></li>
<li class="chapter" data-level="7.2.4" data-path="chap-logistic.html"><a href="chap-logistic.html#loss-function-cross-entropy"><i class="fa fa-check"></i><b>7.2.4</b> Loss Function: Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chap-logistic.html"><a href="chap-logistic.html#exercises-4"><i class="fa fa-check"></i><b>7.3</b> Exercises</a><ul>
<li class="chapter" data-level="7.3.1" data-path="chap-logistic.html"><a href="chap-logistic.html#edstats-fitting-of-binary-logistic-regression-models"><i class="fa fa-check"></i><b>7.3.1</b> EdStats – Fitting of Binary Logistic Regression Models</a></li>
<li class="chapter" data-level="7.3.2" data-path="chap-logistic.html"><a href="chap-logistic.html#edstats-variable-selection-in-binary-logistic-regression"><i class="fa fa-check"></i><b>7.3.2</b> EdStats – Variable Selection in Binary Logistic Regression (*)</a></li>
<li class="chapter" data-level="7.3.3" data-path="chap-logistic.html"><a href="chap-logistic.html#currency-exchange-rates-growthfall"><i class="fa fa-check"></i><b>7.3.3</b> Currency Exchange Rates Growth/Fall</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="chap-logistic.html"><a href="chap-logistic.html#outro-4"><i class="fa fa-check"></i><b>7.4</b> Outro</a><ul>
<li class="chapter" data-level="7.4.1" data-path="chap-logistic.html"><a href="chap-logistic.html#remarks-5"><i class="fa fa-check"></i><b>7.4.1</b> Remarks</a></li>
<li class="chapter" data-level="7.4.2" data-path="chap-logistic.html"><a href="chap-logistic.html#further-reading-4"><i class="fa fa-check"></i><b>7.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html"><i class="fa fa-check"></i><b>8</b> Continuous Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#introduction-8"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#optimisation-problems"><i class="fa fa-check"></i><b>8.1.1</b> Optimisation Problems</a></li>
<li class="chapter" data-level="8.1.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>8.1.2</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="8.1.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>8.1.3</b> Example Objective over a 2D Domain</a></li>
<li class="chapter" data-level="8.1.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>8.1.4</b> Example Optimisation Problems in Machine Learning</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#iterative-methods"><i class="fa fa-check"></i><b>8.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="8.2.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#introduction-9"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-in-r-3"><i class="fa fa-check"></i><b>8.2.2</b> Example in R</a></li>
<li class="chapter" data-level="8.2.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>8.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="8.2.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#random-restarts"><i class="fa fa-check"></i><b>8.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#gradient-descent"><i class="fa fa-check"></i><b>8.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="8.3.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#function-gradient"><i class="fa fa-check"></i><b>8.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="8.3.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>8.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="8.3.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>8.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="8.3.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-mnist"><i class="fa fa-check"></i><b>8.3.4</b> Example: MNIST (*)</a></li>
<li class="chapter" data-level="8.3.5" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>8.3.5</b> Stochastic Gradient Descent (SGD) (*)</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>8.4</b> A Note on Convex Optimisation (*)</a></li>
<li class="chapter" data-level="8.5" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#outro-5"><i class="fa fa-check"></i><b>8.5</b> Outro</a><ul>
<li class="chapter" data-level="8.5.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#remarks-6"><i class="fa fa-check"></i><b>8.5.1</b> Remarks</a></li>
<li class="chapter" data-level="8.5.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#further-reading-5"><i class="fa fa-check"></i><b>8.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-kmeans.html"><a href="chap-kmeans.html"><i class="fa fa-check"></i><b>9</b> Clustering with K-Means</a><ul>
<li class="chapter" data-level="9.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#k-means-clustering"><i class="fa fa-check"></i><b>9.1</b> K-means Clustering</a><ul>
<li class="chapter" data-level="9.1.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#example-in-r-4"><i class="fa fa-check"></i><b>9.1.1</b> Example in R</a></li>
<li class="chapter" data-level="9.1.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#problem-statement"><i class="fa fa-check"></i><b>9.1.2</b> Problem Statement</a></li>
<li class="chapter" data-level="9.1.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>9.1.3</b> Algorithms for the K-means Problem</a></li>
<li class="chapter" data-level="9.1.4" data-path="chap-kmeans.html"><a href="chap-kmeans.html#k-means-revisited"><i class="fa fa-check"></i><b>9.1.4</b> K-means Revisited</a></li>
<li class="chapter" data-level="9.1.5" data-path="chap-kmeans.html"><a href="chap-kmeans.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>9.1.5</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#exercises-5"><i class="fa fa-check"></i><b>9.2</b> Exercises</a><ul>
<li class="chapter" data-level="9.2.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#clustering-of-the-world-factbook"><i class="fa fa-check"></i><b>9.2.1</b> Clustering of the World Factbook</a></li>
<li class="chapter" data-level="9.2.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#unbalance-dataset-k-means-needs-multiple-starts"><i class="fa fa-check"></i><b>9.2.2</b> Unbalance Dataset – K-Means Needs Multiple Starts</a></li>
<li class="chapter" data-level="9.2.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#clustering-of-typical-2d-benchmark-datasets"><i class="fa fa-check"></i><b>9.2.3</b> Clustering of Typical 2D Benchmark Datasets</a></li>
<li class="chapter" data-level="9.2.4" data-path="chap-kmeans.html"><a href="chap-kmeans.html#wine-quality-volatile.acidity-and-sulphates"><i class="fa fa-check"></i><b>9.2.4</b> Wine Quality – <code>volatile.acidity</code> and <code>sulphates</code></a></li>
<li class="chapter" data-level="9.2.5" data-path="chap-kmeans.html"><a href="chap-kmeans.html#wine-quality-chlorides-and-total.sulfur.dioxide"><i class="fa fa-check"></i><b>9.2.5</b> Wine Quality – <code>chlorides</code> and <code>total.sulfur.dioxide</code></a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#outro-6"><i class="fa fa-check"></i><b>9.3</b> Outro</a><ul>
<li class="chapter" data-level="9.3.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#remarks-7"><i class="fa fa-check"></i><b>9.3.1</b> Remarks</a></li>
<li class="chapter" data-level="9.3.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#further-reading-6"><i class="fa fa-check"></i><b>9.3.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html"><i class="fa fa-check"></i><b>10</b> Discrete Optimisation</a><ul>
<li class="chapter" data-level="10.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#introduction-10"><i class="fa fa-check"></i><b>10.1</b> Introduction</a><ul>
<li class="chapter" data-level="10.1.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#recap"><i class="fa fa-check"></i><b>10.1.1</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#outro-7"><i class="fa fa-check"></i><b>10.2</b> Outro</a><ul>
<li class="chapter" data-level="10.2.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#remarks-8"><i class="fa fa-check"></i><b>10.2.1</b> Remarks</a></li>
<li class="chapter" data-level="10.2.2" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#further-reading-7"><i class="fa fa-check"></i><b>10.2.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-images.html"><a href="chap-images.html"><i class="fa fa-check"></i><b>11</b> Shallow and Deep Neural Networks</a><ul>
<li class="chapter" data-level="11.1" data-path="chap-images.html"><a href="chap-images.html#introduction-11"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="chap-images.html"><a href="chap-images.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>11.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="11.1.2" data-path="chap-images.html"><a href="chap-images.html#data-3"><i class="fa fa-check"></i><b>11.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="chap-images.html"><a href="chap-images.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>11.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="chap-images.html"><a href="chap-images.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>11.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="11.2.2" data-path="chap-images.html"><a href="chap-images.html#extending-logistic-regression"><i class="fa fa-check"></i><b>11.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="11.2.3" data-path="chap-images.html"><a href="chap-images.html#softmax-function"><i class="fa fa-check"></i><b>11.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="11.2.4" data-path="chap-images.html"><a href="chap-images.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>11.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="11.2.5" data-path="chap-images.html"><a href="chap-images.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>11.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="11.2.6" data-path="chap-images.html"><a href="chap-images.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>11.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="chap-images.html"><a href="chap-images.html#artificial-neural-networks"><i class="fa fa-check"></i><b>11.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="11.3.1" data-path="chap-images.html"><a href="chap-images.html#artificial-neuron"><i class="fa fa-check"></i><b>11.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="11.3.2" data-path="chap-images.html"><a href="chap-images.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>11.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="11.3.3" data-path="chap-images.html"><a href="chap-images.html#example-in-r-5"><i class="fa fa-check"></i><b>11.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="chap-images.html"><a href="chap-images.html#deep-neural-networks"><i class="fa fa-check"></i><b>11.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="11.4.1" data-path="chap-images.html"><a href="chap-images.html#introduction-12"><i class="fa fa-check"></i><b>11.4.1</b> Introduction</a></li>
<li class="chapter" data-level="11.4.2" data-path="chap-images.html"><a href="chap-images.html#activation-functions"><i class="fa fa-check"></i><b>11.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="11.4.3" data-path="chap-images.html"><a href="chap-images.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>11.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="11.4.4" data-path="chap-images.html"><a href="chap-images.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>11.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="chap-images.html"><a href="chap-images.html#preprocessing-of-data"><i class="fa fa-check"></i><b>11.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="11.5.1" data-path="chap-images.html"><a href="chap-images.html#introduction-13"><i class="fa fa-check"></i><b>11.5.1</b> Introduction</a></li>
<li class="chapter" data-level="11.5.2" data-path="chap-images.html"><a href="chap-images.html#image-deskewing"><i class="fa fa-check"></i><b>11.5.2</b> Image Deskewing</a></li>
<li class="chapter" data-level="11.5.3" data-path="chap-images.html"><a href="chap-images.html#summary-of-all-the-models-considered"><i class="fa fa-check"></i><b>11.5.3</b> Summary of All the Models Considered</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="chap-images.html"><a href="chap-images.html#outro-8"><i class="fa fa-check"></i><b>11.6</b> Outro</a><ul>
<li class="chapter" data-level="11.6.1" data-path="chap-images.html"><a href="chap-images.html#remarks-9"><i class="fa fa-check"></i><b>11.6.1</b> Remarks</a></li>
<li class="chapter" data-level="11.6.2" data-path="chap-images.html"><a href="chap-images.html#beyond-mnist"><i class="fa fa-check"></i><b>11.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="11.6.3" data-path="chap-images.html"><a href="chap-images.html#further-reading-8"><i class="fa fa-check"></i><b>11.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-recommenders.html"><a href="chap-recommenders.html"><i class="fa fa-check"></i><b>12</b> Recommender Systems</a><ul>
<li class="chapter" data-level="12.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#introduction-14"><i class="fa fa-check"></i><b>12.1</b> Introduction</a><ul>
<li class="chapter" data-level="12.1.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#the-netflix-prize"><i class="fa fa-check"></i><b>12.1.1</b> The Netflix Prize</a></li>
<li class="chapter" data-level="12.1.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#main-approaches"><i class="fa fa-check"></i><b>12.1.2</b> Main Approaches</a></li>
<li class="chapter" data-level="12.1.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#formalism-1"><i class="fa fa-check"></i><b>12.1.3</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#collaborative-filtering"><i class="fa fa-check"></i><b>12.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="12.2.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#example"><i class="fa fa-check"></i><b>12.2.1</b> Example</a></li>
<li class="chapter" data-level="12.2.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#similarity-measures"><i class="fa fa-check"></i><b>12.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="12.2.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>12.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="12.2.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>12.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#exercise-the-movielens-dataset"><i class="fa fa-check"></i><b>12.3</b> Exercise: The MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="12.3.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#dataset"><i class="fa fa-check"></i><b>12.3.1</b> Dataset</a></li>
<li class="chapter" data-level="12.3.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#data-cleansing"><i class="fa fa-check"></i><b>12.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="12.3.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#item-item-similarities"><i class="fa fa-check"></i><b>12.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="12.3.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#example-recommendations"><i class="fa fa-check"></i><b>12.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="12.3.5" data-path="chap-recommenders.html"><a href="chap-recommenders.html#clustering-1"><i class="fa fa-check"></i><b>12.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#outro-9"><i class="fa fa-check"></i><b>12.4</b> Outro</a><ul>
<li class="chapter" data-level="12.4.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#remarks-10"><i class="fa fa-check"></i><b>12.4.1</b> Remarks</a></li>
<li class="chapter" data-level="12.4.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#further-reading-9"><i class="fa fa-check"></i><b>12.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="chap-text.html"><a href="chap-text.html"><i class="fa fa-check"></i><b>13</b> Natural Language Processing</a><ul>
<li class="chapter" data-level="13.1" data-path="chap-text.html"><a href="chap-text.html#to-do"><i class="fa fa-check"></i><b>13.1</b> TO DO</a></li>
<li class="chapter" data-level="13.2" data-path="chap-text.html"><a href="chap-text.html#further-reading-10"><i class="fa fa-check"></i><b>13.2</b> Further Reading</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-convention.html"><a href="appendix-convention.html"><i class="fa fa-check"></i><b>A</b> Notation Convention</a></li>
<li class="chapter" data-level="B" data-path="appendix-rintro.html"><a href="appendix-rintro.html"><i class="fa fa-check"></i><b>B</b> Setting Up the R Environment</a><ul>
<li class="chapter" data-level="B.1" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-r"><i class="fa fa-check"></i><b>B.1</b> Installing R</a></li>
<li class="chapter" data-level="B.2" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-an-ide"><i class="fa fa-check"></i><b>B.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="B.3" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-recommended-packages"><i class="fa fa-check"></i><b>B.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="B.4" data-path="appendix-rintro.html"><a href="appendix-rintro.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>B.4</b> First R Script in RStudio</a></li>
<li class="chapter" data-level="B.5" data-path="appendix-rintro.html"><a href="appendix-rintro.html#exercises-6"><i class="fa fa-check"></i><b>B.5</b> Exercises</a><ul>
<li class="chapter" data-level="B.5.1" data-path="appendix-rintro.html"><a href="appendix-rintro.html#first-steps-with-vectors"><i class="fa fa-check"></i><b>B.5.1</b> First Steps with Vectors</a></li>
<li class="chapter" data-level="B.5.2" data-path="appendix-rintro.html"><a href="appendix-rintro.html#basic-plotting"><i class="fa fa-check"></i><b>B.5.2</b> Basic Plotting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appendix-rvector.html"><a href="appendix-rvector.html"><i class="fa fa-check"></i><b>C</b> Vector Algebra in R</a><ul>
<li class="chapter" data-level="C.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#motivation-1"><i class="fa fa-check"></i><b>C.1</b> Motivation</a></li>
<li class="chapter" data-level="C.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#numeric-vectors"><i class="fa fa-check"></i><b>C.2</b> Numeric Vectors</a><ul>
<li class="chapter" data-level="C.2.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>C.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="C.2.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-vector-operations"><i class="fa fa-check"></i><b>C.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#aggregation-functions"><i class="fa fa-check"></i><b>C.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="C.2.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#special-functions"><i class="fa fa-check"></i><b>C.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="C.2.6" data-path="appendix-rvector.html"><a href="appendix-rvector.html#norms-and-distances"><i class="fa fa-check"></i><b>C.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="C.2.7" data-path="appendix-rvector.html"><a href="appendix-rvector.html#dot-product"><i class="fa fa-check"></i><b>C.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="C.2.8" data-path="appendix-rvector.html"><a href="appendix-rvector.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>C.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#logical-vectors"><i class="fa fa-check"></i><b>C.3</b> Logical Vectors</a><ul>
<li class="chapter" data-level="C.3.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-logical-vectors"><i class="fa fa-check"></i><b>C.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="C.3.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#logical-operations"><i class="fa fa-check"></i><b>C.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="C.3.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#comparison-operations"><i class="fa fa-check"></i><b>C.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="C.3.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#aggregation-functions-1"><i class="fa fa-check"></i><b>C.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#character-vectors"><i class="fa fa-check"></i><b>C.4</b> Character Vectors</a><ul>
<li class="chapter" data-level="C.4.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-character-vectors"><i class="fa fa-check"></i><b>C.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="C.4.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>C.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="C.4.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>C.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-subsetting"><i class="fa fa-check"></i><b>C.5</b> Vector Subsetting</a><ul>
<li class="chapter" data-level="C.5.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>C.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="C.5.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>C.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="C.5.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>C.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="C.5.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#replacing-elements"><i class="fa fa-check"></i><b>C.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="C.5.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#other-functions"><i class="fa fa-check"></i><b>C.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="appendix-rvector.html"><a href="appendix-rvector.html#named-vectors"><i class="fa fa-check"></i><b>C.6</b> Named Vectors</a><ul>
<li class="chapter" data-level="C.6.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-named-vectors"><i class="fa fa-check"></i><b>C.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="C.6.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>C.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="C.7" data-path="appendix-rvector.html"><a href="appendix-rvector.html#factors"><i class="fa fa-check"></i><b>C.7</b> Factors</a><ul>
<li class="chapter" data-level="C.7.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-factors"><i class="fa fa-check"></i><b>C.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="C.7.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#levels"><i class="fa fa-check"></i><b>C.7.2</b> Levels</a></li>
<li class="chapter" data-level="C.7.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#internal-representation"><i class="fa fa-check"></i><b>C.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.8" data-path="appendix-rvector.html"><a href="appendix-rvector.html#lists"><i class="fa fa-check"></i><b>C.8</b> Lists</a><ul>
<li class="chapter" data-level="C.8.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-lists"><i class="fa fa-check"></i><b>C.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="C.8.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#named-lists"><i class="fa fa-check"></i><b>C.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="C.8.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>C.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="C.8.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#common-operations"><i class="fa fa-check"></i><b>C.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="appendix-rvector.html"><a href="appendix-rvector.html#exercises-7"><i class="fa fa-check"></i><b>C.9</b> Exercises</a><ul>
<li class="chapter" data-level="C.9.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#audeur-exchange-rates"><i class="fa fa-check"></i><b>C.9.1</b> AUD/EUR Exchange Rates</a></li>
</ul></li>
<li class="chapter" data-level="C.10" data-path="appendix-rvector.html"><a href="appendix-rvector.html#further-reading-11"><i class="fa fa-check"></i><b>C.10</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html"><i class="fa fa-check"></i><b>D</b> Matrix Algebra in R</a><ul>
<li class="chapter" data-level="D.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#creating-matrices"><i class="fa fa-check"></i><b>D.1</b> Creating Matrices</a><ul>
<li class="chapter" data-level="D.1.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix"><i class="fa fa-check"></i><b>D.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="D.1.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#stacking-vectors"><i class="fa fa-check"></i><b>D.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="D.1.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>D.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="D.1.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>D.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="D.1.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#other-methods"><i class="fa fa-check"></i><b>D.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="D.1.6" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#internal-representation-1"><i class="fa fa-check"></i><b>D.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#common-operations-1"><i class="fa fa-check"></i><b>D.2</b> Common Operations</a><ul>
<li class="chapter" data-level="D.2.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-transpose"><i class="fa fa-check"></i><b>D.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="D.2.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>D.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="D.2.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>D.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="D.2.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-multiplication"><i class="fa fa-check"></i><b>D.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="D.2.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>D.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="D.2.6" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#vectorised-special-functions"><i class="fa fa-check"></i><b>D.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="D.2.7" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-vector-operations"><i class="fa fa-check"></i><b>D.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-subsetting"><i class="fa fa-check"></i><b>D.3</b> Matrix Subsetting</a><ul>
<li class="chapter" data-level="D.3.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-individual-elements"><i class="fa fa-check"></i><b>D.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="D.3.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>D.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="D.3.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-submatrices"><i class="fa fa-check"></i><b>D.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="D.3.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>D.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="D.3.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>D.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#exercises-8"><i class="fa fa-check"></i><b>D.4</b> Exercises</a><ul>
<li class="chapter" data-level="D.4.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#currency-exchange-rates"><i class="fa fa-check"></i><b>D.4.1</b> Currency Exchange Rates</a></li>
<li class="chapter" data-level="D.4.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#currency-exchange-rates-relative-to-1999"><i class="fa fa-check"></i><b>D.4.2</b> Currency Exchange Rates Relative to 1999</a></li>
</ul></li>
<li class="chapter" data-level="D.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#further-reading-12"><i class="fa fa-check"></i><b>D.5</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="appendix-rdf.html"><a href="appendix-rdf.html"><i class="fa fa-check"></i><b>E</b> Data Frame Wrangling in R</a><ul>
<li class="chapter" data-level="E.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#creating-data-frames"><i class="fa fa-check"></i><b>E.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="E.2" data-path="appendix-rdf.html"><a href="appendix-rdf.html#importing-data-frames"><i class="fa fa-check"></i><b>E.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="E.3" data-path="appendix-rdf.html"><a href="appendix-rdf.html#data-frame-subsetting"><i class="fa fa-check"></i><b>E.3</b> Data Frame Subsetting</a><ul>
<li class="chapter" data-level="E.3.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>E.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="E.3.2" data-path="appendix-rdf.html"><a href="appendix-rdf.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>E.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="appendix-rdf.html"><a href="appendix-rdf.html#common-operations-2"><i class="fa fa-check"></i><b>E.4</b> Common Operations</a></li>
<li class="chapter" data-level="E.5" data-path="appendix-rdf.html"><a href="appendix-rdf.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>E.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="E.6" data-path="appendix-rdf.html"><a href="appendix-rdf.html#exercises-9"><i class="fa fa-check"></i><b>E.6</b> Exercises</a><ul>
<li class="chapter" data-level="E.6.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#urban-forest"><i class="fa fa-check"></i><b>E.6.1</b> Urban Forest</a></li>
</ul></li>
<li class="chapter" data-level="E.7" data-path="appendix-rdf.html"><a href="appendix-rdf.html#air-quality"><i class="fa fa-check"></i><b>E.7</b> Air Quality</a></li>
<li class="chapter" data-level="E.8" data-path="appendix-rdf.html"><a href="appendix-rdf.html#further-reading-13"><i class="fa fa-check"></i><b>E.8</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="appendix-datasets.html"><a href="appendix-datasets.html"><i class="fa fa-check"></i><b>F</b> Datasets</a><ul>
<li class="chapter" data-level="F.1" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sustainable-society-indices"><i class="fa fa-check"></i><b>F.1</b> Sustainable Society Indices</a></li>
<li class="chapter" data-level="F.2" data-path="appendix-datasets.html"><a href="appendix-datasets.html#air-quality-1"><i class="fa fa-check"></i><b>F.2</b> Air Quality</a></li>
<li class="chapter" data-level="F.3" data-path="appendix-datasets.html"><a href="appendix-datasets.html#currency-exchange-rates-1"><i class="fa fa-check"></i><b>F.3</b> Currency Exchange Rates</a></li>
<li class="chapter" data-level="F.4" data-path="appendix-datasets.html"><a href="appendix-datasets.html#urban-forest-1"><i class="fa fa-check"></i><b>F.4</b> Urban Forest</a></li>
<li class="chapter" data-level="F.5" data-path="appendix-datasets.html"><a href="appendix-datasets.html#wine-quality"><i class="fa fa-check"></i><b>F.5</b> Wine Quality</a></li>
<li class="chapter" data-level="F.6" data-path="appendix-datasets.html"><a href="appendix-datasets.html#the-world-factbook-countries-of-the-world"><i class="fa fa-check"></i><b>F.6</b> The World Factbook (Countries of the World)</a></li>
<li class="chapter" data-level="F.7" data-path="appendix-datasets.html"><a href="appendix-datasets.html#edstats-country-level-education-statistics"><i class="fa fa-check"></i><b>F.7</b> EdStats (Country-Level Education Statistics)</a></li>
<li class="chapter" data-level="F.8" data-path="appendix-datasets.html"><a href="appendix-datasets.html#food-and-nutrient-database-for-dietary-studies-fndds"><i class="fa fa-check"></i><b>F.8</b> Food and Nutrient Database for Dietary Studies (FNDDS)</a></li>
<li class="chapter" data-level="F.9" data-path="appendix-datasets.html"><a href="appendix-datasets.html#clustering-benchmarks"><i class="fa fa-check"></i><b>F.9</b> Clustering Benchmarks</a></li>
<li class="chapter" data-level="F.10" data-path="appendix-datasets.html"><a href="appendix-datasets.html#movie-lens-todo"><i class="fa fa-check"></i><b>F.10</b> Movie Lens (TODO)</a></li>
<li class="chapter" data-level="F.11" data-path="appendix-datasets.html"><a href="appendix-datasets.html#other-todo"><i class="fa fa-check"></i><b>F.11</b> Other (TODO)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.3 2020-06-19 21:18 (d30643a)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap:hclust" class="section level1">
<h1><span class="header-section-number">2</span> Agglomerative Hierarchical Clustering</h1>
<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->
<!--

Exercise: dimensionality reduction with auto-encoders?

Exercise: apply genieclust

Exercise: implement MDS with optim?

-->
<p>The aim of clustering (a.k.a. data segmentation or quantisation)
is to split the input dataset into <em>interesting</em>
subgroups in an unsupervised manner. Example applications of clustering include:</p>
<ul>
<li><p>taxonomisation, e.g.,
partition consumers to more “uniform”
groups to better understand who they are and what do they need,</p></li>
<li><p>aggregation, e.g., to reduce the number of observations
by substituting them by “group representatives” or “prototypes”,</p></li>
<li><p>object detection in images, e.g.,
tumour tissues on PET/CT scans,</p></li>
<li><p>complex networks analysis,
e.g., detecting communities in friendship,
retweets and other networks,</p></li>
<li><p>spatial data analysis, e.g., identifying densely populated areas
or traffic jams on maps,</p></li>
<li><p>text analysis, e.g., for grouping documents into automatically
detected topics,</p></li>
<li><p>fine-tuning of supervised learning algorithms,
e.g., recommender systems indicating content
that was rated highly by users from the same group
or learning multiple manifolds in a dimension reduction task.</p></li>
</ul>
<div id="dataset-partitions" class="section level2">
<h2><span class="header-section-number">2.1</span> Dataset Partitions</h2>
<div id="label-vectors" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Label Vectors</h3>
<p>Let’s assume that the input data set <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span>
consists of <span class="math inline">\(n\)</span> observations and that we wish to identify
<span class="math inline">\(K\)</span> clusters, for some <span class="math inline">\(K\ge 2\)</span> which gives the number of subgroups.
Such a clustering of <span class="math inline">\(\mathbf{X}\)</span> will be represented
by a vector <span class="math inline">\(\boldsymbol{y}\in\{1, 2, \dots, K\}^n\)</span>
such that <span class="math inline">\(y_i\)</span> gives the <em>cluster ID</em> (cluster identifier or number,
an integer between
1 and K) of the <span class="math inline">\(i\)</span>-th observation, <span class="math inline">\(i=1,2,\dots,n\)</span>.
Thus, we can think of clustering as of the process of labelling
or assigning colours to each data point (for example, 1 = black, 2 = red,
3 = green, 4 = blue etc.).
The only restriction we impose on each such <em>label</em> vector is that
<em>each of the <span class="math inline">\(K\)</span> labels must occur at least once</em>.
This will guarantee that we have indeed a division of <span class="math inline">\(\mathbf{X}\)</span>
into <span class="math inline">\(K\)</span> subgroups, and not less.</p>
<div class="remark"><strong>Remark.</strong>
<p>Given an integer <span class="math inline">\(K\)</span>
and a vector <span class="math inline">\(\boldsymbol{y}\)</span> with elements <span class="math inline">\(\{1, 2, \dots, K\}\)</span>,
for instance:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" title="1">y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb29-2" title="2">K &lt;-<span class="st"> </span><span class="dv">3</span> <span class="co"># by the way, it&#39;s max(y)</span></a></code></pre></div>
<p>we can check whether it represents a proper label vector by
computing the number of occurrences of each label:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" title="1"><span class="kw">tabulate</span>(y, K) <span class="co"># how many 1s, ..., Ks are there in y, respectively</span></a></code></pre></div>
<pre><code>## [1] 6 3 2</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" title="1"><span class="kw">all</span>(<span class="kw">tabulate</span>(y, K) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) <span class="co"># do we have at least 1 occurrence of each label?</span></a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>or, for example, determining the set of all unique
values in <span class="math inline">\(\boldsymbol{y}\)</span>:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" title="1"><span class="kw">unique</span>(y)</a></code></pre></div>
<pre><code>## [1] 3 1 2</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" title="1"><span class="kw">length</span>(<span class="kw">unique</span>(y)) <span class="op">==</span><span class="st"> </span>K <span class="co"># do we have K unique values in y?</span></a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<p>Each such label vector yields a <span class="math inline">\(K\)</span>-partition
of the input data set, that is, its grouping into
<span class="math inline">\(K\)</span> disjoint subsets – every element in <span class="math inline">\(\mathbf{X}\)</span>
belongs to one (and only one) cluster.</p>
</div>
<div id="k-partitions" class="section level3">
<h3><span class="header-section-number">2.1.2</span> K-Partitions</h3>
<p>As it is beneficial for us to be <em>gently</em> exposed to as many easy
mathematical formalisms as possible, let us use some set-theoretic notation
to come up with a rigorous definition of a dataset grouping.
Formally, <em>clustering</em> aims to find a <em>special kind</em>
of a <em><span class="math inline">\(K\)</span>-partition</em> of the input dataset,
which – without loss of generality – we can identify
with the set <span class="math inline">\(\{1, 2, \dots, n\}\)</span> of indexes of observations (rows)
in <span class="math inline">\(\mathbf{X}\)</span>.</p>
<div class="definition"><strong>Definition.</strong>
<p>We say that <span class="math inline">\(\mathcal{C}=\{C_1,C_2,\dots,C_K\}\)</span> forms
a <em><span class="math inline">\(K\)</span>-partition</em> of the set <span class="math inline">\(\{1, 2, \dots, n\}\)</span>,
whenever:</p>
<ul>
<li><p><span class="math inline">\(\bigcup_{k=1}^K C_k=C_1\cup C_2\cup\dots\cup C_K=\{1, 2, \dots, n\}\)</span>
(the union of all clusters
is the whole set; every point is assigned to some cluster;
no point is neglected),</p></li>
<li><p><span class="math inline">\(C_k\cap C_l=\emptyset\)</span> for all <span class="math inline">\(k\neq l\)</span> (clusters are pairwise disjoint;
their intersection is empty; they share no common elements),</p></li>
<li><p><span class="math inline">\(C_k\neq\emptyset\)</span> for all <span class="math inline">\(k\)</span> (each cluster is nonempty).</p></li>
</ul>
<!-- * $C_k\subseteq\{1, 2, \dots, n\}$ for all $k$, -->
</div>
<!--(\*) There is a one-to-one correspondence between partitions
and equivalence relations-->
<p>In the label vector representation,
we assume that <span class="math inline">\(y_i = k\)</span> iff <span class="math inline">\(i\in C_k\)</span>, i.e.,
the <span class="math inline">\(i\)</span>-th point is assigned the <span class="math inline">\(k\)</span>-th label if and only if
it belongs to the <span class="math inline">\(k\)</span>-th cluster.
It is easily seen that each label vector yields
a grouping of <span class="math inline">\(\{1, 2, \dots, n\}\)</span> that obviously fulfil
the first two conditions in the above condition
and that the third property is fulfilled by assuming
that <span class="math inline">\((\forall k\in\{1,2,\dots,K\})\)</span> <span class="math inline">\((\exists i\in\{1,2,\dots,n\})\)</span>
<span class="math inline">\(y_i = k\)</span> (read: <em>for every <span class="math inline">\(k\)</span> there exists <span class="math inline">\(i\)</span> such that…</em>),
i.e., each label occurs at least once in <span class="math inline">\(\boldsymbol{y}\)</span>.</p>
<div class="remark"><strong>Remark.</strong>
<p>The benefit of using mathematical notation is that we are super-precise,
efficient and universal. Natural tongue is vague and leaves much room
for interpretation. We used so many <em>words</em> to explain the above concepts.
As soon as we further develop our skills of <em>speaking</em> the language
of mathematics
(note that we’ve started with basic phrases such as <em>Good Afternoon</em>,
<em>I am Sorry</em> and <em>Thank You</em>), it’ll be more natural for
us to just write:</p>
<p>“Let <span class="math inline">\(\mathcal{C}=\{ C_1,\dots, C_K \}\)</span> s.t.
<span class="math inline">\(\textstyle\bigcup_{k=1}^K C_k = \{1,\dots, n\}\)</span>,
<span class="math inline">\((\forall k\neq l)\)</span> <span class="math inline">\(C_k\cap C_l = \emptyset\)</span>, <span class="math inline">\(C_k\neq \emptyset\)</span>”</p>
<p>without all the talking. This is what more advanced books related
to machine learning and statistics do. We’ll get to that.
By the way, thanks to this it’s relatively easy to read maths
papers (especially those from the 1950s) written in other languages
(such as Russian, German or French).</p>
</div>
<p>In theory, the number of possible <span class="math inline">\(K\)</span>-partitions of a set
with <span class="math inline">\(n\)</span> elements is given by
<em>the Stirling number of the second kind</em>:
<span class="math display">\[
\left\{{n \atop K}\right\}=\sum_{{j=0}}^{{K}} \frac{(-1)^{{j}}
(K-j)^{n}}{j! (K-j)!} = \frac{K^n}{K!}
- \frac{(K-1)^n}{(K-1)!}
+ \frac{(K-2)^n}{2 (K-2)!}
- \frac{(K-3)^n}{6 (K-3)!}
+ \dots,
\]</span>
where <span class="math inline">\(n!=1\cdot 2 \cdot 3\cdot \dots\cdot n\)</span> with <span class="math inline">\(0!=1\)</span> (<span class="math inline">\(n\)</span>-factorial).
In particular, already <span class="math inline">\(\left\{{n \atop 2}\right\}=2^{n-1}-1\)</span>
and <span class="math inline">\(\left\{{n \atop K}\right\}\simeq K^n/K!\)</span> for large <span class="math inline">\(n\)</span> – that is a lot.</p>
<table style="width:100%;">
<caption><span id="tab:stirling">Table 2.1: </span> The number of possible partitions of a dataset with <span class="math inline">\(n\)</span> elements to <span class="math inline">\(K\)</span> clusters, denoted <span class="math inline">\(\left\{{n \atop K}\right\}\)</span>, grows rapidly as <span class="math inline">\(n\)</span> increases</caption>
<colgroup>
<col width="1%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th align="right"><span class="math inline">\(n\)</span></th>
<th align="right"><span class="math inline">\(\left\{{n \atop 2}\right\}\)</span></th>
<th align="right"><span class="math inline">\(\left\{{n \atop 3}\right\}\)</span></th>
<th align="right"><span class="math inline">\(\left\{{n \atop 4}\right\}\)</span></th>
<th align="right"><span class="math inline">\(\left\{{n \atop 5}\right\}\)</span></th>
<th align="right"><span class="math inline">\(\left\{{n \atop 6}\right\}\)</span></th>
<th align="right"><span class="math inline">\(\left\{{n \atop 7}\right\}\)</span></th>
<th align="right"><span class="math inline">\(\left\{{n \atop 8}\right\}\)</span></th>
<th align="right"><span class="math inline">\(\left\{{n \atop 9}\right\}\)</span></th>
<th align="right"><span class="math inline">\(\left\{{n \atop 10}\right\}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
<td align="right">1</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">3</td>
<td align="right"></td>
<td align="right">1</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">7</td>
<td align="right"></td>
<td align="right">6</td>
<td align="right">1</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">15</td>
<td align="right">25</td>
<td align="right">10</td>
<td align="right">1</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">31</td>
<td align="right">90</td>
<td align="right">65</td>
<td align="right">15</td>
<td align="right">1</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">63</td>
<td align="right">301</td>
<td align="right">350</td>
<td align="right">140</td>
<td align="right">21</td>
<td align="right">1</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">8</td>
<td align="right">127</td>
<td align="right">966</td>
<td align="right">1701</td>
<td align="right">1050</td>
<td align="right">266</td>
<td align="right">28</td>
<td align="right">1</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="right">255</td>
<td align="right">3025</td>
<td align="right">7770</td>
<td align="right">6951</td>
<td align="right">2646</td>
<td align="right">462</td>
<td align="right">36</td>
<td align="right">1</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">10</td>
<td align="right">511</td>
<td align="right">9330</td>
<td align="right">34105</td>
<td align="right">42525</td>
<td align="right">22827</td>
<td align="right">5880</td>
<td align="right">750</td>
<td align="right">45</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
</div>
<div id="interesting-partitions" class="section level3">
<h3><span class="header-section-number">2.1.3</span> “Interesting” Partitions</h3>
<p>We are not just interested in “any” partition, because
there are simply way too plentiful (compare Table <a href="chap-hclust.html#tab:stirling">2.1</a>;
some of them will certainly be more meaningful or valuable than others.
However, even one of the most famous ML textbooks provides us with only
a vague hint of what we might be looking for:</p>
<div class="definition"><strong>Definition.</strong>
<p>(or, rather, a “definition”).
Clustering concerns “segmenting a collection of objects into subsets
so that those within each cluster are more <strong>closely related</strong>
to one another than objects assigned to different clusters” <span class="citation">(Hastie et al. <a href="references.html#ref-esl">2017</a>)</span>.</p>
</div>
<p>It is not uncommon (see, e.g., <span class="citation">(Estivill-Castro <a href="references.html#ref-many_clustering_algorithms">2002</a>)</span>)
to equate the general definition of data clustering problems with… the
particular outputs generated by specific clustering algorithms. It some sense,
that sounds fair – clustering is both art and science
(compare <span class="citation">(Luxburg et al. <a href="references.html#ref-clustering_science_art">2012</a>)</span>).
From this perspective, we might be interested in
identifying the two main types of clustering algorithms:</p>
<ul>
<li><p><em>parametric</em> (model-based) –
find clusters of specific shapes or following specific multidimensional
probability distributions,
e.g., K-means,
expectation-maximisation for Gaussian mixtures (EM),
average linkage agglomerative clustering;</p></li>
<li><p><em>nonparametric</em> (model-free) – identify high-density or
well-separable regions, perhaps in the presence of noise points,
e.g., single linkage agglomerative clustering, Genie, (H)DBSCAN, BIRCH.</p></li>
</ul>
<p>In this chapter we’ll take a look at
<em>(agglomerative) hierarchical clustering</em> algorithms which build
a cluster structure
<em>from the bottom</em>, i.e., by merging small points groups to form larger ones.
In Chapter <a href="chap-kmeans.html#chap:kmeans">9</a>, we’ll discuss the K-means algorithm,
which seeks “good” cluster prototypes.</p>
</div>
</div>
<div id="sec:euclidean" class="section level2">
<h2><span class="header-section-number">2.2</span> Euclidean Distance</h2>
<p>In order to build our first clustering algorithm,
we need to refer to the notion of a <em>distance</em>,
which
quantifies the extent to which two observations
in a dataset (two rows in <span class="math inline">\(\mathbf{X}\)</span>) are different from
or similar to each other.</p>
<p>In this chapter we will be dealing with the most natural,
“school” straight-line distance, called the Euclidean metric.
It works as if we were measuring how two points
are far away from each other with a ruler.</p>
<p>Given two <span class="math inline">\(p\)</span>-tuples <span class="math inline">\(\boldsymbol{u}=(u_1,\dots,u_p)\)</span>
and <span class="math inline">\(\boldsymbol{v}=(p_1,\dots,v_p)\)</span>, the <em>Euclidean metric</em>
is a function <span class="math inline">\(d\)</span> such that:
<span class="math display">\[
d(\boldsymbol{u}, \boldsymbol{v}) = \sqrt{
(u_1-v_1)^2 + (u_2-v_2)^2 + \dots + (u_p-v_p)^2
}
=
\sqrt{
\sum_{i=1}^p (u_i-v_i)^2
}.
\]</span>
The Euclidean metric will often also be denoted with
<span class="math inline">\(\|\boldsymbol{u}-\boldsymbol{v}\|\)</span>.</p>
<div class="remark"><strong>Remark.</strong>
<p><span class="math inline">\(\sum\)</span> (Greek capital letter Sigma) denotes summation.
Read “<span class="math inline">\(\sum_{i=1}^p z_i\)</span>” as “the sum of <span class="math inline">\(z_i\)</span> for <span class="math inline">\(i\)</span> from <span class="math inline">\(1\)</span> to <span class="math inline">\(p\)</span>”;
this is just a convenient shorthand for <span class="math inline">\(z_1+z_2+\dots+z_p\)</span>.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Consider the following <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{3\times 2}\)</span>:
<span class="math display">\[
\mathbf{X} = \left[\begin{array}{cc}
0 &amp; 0 \\
1 &amp; 0 \\
\frac{1}{4} &amp; 1 \\
\end{array}\right]
\]</span>
Calculate (by hand)
<span class="math inline">\(d(\mathbf{x}_{1,\cdot}, \mathbf{x}_{2,\cdot})\)</span>,
<span class="math inline">\(d(\mathbf{x}_{1,\cdot}, \mathbf{x}_{3,\cdot})\)</span>,
<span class="math inline">\(d(\mathbf{x}_{2,\cdot}, \mathbf{x}_{3,\cdot})\)</span>,
<span class="math inline">\(d(\mathbf{x}_{1,\cdot}, \mathbf{x}_{1,\cdot})\)</span> and
<span class="math inline">\(d(\mathbf{x}_{2,\cdot}, \mathbf{x}_{1,\cdot})\)</span>.</p>
</div>
<p>In order to compute all pairwise distances between the rows in a
matrix with R, we can call the <code>dist()</code> function
(see also Figure <a href="chap-hclust.html#fig:euclidean-distance-fig">2.1</a> for a graphical illustration):</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" title="1">X &lt;-<span class="st"> </span><span class="kw">rbind</span>(</a>
<a class="sourceLine" id="cb38-2" title="2">    <span class="kw">c</span>(<span class="dv">0</span>,    <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb38-3" title="3">    <span class="kw">c</span>(<span class="dv">1</span>,    <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb38-4" title="4">    <span class="kw">c</span>(<span class="fl">0.25</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb38-5" title="5">)</a>
<a class="sourceLine" id="cb38-6" title="6"><span class="kw">print</span>(X)</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,] 0.00    0
## [2,] 1.00    0
## [3,] 0.25    1</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" title="1"><span class="kw">dist</span>(X)</a></code></pre></div>
<pre><code>##        1      2
## 2 1.0000       
## 3 1.0308 1.2500</code></pre>
<div class="figure"><span id="fig:euclidean-distance-fig"></span>
<img src="02-hclust-figures/euclidean-distance-fig-1.svg" alt=" Example points and Euclidean distances between them" />
<p class="caption">Figure 2.1:  Example points and Euclidean distances between them</p>
</div>
<p>Note that only 3 values were reported. This is due to the fact
that:</p>
<ul>
<li><p>the distance between a point and itself is always 0,
i.e., <span class="math inline">\(d(\boldsymbol{u},\boldsymbol{u})=0\)</span>;</p></li>
<li><p>the order of the points (from-to or to-from) doesn’t matter,
we have <span class="math inline">\(d(\boldsymbol{u},\boldsymbol{v})=d(\boldsymbol{v},\boldsymbol{u})\)</span>
(this is called <em>symmetry</em>).</p></li>
</ul>
<p>If we want the full distance matrix, we can call:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" title="1"><span class="kw">as.matrix</span>(<span class="kw">dist</span>(X))</a></code></pre></div>
<pre><code>##        1    2      3
## 1 0.0000 1.00 1.0308
## 2 1.0000 0.00 1.2500
## 3 1.0308 1.25 0.0000</code></pre>
<p>Note the zeros on the main diagonal and that the matrix is symmetric
around it.</p>
<div class="remark"><strong>Remark.</strong>
<p>Overall, there are <span class="math inline">\(n(n-1)/2\)</span> “interesting” pairwise distances
for a dataset of <span class="math inline">\(n\)</span> points. In R, these are stored using <code>numeric</code> type,
each being an 8-byte floating point number. Therefore, by calling <code>dist()</code>
on matrices with too many rows we can easily run out of memory;
already for <span class="math inline">\(n=100{,}000\)</span> we need ca. 40 GB of RAM.</p>
</div>
<div class="remark"><strong>Remark.</strong>
<p>Later <!-- TODO -->
we will mention that there are many possible distances,
allowing to measure the similarity of points not only in <span class="math inline">\(\mathbb{R}^p\)</span>,
but also character strings, protein sequences, film ratings, etc.;
there is even an encyclopedia of distances (see <span class="citation">(Deza &amp; Deza <a href="references.html#ref-encyclopedia_distances">2014</a>)</span>)!</p>
</div>
</div>
<div id="hierarchical-clustering-at-a-glance" class="section level2">
<h2><span class="header-section-number">2.3</span> Hierarchical Clustering at a Glance</h2>
<p>Hierarchical methods generate a whole hierarchy
of <em>nested</em> partitions. A <span class="math inline">\(K\)</span>-partition for any <span class="math inline">\(K\)</span>
can be extracted later at any time.
In the case of <em>agglomerative</em> hierarchical algorithms:</p>
<ul>
<li><p>at the lowest level of the hierarchy, each point belongs to its own
cluster (there are <span class="math inline">\(n\)</span> singletons);</p></li>
<li><p>at the highest level of the hierarchy,
there is one cluster that embraces all the points;</p></li>
<li><p>moving from the <span class="math inline">\(i\)</span>-th to the <span class="math inline">\((i+1)\)</span>-th level,
we select (somehow; see below) a pair of clusters to be merged.</p></li>
</ul>
<p>Let’s consider the Sustainable Society Indices Dataset
(see Appendix <a href="appendix-datasets.html#appendix:datasets">F</a>)
that measures the Human,
Environmental and Economic Wellbeing
in each country on the scale <span class="math inline">\([0,10]\)</span>.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" title="1">ssi &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/ssi_2016_dimensions.csv&quot;</span>,</a>
<a class="sourceLine" id="cb44-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a>
<a class="sourceLine" id="cb44-3" title="3"><span class="kw">head</span>(ssi)</a></code></pre></div>
<pre><code>##     Country HumanWellbeing EnvironmentalWellbeing EconomicWellbeing
## 1   Albania         8.1162                 5.3961            2.6317
## 2   Algeria         6.5283                 3.5698            4.6622
## 3    Angola         4.1749                 6.4411            2.8579
## 4 Argentina         6.8385                 4.0929            5.7379
## 5   Armenia         7.6110                 4.0443            3.1948
## 6 Australia         8.0647                 2.4262            7.5716</code></pre>
<p>In other to better understand how the algorithms of interest work,
we’ll restrict ourselves a smaller sample of countries, say,
to 37 members of the OECD:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" title="1">oecd &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Australia&quot;</span>, <span class="st">&quot;Austria&quot;</span>, <span class="st">&quot;Belgium&quot;</span>, <span class="st">&quot;Canada&quot;</span>, <span class="st">&quot;Chile&quot;</span>, <span class="st">&quot;Colombia&quot;</span>,</a>
<a class="sourceLine" id="cb46-2" title="2"><span class="st">&quot;Czech Republic&quot;</span>, <span class="st">&quot;Denmark&quot;</span>, <span class="st">&quot;Estonia&quot;</span>, <span class="st">&quot;Finland&quot;</span>, <span class="st">&quot;France&quot;</span>, <span class="st">&quot;Germany&quot;</span>,</a>
<a class="sourceLine" id="cb46-3" title="3"><span class="st">&quot;Greece&quot;</span>, <span class="st">&quot;Hungary&quot;</span>, <span class="st">&quot;Iceland&quot;</span>, <span class="st">&quot;Ireland&quot;</span>, <span class="st">&quot;Israel&quot;</span>, <span class="st">&quot;Italy&quot;</span>, <span class="st">&quot;Japan&quot;</span>,</a>
<a class="sourceLine" id="cb46-4" title="4"><span class="st">&quot;Korea, South&quot;</span>, <span class="st">&quot;Latvia&quot;</span>, <span class="st">&quot;Lithuania&quot;</span>, <span class="st">&quot;Luxembourg&quot;</span>, <span class="st">&quot;Mexico&quot;</span>,</a>
<a class="sourceLine" id="cb46-5" title="5"><span class="st">&quot;Netherlands&quot;</span>, <span class="st">&quot;New Zealand&quot;</span>, <span class="st">&quot;Norway&quot;</span>, <span class="st">&quot;Poland&quot;</span>, <span class="st">&quot;Portugal&quot;</span>,</a>
<a class="sourceLine" id="cb46-6" title="6"><span class="st">&quot;Slovak Republic&quot;</span>, <span class="st">&quot;Slovenia&quot;</span>, <span class="st">&quot;Spain&quot;</span>, <span class="st">&quot;Sweden&quot;</span>, <span class="st">&quot;Switzerland&quot;</span>,</a>
<a class="sourceLine" id="cb46-7" title="7"><span class="st">&quot;Turkey&quot;</span>, <span class="st">&quot;United Kingdom&quot;</span>, <span class="st">&quot;United States&quot;</span>)</a>
<a class="sourceLine" id="cb46-8" title="8">ssi &lt;-<span class="st"> </span>ssi[ssi[,<span class="st">&quot;Country&quot;</span>] <span class="op">%in%</span><span class="st"> </span>oecd, ]</a>
<a class="sourceLine" id="cb46-9" title="9">X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(ssi[,<span class="op">-</span><span class="dv">1</span>]) <span class="co"># everything except the Country column</span></a>
<a class="sourceLine" id="cb46-10" title="10"><span class="kw">dimnames</span>(X)[[<span class="dv">1</span>]] &lt;-<span class="st"> </span>ssi[,<span class="dv">1</span>] <span class="co"># set row names</span></a>
<a class="sourceLine" id="cb46-11" title="11"><span class="kw">head</span>(X)</a></code></pre></div>
<pre><code>##           HumanWellbeing EnvironmentalWellbeing EconomicWellbeing
## Australia         8.0647                 2.4262            7.5716
## Austria           8.4803                 4.5422            5.6352
## Belgium           8.6593                 2.6583            4.7965
## Canada            8.2536                 2.4219            4.2430
## Chile             6.8764                 4.3150            5.1530
## Colombia          5.9280                 5.9480            4.1480</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" title="1"><span class="kw">dim</span>(X) <span class="co"># n and p</span></a></code></pre></div>
<pre><code>## [1] 37  3</code></pre>
<p>Hence, we have <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{37 \times 3}\)</span>.
Note that the matrix in R has row and column names set
for greater readability.</p>
<div style="margin-top: 1em">

</div>
<p>The most basic implementation of a few
agglomerative hierarchical clustering algorithms
is provided by the <code>stats::hclust()</code> function, which works on a pairwise
distance matrix.
By default, the method computes the so-called <em>complete</em> linkage.
We’ll explain what that means later, let’s just simply use it in
a “black-box” fashion now, i.e., without going into details:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb50-1" title="1"><span class="co"># Euclidean distances between all pairs of points:</span></a>
<a class="sourceLine" id="cb50-2" title="2">D &lt;-<span class="st"> </span><span class="kw">dist</span>(X)</a>
<a class="sourceLine" id="cb50-3" title="3">h &lt;-<span class="st"> </span><span class="kw">hclust</span>(D) <span class="co"># method=&quot;complete&quot;</span></a>
<a class="sourceLine" id="cb50-4" title="4"><span class="kw">print</span>(h)</a></code></pre></div>
<pre><code>## 
## Call:
## hclust(d = D)
## 
## Cluster method   : complete 
## Distance         : euclidean 
## Number of objects: 37</code></pre>
<div class="remark"><strong>Remark.</strong>
<p><code>stats::hclust()</code> refers to the <code>hclust()</code> function
in the <code>stats</code> package. This is a package that comes with R, therefore
there should be no need to load it explicitly. Otherwise, we would
have to use the complete descriptor (with the <code>stats::</code> prefix)
or call <code>library("stats")</code> to attach the package’s namespace.</p>
</div>
<p>The obtained hierarchy can be <em>cut</em> at an arbitrary level
by applying the <code>cutree()</code> function. Let’s use <span class="math inline">\(K=3\)</span>:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb52-1" title="1">K &lt;-<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb52-2" title="2">y &lt;-<span class="st"> </span><span class="kw">cutree</span>(h, K) <span class="co"># extract the 3-partition</span></a></code></pre></div>
<p>A picture is worth a thousand words, therefore
let’s generate a map with the <code>rworldmap</code> package (see Figure <a href="chap-hclust.html#fig:ssi-map">2.2</a>):</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" title="1"><span class="kw">library</span>(<span class="st">&quot;rworldmap&quot;</span>) <span class="co"># see the package&#39;s manual for details</span></a>
<a class="sourceLine" id="cb53-2" title="2">mapdata &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Country=</span><span class="kw">dimnames</span>(X)[[<span class="dv">1</span>]], <span class="dt">Cluster=</span>y)</a>
<a class="sourceLine" id="cb53-3" title="3">mapdata &lt;-<span class="st"> </span><span class="kw">joinCountryData2Map</span>(mapdata, <span class="dt">joinCode=</span><span class="st">&quot;NAME&quot;</span>,</a>
<a class="sourceLine" id="cb53-4" title="4">  <span class="dt">nameJoinColumn=</span><span class="st">&quot;Country&quot;</span>)</a>
<a class="sourceLine" id="cb53-5" title="5"><span class="kw">mapCountryData</span>(mapdata, <span class="dt">nameColumnToPlot=</span><span class="st">&quot;Cluster&quot;</span>,</a>
<a class="sourceLine" id="cb53-6" title="6">    <span class="dt">catMethod=</span><span class="st">&quot;categorical&quot;</span>, <span class="dt">missingCountryCol=</span><span class="st">&quot;white&quot;</span>,</a>
<a class="sourceLine" id="cb53-7" title="7">    <span class="dt">colourPalette=</span><span class="kw">palette.colors</span>(K, <span class="st">&quot;Okabe-Ito&quot;</span>),</a>
<a class="sourceLine" id="cb53-8" title="8">    <span class="dt">mapTitle=</span><span class="st">&quot;&quot;</span>, <span class="dt">nameColumnToHatch=</span><span class="st">&quot;Cluster&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:ssi-map"></span>
<img src="02-hclust-figures/ssi-map-1.svg" alt=" OECD countries grouped w.r.t. the SSI dimensions" />
<p class="caption">Figure 2.2:  OECD countries grouped w.r.t. the SSI dimensions</p>
</div>
<p>We have so many questions now! First of all, why the countries were grouped
in such a way? Well, this is because the algorithm is programmed
to perform certain computational steps that lead to this particular solution.
We’ll discuss them in very detail soon.</p>
<p>A better question is: what characterises the countries in each cluster?
To explore possible answers to this question, we can compute, e.g.,
the average indicators in each group:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb54-1" title="1"><span class="kw">aggregate</span>(<span class="kw">as.data.frame</span>(X), <span class="kw">list</span>(<span class="dt">Cluster=</span>y), mean)</a></code></pre></div>
<pre><code>##   Cluster HumanWellbeing EnvironmentalWellbeing EconomicWellbeing
## 1       1         8.4195                 3.5158            7.2490
## 2       2         8.0008                 3.9414            5.0226
## 3       3         7.4919                 5.2490            3.7474</code></pre>
<p>Therefore, on average, the countries in the 1st cluster
have higher Economic Wellbeing scores than the countries in the 2nd cluster,
and these outperform those from the 3rd cluster. On the other hand,
the 3rd cluster countries score much more highly on the Environmental
Wellbeing dimension. This summary, however, is valid only if
the countries in each cluster are score similarly on each scale
(are homogeneous).</p>
<p>Luckily, we have only 3 dimensions, therefore we can visualise
different two-dimensional projections of this dataset.
Figure <a href="chap-hclust.html#fig:ssi-complete-pairs">2.3</a>
depicts a slightly “tuned up” (we’ve added the ISO 3-letter country codes)
version of a scatter plot matrix
that we would normally obtain by calling <code>pairs(X, col=y, pch=y)</code>.</p>
<div class="figure"><span id="fig:ssi-complete-pairs"></span>
<img src="02-hclust-figures/ssi-complete-pairs-1.svg" alt=" Scatterplot matrix for the SSI dimensions with the 3-partition generated by complete linkage" />
<p class="caption">Figure 2.3:  Scatterplot matrix for the SSI dimensions with the 3-partition generated by complete linkage</p>
</div>
<p>In seems that Economic Wellbeing is the deciding factor for distinguishing
between these countries. We see on the Economic vs. Environmental Wellbeing plot
that the clusters are nicely separated. Human Wellbeing, on the other hand,
is pretty “mixed up” across the clusters.</p>
</div>
<div id="cluster-dendrograms" class="section level2">
<h2><span class="header-section-number">2.4</span> Cluster Dendrograms</h2>
<p>A <em>dendrogram</em> can provide us with some insight into the underlying
data structure as well as with some hints about how many clusters (K)
should extract.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" title="1"><span class="co"># Plotting of dendrogram: essentially plot(hclust(dist(X)))</span></a>
<a class="sourceLine" id="cb56-2" title="2"><span class="kw">plot</span>(h, <span class="dt">xlab=</span><span class="ot">NA</span>, <span class="dt">main=</span><span class="ot">NA</span>); <span class="kw">box</span>()</a></code></pre></div>
<div class="figure"><span id="fig:ssi-dendrogram-complete"></span>
<img src="02-hclust-figures/ssi-dendrogram-complete-1.svg" alt=" Cluster dendrogram generated by complete linkage" />
<p class="caption">Figure 2.4:  Cluster dendrogram generated by complete linkage</p>
</div>
<p>Figure <a href="chap-hclust.html#fig:ssi-dendrogram-complete">2.4</a> depicts
the dendrogram corresponding to the performed
agglomerative hierarchical clustering.</p>
<p>A dendrogram is a tree-like diagram whose nodes represent
clusters at different stages of the algorithm’s run:</p>
<ul>
<li><p>The <em>leaves</em> of the tree, i.e., the nodes at the very bottom,
stand for the individual data points (recall that in agglomerative clustering
we start with <span class="math inline">\(n\)</span> singletons). In our case, these are the 37 OECD countries.</p></li>
<li><p>Each two-teeth “fork” represents two clusters being merged at some
iteration. For example, at some stages we have clusters like:
{Italy} and {Portugal, Spain} which are merged so that we obtain
{Italy, Portugal, Spain}.
Moreover, the union of {Chile, Israel} and {Mexico, Turkey}
gives {Chile, Israel, Mexico, Turkey}.</p></li>
<li><p>At the tree top (“root” – the tree is plotted upside down)
we have a state where all the clusters
have been merged with each other (there is one cluster consisting
of all the points).</p></li>
<li><p>The tree can be “cut” at any level so as to obtain
a clustering to any number of subgroups.
In particular, if we cut the tree at height <span class="math inline">\(\simeq 4\)</span>, we will
get a 3-partition from Figure <a href="chap-hclust.html#fig:ssi-complete-pairs">2.3</a>.</p></li>
</ul>
</div>
<div id="linkage-functions" class="section level2">
<h2><span class="header-section-number">2.5</span> Linkage Functions</h2>
<p>Let’s formalise the agglomerative hierarchical clustering process.
Initially (“iteration 0”),
each of the <span class="math inline">\(n\)</span> points is a member of its own cluster.
Let <span class="math inline">\(\mathcal{C}^{(0)}\)</span> denote this <span class="math inline">\(n\)</span>-partition:
<span class="math display">\[
\mathcal{C}^{(0)}=\left\{
    C_1^{(0)}, C_2^{(0)}, \dots, C_n^{(0)}
\right\},
\]</span>
where <span class="math inline">\(C_i^{(0)}\)</span> is the cluster that consists of the <span class="math inline">\(i\)</span>-th point only,
i.e., <span class="math inline">\(C_i^{(0)}=\{i\}\)</span>.</p>
<p>While an agglomerative hierarchical clustering algorithm is being computed,
there are <span class="math inline">\(n-k\)</span> clusters at the <span class="math inline">\(k\)</span>-th step of the procedure,
<span class="math display">\[
\mathcal{C}^{(k)}=\left\{
C_1^{(k)},C_2^{(k)},\dots,C_{u-1}^{(k)},
C_u^{(k)},
C_{u+1}^{(k)},\dots,C_{v-1}^{(k)},
C_v^{(k)},
C_{v+1}^{(k)},\dots,C_{n-k}^{(k)}
\right\},
\]</span>
where <span class="math inline">\(k=0,1,\dots,n-1\)</span>.</p>
<p>When proceeding from step <span class="math inline">\(k\)</span> to <span class="math inline">\(k+1\)</span>,
we determine two clusters <span class="math inline">\(C_u^{(k)}\)</span> and <span class="math inline">\(C_v^{(k)}\)</span>, <span class="math inline">\(u&lt;v\)</span>,
to be <em>merged</em> so that the partition at the higher level
is of the form:
<span class="math display">\[
\mathcal{C}^{(k+1)} = \left\{
C_1^{(k)},C_2^{(k)},\dots,C_{u-1}^{(k)},
C_u^{(k)}{\cup C_v^{(k)}},
C_{u+1}^{(k)},\dots,C_{v-1}^{(k)},
C_{v+1}^{(k)},\dots,C_{n-k}^{(k)}
\right\}.
\]</span></p>
<!--
Thus, $(\mathcal{C}^{(0)}, \mathcal{C}^{(1)}, \dots, \mathcal{C}^{(n-1)})$
form a sequence of *nested* partitions of
the input dataset
with the last level being just one big cluster,
$\mathcal{C}^{(n-1)}=\left\{ \{\mathbf{x}_{1,\cdot},\mathbf{x}_{2,\cdot},\dots,\mathbf{x}_{n,\cdot}\} \right\}$.
-->
<p>It is evident that this way we will arrive at a 1-partition (a single
boring cluster consisting of all the points),
<span class="math inline">\(\mathcal{C}^{(n-1)}=\left\{ C_1^{(n-1)} \right\}\)</span>
with <span class="math inline">\(C_1^{(n-1)}=\{1,2,\dots,n\}\)</span>.</p>
<div class="remark"><strong>Example.</strong>
<p>Restricting ourselves to the first 5 countries
on the left in Figure <a href="chap-hclust.html#fig:ssi-dendrogram-complete">2.4</a>, we have:</p>
<p><span class="math display">\[
\begin{array}{llll}
\mathcal{C}^{(0)} &amp; = &amp; \left\{ \{\text{Latvia}\}, \{\text{Lithuania}\}, \{\text{Poland}\}, \{\text{Denmark}\}, \{\text{Switzerland}\} \right\},&amp; {\footnotesize (C_{2}^{(0)}\cup C_{3}^{(0)})}\\
\mathcal{C}^{(1)} &amp; = &amp; \left\{ \{\text{Latvia}\}, \{\text{Lithuania}, \text{Poland}\}, \{\text{Denmark}\}, \{\text{Switzerland}\} \right\},    &amp; {\footnotesize (C_{1}^{(1)}\cup C_{2}^{(1)})}\\
\mathcal{C}^{(2)} &amp; = &amp; \left\{ \{\text{Latvia}, \text{Lithuania}, \text{Poland}\}, \{\text{Denmark}\}, \{\text{Switzerland}\} \right\},        &amp; {\footnotesize (C_{2}^{(2)}\cup C_{3}^{(2)})}\\
\mathcal{C}^{(3)} &amp; = &amp; \left\{ \{\text{Latvia}, \text{Lithuania}, \text{Poland}\}, \{\text{Denmark}, \text{Switzerland}\} \right\},            &amp; {\footnotesize (C_{1}^{(3)}\cup C_{2}^{(3)})}\\
\mathcal{C}^{(4)} &amp; = &amp; \left\{ \{\text{Latvia}, \text{Lithuania}, \text{Poland}, \text{Denmark}, \text{Switzerland}\} \right\}.                &amp; \\
\end{array}
\]</span></p>
</div>
<p>There is one component missing – how to determine the pair
of clusters <span class="math inline">\(C_u^{(k)}\)</span> and <span class="math inline">\(C_v^{(k)}\)</span> to be merged
at the <span class="math inline">\(k\)</span>-th iteration?
For classic agglomerative clustering algorithms,
we choose two clusters that are <em>closest to each other</em> – they minimise
the inter-cluster distance.</p>
<p>Formally, we write the condition upon our decision is based as:
<span class="math display">\[
\min_{u,v: u &lt; v} d^*(C_u^{(k)}, C_v^{(k)}),
\]</span>
(read: “find <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> such that <span class="math inline">\(u&lt;v\)</span> and for which
<span class="math inline">\(d^*(C_u^{(k)}, C_v^{(k)})\)</span> is the smallest”),
where <span class="math inline">\(d^*(C_u^{(k)}, C_v^{(k)})\)</span> is the <em>distance</em> between two clusters
<span class="math inline">\(C_u^{(k)}\)</span> and <span class="math inline">\(C_v^{(k)}\)</span>.</p>
<p>Note that usually we consider only the distances between <em>individual points</em>,
not sets of points, at least this is what we’ve done in
Section <a href="chap-hclust.html#sec:euclidean">2.2</a>.
Therefore, what we need to do is to introduce <span class="math inline">\(d^*\)</span> as a suitable extension
of the Euclidean metric <span class="math inline">\(d\)</span>.</p>
<p>First, we will assume that <span class="math inline">\(d^*(\{i\}, \{j\})= d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot})\)</span>, i.e., the distance between
singleton clusters is the same as the distance between the corresponding
points (rows in <span class="math inline">\(\mathbf{X}\)</span>) themselves.</p>
<p>As far as more populous point groups are concerned, there are many popular
choices of <span class="math inline">\(d^*\)</span> (which in the context of hierarchical clustering we call
<em>linkage functions</em>):</p>
<ul>
<li><p>single linkage – the distance between two clusters is equal to
the distance between the closest pair of points:</p>
<p><span class="math display">\[
  d_\text{S}^*(C_u^{(k)}, C_v^{(k)}) =
  \min_{i\in C_u^{(k)}, j\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}),
  \]</span></p></li>
<li><p>complete linkage – we choose the pair of points farthest away:</p>
<p><span class="math display">\[
  d_\text{C}^*(C_u^{(k)}, C_v^{(k)}) =
  \max_{i\in C_u^{(k)}, j\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}),
  \]</span></p></li>
<li><p>average linkage – we consider the average distance:</p>
<p><span class="math display">\[
  d_\text{A}^*(C_u^{(k)}, C_v^{(k)}) =
  \frac{1}{|C_u^{(k)}| |C_v^{(k)}|} \sum_{i\in C_u^{(k)}}\sum_{j\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot})
  \]</span>
where <span class="math inline">\(|C_u^{(k)}|\)</span> and <span class="math inline">\(|C_v^{(k)}|\)</span> denote the total number of
points in these two clusters.</p></li>
</ul>
<p>An illustration of the way different linkages are computed
is given in Figure <a href="chap-hclust.html#fig:linkages">2.5</a>.</p>
<div class="figure"><span id="fig:linkages"></span>
<img src="02-hclust-figures/linkages-1.svg" alt=" In single linkage, we find the closest pair of points; in complete linkage, we seek the pair furthest away from each other; in average linkage, we determine the arithmetic mean of all inter-cluster pairwise distances" />
<p class="caption">Figure 2.5:  In single linkage, we find the closest pair of points; in complete linkage, we seek the pair furthest away from each other; in average linkage, we determine the arithmetic mean of all inter-cluster pairwise distances</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Here are the pairwise distances between the first 5 countries
(w.r.t. the SSI dimensions)
on the left in Figure <a href="chap-hclust.html#fig:ssi-dendrogram-complete">2.4</a>:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" title="1"><span class="kw">as.matrix</span>(</a>
<a class="sourceLine" id="cb57-2" title="2">    <span class="kw">dist</span>(X[<span class="kw">c</span>(<span class="st">&quot;Latvia&quot;</span>, <span class="st">&quot;Lithuania&quot;</span>, <span class="st">&quot;Poland&quot;</span>, <span class="st">&quot;Denmark&quot;</span>, <span class="st">&quot;Switzerland&quot;</span>),])</a>
<a class="sourceLine" id="cb57-3" title="3">)</a></code></pre></div>
<pre><code>##              Latvia Lithuania  Poland Denmark Switzerland
## Latvia      0.00000   0.77607 0.61706 1.21210      1.8724
## Lithuania   0.77607   0.00000 0.48823 0.99683      1.9058
## Poland      0.61706   0.48823 0.00000 1.08317      2.0167
## Denmark     1.21210   0.99683 1.08317 0.00000      1.0502
## Switzerland 1.87241   1.90577 2.01673 1.05017      0.0000</code></pre>
<p>Calculate (by hand):</p>
<ul>
<li><p><span class="math inline">\(d_\text{S}\left(\{\text{Latvia}\}, \{\text{Lithuania}, \text{Poland}\}\right)\)</span>,</p></li>
<li><p><span class="math inline">\(d_\text{C}\left(\{\text{Latvia}\}, \{\text{Lithuania}, \text{Poland}\}\right)\)</span>,</p></li>
<li><p><span class="math inline">\(d_\text{A}\left(\{\text{Latvia}\}, \{\text{Lithuania}, \text{Poland}\}\right)\)</span>,</p></li>
<li><p><span class="math inline">\(d_\text{S}\left(\{\text{Latvia}, \text{Lithuania}, \text{Poland}\}, \{\text{Denmark}, \text{Switzerland}\}\right)\)</span>,</p></li>
<li><p><span class="math inline">\(d_\text{C}\left(\{\text{Latvia}, \text{Lithuania}, \text{Poland}\}, \{\text{Denmark}, \text{Switzerland}\}\right)\)</span>,</p></li>
<li><span class="math inline">\(d_\text{A}\left(\{\text{Latvia}, \text{Lithuania}, \text{Poland}\}, \{\text{Denmark}, \text{Switzerland}\}\right)\)</span>.</li>
</ul>
</div>
<div style="margin-top: 1em">

</div>
<!--Assuming $d_\text{S}^*$, $d_\text{C}^*$ or $d_\text{A}^*$
in the aforementioned procedure leads to single, complete or average
linkage-based agglomerative hierarchical clustering algorithms,
respectively
(referred to as single linkage etc. for brevity).-->
<p>We can perform agglomerative clustering of our dataset with respect
to different linkages by setting the <code>method</code> argument in the <code>hclust()</code>
function:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" title="1">D &lt;-<span class="st"> </span><span class="kw">dist</span>(X)</a>
<a class="sourceLine" id="cb59-2" title="2">hs &lt;-<span class="st"> </span><span class="kw">hclust</span>(D, <span class="dt">method=</span><span class="st">&quot;single&quot;</span>)</a>
<a class="sourceLine" id="cb59-3" title="3">hc &lt;-<span class="st"> </span><span class="kw">hclust</span>(D, <span class="dt">method=</span><span class="st">&quot;complete&quot;</span>) <span class="co"># the default</span></a>
<a class="sourceLine" id="cb59-4" title="4">ha &lt;-<span class="st"> </span><span class="kw">hclust</span>(D, <span class="dt">method=</span><span class="st">&quot;average&quot;</span>)</a></code></pre></div>
<p>Figure <a href="chap-hclust.html#fig:ssi-dendrograms">2.6</a> depicts the dendrograms
corresponding to single and average linkages,
which we can plot by calling <code>plot(hs)</code> and <code>plot(ha)</code>.</p>
<div class="figure"><span id="fig:ssi-dendrograms"></span>
<img src="02-hclust-figures/ssi-dendrograms-1.svg" alt=" Cluster dendrograms generated by single and average linkages" />
<p class="caption">Figure 2.6:  Cluster dendrograms generated by single and average linkages</p>
</div>
<div class="remark"><strong>Remark.</strong>
<p>The <code>height</code> on the Y axis in each dendrogram’s plot
represents the distance between the merged clusters (as measured
by the assumed linkage function).</p>
</div>
<p>Both complete and average linkages give us nicely “balanced” cluster hierarchy
(average linkage identified Colombia as an “outlier” though).
The obtained groupings are similar to some extent, yet, they are not
identical – both algorithms are based on different heuristics.
However, single linkage tends to produce somehow “ugly” trees – it’s
often be the case that it’ll produce few large clusters and lots of small ones.</p>
<p>The reader is encouraged to perform a similar “debugging” of the obtained
clusterings (say, for the 3- and 4-partitions) as we performed above
(compute the average features in each cluster, draw pairwise scatterplots).</p>
<!--
Figure \@ref(fig:linkages-hier52) compares the 5-, 4- and 3-partitions
obtained by applying the 3 above linkages. Note that it's in very nature of
the single linkage algorithm that it's highly sensitive to outliers.


-->
</div>
<div id="exercises-in-r" class="section level2">
<h2><span class="header-section-number">2.6</span> Exercises in R</h2>
<div class="exercise"><strong>Exercise.</strong>
<p>Perform cluster analysis of the OECD countries
based on the 7 SSI categories <code>ssi_2016_categories.csv</code>.
Apply single, complete and average linkage and draw the
corresponding dengrograms. Compute the averages of all indicators
in each point group for clusterings of different sizes.
Draw scatter plots for different pairs of variables (e.g., by calling
<code>pairs()</code>), where clusters are represented by points of different
shapes and colours.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Perform cluster analysis of the OECD countries
based on the 21 SSI indicators <code>ssi_2016_indicators.csv</code></p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Perform cluster analysis for all the countries covered by the
Sustainable Society Indices.</p>
</div>
</div>
<div id="remarks" class="section level2">
<h2><span class="header-section-number">2.7</span> Remarks</h2>
<p>there are other linkages - e.g., the Ward linkage to minimise the variance
of within-cluster distances, together with the linkages presented in this
chapter they are generalised by the Lance–Williams formula <span class="citation">(Lance &amp; Williams <a href="references.html#ref-lancewilliams">1967</a>)</span>,
see also <span class="citation">(Müllner <a href="references.html#ref-muellner">2011</a>)</span>.</p>
<p>Hierarchical clustering algorithms are great, because they
output a whole hierarchy of <em>nested</em> partitions – it’s not that
when we cut it at the <span class="math inline">\(k\)</span>-th level we’ll obtain something totally unrelated
to what can we find at the <span class="math inline">\(l\)</span>-th one (<span class="math inline">\(k\neq l\)</span>).
However, this comes at a price – these algorithms are quite slow
for large datasets.
In particular, the implementations included in the <code>stats::hclust()</code> function
(which we have used above) have <span class="math inline">\(O(n^3)\)</span> time complexity.
Therefore, in practice it’s better to use
function <code>fastcluster::hclust()</code> <span class="citation">(Müllner <a href="references.html#ref-fastcluster">2013</a>)</span>,
which provides a drop-in replacement
for the original routine.</p>
<p>Single linkage is the fastest – even though it has <span class="math inline">\(O(n^2)\)</span>
worst-case time complexity in generic metric spaces,
however, it can run much faster in real spaces
of small dimensionality (e.g., when analysing spatial data), as it
can be computed based on a minimum spanning tree of the pairwise
distance graph.</p>
<p>For a comprehensive overview of the efficient algorithms to compute
the aforementioned linkages, see <span class="citation">(Müllner <a href="references.html#ref-muellner">2011</a>)</span>.</p>
<p>some history – bibliography</p>
<p>Note that all the indicators were on the same scale.
We’ll deal with different datasets in the next chapter</p>
<p>otherwise, we need to perform some feature weighting or calibration;
e.g., standardisation</p>
<p>Agglomerative clustering algorithms utilise the “bottom-up” approach</p>
<p>there are also divisive algorithms (top-down, start with the <span class="math inline">\(1\)</span>-partition,
split clusters into smaller chunks), but they tend to be much more computationally
intensive, see, Mueller’s ITM, however</p>
<p>what we did above, we call a greedy strategy btw</p>
<p>There are modification to this scheme, e.g., the Genie algorithm
(see package <code>genieclust</code>) <span class="citation">(Gagolewski et al. <a href="references.html#ref-genie">2016</a>)</span> merges clusters based
on the closest pairs of points (just as single linkage), however
under the constraint that the inequality of the cluster size distribution
cannot go beyond some threshold – this prevents leaving out very small
clusters at higher levels of the hierarchy.</p>
<p>Note that we have studied “crisp” (disjoint) partitions only.</p>
<p><span class="citation">(Jain <a href="references.html#ref-jainsurvey">2010</a>)</span>
<span class="citation">(Wierzchoń &amp; Kłopotek <a href="references.html#ref-wierzchon_klopotek">2018</a>)</span></p>
<p>As we’ve said, there are <span class="math inline">\(n(n-1)/2\)</span> unique pairwise distances between <span class="math inline">\(n\)</span> points.
Don’t try calling <code>dist(hclust(X))</code> on large data matrices;
packages <code>fastcluster</code> and , among others,
aim to solve this problem.</p>
<p><code>fastcluster::hclust.vector()</code> implements single and Ward (but not complete
and average) linkages without the need of precomputing the whole distance
matrix.</p>
<p>is the obtained clustering a good one?
well, if it leads to interesting “discoveries” or something useful,
it is;
more discussion in Chapter <a href="chap-kmeans.html#chap:kmeans">9</a></p>
<p>we mention genieclust and centroid-based linkages later</p>
<p>will we play with HDBSCAN*?</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap-introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-knn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
