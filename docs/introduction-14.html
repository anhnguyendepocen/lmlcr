<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8.1 Introduction | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="8.1 Introduction | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8.1 Introduction | Lightweight Machine Learning Classics with R" />
  
  <meta name="twitter:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  

<meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="optimisation-with-genetic-algorithms.html"/>
<link rel="next" href="a-note-on-convex-optimisation.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>{</a></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a><ul>
<li class="chapter" data-level="1.1.1" data-path="machine-learning.html"><a href="machine-learning.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.1.2" data-path="machine-learning.html"><a href="machine-learning.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.2.1" data-path="supervised-learning.html"><a href="supervised-learning.html#formalism"><i class="fa fa-check"></i><b>1.2.1</b> Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="supervised-learning.html"><a href="supervised-learning.html#desired-outputs"><i class="fa fa-check"></i><b>1.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="1.2.3" data-path="supervised-learning.html"><a href="supervised-learning.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple-regression.html"><a href="simple-regression.html"><i class="fa fa-check"></i><b>1.3</b> Simple Regression</a><ul>
<li class="chapter" data-level="1.3.1" data-path="simple-regression.html"><a href="simple-regression.html#introduction"><i class="fa fa-check"></i><b>1.3.1</b> Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-regression.html"><a href="simple-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>1.3.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.4.1" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> Introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html#solution-in-r"><i class="fa fa-check"></i><b>1.4.2</b> Solution in R</a></li>
<li class="chapter" data-level="1.4.3" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>1.4.3</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="outro.html"><a href="outro.html"><i class="fa fa-check"></i><b>1.5</b> Outro</a><ul>
<li class="chapter" data-level="1.5.1" data-path="outro.html"><a href="outro.html#remarks"><i class="fa fa-check"></i><b>1.5.1</b> Remarks</a></li>
<li class="chapter" data-level="1.5.2" data-path="outro.html"><a href="outro.html#further-reading"><i class="fa fa-check"></i><b>1.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>2.1</b> Introduction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-2.html"><a href="introduction-2.html#formalism-1"><i class="fa fa-check"></i><b>2.1.1</b> Formalism</a></li>
<li class="chapter" data-level="2.1.2" data-path="introduction-2.html"><a href="introduction-2.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>2.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#problem-formulation"><i class="fa fa-check"></i><b>2.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>2.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html"><i class="fa fa-check"></i><b>2.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="2.3.1" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#model-diagnostics"><i class="fa fa-check"></i><b>2.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.3.2" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#variable-selection"><i class="fa fa-check"></i><b>2.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="2.3.3" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#variable-transformation"><i class="fa fa-check"></i><b>2.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="2.3.4" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>2.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="outro-1.html"><a href="outro-1.html"><i class="fa fa-check"></i><b>2.4</b> Outro</a><ul>
<li class="chapter" data-level="2.4.1" data-path="outro-1.html"><a href="outro-1.html#remarks-1"><i class="fa fa-check"></i><b>2.4.1</b> Remarks</a></li>
<li class="chapter" data-level="2.4.2" data-path="outro-1.html"><a href="outro-1.html#other-methods-for-regression"><i class="fa fa-check"></i><b>2.4.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="2.4.3" data-path="outro-1.html"><a href="outro-1.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>2.4.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="2.4.4" data-path="outro-1.html"><a href="outro-1.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>2.4.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="2.4.5" data-path="outro-1.html"><a href="outro-1.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>2.4.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="2.4.6" data-path="outro-1.html"><a href="outro-1.html#further-reading-1"><i class="fa fa-check"></i><b>2.4.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-3.html"><a href="introduction-3.html"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-3.html"><a href="introduction-3.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction-3.html"><a href="introduction-3.html#factor-data-type"><i class="fa fa-check"></i><b>3.1.2</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction-3.html"><a href="introduction-3.html#data"><i class="fa fa-check"></i><b>3.1.3</b> Data</a></li>
<li class="chapter" data-level="3.1.4" data-path="introduction-3.html"><a href="introduction-3.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.4</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.5" data-path="introduction-3.html"><a href="introduction-3.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.5</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a><ul>
<li class="chapter" data-level="3.2.1" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#introduction-4"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#different-metrics"><i class="fa fa-check"></i><b>3.2.3</b> Different Metrics (*)</a></li>
<li class="chapter" data-level="3.2.4" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#standardisation-of-independent-variables"><i class="fa fa-check"></i><b>3.2.4</b> Standardisation of Independent Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html"><i class="fa fa-check"></i><b>3.3</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#main-routine"><i class="fa fa-check"></i><b>3.3.1</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.3.2" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#mode"><i class="fa fa-check"></i><b>3.3.2</b> Mode</a></li>
<li class="chapter" data-level="3.3.3" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#nn-search-routines"><i class="fa fa-check"></i><b>3.3.3</b> NN Search Routines (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="outro-2.html"><a href="outro-2.html"><i class="fa fa-check"></i><b>3.4</b> Outro</a><ul>
<li class="chapter" data-level="3.4.1" data-path="outro-2.html"><a href="outro-2.html#remarks-2"><i class="fa fa-check"></i><b>3.4.1</b> Remarks</a></li>
<li class="chapter" data-level="3.4.2" data-path="outro-2.html"><a href="outro-2.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.4.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.4.3" data-path="outro-2.html"><a href="outro-2.html#further-reading-2"><i class="fa fa-check"></i><b>3.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html"><i class="fa fa-check"></i><b>4</b> Classification with Trees and Linear Models</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction-5.html"><a href="introduction-5.html"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="introduction-5.html"><a href="introduction-5.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="introduction-5.html"><a href="introduction-5.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
<li class="chapter" data-level="4.1.3" data-path="introduction-5.html"><a href="introduction-5.html#discussed-methods-1"><i class="fa fa-check"></i><b>4.1.3</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html"><i class="fa fa-check"></i><b>4.2</b> Model Assessment and Selection</a><ul>
<li class="chapter" data-level="4.2.1" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html#performance-metrics"><i class="fa fa-check"></i><b>4.2.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="4.2.2" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>4.2.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="4.2.3" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>4.2.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>4.3</b> Decision Trees</a><ul>
<li class="chapter" data-level="4.3.1" data-path="decision-trees.html"><a href="decision-trees.html#introduction-6"><i class="fa fa-check"></i><b>4.3.1</b> Introduction</a></li>
<li class="chapter" data-level="4.3.2" data-path="decision-trees.html"><a href="decision-trees.html#example-in-r-1"><i class="fa fa-check"></i><b>4.3.2</b> Example in R</a></li>
<li class="chapter" data-level="4.3.3" data-path="decision-trees.html"><a href="decision-trees.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.3.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html"><i class="fa fa-check"></i><b>4.4</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="4.4.1" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#motivation"><i class="fa fa-check"></i><b>4.4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.4.2" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>4.4.2</b> Logistic Model</a></li>
<li class="chapter" data-level="4.4.3" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#example-in-r-2"><i class="fa fa-check"></i><b>4.4.3</b> Example in R</a></li>
<li class="chapter" data-level="4.4.4" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#loss-function"><i class="fa fa-check"></i><b>4.4.4</b> Loss Function</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="outro-3.html"><a href="outro-3.html"><i class="fa fa-check"></i><b>4.5</b> Outro</a><ul>
<li class="chapter" data-level="4.5.1" data-path="outro-3.html"><a href="outro-3.html#remarks-3"><i class="fa fa-check"></i><b>4.5.1</b> Remarks</a></li>
<li class="chapter" data-level="4.5.2" data-path="outro-3.html"><a href="outro-3.html#further-reading-3"><i class="fa fa-check"></i><b>4.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Shallow and Deep Neural Networks</a><ul>
<li class="chapter" data-level="5.1" data-path="introduction-7.html"><a href="introduction-7.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="introduction-7.html"><a href="introduction-7.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>5.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="5.1.2" data-path="introduction-7.html"><a href="introduction-7.html#data-2"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>5.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="5.2.2" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#extending-logistic-regression"><i class="fa fa-check"></i><b>5.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#softmax-function"><i class="fa fa-check"></i><b>5.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="5.2.4" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>5.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="5.2.5" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>5.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="5.2.6" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>5.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html"><i class="fa fa-check"></i><b>5.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="5.3.1" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>5.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="5.3.2" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>5.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="5.3.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#example-in-r-3"><i class="fa fa-check"></i><b>5.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html"><i class="fa fa-check"></i><b>5.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="5.4.1" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#introduction-8"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>5.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="5.4.3" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>5.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="5.4.4" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>5.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="preprocessing-of-data.html"><a href="preprocessing-of-data.html"><i class="fa fa-check"></i><b>5.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="5.5.1" data-path="preprocessing-of-data.html"><a href="preprocessing-of-data.html#introduction-9"><i class="fa fa-check"></i><b>5.5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.5.2" data-path="preprocessing-of-data.html"><a href="preprocessing-of-data.html#image-deskewing"><i class="fa fa-check"></i><b>5.5.2</b> Image Deskewing</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="outro-4.html"><a href="outro-4.html"><i class="fa fa-check"></i><b>5.6</b> Outro</a><ul>
<li class="chapter" data-level="5.6.1" data-path="outro-4.html"><a href="outro-4.html#remarks-4"><i class="fa fa-check"></i><b>5.6.1</b> Remarks</a></li>
<li class="chapter" data-level="5.6.2" data-path="outro-4.html"><a href="outro-4.html#beyond-mnist"><i class="fa fa-check"></i><b>5.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="5.6.3" data-path="outro-4.html"><a href="outro-4.html#further-reading-4"><i class="fa fa-check"></i><b>5.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>6</b> Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-10.html"><a href="introduction-10.html"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="introduction-10.html"><a href="introduction-10.html#optimisation-problem"><i class="fa fa-check"></i><b>6.1.1</b> Optimisation Problem</a></li>
<li class="chapter" data-level="6.1.2" data-path="introduction-10.html"><a href="introduction-10.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>6.1.2</b> Example Optimisation Problems in Machine Learning</a></li>
<li class="chapter" data-level="6.1.3" data-path="introduction-10.html"><a href="introduction-10.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>6.1.3</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="6.1.4" data-path="introduction-10.html"><a href="introduction-10.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>6.1.4</b> Example Objective over a 2D Domain</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="iterative-methods.html"><a href="iterative-methods.html"><i class="fa fa-check"></i><b>6.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="iterative-methods.html"><a href="iterative-methods.html#introduction-11"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="iterative-methods.html"><a href="iterative-methods.html#example-in-r-4"><i class="fa fa-check"></i><b>6.2.2</b> Example in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="iterative-methods.html"><a href="iterative-methods.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>6.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="6.2.4" data-path="iterative-methods.html"><a href="iterative-methods.html#random-restarts"><i class="fa fa-check"></i><b>6.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gradient-descent.html"><a href="gradient-descent.html"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient-descent.html"><a href="gradient-descent.html#function-gradient"><i class="fa fa-check"></i><b>6.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="6.3.2" data-path="gradient-descent.html"><a href="gradient-descent.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>6.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="6.3.3" data-path="gradient-descent.html"><a href="gradient-descent.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>6.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="6.3.4" data-path="gradient-descent.html"><a href="gradient-descent.html#example-mnist"><i class="fa fa-check"></i><b>6.3.4</b> Example: MNIST</a></li>
<li class="chapter" data-level="6.3.5" data-path="gradient-descent.html"><a href="gradient-descent.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>6.3.5</b> Stochastic Gradient Descent (SGD)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="outro-5.html"><a href="outro-5.html"><i class="fa fa-check"></i><b>6.4</b> Outro</a><ul>
<li class="chapter" data-level="6.4.1" data-path="outro-5.html"><a href="outro-5.html#remarks-5"><i class="fa fa-check"></i><b>6.4.1</b> Remarks</a></li>
<li class="chapter" data-level="6.4.2" data-path="outro-5.html"><a href="outro-5.html#optimisers-in-keras"><i class="fa fa-check"></i><b>6.4.2</b> Optimisers in Keras</a></li>
<li class="chapter" data-level="6.4.3" data-path="outro-5.html"><a href="outro-5.html#note-on-search-spaces"><i class="fa fa-check"></i><b>6.4.3</b> Note on Search Spaces</a></li>
<li class="chapter" data-level="6.4.4" data-path="outro-5.html"><a href="outro-5.html#further-reading-5"><i class="fa fa-check"></i><b>6.4.4</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a><ul>
<li class="chapter" data-level="7.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="7.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#introduction-12"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>7.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="7.1.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering-1"><i class="fa fa-check"></i><b>7.1.3</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>7.2</b> K-means Clustering</a><ul>
<li class="chapter" data-level="7.2.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>7.2.1</b> Example in R</a></li>
<li class="chapter" data-level="7.2.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#problem-statement"><i class="fa fa-check"></i><b>7.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="7.2.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>7.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html"><i class="fa fa-check"></i><b>7.3</b> Hierarchical Methods</a><ul>
<li class="chapter" data-level="7.3.1" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#introduction-13"><i class="fa fa-check"></i><b>7.3.1</b> Introduction</a></li>
<li class="chapter" data-level="7.3.2" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#example-in-r-6"><i class="fa fa-check"></i><b>7.3.2</b> Example in R</a></li>
<li class="chapter" data-level="7.3.3" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>7.3.3</b> Agglomerative Hierarchical Clustering</a></li>
<li class="chapter" data-level="7.3.4" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#linkage-functions"><i class="fa fa-check"></i><b>7.3.4</b> Linkage Functions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="outro-6.html"><a href="outro-6.html"><i class="fa fa-check"></i><b>7.4</b> Outro</a><ul>
<li class="chapter" data-level="7.4.1" data-path="outro-6.html"><a href="outro-6.html#remarks-6"><i class="fa fa-check"></i><b>7.4.1</b> Remarks</a></li>
<li class="chapter" data-level="7.4.2" data-path="outro-6.html"><a href="outro-6.html#other-noteworthy-clustering-algorithms"><i class="fa fa-check"></i><b>7.4.2</b> Other Noteworthy Clustering Algorithms</a></li>
<li class="chapter" data-level="7.4.3" data-path="outro-6.html"><a href="outro-6.html#further-reading-6"><i class="fa fa-check"></i><b>7.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html"><i class="fa fa-check"></i><b>8</b> Optimisation with Genetic Algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="introduction-14.html"><a href="introduction-14.html"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="introduction-14.html"><a href="introduction-14.html#recap"><i class="fa fa-check"></i><b>8.1.1</b> Recap</a></li>
<li class="chapter" data-level="8.1.2" data-path="introduction-14.html"><a href="introduction-14.html#k-means-revisited"><i class="fa fa-check"></i><b>8.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="8.1.3" data-path="introduction-14.html"><a href="introduction-14.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>8.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html"><i class="fa fa-check"></i><b>8.2</b> A Note on Convex Optimisation (*)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#introduction-15"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#convex-combinations"><i class="fa fa-check"></i><b>8.2.2</b> Convex Combinations (*)</a></li>
<li class="chapter" data-level="8.2.3" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#convex-functions"><i class="fa fa-check"></i><b>8.2.3</b> Convex Functions (*)</a></li>
<li class="chapter" data-level="8.2.4" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#examples"><i class="fa fa-check"></i><b>8.2.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html"><i class="fa fa-check"></i><b>8.3</b> Genetic Algorithms</a><ul>
<li class="chapter" data-level="8.3.1" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html#introduction-16"><i class="fa fa-check"></i><b>8.3.1</b> Introduction</a></li>
<li class="chapter" data-level="8.3.2" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html#overview-of-the-method"><i class="fa fa-check"></i><b>8.3.2</b> Overview of the Method</a></li>
<li class="chapter" data-level="8.3.3" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html#example-implementation---ga-for-k-means"><i class="fa fa-check"></i><b>8.3.3</b> Example Implementation - GA for K-means</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="outro-7.html"><a href="outro-7.html"><i class="fa fa-check"></i><b>8.4</b> Outro</a><ul>
<li class="chapter" data-level="8.4.1" data-path="outro-7.html"><a href="outro-7.html#remarks-7"><i class="fa fa-check"></i><b>8.4.1</b> Remarks</a></li>
<li class="chapter" data-level="8.4.2" data-path="outro-7.html"><a href="outro-7.html#further-reading-7"><i class="fa fa-check"></i><b>8.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>9</b> Recommender Systems</a><ul>
<li class="chapter" data-level="9.1" data-path="introduction-17.html"><a href="introduction-17.html"><i class="fa fa-check"></i><b>9.1</b> Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="introduction-17.html"><a href="introduction-17.html#what-is-a-recommender-system"><i class="fa fa-check"></i><b>9.1.1</b> What is a Recommender System?</a></li>
<li class="chapter" data-level="9.1.2" data-path="introduction-17.html"><a href="introduction-17.html#the-netflix-prize"><i class="fa fa-check"></i><b>9.1.2</b> The Netflix Prize</a></li>
<li class="chapter" data-level="9.1.3" data-path="introduction-17.html"><a href="introduction-17.html#main-approaches"><i class="fa fa-check"></i><b>9.1.3</b> Main Approaches</a></li>
<li class="chapter" data-level="9.1.4" data-path="introduction-17.html"><a href="introduction-17.html#formalism-2"><i class="fa fa-check"></i><b>9.1.4</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html"><i class="fa fa-check"></i><b>9.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="9.2.1" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#example"><i class="fa fa-check"></i><b>9.2.1</b> Example</a></li>
<li class="chapter" data-level="9.2.2" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#similarity-measures"><i class="fa fa-check"></i><b>9.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="9.2.3" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="9.2.4" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="movielens-dataset.html"><a href="movielens-dataset.html"><i class="fa fa-check"></i><b>9.3</b> MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="9.3.1" data-path="movielens-dataset.html"><a href="movielens-dataset.html#dataset"><i class="fa fa-check"></i><b>9.3.1</b> Dataset</a></li>
<li class="chapter" data-level="9.3.2" data-path="movielens-dataset.html"><a href="movielens-dataset.html#data-cleansing"><i class="fa fa-check"></i><b>9.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="9.3.3" data-path="movielens-dataset.html"><a href="movielens-dataset.html#item-item-similarities"><i class="fa fa-check"></i><b>9.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="9.3.4" data-path="movielens-dataset.html"><a href="movielens-dataset.html#example-recommendations"><i class="fa fa-check"></i><b>9.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="9.3.5" data-path="movielens-dataset.html"><a href="movielens-dataset.html#clustering-2"><i class="fa fa-check"></i><b>9.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="outro-8.html"><a href="outro-8.html"><i class="fa fa-check"></i><b>9.4</b> Outro</a><ul>
<li class="chapter" data-level="9.4.1" data-path="outro-8.html"><a href="outro-8.html#remarks-8"><i class="fa fa-check"></i><b>9.4.1</b> Remarks</a></li>
<li class="chapter" data-level="9.4.2" data-path="outro-8.html"><a href="outro-8.html#issues"><i class="fa fa-check"></i><b>9.4.2</b> Issues</a></li>
<li class="chapter" data-level="9.4.3" data-path="outro-8.html"><a href="outro-8.html#further-reading-8"><i class="fa fa-check"></i><b>9.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="section-15.html"><a href="section-15.html"><i class="fa fa-check"></i>}</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html"><i class="fa fa-check"></i><b>A</b> Setting Up the R Environment</a><ul>
<li class="chapter" data-level="A.1" data-path="installing-r.html"><a href="installing-r.html"><i class="fa fa-check"></i><b>A.1</b> Installing R</a></li>
<li class="chapter" data-level="A.2" data-path="installing-an-ide.html"><a href="installing-an-ide.html"><i class="fa fa-check"></i><b>A.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="A.3" data-path="installing-recommended-packages.html"><a href="installing-recommended-packages.html"><i class="fa fa-check"></i><b>A.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="A.4" data-path="first-r-script.html"><a href="first-r-script.html"><i class="fa fa-check"></i><b>A.4</b> First R Script</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html"><i class="fa fa-check"></i><b>B</b> Vector Algebra in R</a><ul>
<li class="chapter" data-level="B.1" data-path="motivation-1.html"><a href="motivation-1.html"><i class="fa fa-check"></i><b>B.1</b> Motivation</a></li>
<li class="chapter" data-level="B.2" data-path="numeric-vectors.html"><a href="numeric-vectors.html"><i class="fa fa-check"></i><b>B.2</b> Numeric Vectors</a><ul>
<li class="chapter" data-level="B.2.1" data-path="numeric-vectors.html"><a href="numeric-vectors.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>B.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="B.2.2" data-path="numeric-vectors.html"><a href="numeric-vectors.html#vector-scalar-operations"><i class="fa fa-check"></i><b>B.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="B.2.3" data-path="numeric-vectors.html"><a href="numeric-vectors.html#vector-vector-operations"><i class="fa fa-check"></i><b>B.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="B.2.4" data-path="numeric-vectors.html"><a href="numeric-vectors.html#aggregation-functions"><i class="fa fa-check"></i><b>B.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="B.2.5" data-path="numeric-vectors.html"><a href="numeric-vectors.html#special-functions"><i class="fa fa-check"></i><b>B.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="B.2.6" data-path="numeric-vectors.html"><a href="numeric-vectors.html#norms-and-distances"><i class="fa fa-check"></i><b>B.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="B.2.7" data-path="numeric-vectors.html"><a href="numeric-vectors.html#dot-product"><i class="fa fa-check"></i><b>B.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="B.2.8" data-path="numeric-vectors.html"><a href="numeric-vectors.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>B.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="logical-vectors.html"><a href="logical-vectors.html"><i class="fa fa-check"></i><b>B.3</b> Logical Vectors</a><ul>
<li class="chapter" data-level="B.3.1" data-path="logical-vectors.html"><a href="logical-vectors.html#creating-logical-vectors"><i class="fa fa-check"></i><b>B.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="B.3.2" data-path="logical-vectors.html"><a href="logical-vectors.html#logical-operations"><i class="fa fa-check"></i><b>B.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="B.3.3" data-path="logical-vectors.html"><a href="logical-vectors.html#comparison-operations"><i class="fa fa-check"></i><b>B.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="B.3.4" data-path="logical-vectors.html"><a href="logical-vectors.html#aggregation-functions-1"><i class="fa fa-check"></i><b>B.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="character-vectors.html"><a href="character-vectors.html"><i class="fa fa-check"></i><b>B.4</b> Character Vectors</a><ul>
<li class="chapter" data-level="B.4.1" data-path="character-vectors.html"><a href="character-vectors.html#creating-character-vectors"><i class="fa fa-check"></i><b>B.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="B.4.2" data-path="character-vectors.html"><a href="character-vectors.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>B.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="B.4.3" data-path="character-vectors.html"><a href="character-vectors.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>B.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="vector-subsetting.html"><a href="vector-subsetting.html"><i class="fa fa-check"></i><b>B.5</b> Vector Subsetting</a><ul>
<li class="chapter" data-level="B.5.1" data-path="vector-subsetting.html"><a href="vector-subsetting.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>B.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="B.5.2" data-path="vector-subsetting.html"><a href="vector-subsetting.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>B.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="B.5.3" data-path="vector-subsetting.html"><a href="vector-subsetting.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>B.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="B.5.4" data-path="vector-subsetting.html"><a href="vector-subsetting.html#replacing-elements"><i class="fa fa-check"></i><b>B.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="B.5.5" data-path="vector-subsetting.html"><a href="vector-subsetting.html#other-functions"><i class="fa fa-check"></i><b>B.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="B.6" data-path="named-vectors.html"><a href="named-vectors.html"><i class="fa fa-check"></i><b>B.6</b> Named Vectors</a><ul>
<li class="chapter" data-level="B.6.1" data-path="named-vectors.html"><a href="named-vectors.html#creating-named-vectors"><i class="fa fa-check"></i><b>B.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="B.6.2" data-path="named-vectors.html"><a href="named-vectors.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>B.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="B.7" data-path="factors.html"><a href="factors.html"><i class="fa fa-check"></i><b>B.7</b> Factors</a><ul>
<li class="chapter" data-level="B.7.1" data-path="factors.html"><a href="factors.html#creating-factors"><i class="fa fa-check"></i><b>B.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="B.7.2" data-path="factors.html"><a href="factors.html#levels"><i class="fa fa-check"></i><b>B.7.2</b> Levels</a></li>
<li class="chapter" data-level="B.7.3" data-path="factors.html"><a href="factors.html#internal-representation"><i class="fa fa-check"></i><b>B.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="B.8" data-path="lists.html"><a href="lists.html"><i class="fa fa-check"></i><b>B.8</b> Lists</a><ul>
<li class="chapter" data-level="B.8.1" data-path="lists.html"><a href="lists.html#creating-lists"><i class="fa fa-check"></i><b>B.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="B.8.2" data-path="lists.html"><a href="lists.html#named-lists"><i class="fa fa-check"></i><b>B.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="B.8.3" data-path="lists.html"><a href="lists.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>B.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="B.8.4" data-path="lists.html"><a href="lists.html#common-operations"><i class="fa fa-check"></i><b>B.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="B.9" data-path="further-reading-9.html"><a href="further-reading-9.html"><i class="fa fa-check"></i><b>B.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html"><i class="fa fa-check"></i><b>C</b> Matrix Algebra in R</a><ul>
<li class="chapter" data-level="C.1" data-path="creating-matrices.html"><a href="creating-matrices.html"><i class="fa fa-check"></i><b>C.1</b> Creating Matrices</a><ul>
<li class="chapter" data-level="C.1.1" data-path="creating-matrices.html"><a href="creating-matrices.html#matrix"><i class="fa fa-check"></i><b>C.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="C.1.2" data-path="creating-matrices.html"><a href="creating-matrices.html#stacking-vectors"><i class="fa fa-check"></i><b>C.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="C.1.3" data-path="creating-matrices.html"><a href="creating-matrices.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>C.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="C.1.4" data-path="creating-matrices.html"><a href="creating-matrices.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>C.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="C.1.5" data-path="creating-matrices.html"><a href="creating-matrices.html#other-methods"><i class="fa fa-check"></i><b>C.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="C.1.6" data-path="creating-matrices.html"><a href="creating-matrices.html#internal-representation-1"><i class="fa fa-check"></i><b>C.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="algebraic-operations.html"><a href="algebraic-operations.html"><i class="fa fa-check"></i><b>C.2</b> Algebraic Operations</a><ul>
<li class="chapter" data-level="C.2.1" data-path="algebraic-operations.html"><a href="algebraic-operations.html#matrix-transpose"><i class="fa fa-check"></i><b>C.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="C.2.2" data-path="algebraic-operations.html"><a href="algebraic-operations.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="algebraic-operations.html"><a href="algebraic-operations.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>C.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="algebraic-operations.html"><a href="algebraic-operations.html#matrix-multiplication"><i class="fa fa-check"></i><b>C.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="C.2.5" data-path="algebraic-operations.html"><a href="algebraic-operations.html#matrix-vector-operations"><i class="fa fa-check"></i><b>C.2.5</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="matrix-subsetting.html"><a href="matrix-subsetting.html"><i class="fa fa-check"></i><b>C.3</b> Matrix Subsetting</a><ul>
<li class="chapter" data-level="C.3.1" data-path="matrix-subsetting.html"><a href="matrix-subsetting.html#selecting-individual-elements"><i class="fa fa-check"></i><b>C.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="C.3.2" data-path="matrix-subsetting.html"><a href="matrix-subsetting.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>C.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="C.3.3" data-path="matrix-subsetting.html"><a href="matrix-subsetting.html#selecting-submatrices"><i class="fa fa-check"></i><b>C.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="C.3.4" data-path="matrix-subsetting.html"><a href="matrix-subsetting.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>C.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="C.3.5" data-path="matrix-subsetting.html"><a href="matrix-subsetting.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>C.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="further-reading-10.html"><a href="further-reading-10.html"><i class="fa fa-check"></i><b>C.4</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html"><i class="fa fa-check"></i><b>D</b> Data Frame Wrangling in R</a><ul>
<li class="chapter" data-level="D.1" data-path="creating-data-frames.html"><a href="creating-data-frames.html"><i class="fa fa-check"></i><b>D.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="D.2" data-path="importing-data-frames.html"><a href="importing-data-frames.html"><i class="fa fa-check"></i><b>D.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="D.3" data-path="data-frame-subsetting.html"><a href="data-frame-subsetting.html"><i class="fa fa-check"></i><b>D.3</b> Data Frame Subsetting</a><ul>
<li class="chapter" data-level="D.3.1" data-path="data-frame-subsetting.html"><a href="data-frame-subsetting.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>D.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="D.3.2" data-path="data-frame-subsetting.html"><a href="data-frame-subsetting.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>D.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="common-operations-1.html"><a href="common-operations-1.html"><i class="fa fa-check"></i><b>D.4</b> Common Operations</a></li>
<li class="chapter" data-level="D.5" data-path="metaprogramming-and-formulas.html"><a href="metaprogramming-and-formulas.html"><i class="fa fa-check"></i><b>D.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="D.6" data-path="further-reading-11.html"><a href="further-reading-11.html"><i class="fa fa-check"></i><b>D.6</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.2 2020-03-16 14:13 (faef6c9)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-14" class="section level2">
<h2><span class="header-section-number">8.1</span> Introduction</h2>
<div id="recap" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Recap</h3>
<hr />
<p>Recall that an <strong>optimisation task</strong> deals with finding an element <span class="math inline">\({x}\)</span>
in a <strong>search space</strong> <span class="math inline">\(D\)</span>,
that minimises or maximises an <strong>objective function</strong> <span class="math inline">\(f:D\to\mathbb{R}\)</span>:
<span class="math display">\[
\min_{{x}\in D} f({x}) \quad\text{or}\quad\max_{{x}\in D} f({x}),
\]</span></p>
<p>In one of the previous chapters, we were dealing with
<strong>unconstrained continuous optimisation</strong>,
i.e., we assumed the search space is <span class="math inline">\(D=\mathbb{R}^p\)</span> for some <span class="math inline">\(p\)</span>.</p>
<p> <br />
</p>
<p>Example problems of this kind: minimising mean squared error
in linear regression
or cross-entropy in logistic regression.</p>
<hr />
<p>The class of general-purpose iterative algorithms we’ve previously studied
fit into the following scheme:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbf{x}^{(0)}\)</span> – initial guess (e.g., generated at random)</p></li>
<li>for <span class="math inline">\(i=1,...,M\)</span>:
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\mathbf{x}^{(i)} = \mathbf{x}^{(i-1)}+\text{[guessed direction, e.g.,}-\eta\nabla f(\mathbf{x})\text{]}\)</span></li>
<li>if <span class="math inline">\(|f(\mathbf{x}^{(i)})-f(\mathbf{x}^{(i-1)})| &lt; \varepsilon\)</span> break</li>
</ol></li>
<li><p>return <span class="math inline">\(\mathbf{x}^{(i)}\)</span> as result</p></li>
</ol>
<p>where:</p>
<ul>
<li><span class="math inline">\(M\)</span> = maximum number of iterations</li>
<li><span class="math inline">\(\varepsilon\)</span> = tolerance, e.g, <span class="math inline">\(10^{-8}\)</span></li>
<li><span class="math inline">\(\eta&gt;0\)</span> = learning rate</li>
</ul>
<hr />
<p>The algorithms such as gradient descent and BFGS (see <code>optim()</code>)
give satisfactory results in the case of <strong>smooth and well-behaving objective functions</strong>.</p>
<p>However, if an objective has, e.g., many plateaus (regions where it is almost constant),
those methods might easily get stuck in local minima.</p>
<p>The K-means clustering’s objective function is a not particularly pleasant
one – it involves a nested search for the closest cluster, with the use of the <span class="math inline">\(\min\)</span> operator.</p>
</div>
<div id="k-means-revisited" class="section level3">
<h3><span class="header-section-number">8.1.2</span> K-means Revisited</h3>
<hr />
<p>In <strong>K-means clustering</strong> we are minimising the squared Euclidean distance
to each point’s cluster centre:
<span class="math display">\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{k=1,\dots,K}
\sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\right).
\]</span></p>
<p>This is an (NP-)hard problem!
There is no efficient exact algorithm.</p>
<p>We need approximations. In the last chapter, we have
discussed the iterative Lloyd’s algorithm (1957),
which is amongst a few procedures implemented in the <code>kmeans()</code> function.</p>
<hr />
<p>To recall, Lloyd’s algorithm (1957) is sometimes referred to as “the” K-means algorithm:</p>
<ol style="list-style-type: decimal">
<li><p>Start with random cluster centres <span class="math inline">\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)</span>.</p></li>
<li><p>For each point <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span>, determine its closest centre <span class="math inline">\(C(i)\in\{1,\dots,K\}\)</span>.</p></li>
<li><p>For each cluster <span class="math inline">\(k\in\{1,\dots,K\}\)</span>, compute the new cluster centre <span class="math inline">\(\boldsymbol\mu_{k,\cdot}\)</span> as the componentwise arithmetic mean
of the coordinates of all the point indexes <span class="math inline">\(i\)</span> such that <span class="math inline">\(C(i)=k\)</span>.</p></li>
<li><p>If the cluster centres changed since last iteration, go to step 2, otherwise stop and return the result.</p></li>
</ol>
<p> <br />
</p>
<p>As the procedure might get stuck in a local minimum,
a few restarts are recommended (as usual).</p>
<p>Hence, we are used to calling:</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb461-1" title="1"><span class="kw">kmeans</span>(X, <span class="dt">centers=</span>k, <span class="dt">nstart=</span><span class="dv">10</span>)</a></code></pre></div>
</div>
<div id="optim-vs.-kmeans" class="section level3">
<h3><span class="header-section-number">8.1.3</span> optim() vs. kmeans()</h3>
<hr />
<p>Let us compare how a general-purpose optimiser such as the BFGS algorithm
implemented in <code>optim()</code> compares with a customised, problem-specific solver.</p>
<p>We will need some benchmark data.</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb462-1" title="1">gen_cluster &lt;-<span class="st"> </span><span class="cf">function</span>(n, p, m, s) {</a>
<a class="sourceLine" id="cb462-2" title="2">    vectors &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p)</a>
<a class="sourceLine" id="cb462-3" title="3">    unit_vectors &lt;-<span class="st"> </span>vectors<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">rowSums</span>(vectors<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb462-4" title="4">    unit_vectors<span class="op">*</span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, s)<span class="op">+</span><span class="kw">rep</span>(m, <span class="dt">each=</span>n)</a>
<a class="sourceLine" id="cb462-5" title="5">}</a></code></pre></div>
<p>The above function generates <span class="math inline">\(n\)</span> points in <span class="math inline">\(\mathbb{R}^p\)</span>
from a distribution centred at <span class="math inline">\(\mathbf{m}\in\mathbb{R}^p\)</span>,
spread randomly in every possible direction with scale factor <span class="math inline">\(s\)</span>.</p>
<hr />
<p>Two example clusters in <span class="math inline">\(\mathbb{R}^2\)</span>:</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb463-1" title="1"><span class="co"># plot the &quot;black&quot; cluster</span></a>
<a class="sourceLine" id="cb463-2" title="2"><span class="kw">plot</span>(<span class="kw">gen_cluster</span>(<span class="dv">500</span>, <span class="dv">2</span>, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="dv">1</span>), <span class="dt">col=</span><span class="st">&quot;#00000022&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>,</a>
<a class="sourceLine" id="cb463-3" title="3">    <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">4</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">4</span>), <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">las=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb463-4" title="4"><span class="co"># plot the &quot;red&quot; cluster</span></a>
<a class="sourceLine" id="cb463-5" title="5"><span class="kw">points</span>(<span class="kw">gen_cluster</span>(<span class="dv">250</span>, <span class="dv">2</span>, <span class="kw">c</span>(<span class="fl">1.5</span>, <span class="dv">1</span>), <span class="fl">0.5</span>), <span class="dt">col=</span><span class="st">&quot;#ff000022&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a></code></pre></div>
<p><img src="08-optimisation-genetic-figures/gendata_example-1.png" alt="plot of chunk gendata_example" style="width:50.0%" /></p>
<hr />
<p>Let’s generate the benchmark dataset <span class="math inline">\(\mathbf{X}\)</span>
that consists of three clusters in a high-dimensional space.</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb464-1" title="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb464-2" title="2">p  &lt;-<span class="st"> </span><span class="dv">32</span></a>
<a class="sourceLine" id="cb464-3" title="3">Ns &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb464-4" title="4">Ms &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb464-5" title="5">s  &lt;-<span class="st"> </span><span class="fl">1.5</span><span class="op">*</span>p</a>
<a class="sourceLine" id="cb464-6" title="6">K  &lt;-<span class="st"> </span><span class="kw">length</span>(Ns)</a>
<a class="sourceLine" id="cb464-7" title="7"></a>
<a class="sourceLine" id="cb464-8" title="8">X &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>K, <span class="cf">function</span>(k)</a>
<a class="sourceLine" id="cb464-9" title="9">    <span class="kw">gen_cluster</span>(Ns[k], p, <span class="kw">rep</span>(Ms[k], p), s))</a>
<a class="sourceLine" id="cb464-10" title="10">X &lt;-<span class="st"> </span><span class="kw">do.call</span>(rbind, X) <span class="co"># rbind(X[[1]], X[[2]], X[[3]])</span></a></code></pre></div>
<hr />
<p>The objective function for the K-means clustering problem:</p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb465-1" title="1"><span class="kw">library</span>(<span class="st">&quot;FNN&quot;</span>)</a>
<a class="sourceLine" id="cb465-2" title="2">get_fitness &lt;-<span class="st"> </span><span class="cf">function</span>(mu, X) {</a>
<a class="sourceLine" id="cb465-3" title="3">    <span class="co"># For each point in X,</span></a>
<a class="sourceLine" id="cb465-4" title="4">    <span class="co"># get the index of the closest point in mu:</span></a>
<a class="sourceLine" id="cb465-5" title="5">    memb &lt;-<span class="st"> </span>FNN<span class="op">::</span><span class="kw">get.knnx</span>(mu, X, <span class="dv">1</span>)<span class="op">$</span>nn.index</a>
<a class="sourceLine" id="cb465-6" title="6"></a>
<a class="sourceLine" id="cb465-7" title="7">    <span class="co"># compute the sum of squared distances</span></a>
<a class="sourceLine" id="cb465-8" title="8">    <span class="co"># between each point and its closes cluster centre:</span></a>
<a class="sourceLine" id="cb465-9" title="9">    <span class="kw">sum</span>((X<span class="op">-</span>mu[memb,])<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb465-10" title="10">}</a></code></pre></div>
<hr />
<p>Setting up the solvers:</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb466-1" title="1">min_HartiganWong &lt;-<span class="st"> </span><span class="cf">function</span>(mu0, X)</a>
<a class="sourceLine" id="cb466-2" title="2">    <span class="kw">get_fitness</span>(</a>
<a class="sourceLine" id="cb466-3" title="3">        <span class="co"># algorithm=&quot;Hartigan-Wong&quot;</span></a>
<a class="sourceLine" id="cb466-4" title="4">        <span class="kw">kmeans</span>(X, mu0, <span class="dt">iter.max=</span><span class="dv">100</span>)<span class="op">$</span>centers,</a>
<a class="sourceLine" id="cb466-5" title="5">    X)</a>
<a class="sourceLine" id="cb466-6" title="6">min_Lloyd &lt;-<span class="st"> </span><span class="cf">function</span>(mu0, X)</a>
<a class="sourceLine" id="cb466-7" title="7">    <span class="kw">get_fitness</span>(</a>
<a class="sourceLine" id="cb466-8" title="8">        <span class="kw">kmeans</span>(X, mu0, <span class="dt">iter.max=</span><span class="dv">100</span>, <span class="dt">algorithm=</span><span class="st">&quot;Lloyd&quot;</span>)<span class="op">$</span>centers,</a>
<a class="sourceLine" id="cb466-9" title="9">    X)</a>
<a class="sourceLine" id="cb466-10" title="10">min_optim &lt;-<span class="st"> </span><span class="cf">function</span>(mu0, X)</a>
<a class="sourceLine" id="cb466-11" title="11">    <span class="kw">optim</span>(mu0,</a>
<a class="sourceLine" id="cb466-12" title="12">        <span class="cf">function</span>(mu, X) {</a>
<a class="sourceLine" id="cb466-13" title="13">            <span class="kw">get_fitness</span>(<span class="kw">matrix</span>(mu, <span class="dt">nrow=</span><span class="kw">nrow</span>(mu0)), X)</a>
<a class="sourceLine" id="cb466-14" title="14">        }, <span class="dt">X=</span>X, <span class="dt">method=</span><span class="st">&quot;BFGS&quot;</span>, <span class="dt">control=</span><span class="kw">list</span>(<span class="dt">reltol=</span><span class="fl">1e-16</span>)</a>
<a class="sourceLine" id="cb466-15" title="15">    )<span class="op">$</span>val</a></code></pre></div>
<hr />
<p>Running the simulation:</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb467-1" title="1">nstart &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb467-2" title="2"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb467-3" title="3">res &lt;-<span class="st"> </span><span class="kw">replicate</span>(nstart, {</a>
<a class="sourceLine" id="cb467-4" title="4">  mu0 &lt;-<span class="st"> </span>X[<span class="kw">sample</span>(<span class="kw">nrow</span>(X), K),]</a>
<a class="sourceLine" id="cb467-5" title="5">    <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb467-6" title="6">        <span class="dt">HartiganWong=</span><span class="kw">min_HartiganWong</span>(mu0, X),</a>
<a class="sourceLine" id="cb467-7" title="7">        <span class="dt">Lloyd=</span><span class="kw">min_Lloyd</span>(mu0, X),</a>
<a class="sourceLine" id="cb467-8" title="8">        <span class="dt">optim=</span><span class="kw">min_optim</span>(mu0, X)</a>
<a class="sourceLine" id="cb467-9" title="9">    )</a>
<a class="sourceLine" id="cb467-10" title="10">})</a></code></pre></div>
<hr />
<p>Notice a considerable variability of the
objective function at the local minima found:</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb468-1" title="1"><span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">2</span>, <span class="fl">6.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>)) <span class="co"># figure margins</span></a>
<a class="sourceLine" id="cb468-2" title="2"><span class="kw">boxplot</span>(<span class="kw">as.data.frame</span>(<span class="kw">t</span>(res)), <span class="dt">horizontal=</span><span class="ot">TRUE</span>, <span class="dt">las=</span><span class="dv">1</span>)</a></code></pre></div>
<p><img src="08-optimisation-genetic-figures/gendata5-1.png" alt="plot of chunk gendata5" style="width:50.0%" /></p>
<hr />
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb469-1" title="1"><span class="kw">print</span>(<span class="kw">apply</span>(res, <span class="dv">1</span>, <span class="cf">function</span>(x)</a>
<a class="sourceLine" id="cb469-2" title="2">    <span class="kw">c</span>(<span class="kw">summary</span>(x), <span class="dt">sd=</span><span class="kw">sd</span>(x))</a>
<a class="sourceLine" id="cb469-3" title="3">))</a></code></pre></div>
<pre><code>##         HartiganWong      Lloyd    optim
## Min.      421889.463 425119.482 422989.2
## 1st Qu.   424662.768 433669.308 432445.6
## Median    427128.673 438502.186 440032.9
## Mean      426557.050 438074.991 440635.3
## 3rd Qu.   428242.881 441381.268 446614.2
## Max.      431868.537 450469.678 466302.5
## sd          2300.955   5709.282  10888.4</code></pre>
<p>Of course, we are interested in the smallest value of the objective,
because we’re trying to pinpoint the global minimum.</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb471-1" title="1"><span class="kw">print</span>(<span class="kw">apply</span>(res, <span class="dv">1</span>, min))</a></code></pre></div>
<pre><code>## HartiganWong        Lloyd        optim 
##     421889.5     425119.5     422989.2</code></pre>
<hr />
<p>The Hartigan-Wong algorithm (the default one in <code>kmeans()</code>)
is the most reliable one of the three:</p>
<ul>
<li>it gives the best solution (low bias)</li>
<li>the solutions have the lowest degree of variability (low variance)</li>
<li>it is the fastest:</li>
</ul>
<hr />
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb473-1" title="1"><span class="kw">library</span>(<span class="st">&quot;microbenchmark&quot;</span>)</a>
<a class="sourceLine" id="cb473-2" title="2"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb473-3" title="3">mu0 &lt;-<span class="st"> </span>X[<span class="kw">sample</span>(<span class="kw">nrow</span>(X), K),]</a>
<a class="sourceLine" id="cb473-4" title="4"><span class="kw">summary</span>(<span class="kw">microbenchmark</span>(</a>
<a class="sourceLine" id="cb473-5" title="5">    <span class="dt">HartiganWong=</span><span class="kw">min_HartiganWong</span>(mu0, X),</a>
<a class="sourceLine" id="cb473-6" title="6">    <span class="dt">Lloyd=</span><span class="kw">min_Lloyd</span>(mu0, X),</a>
<a class="sourceLine" id="cb473-7" title="7">    <span class="dt">optim=</span><span class="kw">min_optim</span>(mu0, X),</a>
<a class="sourceLine" id="cb473-8" title="8">    <span class="dt">times=</span><span class="dv">10</span></a>
<a class="sourceLine" id="cb473-9" title="9">), <span class="dt">unit=</span><span class="st">&quot;relative&quot;</span>)</a></code></pre></div>
<pre><code>##           expr        min         lq        mean      median
## 1 HartiganWong    1.10129    1.12666    1.204456    1.260605
## 2        Lloyd    1.00000    1.00000    1.000000    1.000000
## 3        optim 1608.58910 1601.12095 1495.478568 1567.427931
##            uq         max neval
## 1    1.356686    1.088029    10
## 2    1.000000    1.000000    10
## 3 1529.904081 1110.287899    10</code></pre>
<hr />
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb475-1" title="1"><span class="kw">print</span>(<span class="kw">min</span>(res))</a></code></pre></div>
<pre><code>## [1] 421889.5</code></pre>
<p>Is it the global minimum?</p>
<blockquote>
<p>We don’t know, we just didn’t happen to find anything better (yet).</p>
</blockquote>
<p>Did we put enough effort to find it?</p>
<blockquote>
<p>Well, maybe. We can try more random restarts:</p>
</blockquote>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb477-1" title="1">res_tried_very_hard &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, K, <span class="dt">nstart=</span><span class="dv">100000</span>, <span class="dt">iter.max=</span><span class="dv">10000</span>)<span class="op">$</span>centers</a>
<a class="sourceLine" id="cb477-2" title="2"><span class="kw">print</span>(<span class="kw">get_fitness</span>(res_tried_very_hard, X))</a></code></pre></div>
<pre><code>## [1] 421889.5</code></pre>
<p>Is it good enough?</p>
<blockquote>
<p>It depends what we’d like to do with this. Does it make your boss happy?
Does it generate revenue? Does it help solve any other problem? Is it useful anyhow?
Are you really looking for the global minimum?</p>
</blockquote>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="optimisation-with-genetic-algorithms.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-note-on-convex-optimisation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
