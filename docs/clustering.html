<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Clustering | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Clustering | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Clustering | Lightweight Machine Learning Classics with R" />
  
  <meta name="twitter:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  

<meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="optimisation-with-iterative-algorithms.html"/>
<link rel="next" href="optimisation-with-genetic-algorithms.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

</style>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>{</a></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a><ul>
<li class="chapter" data-level="1.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#supervised-learning"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#formalism"><i class="fa fa-check"></i><b>1.2.1</b> Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#desired-outputs"><i class="fa fa-check"></i><b>1.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="1.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-regression"><i class="fa fa-check"></i><b>1.3</b> Simple Regression</a><ul>
<li class="chapter" data-level="1.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>1.3.1</b> Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>1.3.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> Introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#solution-in-r"><i class="fa fa-check"></i><b>1.4.2</b> Solution in R</a></li>
<li class="chapter" data-level="1.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analytic-solution"><i class="fa fa-check"></i><b>1.4.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="1.4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>1.4.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-in-r"><i class="fa fa-check"></i><b>1.5</b> Exercises in R</a><ul>
<li class="chapter" data-level="1.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>1.5.1</b> The Anscombe Quartet</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outro"><i class="fa fa-check"></i><b>1.6</b> Outro</a><ul>
<li class="chapter" data-level="1.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#remarks"><i class="fa fa-check"></i><b>1.6.1</b> Remarks</a></li>
<li class="chapter" data-level="1.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#further-reading"><i class="fa fa-check"></i><b>1.6.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#formalism-1"><i class="fa fa-check"></i><b>2.1.1</b> Formalism</a></li>
<li class="chapter" data-level="2.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>2.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>2.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#problem-formulation"><i class="fa fa-check"></i><b>2.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>2.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="multiple-regression.html"><a href="multiple-regression.html#finding-the-best-model"><i class="fa fa-check"></i><b>2.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>2.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.3.2" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-selection"><i class="fa fa-check"></i><b>2.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="2.3.3" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-transformation"><i class="fa fa-check"></i><b>2.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="2.3.4" data-path="multiple-regression.html"><a href="multiple-regression.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>2.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multiple-regression.html"><a href="multiple-regression.html#exercises-in-r-1"><i class="fa fa-check"></i><b>2.4</b> Exercises in R</a><ul>
<li class="chapter" data-level="2.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>2.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="2.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="2.4.3" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>2.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="2.4.4" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="2.4.5" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>2.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="multiple-regression.html"><a href="multiple-regression.html#outro-1"><i class="fa fa-check"></i><b>2.5</b> Outro</a><ul>
<li class="chapter" data-level="2.5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#remarks-1"><i class="fa fa-check"></i><b>2.5.1</b> Remarks</a></li>
<li class="chapter" data-level="2.5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#other-methods-for-regression"><i class="fa fa-check"></i><b>2.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="2.5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>2.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="2.5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>2.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="2.5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>2.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="2.5.6" data-path="multiple-regression.html"><a href="multiple-regression.html#further-reading-1"><i class="fa fa-check"></i><b>2.5.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours</a><ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#data"><i class="fa fa-check"></i><b>3.1.2</b> Data</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.3</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.4</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#k-nearest-neighbour-classifier"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a><ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-4"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#feature-engineering"><i class="fa fa-check"></i><b>3.2.3</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#model-assessment-and-selection"><i class="fa fa-check"></i><b>3.3</b> Model Assessment and Selection</a><ul>
<li class="chapter" data-level="3.3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#performance-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>3.3.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="3.3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>3.3.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.4</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#factor-data-type"><i class="fa fa-check"></i><b>3.4.1</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.4.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#main-routine"><i class="fa fa-check"></i><b>3.4.2</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.4.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#mode"><i class="fa fa-check"></i><b>3.4.3</b> Mode</a></li>
<li class="chapter" data-level="3.4.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#nn-search-routines"><i class="fa fa-check"></i><b>3.4.4</b> NN Search Routines (*)</a></li>
<li class="chapter" data-level="3.4.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#different-metrics"><i class="fa fa-check"></i><b>3.4.5</b> Different Metrics (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#outro-2"><i class="fa fa-check"></i><b>3.5</b> Outro</a><ul>
<li class="chapter" data-level="3.5.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#remarks-2"><i class="fa fa-check"></i><b>3.5.1</b> Remarks</a></li>
<li class="chapter" data-level="3.5.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.5.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.5.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#further-reading-2"><i class="fa fa-check"></i><b>3.5.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html"><i class="fa fa-check"></i><b>4</b> Classification with Trees and Linear Models</a><ul>
<li class="chapter" data-level="4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision Trees</a><ul>
<li class="chapter" data-level="4.2.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-6"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-1"><i class="fa fa-check"></i><b>4.2.2</b> Example in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>4.3</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="4.3.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#motivation"><i class="fa fa-check"></i><b>4.3.1</b> Motivation</a></li>
<li class="chapter" data-level="4.3.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#logistic-model"><i class="fa fa-check"></i><b>4.3.2</b> Logistic Model</a></li>
<li class="chapter" data-level="4.3.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-2"><i class="fa fa-check"></i><b>4.3.3</b> Example in R</a></li>
<li class="chapter" data-level="4.3.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#loss-function"><i class="fa fa-check"></i><b>4.3.4</b> Loss Function</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#outro-3"><i class="fa fa-check"></i><b>4.4</b> Outro</a><ul>
<li class="chapter" data-level="4.4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#remarks-3"><i class="fa fa-check"></i><b>4.4.1</b> Remarks</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#further-reading-3"><i class="fa fa-check"></i><b>4.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Shallow and Deep Neural Networks</a><ul>
<li class="chapter" data-level="5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-7"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>5.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="5.1.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#data-2"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>5.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="5.2.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#extending-logistic-regression"><i class="fa fa-check"></i><b>5.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#softmax-function"><i class="fa fa-check"></i><b>5.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="5.2.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>5.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="5.2.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>5.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="5.2.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>5.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="5.3.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>5.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="5.3.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>5.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="5.3.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r-3"><i class="fa fa-check"></i><b>5.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="5.4.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-8"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>5.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="5.4.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>5.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="5.4.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>5.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#preprocessing-of-data"><i class="fa fa-check"></i><b>5.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="5.5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-9"><i class="fa fa-check"></i><b>5.5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#image-deskewing"><i class="fa fa-check"></i><b>5.5.2</b> Image Deskewing</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#outro-4"><i class="fa fa-check"></i><b>5.6</b> Outro</a><ul>
<li class="chapter" data-level="5.6.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#remarks-4"><i class="fa fa-check"></i><b>5.6.1</b> Remarks</a></li>
<li class="chapter" data-level="5.6.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#beyond-mnist"><i class="fa fa-check"></i><b>5.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="5.6.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#further-reading-4"><i class="fa fa-check"></i><b>5.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>6</b> Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="6.1" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#introduction-10"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#optimisation-problem"><i class="fa fa-check"></i><b>6.1.1</b> Optimisation Problem</a></li>
<li class="chapter" data-level="6.1.2" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>6.1.2</b> Example Optimisation Problems in Machine Learning</a></li>
<li class="chapter" data-level="6.1.3" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>6.1.3</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="6.1.4" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>6.1.4</b> Example Objective over a 2D Domain</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#iterative-methods"><i class="fa fa-check"></i><b>6.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#introduction-11"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#example-in-r-4"><i class="fa fa-check"></i><b>6.2.2</b> Example in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>6.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="6.2.4" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#random-restarts"><i class="fa fa-check"></i><b>6.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.3.1" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#function-gradient"><i class="fa fa-check"></i><b>6.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="6.3.2" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>6.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="6.3.3" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>6.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="6.3.4" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#example-mnist"><i class="fa fa-check"></i><b>6.3.4</b> Example: MNIST</a></li>
<li class="chapter" data-level="6.3.5" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>6.3.5</b> Stochastic Gradient Descent (SGD)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#outro-5"><i class="fa fa-check"></i><b>6.4</b> Outro</a><ul>
<li class="chapter" data-level="6.4.1" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#remarks-5"><i class="fa fa-check"></i><b>6.4.1</b> Remarks</a></li>
<li class="chapter" data-level="6.4.2" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#optimisers-in-keras"><i class="fa fa-check"></i><b>6.4.2</b> Optimisers in Keras</a></li>
<li class="chapter" data-level="6.4.3" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#note-on-search-spaces"><i class="fa fa-check"></i><b>6.4.3</b> Note on Search Spaces</a></li>
<li class="chapter" data-level="6.4.4" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#further-reading-5"><i class="fa fa-check"></i><b>6.4.4</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a><ul>
<li class="chapter" data-level="7.1" data-path="clustering.html"><a href="clustering.html#unsupervised-learning"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="7.1.1" data-path="clustering.html"><a href="clustering.html#introduction-12"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="clustering.html"><a href="clustering.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>7.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="7.1.3" data-path="clustering.html"><a href="clustering.html#clustering-1"><i class="fa fa-check"></i><b>7.1.3</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>7.2</b> K-means Clustering</a><ul>
<li class="chapter" data-level="7.2.1" data-path="clustering.html"><a href="clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>7.2.1</b> Example in R</a></li>
<li class="chapter" data-level="7.2.2" data-path="clustering.html"><a href="clustering.html#problem-statement"><i class="fa fa-check"></i><b>7.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="7.2.3" data-path="clustering.html"><a href="clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>7.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="clustering.html"><a href="clustering.html#hierarchical-methods"><i class="fa fa-check"></i><b>7.3</b> Hierarchical Methods</a><ul>
<li class="chapter" data-level="7.3.1" data-path="clustering.html"><a href="clustering.html#introduction-13"><i class="fa fa-check"></i><b>7.3.1</b> Introduction</a></li>
<li class="chapter" data-level="7.3.2" data-path="clustering.html"><a href="clustering.html#example-in-r-6"><i class="fa fa-check"></i><b>7.3.2</b> Example in R</a></li>
<li class="chapter" data-level="7.3.3" data-path="clustering.html"><a href="clustering.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>7.3.3</b> Agglomerative Hierarchical Clustering</a></li>
<li class="chapter" data-level="7.3.4" data-path="clustering.html"><a href="clustering.html#linkage-functions"><i class="fa fa-check"></i><b>7.3.4</b> Linkage Functions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="clustering.html"><a href="clustering.html#outro-6"><i class="fa fa-check"></i><b>7.4</b> Outro</a><ul>
<li class="chapter" data-level="7.4.1" data-path="clustering.html"><a href="clustering.html#remarks-6"><i class="fa fa-check"></i><b>7.4.1</b> Remarks</a></li>
<li class="chapter" data-level="7.4.2" data-path="clustering.html"><a href="clustering.html#other-noteworthy-clustering-algorithms"><i class="fa fa-check"></i><b>7.4.2</b> Other Noteworthy Clustering Algorithms</a></li>
<li class="chapter" data-level="7.4.3" data-path="clustering.html"><a href="clustering.html#further-reading-6"><i class="fa fa-check"></i><b>7.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html"><i class="fa fa-check"></i><b>8</b> Optimisation with Genetic Algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-14"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#recap"><i class="fa fa-check"></i><b>8.1.1</b> Recap</a></li>
<li class="chapter" data-level="8.1.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#k-means-revisited"><i class="fa fa-check"></i><b>8.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="8.1.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>8.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>8.2</b> A Note on Convex Optimisation (*)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-15"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#convex-combinations"><i class="fa fa-check"></i><b>8.2.2</b> Convex Combinations (*)</a></li>
<li class="chapter" data-level="8.2.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#convex-functions"><i class="fa fa-check"></i><b>8.2.3</b> Convex Functions (*)</a></li>
<li class="chapter" data-level="8.2.4" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#examples"><i class="fa fa-check"></i><b>8.2.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#genetic-algorithms"><i class="fa fa-check"></i><b>8.3</b> Genetic Algorithms</a><ul>
<li class="chapter" data-level="8.3.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-16"><i class="fa fa-check"></i><b>8.3.1</b> Introduction</a></li>
<li class="chapter" data-level="8.3.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#overview-of-the-method"><i class="fa fa-check"></i><b>8.3.2</b> Overview of the Method</a></li>
<li class="chapter" data-level="8.3.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#example-implementation---ga-for-k-means"><i class="fa fa-check"></i><b>8.3.3</b> Example Implementation - GA for K-means</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#outro-7"><i class="fa fa-check"></i><b>8.4</b> Outro</a><ul>
<li class="chapter" data-level="8.4.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#remarks-7"><i class="fa fa-check"></i><b>8.4.1</b> Remarks</a></li>
<li class="chapter" data-level="8.4.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#further-reading-7"><i class="fa fa-check"></i><b>8.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>9</b> Recommender Systems</a><ul>
<li class="chapter" data-level="9.1" data-path="recommender-systems.html"><a href="recommender-systems.html#introduction-17"><i class="fa fa-check"></i><b>9.1</b> Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="recommender-systems.html"><a href="recommender-systems.html#what-is-a-recommender-system"><i class="fa fa-check"></i><b>9.1.1</b> What is a Recommender System?</a></li>
<li class="chapter" data-level="9.1.2" data-path="recommender-systems.html"><a href="recommender-systems.html#the-netflix-prize"><i class="fa fa-check"></i><b>9.1.2</b> The Netflix Prize</a></li>
<li class="chapter" data-level="9.1.3" data-path="recommender-systems.html"><a href="recommender-systems.html#main-approaches"><i class="fa fa-check"></i><b>9.1.3</b> Main Approaches</a></li>
<li class="chapter" data-level="9.1.4" data-path="recommender-systems.html"><a href="recommender-systems.html#formalism-2"><i class="fa fa-check"></i><b>9.1.4</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="recommender-systems.html"><a href="recommender-systems.html#collaborative-filtering"><i class="fa fa-check"></i><b>9.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="9.2.1" data-path="recommender-systems.html"><a href="recommender-systems.html#example"><i class="fa fa-check"></i><b>9.2.1</b> Example</a></li>
<li class="chapter" data-level="9.2.2" data-path="recommender-systems.html"><a href="recommender-systems.html#similarity-measures"><i class="fa fa-check"></i><b>9.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="9.2.3" data-path="recommender-systems.html"><a href="recommender-systems.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="9.2.4" data-path="recommender-systems.html"><a href="recommender-systems.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="recommender-systems.html"><a href="recommender-systems.html#movielens-dataset"><i class="fa fa-check"></i><b>9.3</b> MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="9.3.1" data-path="recommender-systems.html"><a href="recommender-systems.html#dataset"><i class="fa fa-check"></i><b>9.3.1</b> Dataset</a></li>
<li class="chapter" data-level="9.3.2" data-path="recommender-systems.html"><a href="recommender-systems.html#data-cleansing"><i class="fa fa-check"></i><b>9.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="9.3.3" data-path="recommender-systems.html"><a href="recommender-systems.html#item-item-similarities"><i class="fa fa-check"></i><b>9.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="9.3.4" data-path="recommender-systems.html"><a href="recommender-systems.html#example-recommendations"><i class="fa fa-check"></i><b>9.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="9.3.5" data-path="recommender-systems.html"><a href="recommender-systems.html#clustering-2"><i class="fa fa-check"></i><b>9.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="recommender-systems.html"><a href="recommender-systems.html#outro-8"><i class="fa fa-check"></i><b>9.4</b> Outro</a><ul>
<li class="chapter" data-level="9.4.1" data-path="recommender-systems.html"><a href="recommender-systems.html#remarks-8"><i class="fa fa-check"></i><b>9.4.1</b> Remarks</a></li>
<li class="chapter" data-level="9.4.2" data-path="recommender-systems.html"><a href="recommender-systems.html#issues"><i class="fa fa-check"></i><b>9.4.2</b> Issues</a></li>
<li class="chapter" data-level="9.4.3" data-path="recommender-systems.html"><a href="recommender-systems.html#further-reading-8"><i class="fa fa-check"></i><b>9.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="section-6.html"><a href="section-6.html"><i class="fa fa-check"></i>}</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html"><i class="fa fa-check"></i><b>A</b> Setting Up the R Environment</a><ul>
<li class="chapter" data-level="A.1" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-r"><i class="fa fa-check"></i><b>A.1</b> Installing R</a></li>
<li class="chapter" data-level="A.2" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-an-ide"><i class="fa fa-check"></i><b>A.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="A.3" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-recommended-packages"><i class="fa fa-check"></i><b>A.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="A.4" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>A.4</b> First R Script in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html"><i class="fa fa-check"></i><b>B</b> Vector Algebra in R</a><ul>
<li class="chapter" data-level="B.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#motivation-1"><i class="fa fa-check"></i><b>B.1</b> Motivation</a></li>
<li class="chapter" data-level="B.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#numeric-vectors"><i class="fa fa-check"></i><b>B.2</b> Numeric Vectors</a><ul>
<li class="chapter" data-level="B.2.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>B.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="B.2.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-scalar-operations"><i class="fa fa-check"></i><b>B.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="B.2.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-vector-operations"><i class="fa fa-check"></i><b>B.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="B.2.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions"><i class="fa fa-check"></i><b>B.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="B.2.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#special-functions"><i class="fa fa-check"></i><b>B.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="B.2.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#norms-and-distances"><i class="fa fa-check"></i><b>B.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="B.2.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#dot-product"><i class="fa fa-check"></i><b>B.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="B.2.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>B.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-vectors"><i class="fa fa-check"></i><b>B.3</b> Logical Vectors</a><ul>
<li class="chapter" data-level="B.3.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-logical-vectors"><i class="fa fa-check"></i><b>B.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="B.3.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-operations"><i class="fa fa-check"></i><b>B.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="B.3.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#comparison-operations"><i class="fa fa-check"></i><b>B.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="B.3.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions-1"><i class="fa fa-check"></i><b>B.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#character-vectors"><i class="fa fa-check"></i><b>B.4</b> Character Vectors</a><ul>
<li class="chapter" data-level="B.4.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-character-vectors"><i class="fa fa-check"></i><b>B.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="B.4.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>B.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="B.4.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>B.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-subsetting"><i class="fa fa-check"></i><b>B.5</b> Vector Subsetting</a><ul>
<li class="chapter" data-level="B.5.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>B.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="B.5.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>B.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="B.5.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>B.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="B.5.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#replacing-elements"><i class="fa fa-check"></i><b>B.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="B.5.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#other-functions"><i class="fa fa-check"></i><b>B.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="B.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-vectors"><i class="fa fa-check"></i><b>B.6</b> Named Vectors</a><ul>
<li class="chapter" data-level="B.6.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-named-vectors"><i class="fa fa-check"></i><b>B.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="B.6.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>B.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="B.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#factors"><i class="fa fa-check"></i><b>B.7</b> Factors</a><ul>
<li class="chapter" data-level="B.7.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-factors"><i class="fa fa-check"></i><b>B.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="B.7.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#levels"><i class="fa fa-check"></i><b>B.7.2</b> Levels</a></li>
<li class="chapter" data-level="B.7.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#internal-representation"><i class="fa fa-check"></i><b>B.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="B.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#lists"><i class="fa fa-check"></i><b>B.8</b> Lists</a><ul>
<li class="chapter" data-level="B.8.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-lists"><i class="fa fa-check"></i><b>B.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="B.8.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-lists"><i class="fa fa-check"></i><b>B.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="B.8.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>B.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="B.8.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#common-operations"><i class="fa fa-check"></i><b>B.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="B.9" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#further-reading-9"><i class="fa fa-check"></i><b>B.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html"><i class="fa fa-check"></i><b>C</b> Matrix Algebra in R</a><ul>
<li class="chapter" data-level="C.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#creating-matrices"><i class="fa fa-check"></i><b>C.1</b> Creating Matrices</a><ul>
<li class="chapter" data-level="C.1.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix"><i class="fa fa-check"></i><b>C.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="C.1.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#stacking-vectors"><i class="fa fa-check"></i><b>C.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="C.1.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>C.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="C.1.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>C.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="C.1.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#other-methods"><i class="fa fa-check"></i><b>C.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="C.1.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#internal-representation-1"><i class="fa fa-check"></i><b>C.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#common-operations-1"><i class="fa fa-check"></i><b>C.2</b> Common Operations</a><ul>
<li class="chapter" data-level="C.2.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-transpose"><i class="fa fa-check"></i><b>C.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="C.2.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>C.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-multiplication"><i class="fa fa-check"></i><b>C.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="C.2.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>C.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="C.2.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#vectorised-special-functions"><i class="fa fa-check"></i><b>C.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="C.2.7" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-vector-operations"><i class="fa fa-check"></i><b>C.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-subsetting"><i class="fa fa-check"></i><b>C.3</b> Matrix Subsetting</a><ul>
<li class="chapter" data-level="C.3.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-individual-elements"><i class="fa fa-check"></i><b>C.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="C.3.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>C.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="C.3.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-submatrices"><i class="fa fa-check"></i><b>C.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="C.3.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>C.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="C.3.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>C.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#further-reading-10"><i class="fa fa-check"></i><b>C.4</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html"><i class="fa fa-check"></i><b>D</b> Data Frame Wrangling in R</a><ul>
<li class="chapter" data-level="D.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#creating-data-frames"><i class="fa fa-check"></i><b>D.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="D.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#importing-data-frames"><i class="fa fa-check"></i><b>D.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="D.3" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#data-frame-subsetting"><i class="fa fa-check"></i><b>D.3</b> Data Frame Subsetting</a><ul>
<li class="chapter" data-level="D.3.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>D.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="D.3.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>D.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#common-operations-2"><i class="fa fa-check"></i><b>D.4</b> Common Operations</a></li>
<li class="chapter" data-level="D.5" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>D.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="D.6" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#further-reading-11"><i class="fa fa-check"></i><b>D.6</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.2 2020-04-17 17:32 (970e421)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering" class="section level1">
<h1><span class="header-section-number">7</span> Clustering</h1>
<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->
<div id="unsupervised-learning" class="section level2">
<h2><span class="header-section-number">7.1</span> Unsupervised Learning</h2>
<div id="introduction-12" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Introduction</h3>
<div style="margin-top: 1em">

</div>
<p>In <strong>unsupervised learning</strong> (learning without a teacher),
the input data points <span class="math inline">\(\mathbf{x}_{1,\cdot},\dots,\mathbf{x}_{n,\cdot}\)</span>
are not assigned any reference labels.</p>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-1-1.svg" alt="Figure 1: plot of chunk unnamed-chunk-1" id="fig:unnamed-chunk-1" />
<p class="caption">Figure 1: plot of chunk unnamed-chunk-1</p>
</div>
<p>Our aim now is to discover the <strong>underlying structure in the data</strong>.</p>
</div>
<div id="main-types-of-unsupervised-learning-problems" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Main Types of Unsupervised Learning Problems</h3>
<div style="margin-top: 1em">

</div>
<p>In <strong>dimensionality reduction</strong> we seek a meaningful <em>projection</em> of a high dimensional
space (think: many variables/columns).</p>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-2-1.svg" alt="Figure 2: plot of chunk unnamed-chunk-2" id="fig:unnamed-chunk-2" />
<p class="caption">Figure 2: plot of chunk unnamed-chunk-2</p>
</div>
<p>This might enable us to plot high-dimensional data or understand its structure better.</p>
<div style="margin-top: 1em">

</div>
<p>Example methods:</p>
<ul>
<li>Multidimensional scaling (MDS)</li>
<li>Principal component analysis (PCA)</li>
<li>Kernel PCA</li>
<li>t-SNE</li>
<li>Autoencoders (deep learning)</li>
</ul>
<div style="margin-top: 1em">

</div>
<p>In <strong>anomaly detection</strong>, out task is to identify rare, suspicious, ab-normal
or out-standing items.</p>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-3-1.svg" alt="Figure 3: plot of chunk unnamed-chunk-3" id="fig:unnamed-chunk-3" />
<p class="caption">Figure 3: plot of chunk unnamed-chunk-3</p>
</div>
<p>For example, these can be cars on walkways in a park’s security camera footage.</p>
<div style="margin-top: 1em">

</div>
<p>The aim of <strong>clustering</strong> is to automatically discover some <em>naturally occurring</em>
subgroups in the data set.</p>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-4-1.svg" alt="Figure 4: plot of chunk unnamed-chunk-4" id="fig:unnamed-chunk-4" />
<p class="caption">Figure 4: plot of chunk unnamed-chunk-4</p>
</div>
<p>For example, these may be customers having different shopping patterns
(such as “young parents”, “students”, “boomers”).</p>
</div>
<div id="clustering-1" class="section level3">
<h3><span class="header-section-number">7.1.3</span> Clustering</h3>
<div style="margin-top: 1em">

</div>
<p>More formally, given <span class="math inline">\(K\ge 2\)</span>, <strong>clustering</strong> aims is to find a <em>special kind</em>
of a <strong><span class="math inline">\(K\)</span>-partition</strong> of the input data set <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p><span class="math inline">\(\mathcal{C}=\{C_1,\dots,C_K\}\)</span> is a <span class="math inline">\(K\)</span>-partition of <span class="math inline">\(\mathbf{X}\)</span> of size <span class="math inline">\(n\)</span>,
whenever:</p>
<ul>
<li><span class="math inline">\(C_k\neq\emptyset\)</span> for all <span class="math inline">\(k\)</span> (each set is nonempty),</li>
<li><span class="math inline">\(C_k\cap C_l=\emptyset\)</span> for all <span class="math inline">\(k\neq l\)</span> (sets are pairwise disjoint),</li>
<li><span class="math inline">\(\bigcup_{k=1}^K C_k=\{1,\dots,n\}\)</span> (no point is neglected).</li>
</ul>
<p>This can also be thought of as assigning each point a unique label <span class="math inline">\(\{1,\dots,K\}\)</span>
(think: colouring of the points, where each number has a colour).</p>
<blockquote>
<p>“<span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> is labelled <span class="math inline">\(j\)</span> iff it belongs to cluster <span class="math inline">\(C_j\)</span>,
i.e., <span class="math inline">\(i\in C_j\)</span>”.</p>
</blockquote>
<div style="margin-top: 1em">

</div>
<p>Example applications of clustering:</p>
<ul>
<li><em>taxonomization</em>: e.g.,
partition the consumers to more “uniform”
groups to better understand who they are and what do they need,</li>
<li><em>image processing</em>:
e.g., object detection, like tumour tissues on medical images,</li>
<li><em>complex networks analysis</em>:
e.g., detecting communities in friendship,
retweets and other networks,</li>
<li><em>fine-tuning supervised learning algorithms</em>:
e.g., recommender systems indicating content
that was rated highly by users from the same group
or learning multiple manifolds in a dimension reduction task.</li>
</ul>
<div style="margin-top: 1em">

</div>
<p>The number of possible <span class="math inline">\(K\)</span>-partitions of a set with <span class="math inline">\(n\)</span> elements is given by
<em>the Stirling number of the second kind</em>:</p>
<p><span class="math display">\[
\left\{{n \atop K}\right\}={\frac  {1}{K!}}\sum _{{j=0}}^{{K}}(-1)^{{K-j}}{\binom  {K}{j}}j^{n};
\]</span></p>
<p>e.g., already <span class="math inline">\(\left\{{n \atop 2}\right\}=2^{n-1}-1\)</span>
and <span class="math inline">\(\left\{{n \atop 3}\right\}=O(3^n)\)</span> – that is a lot.</p>
<p>Certainly, we are not just interested in “any” partition.</p>
<p>However, even one of the most famous textbooks provides us with only a vague hint:</p>
<blockquote>
<p>Clustering concerns “segmenting a collection of objects into subsets
so that those within each cluster are more <strong>closely related</strong>
to one another than objects assigned to different clusters” <span class="citation">(Hastie et al. <a href="references.html#ref-esl">2017</a>)</span>.</p>
</blockquote>
<div style="margin-top: 1em">

</div>
<p>There are two main types of clustering algorithms:</p>
<ul>
<li><strong>parametric</strong> (model-based):
<ul>
<li>find clusters of specific shapes, or following specific multidimensional
probability distributions,</li>
<li>e.g., <span class="math inline">\(K\)</span>-means, expectation-maximization for Gaussian mixtures (EM),
average linkage agglomerative clustering;</li>
</ul></li>
<li><strong>nonparametric</strong> (model-free):
<ul>
<li>identify high-density or well-separable regions,
perhaps in the presence of noise points,</li>
<li>e.g., single linkage agglomerative clustering, Genie, (H)DBSCAN, BIRCH.</li>
</ul></li>
</ul>
<div style="margin-top: 1em">

</div>
<p>In this chapter we’ll take a look at two clustering approaches:</p>
<ul>
<li><em>K-means clustering</em> that looks for a specific number of clusters</li>
<li><em>(agglomerative) hierarchical clustering</em> that outputs a whole hierarchy
of nested data partitions</li>
</ul>
</div>
</div>
<div id="k-means-clustering" class="section level2">
<h2><span class="header-section-number">7.2</span> K-means Clustering</h2>
<div id="example-in-r-5" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Example in R</h3>
<div style="margin-top: 1em">

</div>
<p>Let us apply <span class="math inline">\(K\)</span>-means clustering to find <span class="math inline">\(K=3\)</span> groups
in the famous Fisher’s <code>iris</code> data set (variables <code>Sepal.Width</code>
and <code>Petal.Length</code> variables only)</p>
<div class="sourceCode" id="cb555"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb555-1" title="1">X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(iris[,<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">2</span>)])</a>
<a class="sourceLine" id="cb555-2" title="2"><span class="co"># never forget to set nstart&gt;&gt;1!</span></a>
<a class="sourceLine" id="cb555-3" title="3">km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, <span class="dt">centers=</span><span class="dv">3</span>, <span class="dt">nstart=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb555-4" title="4">km<span class="op">$</span>cluster <span class="co"># assigned labels</span></a></code></pre></div>
<pre><code>##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [30] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1
##  [59] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1
##  [88] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2
## [117] 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2
## [146] 2 1 2 2 2</code></pre>
<blockquote>
<p>Later we’ll see that <code>nstart</code> is responsible for random restarting the
(local) optimisation procedure, just like we did in the previous chapter.</p>
</blockquote>
<div style="margin-top: 1em">

</div>
<div class="sourceCode" id="cb557"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb557-1" title="1"><span class="kw">plot</span>(X, <span class="dt">col=</span>km<span class="op">$</span>cluster, <span class="dt">pch=</span><span class="kw">as.numeric</span>(iris<span class="op">$</span>Species), <span class="dt">las=</span><span class="dv">1</span>)</a></code></pre></div>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-6-1.svg" alt="Figure 5: plot of chunk unnamed-chunk-6" id="fig:unnamed-chunk-6" />
<p class="caption">Figure 5: plot of chunk unnamed-chunk-6</p>
</div>
<p>The colours indicate the detected clusters,</p>
<p>while the plotting characters – the true iris species</p>
<blockquote>
<p>Note that they were not used during the clustering procedure!<br />
(we’re dealing with unsupervised learning)</p>
</blockquote>
<div style="margin-top: 1em">

</div>
<p>A contingency table for detected vs. true clusters:</p>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb558-1" title="1">(C &lt;-<span class="st"> </span><span class="kw">table</span>(km<span class="op">$</span>cluster, iris<span class="op">$</span>Species))</a></code></pre></div>
<pre><code>##    
##     setosa versicolor virginica
##   1      0         48         9
##   2      0          2        41
##   3     50          0         0</code></pre>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb560-1" title="1"><span class="kw">sum</span>(<span class="kw">apply</span>(C, <span class="dv">1</span>, max))<span class="op">/</span><span class="kw">sum</span>(C) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.9266667</code></pre>
<p>The discovered partition matches the original one very well.</p>
</div>
<div id="problem-statement" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Problem Statement</h3>
<div style="margin-top: 1em">

</div>
<p>The aim of <span class="math inline">\(K\)</span>-means clustering is to find <span class="math inline">\(K\)</span> “good” cluster centres
<span class="math inline">\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)</span>.</p>
<p>Then, a point <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> will be assigned to the
cluster represented by the closest centre.</p>
<p>Closest == w.r.t. the squared Euclidean distance.</p>
<p>Assuming all the points are in a <span class="math inline">\(p\)</span>-dimensional space, <span class="math inline">\(\mathbb{R}^p\)</span>,
<span class="math display">\[
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}) = \| \mathbf{x}_{i,\cdot} - \boldsymbol\mu_{k,\cdot} \|^2 =  \sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\]</span></p>
<div style="margin-top: 1em">

</div>
<p>The <span class="math inline">\(i\)</span>-th point’s cluster is determined by:
<span class="math display">\[
\mathrm{C}(i) = \mathrm{arg}\min_{k=1,\dots,K}
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}),
\]</span>
where <span class="math inline">\(\mathrm{arg}\min\)</span> == argument minimum == the index <span class="math inline">\(k\)</span> that minimises
the given expression.</p>
<div style="margin-top: 1em">

</div>
<p>In the previous example, we have:</p>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb562-1" title="1">km<span class="op">$</span>centers</a></code></pre></div>
<pre><code>##   Petal.Length Sepal.Width
## 1     4.328070    2.750877
## 2     5.672093    3.032558
## 3     1.462000    3.428000</code></pre>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb564-1" title="1"><span class="kw">plot</span>(X, <span class="dt">col=</span>km<span class="op">$</span>cluster, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">asp=</span><span class="dv">1</span>) <span class="co"># asp=1 gives the same scale on both axes</span></a>
<a class="sourceLine" id="cb564-2" title="2"><span class="kw">points</span>(km<span class="op">$</span>centers, <span class="dt">cex=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="dv">4</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a></code></pre></div>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-8-1.svg" alt="Figure 6: plot of chunk unnamed-chunk-8" id="fig:unnamed-chunk-8" />
<p class="caption">Figure 6: plot of chunk unnamed-chunk-8</p>
</div>
<div style="margin-top: 1em">

</div>
<p>Here is the partition of the whole <span class="math inline">\(\mathbb{R}^2\)</span> space
into clusters based on the closeness to the three cluster centres:</p>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-9-1.svg" alt="Figure 7: plot of chunk unnamed-chunk-9" id="fig:unnamed-chunk-9" />
<p class="caption">Figure 7: plot of chunk unnamed-chunk-9</p>
</div>
<blockquote>
<p>(*) For the interested, see “Voronoi diagrams”.</p>
</blockquote>
<div style="margin-top: 1em">

</div>
<p>To compute the pairwise distances, we may call <code>pdist::pdist()</code>:</p>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb565-1" title="1"><span class="kw">library</span>(<span class="st">&quot;pdist&quot;</span>)</a>
<a class="sourceLine" id="cb565-2" title="2">D &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">pdist</span>(X, km<span class="op">$</span>centers))</a>
<a class="sourceLine" id="cb565-3" title="3"><span class="kw">head</span>(D) <span class="co"># D[i,j] - distance between x[i,] and µ[j,]</span></a></code></pre></div>
<pre><code>##          [,1]     [,2]       [,3]
## [1,] 3.022380 4.297590 0.09501583
## [2,] 2.938649 4.272217 0.43246734
## [3,] 3.061196 4.375298 0.27969268
## [4,] 2.849538 4.172638 0.33019394
## [5,] 3.048705 4.309613 0.18283319
## [6,] 2.868316 4.065708 0.52860963</code></pre>
<p>…</p>
<div style="margin-top: 1em">

</div>
<p>Therefore, the cluster memberships (<span class="math inline">\(\mathrm{arg}\min\)</span>s) can be determined by:</p>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb567-1" title="1">(idx &lt;-<span class="st"> </span><span class="kw">apply</span>(D, <span class="dv">1</span>, which.min))</a></code></pre></div>
<pre><code>##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [30] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1
##  [59] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1
##  [88] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2
## [117] 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2
## [146] 2 1 2 2 2</code></pre>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb569-1" title="1"><span class="kw">all</span>(km<span class="op">$</span>cluster <span class="op">==</span><span class="st"> </span>idx) <span class="co"># sanity check</span></a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="algorithms-for-the-k-means-problem" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Algorithms for the K-means Problem</h3>
<div style="margin-top: 1em">

</div>
<p>How to find “good” cluster centres?</p>
<p>In the <span class="math inline">\(K\)</span>-means clustering, we wish to find <span class="math inline">\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)</span>
that minimise the total within-cluster distances (distances from each point
to each own cluster centre):
<span class="math display">\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{C(i),\cdot}),
\]</span>
Note that the <span class="math inline">\(\boldsymbol\mu\)</span>s are also “hidden” inside the point-to-cluster
belongingness mapping, <span class="math inline">\(\mathrm{C}\)</span>.</p>
<p>Expanding the above yields:
<span class="math display">\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{k=1,\dots,K}
\sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\right).
\]</span>
Unfortunately, the <span class="math inline">\(\min\)</span> operator in the objective function
makes this optimisation problem not tractable
with the methods discussed in the previous chapter.</p>
<div style="margin-top: 1em">

</div>
<p>The above problem is <em>hard</em> to solve.</p>
<blockquote>
<p>(*) More precisely, it is an NP-hard problem.</p>
</blockquote>
<p>Therefore, in practice we use various heuristics to solve it.</p>
<p><code>kmeans()</code> in R implements the Hartigan-Wong, Lloyd, Forgy and MacQueen
algorithms.</p>
<blockquote>
<p>(*) Technically, there is no such thing as “the K-means algorithm” –
all the above are particular heuristic approaches to solving the K-means
clustering problem as specified by the aforementioned optimisation task.</p>
<p>By setting <code>nstart = 10</code> above, we ask the (Hartigan-Wong) algorithm
to find 10 solution candidates obtained by considering different random
initial clusterings and choose the best one (with respect to the sum
of within-cluster distance) amongst them. This does not guarantee
finding the optimal solution, especially in high-dimensional spaces,
but increases the likelihood of such.</p>
</blockquote>
<div style="margin-top: 1em">

</div>
<p>Lloyd’s algorithm (1957) is sometimes referred to as “the” K-means algorithm:</p>
<ol style="list-style-type: decimal">
<li><p>Start with random cluster centres <span class="math inline">\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)</span>.</p></li>
<li><p>For each point <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span>, determine its closest centre <span class="math inline">\(C(i)\in\{1,\dots,K\}\)</span>.</p></li>
<li><p>For each cluster <span class="math inline">\(k\in\{1,\dots,K\}\)</span>, compute the new cluster centre <span class="math inline">\(\boldsymbol\mu_{k,\cdot}\)</span> as the componentwise arithmetic mean
of the coordinates of all the point indices <span class="math inline">\(i\)</span> such that <span class="math inline">\(C(i)=k\)</span>.</p></li>
<li><p>If the cluster centres changed since last iteration, go to step 2, otherwise stop and return the result.</p></li>
</ol>
<div style="margin-top: 1em">

</div>
<p>(*) Example implementation:</p>
<div class="sourceCode" id="cb571"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb571-1" title="1">K &lt;-<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb571-2" title="2"></a>
<a class="sourceLine" id="cb571-3" title="3"><span class="co"># Random initial cluster centres</span></a>
<a class="sourceLine" id="cb571-4" title="4">M &lt;-<span class="st"> </span><span class="kw">jitter</span>(X[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(X), K),])</a>
<a class="sourceLine" id="cb571-5" title="5">M</a></code></pre></div>
<pre><code>##      Petal.Length Sepal.Width
## [1,]     1.106172    2.994400
## [2,]     1.394794    3.321308
## [3,]     6.754820    3.808716</code></pre>
<div class="sourceCode" id="cb573"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb573-1" title="1"><span class="co"># Let D[i,k] bet the distance between the i-th point and the k-th centre:</span></a>
<a class="sourceLine" id="cb573-2" title="2">D &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">pdist</span>(X, M))</a>
<a class="sourceLine" id="cb573-3" title="3"><span class="co"># Let idx[i] be the centre closest to the i-th point</span></a>
<a class="sourceLine" id="cb573-4" title="4">idx &lt;-<span class="st"> </span><span class="kw">apply</span>(D, <span class="dv">1</span>, which.min)</a></code></pre></div>
<div style="margin-top: 1em">

</div>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb574-1" title="1"><span class="cf">repeat</span> {</a>
<a class="sourceLine" id="cb574-2" title="2">    <span class="co"># Previous cluster belongingness:</span></a>
<a class="sourceLine" id="cb574-3" title="3">    old_idx &lt;-<span class="st"> </span>idx</a>
<a class="sourceLine" id="cb574-4" title="4">    <span class="co"># Split X into a list of K data frames based on old_idx info</span></a>
<a class="sourceLine" id="cb574-5" title="5">    X_split &lt;-<span class="st"> </span><span class="kw">split</span>(<span class="kw">as.data.frame</span>(X), old_idx)</a>
<a class="sourceLine" id="cb574-6" title="6">    <span class="co"># Compute componentwise arithmetic means of each data frame - new centres</span></a>
<a class="sourceLine" id="cb574-7" title="7">    M &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">sapply</span>(X_split, colMeans))</a>
<a class="sourceLine" id="cb574-8" title="8">    <span class="co"># Recompute cluster belongingness</span></a>
<a class="sourceLine" id="cb574-9" title="9">    D &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">pdist</span>(X, M))</a>
<a class="sourceLine" id="cb574-10" title="10">    idx &lt;-<span class="st"> </span><span class="kw">apply</span>(D, <span class="dv">1</span>, which.min)</a>
<a class="sourceLine" id="cb574-11" title="11">    <span class="co"># Check if converged already:</span></a>
<a class="sourceLine" id="cb574-12" title="12">    <span class="cf">if</span> (<span class="kw">all</span>(idx <span class="op">==</span><span class="st"> </span>old_idx)) <span class="cf">break</span></a>
<a class="sourceLine" id="cb574-13" title="13">}</a></code></pre></div>
<div style="margin-top: 1em">

</div>
<div class="sourceCode" id="cb575"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb575-1" title="1">M <span class="co"># our result</span></a></code></pre></div>
<pre><code>##   Petal.Length Sepal.Width
## 1     1.462000    3.428000
## 2     4.328070    2.750877
## 3     5.672093    3.032558</code></pre>
<div class="sourceCode" id="cb577"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb577-1" title="1">km<span class="op">$</span>center <span class="co"># result of kmeans()</span></a></code></pre></div>
<pre><code>##   Petal.Length Sepal.Width
## 1     4.328070    2.750877
## 2     5.672093    3.032558
## 3     1.462000    3.428000</code></pre>
<!--

# TODO: k-medoids???

## TODO

<div style="margin-top: 1em"></div>

TODO maybe k-medoids, nice for optimisation
plus they allow for different metrics

alternatively, DBSCAN or PCA or MDS or ...



-->
</div>
</div>
<div id="hierarchical-methods" class="section level2">
<h2><span class="header-section-number">7.3</span> Hierarchical Methods</h2>
<div id="introduction-13" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Introduction</h3>
<div style="margin-top: 1em">

</div>
<p>In K-means, we need to specify the number of clusters, <span class="math inline">\(K\)</span>, in advance.</p>
<p>What if we don’t know it?</p>
<p>There is no guarantee that a <span class="math inline">\((K+1)\)</span>-partition is “similar” to the <span class="math inline">\(K\)</span>-one.</p>
<p>Hierarchical methods, on the other hand, output a whole hierarchy
of mutually <em>nested</em> partitions, which increase the interpretability of the results.</p>
<p>A <span class="math inline">\(K\)</span>-partition for any <span class="math inline">\(K\)</span> can be extracted later.</p>
<div style="margin-top: 1em">

</div>
<p>Here we are interested in <strong>agglomerative</strong> algorithms.</p>
<p>At the lowest level of the hierarchy, each point belongs to its own
cluster (there are <span class="math inline">\(n\)</span> singletons).</p>
<p>At the highest level of the hierarchy, there is one cluster containing all the points.</p>
<p>Moving from the <span class="math inline">\(i\)</span>-th to the <span class="math inline">\((i+1)\)</span>-th level,
we select a pair of clusters to be merged.</p>
</div>
<div id="example-in-r-6" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Example in R</h3>
<div style="margin-top: 1em">

</div>
<div class="sourceCode" id="cb579"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb579-1" title="1"><span class="co"># Distances between all pairs of points:</span></a>
<a class="sourceLine" id="cb579-2" title="2">D &lt;-<span class="st"> </span><span class="kw">dist</span>(X)</a>
<a class="sourceLine" id="cb579-3" title="3"><span class="co"># Apply Complete Linkage (the default, see below):</span></a>
<a class="sourceLine" id="cb579-4" title="4">h &lt;-<span class="st"> </span><span class="kw">hclust</span>(D) <span class="co"># method=&quot;complete&quot;</span></a>
<a class="sourceLine" id="cb579-5" title="5"><span class="kw">print</span>(h)</a></code></pre></div>
<pre><code>## 
## Call:
## hclust(d = D)
## 
## Cluster method   : complete 
## Distance         : euclidean 
## Number of objects: 150</code></pre>
<div style="margin-top: 1em">

</div>
<div class="sourceCode" id="cb581"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb581-1" title="1"><span class="kw">cutree</span>(h, <span class="dt">k=</span><span class="dv">3</span>) <span class="co"># extract the 3-partition</span></a></code></pre></div>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [30] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2
##  [59] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
##  [88] 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 2 3 3 2 2 2
## [117] 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 2 2 3 3
## [146] 2 2 2 3 2</code></pre>
<div style="margin-top: 1em">

</div>
<p>Different cuts of the hierarchy:</p>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-18-1.svg" alt="Figure 8: plot of chunk unnamed-chunk-18" id="fig:unnamed-chunk-18" />
<p class="caption">Figure 8: plot of chunk unnamed-chunk-18</p>
</div>
<div style="margin-top: 1em">

</div>
<p>A <strong>dendrogram</strong> depicts the distances (* as defined by the linkage function)
between the clusters merged at every stage. This can provide us with the insight
into the underlying data structure.</p>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb583-1" title="1"><span class="kw">plot</span>(h)</a></code></pre></div>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-19-1.svg" alt="Figure 9: plot of chunk unnamed-chunk-19" id="fig:unnamed-chunk-19" />
<p class="caption">Figure 9: plot of chunk unnamed-chunk-19</p>
</div>
</div>
<div id="agglomerative-hierarchical-clustering" class="section level3">
<h3><span class="header-section-number">7.3.3</span> Agglomerative Hierarchical Clustering</h3>
<div style="margin-top: 1em">

</div>
<p>Initially, <span class="math inline">\(\mathcal{C}^{(0)}=\{\{1\},\dots,\{n\}\}\)</span>,
i.e., each point is a member of its own cluster.</p>
<p>While a <strong>hierarchical agglomerative clustering</strong> algorithm is being computed,
there are <span class="math inline">\(n-k\)</span> clusters at the <span class="math inline">\(k\)</span>-th step of the procedure,
<span class="math inline">\(\mathcal{C}^{(k)}=\{C_1^{(k)},\dots,C_{n-k}^{(k)}\}\)</span>.</p>
<p>When proceeding from step <span class="math inline">\(k\)</span> to <span class="math inline">\(k+1\)</span>, we determine <span class="math inline">\(C_u^{(k)}\)</span> and <span class="math inline">\(C_v^{(k)}\)</span>, <span class="math inline">\(u&lt;v\)</span>,
to be <strong>merged</strong> together so that we get:
<span class="math display">\[
\mathcal{C}^{(k+1)} = \left\{
C_1^{(k)},\dots,C_{u-1}^{(k)},
C_u^{(k)}{\cup C_v^{(k)}},
C_{u+1}^{(k)},\dots,C_{v-1}^{(k)},
C_{v+1}^{(k)},\dots,C_{n-k}^{(k)}
\right\}.
\]</span></p>
<p>Thus, <span class="math inline">\((\mathcal{C}^{(0)}, \mathcal{C}^{(1)}, \dots, \mathcal{C}^{(n-1)})\)</span> is a sequence
of <strong>nested</strong> partitions of <span class="math inline">\(\{1,2,\dots,n\}\)</span>
with <span class="math inline">\(\mathcal{C}^{(n-1)}=\left\{ \{1,2,\dots,n\} \right\}\)</span>.</p>
</div>
<div id="linkage-functions" class="section level3">
<h3><span class="header-section-number">7.3.4</span> Linkage Functions</h3>
<div style="margin-top: 1em">

</div>
<p>A pair of clusters <span class="math inline">\(C_u^{(k)}\)</span> and <span class="math inline">\(C_v^{(k)}\)</span> to be merged with each other
is determined by:
<span class="math display">\[
\mathrm{arg}\min_{u &lt; v} d^*(C_u^{(k)}, C_v^{(k)}),
\]</span>
where <span class="math inline">\(d^*(A,B)\)</span> is the <em>distance</em> between two clusters <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<p>Note that we usually only consider the distances between single points,
not sets of points.</p>
<p><span class="math inline">\(d^*\)</span> is a suitable extension of a pointwise distance <span class="math inline">\(d\)</span> (usually the Euclidean metric)
to whole sets.</p>
<p>We will assume that <span class="math inline">\(d^*(\{\mathbf{x}_{i,\cdot}\},\{\mathbf{x}_{j,\cdot}\})= d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot})\)</span>, i.e., the distance between
singleton clusters is the same as the distance between the points themselves.</p>
<div style="margin-top: 1em">

</div>
<p>There are many popular choices of <span class="math inline">\(d^*\)</span> (which in the context of
hierarchical clustering we call a <strong>linkage function</strong>)</p>
<ul>
<li>Single linkage:
<span class="math display">\[
d_S^*(A,B) = \min_{\mathbf{a}\in A, \mathbf{b}\in B} d(\mathbf{a},\mathbf{b})
\]</span></li>
<li>Complete linkage:
<span class="math display">\[
d_C^*(A,B) = \max_{\mathbf{a}\in A, \mathbf{b}\in B} d(\mathbf{a},\mathbf{b})
\]</span></li>
<li>Average linkage:
<span class="math display">\[
d_A^*(A,B) = \frac{1}{|A| |B|} \sum_{\mathbf{a}\in A}\sum_{\mathbf{b}\in B} d(\mathbf{a},\mathbf{b})
\]</span></li>
</ul>
<div style="margin-top: 1em">

</div>
<p>Computing linkages – an illustration:</p>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-20-1.svg" alt="Figure 10: plot of chunk unnamed-chunk-20" id="fig:unnamed-chunk-20" style="width:100.0%" />
<p class="caption">Figure 10: plot of chunk unnamed-chunk-20</p>
</div>
<div style="margin-top: 1em">

</div>
<p>Complete linkage (again):</p>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-21-1.svg" alt="Figure 11: plot of chunk unnamed-chunk-21" id="fig:unnamed-chunk-21" />
<p class="caption">Figure 11: plot of chunk unnamed-chunk-21</p>
</div>
<div style="margin-top: 1em">

</div>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-22-1.svg" alt="Figure 12: plot of chunk unnamed-chunk-22" id="fig:unnamed-chunk-22" />
<p class="caption">Figure 12: plot of chunk unnamed-chunk-22</p>
</div>
<div style="margin-top: 1em">

</div>
<p>Average linkage:</p>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-23-1.svg" alt="Figure 13: plot of chunk unnamed-chunk-23" id="fig:unnamed-chunk-23" />
<p class="caption">Figure 13: plot of chunk unnamed-chunk-23</p>
</div>
<div style="margin-top: 1em">

</div>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-24-1.svg" alt="Figure 14: plot of chunk unnamed-chunk-24" id="fig:unnamed-chunk-24" />
<p class="caption">Figure 14: plot of chunk unnamed-chunk-24</p>
</div>
<div style="margin-top: 1em">

</div>
<p>Single linkage (this is a typical behaviour!):</p>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-25-1.svg" alt="Figure 15: plot of chunk unnamed-chunk-25" id="fig:unnamed-chunk-25" />
<p class="caption">Figure 15: plot of chunk unnamed-chunk-25</p>
</div>
<div style="margin-top: 1em">

</div>
<div class="figure">
<img src="07-clustering-figures/unnamed-chunk-26-1.svg" alt="Figure 16: plot of chunk unnamed-chunk-26" id="fig:unnamed-chunk-26" />
<p class="caption">Figure 16: plot of chunk unnamed-chunk-26</p>
</div>
<!--Single linkage clustering --
a {\color{blue2}\bf hierarchical agglomerative clustering algorithm}.


\bigskip
{\color{pwsloneczny}\hrule height 2px}
\bigskip

\begin{itemize}


   \item




   \pause\item


% \vspace*{1em}
\end{itemize}-->
<!--Single linkage criterion:
% -- for some \textit{extension} $\tilde{\mathsf{d}}$ of $\mathsf{d}$ to $2^\mathcal{X}$:
$
\displaystyle\argmin_{(u,v), u < v}\ \min\left\{ \mathsf{d}\left(\mathbf{x}^{(i_u)}, \mathbf{x}^{(i_v)}\right):
\mathbf{x}^{(i_u)}\in C_u^{(j)},
\mathbf{x}^{(i_v)}\in C_v^{(j)}
\right\}.
$-->
</div>
</div>
<div id="outro-6" class="section level2">
<h2><span class="header-section-number">7.4</span> Outro</h2>
<div id="remarks-6" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Remarks</h3>
<div style="margin-top: 1em">

</div>
<p>The aim of K-means is to find
<span class="math inline">\(K\)</span> clusters based on the notion of the points’ closeness
to the cluster centres.</p>
<p>Remember that <span class="math inline">\(K\)</span> must be set in advance.</p>
<p>By definition (* via its relation to Voronoi diagrams), all clusters will be of convex shapes.</p>
<p>However, we may try applying <span class="math inline">\(K&#39;\)</span>-means for <span class="math inline">\(K&#39; \gg K\)</span>
to obtained a “fine grained” data compression and then
combine the (sub)clusters into more meaningful groups
using other methods.</p>
<p>Iterative K-means algorithms are very fast even for large data sets,
but they may fail to find a desirable solution, see the next chapter
for discussion.</p>
<div style="margin-top: 1em">

</div>
<p>On the other hand, hierarchical methods output
a whole family of mutually nested partitions, which may provide us
with insight into the underlying data structure.</p>
<p>Unfortunately, there is no easy way to assign new points
to existing clusters.</p>
<p>Linkage scheme must be chosen with care.</p>
<p>These are generally slow – <span class="math inline">\(O(n^2)\)</span> to <span class="math inline">\(O(n^3)\)</span> complexity.</p>
<blockquote>
<p>Note that the <code>fastcluster</code> package provides a more efficient
implementation of some methods available via a call to <code>hclust()</code>.
See also the <code>genie</code> package for a robust algorithm
based on the minimum spanning tree, which can be computed quickly.</p>
</blockquote>
<div style="margin-top: 1em">

</div>
<p>Unsupervised learning is often performed during the data pre-processing
and exploration stage.</p>
<p>Assessing the quality of clustering is particularly challenging as,
unlike in a supervised setting,
we have no access to “ground truth” information.</p>
<p>Moreover, some unsupervised methods do not easily generalise to unobserved data.</p>
<p>Also many clustering methods are based on the notion of pairwise
distances but these tend to behave weirdly in high-dimensional
spaces (“the curse of dimensionality”).</p>
<p>However, clustering methods can aid with supervised tasks
– instead of fitting a single “large model”, it might be
useful to fit separate models to each cluster.</p>
</div>
<div id="other-noteworthy-clustering-algorithms" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Other Noteworthy Clustering Algorithms</h3>
<div style="margin-top: 1em">

</div>
<p>Other noteworthy clustering approaches:</p>
<ul>
<li>Genie (see R package <code>genie</code>) <span class="citation">(Gagolewski et al. <a href="references.html#ref-genie">2016</a>, Cena &amp; Gagolewski <a href="references.html#ref-genieowa">2020</a>)</span></li>
<li>ITM <span class="citation">(Müller et al. <a href="references.html#ref-itm">2012</a>)</span></li>
<li>DBSCAN, HDBSCAN* <span class="citation">(Ling <a href="references.html#ref-pre_dbscan">1973</a>, Ester et al. <a href="references.html#ref-dbscan">1996</a>, Campello et al. <a href="references.html#ref-hdbscan">2015</a>)</span></li>
<li>K-medoids, K-medians</li>
<li>Fuzzy C-means (a.k.a. weighted K-means) <span class="citation">(Bezdek et al. <a href="references.html#ref-cmeans">1984</a>)</span></li>
<li>Spectral clustering; e.g., <span class="citation">(Ng et al. <a href="references.html#ref-spectral_nips">2001</a>)</span></li>
<li>BIRCH <span class="citation">(Zhang et al. <a href="references.html#ref-birch">1996</a>)</span></li>
</ul>
</div>
<div id="further-reading-6" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Further Reading</h3>
<div id="section-3" class="section level4 allowframebreaks unnumbered">
<h4></h4>
<p>Recommended further reading:</p>
<ul>
<li><span class="citation">(James et al. <a href="references.html#ref-islr">2017</a>: Section 10.3)</span></li>
</ul>
<p>Other:</p>
<ul>
<li><span class="citation">(Hastie et al. <a href="references.html#ref-esl">2017</a>: Section 14.3)</span></li>
</ul>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="optimisation-with-iterative-algorithms.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="optimisation-with-genetic-algorithms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
