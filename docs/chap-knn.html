<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Classification with Nearest Neighbours | Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="pandoc, pandoc-citeproc, knitr, bookdown (custom), GitBook, and many more" />

  <meta property="og:title" content="3 Classification with Nearest Neighbours | Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap-hclust.html"/>
<link rel="next" href="chap-feature-engineering.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

div.remark {
    border-left: 2px solid gray;
    padding-left: 1em;
}

div.definition {
    font-style: italic;
    border-left: 2px solid #550000;
    padding-left: 1em;
}

</style>






<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="chap-introduction.html"><a href="chap-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="chap-introduction.html"><a href="chap-introduction.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a><ul>
<li class="chapter" data-level="1.1.1" data-path="chap-introduction.html"><a href="chap-introduction.html#data-sources"><i class="fa fa-check"></i><b>1.1.1</b> Data Sources</a></li>
<li class="chapter" data-level="1.1.2" data-path="chap-introduction.html"><a href="chap-introduction.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chap-introduction.html"><a href="chap-introduction.html#input-data-x"><i class="fa fa-check"></i><b>1.2</b> Input Data, <strong>X</strong></a><ul>
<li class="chapter" data-level="1.2.1" data-path="chap-introduction.html"><a href="chap-introduction.html#abstract-formalism"><i class="fa fa-check"></i><b>1.2.1</b> Abstract Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="chap-introduction.html"><a href="chap-introduction.html#concrete-example"><i class="fa fa-check"></i><b>1.2.2</b> Concrete Example</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="chap-introduction.html"><a href="chap-introduction.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.3</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="1.3.1" data-path="chap-introduction.html"><a href="chap-introduction.html#dimensionality-reduction"><i class="fa fa-check"></i><b>1.3.1</b> Dimensionality Reduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="chap-introduction.html"><a href="chap-introduction.html#anomaly-detection"><i class="fa fa-check"></i><b>1.3.2</b> Anomaly Detection</a></li>
<li class="chapter" data-level="1.3.3" data-path="chap-introduction.html"><a href="chap-introduction.html#clustering"><i class="fa fa-check"></i><b>1.3.3</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="chap-introduction.html"><a href="chap-introduction.html#supervised-learning"><i class="fa fa-check"></i><b>1.4</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.4.1" data-path="chap-introduction.html"><a href="chap-introduction.html#desired-outputs-y"><i class="fa fa-check"></i><b>1.4.1</b> Desired Outputs, <strong>y</strong></a></li>
<li class="chapter" data-level="1.4.2" data-path="chap-introduction.html"><a href="chap-introduction.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.4.2</b> Types of Supervised Learning Problems</a></li>
<li class="chapter" data-level="1.4.3" data-path="chap-introduction.html"><a href="chap-introduction.html#one-dataset-many-problems"><i class="fa fa-check"></i><b>1.4.3</b> One Dataset – Many Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-hclust.html"><a href="chap-hclust.html"><i class="fa fa-check"></i><b>2</b> Agglomerative Hierarchical Clustering</a><ul>
<li class="chapter" data-level="2.1" data-path="chap-hclust.html"><a href="chap-hclust.html#dataset-partitions"><i class="fa fa-check"></i><b>2.1</b> Dataset Partitions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="chap-hclust.html"><a href="chap-hclust.html#label-vectors"><i class="fa fa-check"></i><b>2.1.1</b> Label Vectors</a></li>
<li class="chapter" data-level="2.1.2" data-path="chap-hclust.html"><a href="chap-hclust.html#k-partitions"><i class="fa fa-check"></i><b>2.1.2</b> K-Partitions</a></li>
<li class="chapter" data-level="2.1.3" data-path="chap-hclust.html"><a href="chap-hclust.html#interesting-partitions"><i class="fa fa-check"></i><b>2.1.3</b> “Interesting” Partitions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chap-hclust.html"><a href="chap-hclust.html#sec:euclidean"><i class="fa fa-check"></i><b>2.2</b> Euclidean Distance</a></li>
<li class="chapter" data-level="2.3" data-path="chap-hclust.html"><a href="chap-hclust.html#hierarchical-clustering-at-a-glance"><i class="fa fa-check"></i><b>2.3</b> Hierarchical Clustering at a Glance</a></li>
<li class="chapter" data-level="2.4" data-path="chap-hclust.html"><a href="chap-hclust.html#cluster-dendrograms"><i class="fa fa-check"></i><b>2.4</b> Cluster Dendrograms</a></li>
<li class="chapter" data-level="2.5" data-path="chap-hclust.html"><a href="chap-hclust.html#linkage-functions"><i class="fa fa-check"></i><b>2.5</b> Linkage Functions</a></li>
<li class="chapter" data-level="2.6" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>2.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-knn.html"><a href="chap-knn.html"><i class="fa fa-check"></i><b>3</b> Classification with Nearest Neighbours</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="chap-knn.html"><a href="chap-knn.html#k-nearest-neighbours-classifier"><i class="fa fa-check"></i><b>3.2</b> K-Nearest Neighbours Classifier</a></li>
<li class="chapter" data-level="3.3" data-path="chap-knn.html"><a href="chap-knn.html#example-in-r"><i class="fa fa-check"></i><b>3.3</b> Example in R</a></li>
<li class="chapter" data-level="3.4" data-path="chap-knn.html"><a href="chap-knn.html#classifier-assessment"><i class="fa fa-check"></i><b>3.4</b> Classifier Assessment</a></li>
<li class="chapter" data-level="3.5" data-path="chap-knn.html"><a href="chap-knn.html#classifier-selection"><i class="fa fa-check"></i><b>3.5</b> Classifier Selection</a></li>
<li class="chapter" data-level="3.6" data-path="chap-knn.html"><a href="chap-knn.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.6</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="3.6.1" data-path="chap-knn.html"><a href="chap-knn.html#main-routine"><i class="fa fa-check"></i><b>3.6.1</b> Main Routine</a></li>
<li class="chapter" data-level="3.6.2" data-path="chap-knn.html"><a href="chap-knn.html#sec:mode"><i class="fa fa-check"></i><b>3.6.2</b> Mode</a></li>
<li class="chapter" data-level="3.6.3" data-path="chap-knn.html"><a href="chap-knn.html#nn-search-methods"><i class="fa fa-check"></i><b>3.6.3</b> NN Search Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="chap-knn.html"><a href="chap-knn.html#remarks-1"><i class="fa fa-check"></i><b>3.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html"><i class="fa fa-check"></i><b>4</b> Feature Engineering</a><ul>
<li class="chapter" data-level="4.0.1" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#feature-engineering"><i class="fa fa-check"></i><b>4.0.1</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.0.2" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#different-metrics"><i class="fa fa-check"></i><b>4.0.2</b> Different Metrics (*)</a></li>
<li class="chapter" data-level="4.1" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#exercises-1"><i class="fa fa-check"></i><b>4.1</b> Exercises</a><ul>
<li class="chapter" data-level="4.1.1" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#wine-quality-best-k-nn-parameters-via-cross-validation"><i class="fa fa-check"></i><b>4.1.1</b> Wine Quality – Best K-NN Parameters via Cross-Validation (*)</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#remarks-2"><i class="fa fa-check"></i><b>4.2</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-trees.html"><a href="chap-trees.html"><i class="fa fa-check"></i><b>5</b> Classification with Decision Trees</a><ul>
<li class="chapter" data-level="5.1" data-path="chap-trees.html"><a href="chap-trees.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="chap-trees.html"><a href="chap-trees.html#classification-task"><i class="fa fa-check"></i><b>5.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="5.1.2" data-path="chap-trees.html"><a href="chap-trees.html#data"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="chap-trees.html"><a href="chap-trees.html#decision-trees"><i class="fa fa-check"></i><b>5.2</b> Decision Trees</a><ul>
<li class="chapter" data-level="5.2.1" data-path="chap-trees.html"><a href="chap-trees.html#introduction-2"><i class="fa fa-check"></i><b>5.2.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2.2" data-path="chap-trees.html"><a href="chap-trees.html#example-in-r-1"><i class="fa fa-check"></i><b>5.2.2</b> Example in R</a></li>
<li class="chapter" data-level="5.2.3" data-path="chap-trees.html"><a href="chap-trees.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>5.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="chap-trees.html"><a href="chap-trees.html#exercises-2"><i class="fa fa-check"></i><b>5.3</b> Exercises</a><ul>
<li class="chapter" data-level="5.3.1" data-path="chap-trees.html"><a href="chap-trees.html#edstats-where-girls-are-better-at-maths-than-boys"><i class="fa fa-check"></i><b>5.3.1</b> EdStats – Where Girls Are Better at Maths Than Boys?</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap-trees.html"><a href="chap-trees.html#edstats-and-world-factbook-joining-forces"><i class="fa fa-check"></i><b>5.3.2</b> EdStats and World Factbook – Joining Forces</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap-trees.html"><a href="chap-trees.html#wine-quality-random-forest-and-xgboost"><i class="fa fa-check"></i><b>5.3.3</b> Wine Quality – Random Forest and XGBoost (*)</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap-trees.html"><a href="chap-trees.html#outro"><i class="fa fa-check"></i><b>5.4</b> Outro</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html"><i class="fa fa-check"></i><b>6</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#simple-regression"><i class="fa fa-check"></i><b>6.1</b> Simple Regression</a><ul>
<li class="chapter" data-level="6.1.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#introduction-3"><i class="fa fa-check"></i><b>6.1.1</b> Introduction</a></li>
<li class="chapter" data-level="6.1.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>6.1.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#search-space-and-objective"><i class="fa fa-check"></i><b>6.1.3</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#introduction-4"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#solution-in-r"><i class="fa fa-check"></i><b>6.2.2</b> Solution in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#analytic-solution"><i class="fa fa-check"></i><b>6.2.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="6.2.4" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>6.2.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#exercises-3"><i class="fa fa-check"></i><b>6.3</b> Exercises</a><ul>
<li class="chapter" data-level="6.3.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>6.3.1</b> The Anscombe Quartet</a></li>
<li class="chapter" data-level="6.3.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#median-house-value-in-boston"><i class="fa fa-check"></i><b>6.3.2</b> Median House Value in Boston</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#outro-1"><i class="fa fa-check"></i><b>6.4</b> Outro</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html"><i class="fa fa-check"></i><b>7</b> Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a><ul>
<li class="chapter" data-level="7.1.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#formalism"><i class="fa fa-check"></i><b>7.1.1</b> Formalism</a></li>
<li class="chapter" data-level="7.1.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>7.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#multiple-linear-regression"><i class="fa fa-check"></i><b>7.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#problem-formulation"><i class="fa fa-check"></i><b>7.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="7.2.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>7.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#finding-the-best-model"><i class="fa fa-check"></i><b>7.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="7.3.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#model-diagnostics"><i class="fa fa-check"></i><b>7.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="7.3.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#variable-selection"><i class="fa fa-check"></i><b>7.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="7.3.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#variable-transformation"><i class="fa fa-check"></i><b>7.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="7.3.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>7.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#exercises-4"><i class="fa fa-check"></i><b>7.4</b> Exercises</a><ul>
<li class="chapter" data-level="7.4.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>7.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="7.4.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>7.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="7.4.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>7.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="7.4.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>7.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="7.4.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>7.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
<li class="chapter" data-level="7.4.6" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#median-house-value-in-boston-continued"><i class="fa fa-check"></i><b>7.4.6</b> Median House Value in Boston (Continued)</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#outro-2"><i class="fa fa-check"></i><b>7.5</b> Outro</a><ul>
<li class="chapter" data-level="7.5.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#remarks-3"><i class="fa fa-check"></i><b>7.5.1</b> Remarks</a></li>
<li class="chapter" data-level="7.5.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#other-methods-for-regression"><i class="fa fa-check"></i><b>7.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="7.5.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>7.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="7.5.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>7.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="7.5.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>7.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-logistic.html"><a href="chap-logistic.html"><i class="fa fa-check"></i><b>8</b> Classification with Linear Models</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-logistic.html"><a href="chap-logistic.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="chap-logistic.html"><a href="chap-logistic.html#classification-task-1"><i class="fa fa-check"></i><b>8.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="8.1.2" data-path="chap-logistic.html"><a href="chap-logistic.html#data-1"><i class="fa fa-check"></i><b>8.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="chap-logistic.html"><a href="chap-logistic.html#binary-logistic-regression"><i class="fa fa-check"></i><b>8.2</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="8.2.1" data-path="chap-logistic.html"><a href="chap-logistic.html#motivation"><i class="fa fa-check"></i><b>8.2.1</b> Motivation</a></li>
<li class="chapter" data-level="8.2.2" data-path="chap-logistic.html"><a href="chap-logistic.html#logistic-model"><i class="fa fa-check"></i><b>8.2.2</b> Logistic Model</a></li>
<li class="chapter" data-level="8.2.3" data-path="chap-logistic.html"><a href="chap-logistic.html#example-in-r-2"><i class="fa fa-check"></i><b>8.2.3</b> Example in R</a></li>
<li class="chapter" data-level="8.2.4" data-path="chap-logistic.html"><a href="chap-logistic.html#loss-function-cross-entropy"><i class="fa fa-check"></i><b>8.2.4</b> Loss Function: Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="chap-logistic.html"><a href="chap-logistic.html#exercises-5"><i class="fa fa-check"></i><b>8.3</b> Exercises</a><ul>
<li class="chapter" data-level="8.3.1" data-path="chap-logistic.html"><a href="chap-logistic.html#edstats-fitting-of-binary-logistic-regression-models"><i class="fa fa-check"></i><b>8.3.1</b> EdStats – Fitting of Binary Logistic Regression Models</a></li>
<li class="chapter" data-level="8.3.2" data-path="chap-logistic.html"><a href="chap-logistic.html#edstats-variable-selection-in-binary-logistic-regression"><i class="fa fa-check"></i><b>8.3.2</b> EdStats – Variable Selection in Binary Logistic Regression (*)</a></li>
<li class="chapter" data-level="8.3.3" data-path="chap-logistic.html"><a href="chap-logistic.html#currency-exchange-rates-growthfall"><i class="fa fa-check"></i><b>8.3.3</b> Currency Exchange Rates Growth/Fall</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="chap-logistic.html"><a href="chap-logistic.html#outro-3"><i class="fa fa-check"></i><b>8.4</b> Outro</a><ul>
<li class="chapter" data-level="8.4.1" data-path="chap-logistic.html"><a href="chap-logistic.html#remarks-4"><i class="fa fa-check"></i><b>8.4.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html"><i class="fa fa-check"></i><b>9</b> Continuous Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="9.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#optimisation-problems"><i class="fa fa-check"></i><b>9.1.1</b> Optimisation Problems</a></li>
<li class="chapter" data-level="9.1.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>9.1.2</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="9.1.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>9.1.3</b> Example Objective over a 2D Domain</a></li>
<li class="chapter" data-level="9.1.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>9.1.4</b> Example Optimisation Problems in Machine Learning</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#iterative-methods"><i class="fa fa-check"></i><b>9.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="9.2.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#introduction-8"><i class="fa fa-check"></i><b>9.2.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-in-r-3"><i class="fa fa-check"></i><b>9.2.2</b> Example in R</a></li>
<li class="chapter" data-level="9.2.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>9.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="9.2.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#random-restarts"><i class="fa fa-check"></i><b>9.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#gradient-descent"><i class="fa fa-check"></i><b>9.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="9.3.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#function-gradient"><i class="fa fa-check"></i><b>9.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="9.3.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>9.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="9.3.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>9.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="9.3.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-mnist"><i class="fa fa-check"></i><b>9.3.4</b> Example: MNIST (*)</a></li>
<li class="chapter" data-level="9.3.5" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>9.3.5</b> Stochastic Gradient Descent (SGD) (*)</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>9.4</b> A Note on Convex Optimisation (*)</a></li>
<li class="chapter" data-level="9.5" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#outro-4"><i class="fa fa-check"></i><b>9.5</b> Outro</a><ul>
<li class="chapter" data-level="9.5.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#remarks-5"><i class="fa fa-check"></i><b>9.5.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-kmeans.html"><a href="chap-kmeans.html"><i class="fa fa-check"></i><b>10</b> Clustering with K-Means</a><ul>
<li class="chapter" data-level="10.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#within-cluster-sum-of-squares"><i class="fa fa-check"></i><b>10.1</b> Within-Cluster Sum of Squares</a></li>
<li class="chapter" data-level="10.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#k-means-clustering"><i class="fa fa-check"></i><b>10.2</b> K-means Clustering</a><ul>
<li class="chapter" data-level="10.2.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#example-in-r-4"><i class="fa fa-check"></i><b>10.2.1</b> Example in R</a></li>
<li class="chapter" data-level="10.2.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#problem-statement"><i class="fa fa-check"></i><b>10.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="10.2.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>10.2.3</b> Algorithms for the K-means Problem</a></li>
<li class="chapter" data-level="10.2.4" data-path="chap-kmeans.html"><a href="chap-kmeans.html#k-means-revisited"><i class="fa fa-check"></i><b>10.2.4</b> K-means Revisited</a></li>
<li class="chapter" data-level="10.2.5" data-path="chap-kmeans.html"><a href="chap-kmeans.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>10.2.5</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#exercises-6"><i class="fa fa-check"></i><b>10.3</b> Exercises</a><ul>
<li class="chapter" data-level="10.3.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#clustering-of-the-world-factbook"><i class="fa fa-check"></i><b>10.3.1</b> Clustering of the World Factbook</a></li>
<li class="chapter" data-level="10.3.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#unbalance-dataset-k-means-needs-multiple-starts"><i class="fa fa-check"></i><b>10.3.2</b> Unbalance Dataset – K-Means Needs Multiple Starts</a></li>
<li class="chapter" data-level="10.3.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#clustering-of-typical-2d-benchmark-datasets"><i class="fa fa-check"></i><b>10.3.3</b> Clustering of Typical 2D Benchmark Datasets</a></li>
<li class="chapter" data-level="10.3.4" data-path="chap-kmeans.html"><a href="chap-kmeans.html#wine-quality-volatile.acidity-and-sulphates"><i class="fa fa-check"></i><b>10.3.4</b> Wine Quality – <code>volatile.acidity</code> and <code>sulphates</code></a></li>
<li class="chapter" data-level="10.3.5" data-path="chap-kmeans.html"><a href="chap-kmeans.html#wine-quality-chlorides-and-total.sulfur.dioxide"><i class="fa fa-check"></i><b>10.3.5</b> Wine Quality – <code>chlorides</code> and <code>total.sulfur.dioxide</code></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="chap-kmeans.html"><a href="chap-kmeans.html#outro-5"><i class="fa fa-check"></i><b>10.4</b> Outro</a><ul>
<li class="chapter" data-level="10.4.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#remarks-6"><i class="fa fa-check"></i><b>10.4.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html"><i class="fa fa-check"></i><b>11</b> Discrete Optimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#introduction-9"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#recap"><i class="fa fa-check"></i><b>11.1.1</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#outro-6"><i class="fa fa-check"></i><b>11.2</b> Outro</a><ul>
<li class="chapter" data-level="11.2.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#remarks-7"><i class="fa fa-check"></i><b>11.2.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-feature-selection.html"><a href="chap-feature-selection.html"><i class="fa fa-check"></i><b>12</b> Feature Selection</a><ul>
<li class="chapter" data-level="12.1" data-path="chap-feature-selection.html"><a href="chap-feature-selection.html#introduction-10"><i class="fa fa-check"></i><b>12.1</b> Introduction</a><ul>
<li class="chapter" data-level="12.1.1" data-path="chap-feature-selection.html"><a href="chap-feature-selection.html#recap-1"><i class="fa fa-check"></i><b>12.1.1</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="chap-feature-selection.html"><a href="chap-feature-selection.html#outro-7"><i class="fa fa-check"></i><b>12.2</b> Outro</a><ul>
<li class="chapter" data-level="12.2.1" data-path="chap-feature-selection.html"><a href="chap-feature-selection.html#remarks-8"><i class="fa fa-check"></i><b>12.2.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="chap-images.html"><a href="chap-images.html"><i class="fa fa-check"></i><b>13</b> Shallow and Deep Neural Networks</a><ul>
<li class="chapter" data-level="13.1" data-path="chap-images.html"><a href="chap-images.html#introduction-11"><i class="fa fa-check"></i><b>13.1</b> Introduction</a><ul>
<li class="chapter" data-level="13.1.1" data-path="chap-images.html"><a href="chap-images.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>13.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="13.1.2" data-path="chap-images.html"><a href="chap-images.html#data-2"><i class="fa fa-check"></i><b>13.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="chap-images.html"><a href="chap-images.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>13.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="13.2.1" data-path="chap-images.html"><a href="chap-images.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>13.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="13.2.2" data-path="chap-images.html"><a href="chap-images.html#extending-logistic-regression"><i class="fa fa-check"></i><b>13.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="13.2.3" data-path="chap-images.html"><a href="chap-images.html#softmax-function"><i class="fa fa-check"></i><b>13.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="13.2.4" data-path="chap-images.html"><a href="chap-images.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>13.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="13.2.5" data-path="chap-images.html"><a href="chap-images.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>13.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="13.2.6" data-path="chap-images.html"><a href="chap-images.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>13.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="chap-images.html"><a href="chap-images.html#artificial-neural-networks"><i class="fa fa-check"></i><b>13.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="13.3.1" data-path="chap-images.html"><a href="chap-images.html#artificial-neuron"><i class="fa fa-check"></i><b>13.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="13.3.2" data-path="chap-images.html"><a href="chap-images.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>13.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="13.3.3" data-path="chap-images.html"><a href="chap-images.html#example-in-r-5"><i class="fa fa-check"></i><b>13.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="chap-images.html"><a href="chap-images.html#deep-neural-networks"><i class="fa fa-check"></i><b>13.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="13.4.1" data-path="chap-images.html"><a href="chap-images.html#introduction-12"><i class="fa fa-check"></i><b>13.4.1</b> Introduction</a></li>
<li class="chapter" data-level="13.4.2" data-path="chap-images.html"><a href="chap-images.html#activation-functions"><i class="fa fa-check"></i><b>13.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="13.4.3" data-path="chap-images.html"><a href="chap-images.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>13.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="13.4.4" data-path="chap-images.html"><a href="chap-images.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>13.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="chap-images.html"><a href="chap-images.html#preprocessing-of-data"><i class="fa fa-check"></i><b>13.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="13.5.1" data-path="chap-images.html"><a href="chap-images.html#introduction-13"><i class="fa fa-check"></i><b>13.5.1</b> Introduction</a></li>
<li class="chapter" data-level="13.5.2" data-path="chap-images.html"><a href="chap-images.html#image-deskewing"><i class="fa fa-check"></i><b>13.5.2</b> Image Deskewing</a></li>
<li class="chapter" data-level="13.5.3" data-path="chap-images.html"><a href="chap-images.html#summary-of-all-the-models-considered"><i class="fa fa-check"></i><b>13.5.3</b> Summary of All the Models Considered</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="chap-images.html"><a href="chap-images.html#outro-8"><i class="fa fa-check"></i><b>13.6</b> Outro</a><ul>
<li class="chapter" data-level="13.6.1" data-path="chap-images.html"><a href="chap-images.html#remarks-9"><i class="fa fa-check"></i><b>13.6.1</b> Remarks</a></li>
<li class="chapter" data-level="13.6.2" data-path="chap-images.html"><a href="chap-images.html#beyond-mnist"><i class="fa fa-check"></i><b>13.6.2</b> Beyond MNIST</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="chap-recommenders.html"><a href="chap-recommenders.html"><i class="fa fa-check"></i><b>14</b> Recommender Systems</a><ul>
<li class="chapter" data-level="14.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#introduction-14"><i class="fa fa-check"></i><b>14.1</b> Introduction</a><ul>
<li class="chapter" data-level="14.1.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#the-netflix-prize"><i class="fa fa-check"></i><b>14.1.1</b> The Netflix Prize</a></li>
<li class="chapter" data-level="14.1.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#main-approaches"><i class="fa fa-check"></i><b>14.1.2</b> Main Approaches</a></li>
<li class="chapter" data-level="14.1.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#formalism-1"><i class="fa fa-check"></i><b>14.1.3</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#collaborative-filtering"><i class="fa fa-check"></i><b>14.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="14.2.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#example"><i class="fa fa-check"></i><b>14.2.1</b> Example</a></li>
<li class="chapter" data-level="14.2.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#similarity-measures"><i class="fa fa-check"></i><b>14.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="14.2.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>14.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="14.2.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>14.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#exercise-the-movielens-dataset"><i class="fa fa-check"></i><b>14.3</b> Exercise: The MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="14.3.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#dataset"><i class="fa fa-check"></i><b>14.3.1</b> Dataset</a></li>
<li class="chapter" data-level="14.3.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#data-cleansing"><i class="fa fa-check"></i><b>14.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="14.3.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#item-item-similarities"><i class="fa fa-check"></i><b>14.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="14.3.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#example-recommendations"><i class="fa fa-check"></i><b>14.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="14.3.5" data-path="chap-recommenders.html"><a href="chap-recommenders.html#clustering-1"><i class="fa fa-check"></i><b>14.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#outro-9"><i class="fa fa-check"></i><b>14.4</b> Outro</a><ul>
<li class="chapter" data-level="14.4.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#remarks-10"><i class="fa fa-check"></i><b>14.4.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="chap-text.html"><a href="chap-text.html"><i class="fa fa-check"></i><b>15</b> Natural Language Processing</a><ul>
<li class="chapter" data-level="15.1" data-path="chap-text.html"><a href="chap-text.html#to-do"><i class="fa fa-check"></i><b>15.1</b> TO DO</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-convention.html"><a href="appendix-convention.html"><i class="fa fa-check"></i><b>A</b> Notation Convention</a></li>
<li class="chapter" data-level="B" data-path="appendix-rintro.html"><a href="appendix-rintro.html"><i class="fa fa-check"></i><b>B</b> Setting Up the R Environment</a><ul>
<li class="chapter" data-level="B.1" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-r"><i class="fa fa-check"></i><b>B.1</b> Installing R</a></li>
<li class="chapter" data-level="B.2" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-an-ide"><i class="fa fa-check"></i><b>B.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="B.3" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-recommended-packages"><i class="fa fa-check"></i><b>B.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="B.4" data-path="appendix-rintro.html"><a href="appendix-rintro.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>B.4</b> First R Script in RStudio</a></li>
<li class="chapter" data-level="B.5" data-path="appendix-rintro.html"><a href="appendix-rintro.html#exercises-7"><i class="fa fa-check"></i><b>B.5</b> Exercises</a><ul>
<li class="chapter" data-level="B.5.1" data-path="appendix-rintro.html"><a href="appendix-rintro.html#first-steps-with-vectors"><i class="fa fa-check"></i><b>B.5.1</b> First Steps with Vectors</a></li>
<li class="chapter" data-level="B.5.2" data-path="appendix-rintro.html"><a href="appendix-rintro.html#basic-plotting"><i class="fa fa-check"></i><b>B.5.2</b> Basic Plotting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appendix-rvector.html"><a href="appendix-rvector.html"><i class="fa fa-check"></i><b>C</b> Vector Algebra in R</a><ul>
<li class="chapter" data-level="C.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#motivation-1"><i class="fa fa-check"></i><b>C.1</b> Motivation</a></li>
<li class="chapter" data-level="C.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#numeric-vectors"><i class="fa fa-check"></i><b>C.2</b> Numeric Vectors</a><ul>
<li class="chapter" data-level="C.2.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>C.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="C.2.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-vector-operations"><i class="fa fa-check"></i><b>C.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#aggregation-functions"><i class="fa fa-check"></i><b>C.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="C.2.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#special-functions"><i class="fa fa-check"></i><b>C.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="C.2.6" data-path="appendix-rvector.html"><a href="appendix-rvector.html#norms-and-distances"><i class="fa fa-check"></i><b>C.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="C.2.7" data-path="appendix-rvector.html"><a href="appendix-rvector.html#dot-product"><i class="fa fa-check"></i><b>C.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="C.2.8" data-path="appendix-rvector.html"><a href="appendix-rvector.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>C.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#logical-vectors"><i class="fa fa-check"></i><b>C.3</b> Logical Vectors</a><ul>
<li class="chapter" data-level="C.3.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-logical-vectors"><i class="fa fa-check"></i><b>C.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="C.3.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#logical-operations"><i class="fa fa-check"></i><b>C.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="C.3.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#comparison-operations"><i class="fa fa-check"></i><b>C.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="C.3.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#aggregation-functions-1"><i class="fa fa-check"></i><b>C.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#character-vectors"><i class="fa fa-check"></i><b>C.4</b> Character Vectors</a><ul>
<li class="chapter" data-level="C.4.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-character-vectors"><i class="fa fa-check"></i><b>C.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="C.4.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>C.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="C.4.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>C.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-subsetting"><i class="fa fa-check"></i><b>C.5</b> Vector Subsetting</a><ul>
<li class="chapter" data-level="C.5.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>C.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="C.5.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>C.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="C.5.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>C.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="C.5.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#replacing-elements"><i class="fa fa-check"></i><b>C.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="C.5.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#other-functions"><i class="fa fa-check"></i><b>C.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="appendix-rvector.html"><a href="appendix-rvector.html#named-vectors"><i class="fa fa-check"></i><b>C.6</b> Named Vectors</a><ul>
<li class="chapter" data-level="C.6.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-named-vectors"><i class="fa fa-check"></i><b>C.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="C.6.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>C.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="C.7" data-path="appendix-rvector.html"><a href="appendix-rvector.html#sec:factor"><i class="fa fa-check"></i><b>C.7</b> Factors</a><ul>
<li class="chapter" data-level="C.7.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-factors"><i class="fa fa-check"></i><b>C.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="C.7.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#levels"><i class="fa fa-check"></i><b>C.7.2</b> Levels</a></li>
<li class="chapter" data-level="C.7.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#internal-representation"><i class="fa fa-check"></i><b>C.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.8" data-path="appendix-rvector.html"><a href="appendix-rvector.html#lists"><i class="fa fa-check"></i><b>C.8</b> Lists</a><ul>
<li class="chapter" data-level="C.8.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-lists"><i class="fa fa-check"></i><b>C.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="C.8.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#named-lists"><i class="fa fa-check"></i><b>C.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="C.8.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>C.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="C.8.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#common-operations"><i class="fa fa-check"></i><b>C.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="appendix-rvector.html"><a href="appendix-rvector.html#exercises-8"><i class="fa fa-check"></i><b>C.9</b> Exercises</a><ul>
<li class="chapter" data-level="C.9.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#audeur-exchange-rates"><i class="fa fa-check"></i><b>C.9.1</b> AUD/EUR Exchange Rates</a></li>
</ul></li>
<li class="chapter" data-level="C.10" data-path="appendix-rvector.html"><a href="appendix-rvector.html#further-reading"><i class="fa fa-check"></i><b>C.10</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html"><i class="fa fa-check"></i><b>D</b> Matrix Algebra in R</a><ul>
<li class="chapter" data-level="D.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#creating-matrices"><i class="fa fa-check"></i><b>D.1</b> Creating Matrices</a><ul>
<li class="chapter" data-level="D.1.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix"><i class="fa fa-check"></i><b>D.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="D.1.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#stacking-vectors"><i class="fa fa-check"></i><b>D.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="D.1.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>D.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="D.1.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>D.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="D.1.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#other-methods"><i class="fa fa-check"></i><b>D.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="D.1.6" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#internal-representation-1"><i class="fa fa-check"></i><b>D.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#common-operations-1"><i class="fa fa-check"></i><b>D.2</b> Common Operations</a><ul>
<li class="chapter" data-level="D.2.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-transpose"><i class="fa fa-check"></i><b>D.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="D.2.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>D.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="D.2.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>D.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="D.2.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-multiplication"><i class="fa fa-check"></i><b>D.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="D.2.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>D.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="D.2.6" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#vectorised-special-functions"><i class="fa fa-check"></i><b>D.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="D.2.7" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-vector-operations"><i class="fa fa-check"></i><b>D.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-subsetting"><i class="fa fa-check"></i><b>D.3</b> Matrix Subsetting</a><ul>
<li class="chapter" data-level="D.3.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-individual-elements"><i class="fa fa-check"></i><b>D.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="D.3.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>D.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="D.3.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-submatrices"><i class="fa fa-check"></i><b>D.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="D.3.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>D.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="D.3.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>D.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#exercises-9"><i class="fa fa-check"></i><b>D.4</b> Exercises</a><ul>
<li class="chapter" data-level="D.4.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#currency-exchange-rates"><i class="fa fa-check"></i><b>D.4.1</b> Currency Exchange Rates</a></li>
<li class="chapter" data-level="D.4.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#currency-exchange-rates-relative-to-1999"><i class="fa fa-check"></i><b>D.4.2</b> Currency Exchange Rates Relative to 1999</a></li>
</ul></li>
<li class="chapter" data-level="D.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#further-reading-1"><i class="fa fa-check"></i><b>D.5</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="appendix-rdf.html"><a href="appendix-rdf.html"><i class="fa fa-check"></i><b>E</b> Data Frame Wrangling in R</a><ul>
<li class="chapter" data-level="E.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#creating-data-frames"><i class="fa fa-check"></i><b>E.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="E.2" data-path="appendix-rdf.html"><a href="appendix-rdf.html#importing-data-frames"><i class="fa fa-check"></i><b>E.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="E.3" data-path="appendix-rdf.html"><a href="appendix-rdf.html#data-frame-subsetting"><i class="fa fa-check"></i><b>E.3</b> Data Frame Subsetting</a><ul>
<li class="chapter" data-level="E.3.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>E.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="E.3.2" data-path="appendix-rdf.html"><a href="appendix-rdf.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>E.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="appendix-rdf.html"><a href="appendix-rdf.html#common-operations-2"><i class="fa fa-check"></i><b>E.4</b> Common Operations</a></li>
<li class="chapter" data-level="E.5" data-path="appendix-rdf.html"><a href="appendix-rdf.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>E.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="E.6" data-path="appendix-rdf.html"><a href="appendix-rdf.html#exercises-10"><i class="fa fa-check"></i><b>E.6</b> Exercises</a><ul>
<li class="chapter" data-level="E.6.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#urban-forest"><i class="fa fa-check"></i><b>E.6.1</b> Urban Forest</a></li>
</ul></li>
<li class="chapter" data-level="E.7" data-path="appendix-rdf.html"><a href="appendix-rdf.html#air-quality"><i class="fa fa-check"></i><b>E.7</b> Air Quality</a></li>
<li class="chapter" data-level="E.8" data-path="appendix-rdf.html"><a href="appendix-rdf.html#further-reading-2"><i class="fa fa-check"></i><b>E.8</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="appendix-datasets.html"><a href="appendix-datasets.html"><i class="fa fa-check"></i><b>F</b> Datasets</a><ul>
<li class="chapter" data-level="F.1" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:ssi"><i class="fa fa-check"></i><b>F.1</b> Sustainable Society Indices</a></li>
<li class="chapter" data-level="F.2" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:air-quality"><i class="fa fa-check"></i><b>F.2</b> Air Quality</a></li>
<li class="chapter" data-level="F.3" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:currency-exchange"><i class="fa fa-check"></i><b>F.3</b> Currency Exchange Rates</a></li>
<li class="chapter" data-level="F.4" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:urban-forest"><i class="fa fa-check"></i><b>F.4</b> Urban Forest</a></li>
<li class="chapter" data-level="F.5" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:wine-quality"><i class="fa fa-check"></i><b>F.5</b> Wine Quality</a></li>
<li class="chapter" data-level="F.6" data-path="appendix-datasets.html"><a href="appendix-datasets.html#the-world-factbook-countries-of-the-world"><i class="fa fa-check"></i><b>F.6</b> The World Factbook (Countries of the World)</a></li>
<li class="chapter" data-level="F.7" data-path="appendix-datasets.html"><a href="appendix-datasets.html#edstats-country-level-education-statistics"><i class="fa fa-check"></i><b>F.7</b> EdStats (Country-Level Education Statistics)</a></li>
<li class="chapter" data-level="F.8" data-path="appendix-datasets.html"><a href="appendix-datasets.html#food-and-nutrient-database-for-dietary-studies-fndds"><i class="fa fa-check"></i><b>F.8</b> Food and Nutrient Database for Dietary Studies (FNDDS)</a></li>
<li class="chapter" data-level="F.9" data-path="appendix-datasets.html"><a href="appendix-datasets.html#clustering-benchmarks"><i class="fa fa-check"></i><b>F.9</b> Clustering Benchmarks</a></li>
<li class="chapter" data-level="F.10" data-path="appendix-datasets.html"><a href="appendix-datasets.html#movie-lens-todo"><i class="fa fa-check"></i><b>F.10</b> Movie Lens (TODO)</a></li>
<li class="chapter" data-level="F.11" data-path="appendix-datasets.html"><a href="appendix-datasets.html#other-todo"><i class="fa fa-check"></i><b>F.11</b> Other (TODO)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.3 2020-10-22 13:34 (eb3febb)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap:knn" class="section level1">
<h1><span class="header-section-number">3</span> Classification with Nearest Neighbours</h1>
<p><strong>TODO</strong> In this chapter, we will:</p>
<ul>
<li><p>solve a prediction task with k-nearest neighbour classifier</p></li>
<li><p>discuss performance metrics for binary classification: accuracy,
precision, recall, and F-measure</p></li>
<li><p>introduce best practices of optimal classifier selection – in particular, explain the difference between training, validation,
and test sets</p></li>
</ul>
<!-- (alternative: cross-validation, see exercise TODO) -->
<!-- ROC curve? -->
<!--

Add K-NN Regression



add CHAMELEON and (H)DBSCAN(*) ???
-- beyond supervised learning

Karypis, G., EH. Han, V. Kumar (1999): CHAMELEON: A Hierarchical Clustering Algorithm Using Dynamic Modeling, IEEE Computer, 32(8): 68–75.

-->
<!-- TODO:


2D illustration of NEAREST NEIGHBOURS!!!

exercise -- density, not NN-based classification or
regression - consider the epsilon-neighbourhood????

-->
<div id="introduction" class="section level2">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<p>In the previous chapter, we were only given an input matrix
<span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span> representing
<span class="math inline">\(n\)</span> objects described by means of <span class="math inline">\(p\)</span> numerical features.</p>
<p>In this chapter we are going to be interested in <em>supervised learning</em> tasks;
we assume that with each <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> (<span class="math inline">\(i=1,\dots,n\)</span>)
we associate the desired output <span class="math inline">\(y_i\)</span>:</p>
<p><span class="math display">\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p} \\
x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p} \\
\end{array}
\right],
\qquad
\mathbf{y} = \left[
\begin{array}{c}
{y}_{1} \\
{y}_{2} \\
\vdots\\
{y}_{n} \\
\end{array}
\right].
\]</span></p>
<p>More precisely, we’d now like to focus on <em>classification</em> – we assume that
each <span class="math inline">\(y_i\)</span> is of qualitative type (e.g., a label, a category, a colour).</p>
<p>The classes are usually <em>encoded</em> with consecutive integers,
say, <span class="math inline">\(1, 2, \dots, L\)</span>, where <span class="math inline">\(L\)</span> is the total number of unique cases.
Mathematically, we’ll write that <span class="math inline">\(y_i\in\{1,\dots,L\}\)</span>.</p>
<div class="remark"><strong>Remark.</strong>
<p><code>factor</code> datatype in R (see Section <a href="appendix-rvector.html#sec:factor">C.7</a> for more details)
gives a very convenient means to encode categorical data
(such as <span class="math inline">\(\mathbf{y}\)</span>):</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" title="1">y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb62-2" title="2">(y &lt;-<span class="st"> </span><span class="kw">factor</span>(y, <span class="dt">levels=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;blue&quot;</span>)))</a></code></pre></div>
<pre><code>## [1] black green black red   green green blue  red  
## Levels: black red green blue</code></pre>
<!-- table(y) # every label and the number of occurrences -->
<p>Internally, objects of type <code>factor</code> are represented as integer vectors
with elements in <span class="math inline">\(\{1,\dots,L\}\)</span>, where <span class="math inline">\(L\)</span> is the number of possible levels.
Labels, used to “decipher” the numeric codes, are stored separately
and can always be replaced with other ones.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" title="1"><span class="kw">as.numeric</span>(y) <span class="co"># 1st label, 3rd label, 1st label, and so forth</span></a></code></pre></div>
<pre><code>## [1] 1 3 1 2 3 3 4 2</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" title="1"><span class="kw">length</span>(<span class="kw">levels</span>(y)) <span class="co"># L = number of levels</span></a></code></pre></div>
<pre><code>## [1] 4</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" title="1"><span class="kw">levels</span>(y) <span class="co"># 1=black, 2=red, 3=green, 4=red</span></a></code></pre></div>
<pre><code>## [1] &quot;black&quot; &quot;red&quot;   &quot;green&quot; &quot;blue&quot;</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" title="1"><span class="kw">levels</span>(y) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;k&quot;</span>, <span class="st">&quot;r&quot;</span>, <span class="st">&quot;g&quot;</span>, <span class="st">&quot;b&quot;</span>) <span class="co"># re-encode</span></a>
<a class="sourceLine" id="cb70-2" title="2">y</a></code></pre></div>
<pre><code>## [1] k g k r g g b r
## Levels: k r g b</code></pre>
</div>
<p>In this chapter we’ll assume <span class="math inline">\(L=2\)</span>, i.e., that each <span class="math inline">\(y_i\)</span> can only take one of
two possible values. Such a case is very common in practice;
it has been even given a special name: <em>binary classification</em>.
The two classes are traditionally denoted as <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>,
see Table <a href="chap-knn.html#tab:binary-classification">3.1</a> for some examples.</p>
<table>
<caption><span id="tab:binary-classification">Table 3.1: </span> Some examples of output labels in binary classification tasks</caption>
<thead>
<tr class="header">
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>no</td>
<td>yes</td>
</tr>
<tr class="even">
<td>false</td>
<td>true</td>
</tr>
<tr class="odd">
<td>failure</td>
<td>success</td>
</tr>
<tr class="even">
<td>reject</td>
<td>accept</td>
</tr>
<tr class="odd">
<td>healthy</td>
<td>ill</td>
</tr>
</tbody>
</table>
<div class="remark"><strong>Remark.</strong>
<p><span class="math inline">\(0\)</span>s and <span class="math inline">\(1\)</span>s are mathematically convenient, because
instead of stating “if <span class="math inline">\(y_i=1\)</span>, then let <span class="math inline">\(z=a\)</span> and otherwise take <span class="math inline">\(z=b\)</span>”,
we can simply write “<span class="math inline">\(z=y_i\, a + (1-y_i)\, b\)</span>”
(see, e.g., the definition of cross-entropy in <a href="chap-logistic.html#chap:logistic">8</a>).
Ah, those neat math tricks!</p>
</div>
<p>Figure <a href="chap-knn.html#fig:classify-intro">3.1</a> gives a scatter plot of an example synthetic
two-dimensional dataset (i.e., <span class="math inline">\(p=2\)</span>)
with the reference binary <span class="math inline">\(y\)</span>s depicted using different plotting symbols.
We see that the two classes overlap slightly – they cannot be
separated by means of any simple boundary (e.g., a line).
<!-- The "true" decision boundary is at $X_1=0$ but the classes -->
<!-- slightly overlap (the dataset is a bit noisy). --></p>
<div class="figure"><span id="fig:classify-intro"></span>
<img src="030-knn-figures/classify-intro-1.svg" alt=" A synthetic 2D dataset where each point is assigned one of two distinct labels" />
<p class="caption">Figure 3.1:  A synthetic 2D dataset where each point is assigned one of two distinct labels</p>
</div>
</div>
<div id="k-nearest-neighbours-classifier" class="section level2">
<h2><span class="header-section-number">3.2</span> K-Nearest Neighbours Classifier</h2>
<p>Our main aim will be to “build” an algorithm
that takes a previously unobserved input sequence
<span class="math inline">\(\boldsymbol{x}&#39;\)</span> and which – based what we
know already, i.e., on <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>
generates a corresponding prediction, <span class="math inline">\(\hat{y}&#39;\)</span>
(hopefully being equal to the true <span class="math inline">\(y&#39;\)</span>, whatever it is).</p>
<p>For instance, we may have a set of medical records, where we
store patient data regarding their measurable health-relate parameters
like blood pressure, severity of different symptoms etc.
Moreover, each patient is labelled – we know if they have been diagnosed
a disease or not. But wait, there comes a new patient! We measure their
blood pressure, as some questions, input the data into our algorithm
and – based on the result – advise them to stay in bed.</p>
<div style="margin-top: 1em">

</div>
<p>In this chapter we will consider a very simple classifier,
based on an idea which dates back to at least 1950s (see <span class="citation">(Fix &amp; Hodges <a href="references.html#ref-knn1">1951</a>)</span>, <span class="citation">(Fix &amp; Hodges <a href="references.html#ref-knn2">1952</a>)</span>).</p>
<dl>
<dt>Rule.</dt>
<dd><p>“If you don’t know what to do in a situation, just act like the people around you”</p>
</dd>
</dl>
<p>For some integer <span class="math inline">\(K\ge 1\)</span>, the <em>K-Nearest Neighbour (K-NN) Classifier</em>
proceeds as follows. To classify a new point <span class="math inline">\(\boldsymbol{x}&#39;\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Find the <span class="math inline">\(K\)</span> nearest neighbours of a given point <span class="math inline">\(\boldsymbol{x}&#39;\)</span>
amongst the points in <span class="math inline">\(\mathbf{X}\)</span>, i.e.,
points <span class="math inline">\(\mathbf{x}_{i_1,\cdot}, \dots, \mathbf{x}_{i_K,\cdot}\)</span>
that are the closest to <span class="math inline">\(\boldsymbol{x}&#39;\)</span> w.r.t. the Euclidean distance:
<ol style="list-style-type: lower-alpha">
<li>compute the Euclidean distances between each <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> from the input set and <span class="math inline">\(\boldsymbol{x}&#39;\)</span>:
<span class="math display">\[ d_i = d(\mathbf{x}_{i,\cdot}, \boldsymbol{x}&#39;),\]</span></li>
<li>order <span class="math inline">\(d_i\)</span>s in increasing order,
and fetch the indices <span class="math inline">\(i_1,\dots,i_K\)</span>
which yield:
<span class="math display">\[ d_{i_1} \le d_{i_2} \le \dots \le d_{i_K}.\]</span>
<!--     c. pick first $K$ indices (these are the *nearest* neighbours). --></li>
</ol></li>
<li><p>Fetch the reference labels <span class="math inline">\(y_{i_1}, \dots, y_{i_K}\)</span>
corresponding to the <span class="math inline">\(K\)</span> nearest neighbours.</p></li>
<li><p>Return their <em>mode</em> as a result, i.e., the most frequently occurring label (also called <em>majority vote</em>).</p></li>
</ol>
<!-- > If a mode is not unique, return a randomly chosen mode (ties are broken at random). -->
<div style="margin-top: 1em">

</div>
<p>Let’s illustrate how a <span class="math inline">\(K\)</span>-NN classifier works on the above 2D synthetic dataset.
First we consider <span class="math inline">\(K=1\)</span>, see Figure <a href="chap-knn.html#fig:fig-plot-knn1">3.2</a>.
Dark and light regions depict how new points would be classified (class 0 and
1, respectively).
For instance, the point <span class="math inline">\((0, 2)\)</span> lays in the “dark zone”, therefore
we’d assign it label 0.</p>
<div class="figure"><span id="fig:fig-plot-knn1"></span>
<img src="030-knn-figures/fig-plot-knn1-1.svg" alt=" 1-NN class bounds for the 2D synthetic dataset" />
<p class="caption">Figure 3.2:  1-NN class bounds for the 2D synthetic dataset</p>
</div>
<p>We see that 1-NN is “greedy” or “lazy” in the sense that we just
locate the nearest point.
Increasing <span class="math inline">\(K\)</span> somehow smoothens the decision boundary (this makes it
less “local” and more “global”).
Figure <a href="chap-knn.html#fig:fig-plot-knn3">3.3</a> depicts the <span class="math inline">\(K=3\)</span> case.</p>
<div class="figure"><span id="fig:fig-plot-knn3"></span>
<img src="030-knn-figures/fig-plot-knn3-1.svg" alt=" 3-NN class bounds the our 2D synthetic dataset" />
<p class="caption">Figure 3.3:  3-NN class bounds the our 2D synthetic dataset</p>
</div>
<div class="figure"><span id="fig:fig-plot-knn15"></span>
<img src="030-knn-figures/fig-plot-knn15-1.svg" alt=" 15-NN class bounds the our 2D synthetic dataset" />
<p class="caption">Figure 3.4:  15-NN class bounds the our 2D synthetic dataset</p>
</div>
<p>The 15-NN classifier, see Figure <a href="chap-knn.html#fig:fig-plot-knn15">3.4</a>,
does a quite good job with identifying
a boundary between the two classes – the whole <span class="math inline">\(\mathbb{R}^2\)</span> space
seems to be (more or less) split into two disconnected subregions;
there are no “lakes” or “islands” or significant sizes.</p>
</div>
<div id="example-in-r" class="section level2">
<h2><span class="header-section-number">3.3</span> Example in R</h2>
<p>As a real-world illustration, let’s consider a subset of the Wine Quality dataset
(see Appendix <a href="appendix-datasets.html#sec:wine-quality">F.5</a> for more details)
that features a sample of white wines and includes only 3 of their
physicochemical characteristics:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" title="1">wine_train &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/wine_train.csv&quot;</span>)</a>
<a class="sourceLine" id="cb72-2" title="2"><span class="kw">head</span>(wine_train)</a></code></pre></div>
<pre><code>##   chlorides density volatile.acidity bad
## 1     0.056 0.99680             0.40   1
## 2     0.040 0.99062             0.18   0
## 3     0.050 0.99830             0.24   0
## 4     0.022 0.98915             0.42   0
## 5     0.021 0.99021             0.25   0
## 6     0.025 0.99071             0.30   0</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" title="1"><span class="kw">dim</span>(wine_train) <span class="co"># number of rows, number of columns</span></a></code></pre></div>
<pre><code>## [1] 300   4</code></pre>
<p>The last column determines whether experts claim that a given wine
is of low quality (class <code>1</code>, we don’t want that if we make a party)
or not (class <code>0</code>). We will use it as a
response variable in a wine classification task.</p>
<div class="remark"><strong>Remark.</strong>
<p>Note that above <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are stored together
as a single object, <span class="math inline">\([\mathbf{X}\ \mathbf{y}]\)</span>:</p>
<p><span class="math display">\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p} &amp; y_1\\
x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p} &amp; y_2\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots    &amp; \vdots\\
x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p} &amp; y_n\\
\end{array}
\right],
\]</span></p>
<p>In R, matrices (see Appendix <a href="appendix-rmatrix.html#appendix:rmatrix">D</a>)
are used to store data of the same type.
As <span class="math inline">\(\mathbf{X}\)</span> consists of real numbers,
we might sometimes run into a “type mismatch” error
if we want to extend it with <span class="math inline">\(\mathbf{y}\)</span> that is not numeric.
This is why such “combined” objects are usually
stored as R <code>data.frame</code>s (see Appendix <a href="appendix-rdf.html#appendix:rdf">E</a>);
this class allows columns of mixed types.</p>
</div>
<p>Let’s extract the input matrix
<span class="math inline">\(\mathbf{X}\in\mathbb{R}^{300\times 3}\)</span>
by taking the first 3 columns from the <code>wine_train</code> data frame
and the reference outputs <span class="math inline">\(\mathbf{y}\in\{0,1\}^{300}\)</span>
given by the last column.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" title="1">X_train &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(wine_train[,<span class="op">-</span><span class="dv">4</span>]) <span class="co"># all except the last column</span></a>
<a class="sourceLine" id="cb76-2" title="2">y_train &lt;-<span class="st"> </span><span class="kw">factor</span>(wine_train[,<span class="dv">4</span>])</a></code></pre></div>
<p>Now <span class="math inline">\([\mathbf{X}\ \mathbf{y}]\)</span> is a basis for an interesting (and challenging)
binary classification task. It constitutes our <em>training sample</em> – one
from which we actually <em>learn</em> what levels of chemical features make a
gathering with some vino a disappointing experience.
Yet, we don’t get a certificate in wine tasting only
to hang some fancy diploma in a golden frame on the wall.
We want to <em>apply</em> the knowledge we’ve gained for the bottles whose corks
(or caps) are yet to be popped off.</p>
<p>Here are some bottles for which we can show off our skills:</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" title="1">X_test &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">read.csv</span>(<span class="st">&quot;datasets/wine_test_X.csv&quot;</span>))</a>
<a class="sourceLine" id="cb77-2" title="2"><span class="kw">head</span>(X_test)</a></code></pre></div>
<pre><code>##      chlorides density volatile.acidity
## [1,]     0.056 0.99950             0.19
## [2,]     0.027 0.99240             0.33
## [3,]     0.054 0.99836             0.31
## [4,]     0.057 0.99654             0.28
## [5,]     0.041 0.99280             0.22
## [6,]     0.028 0.99176             0.14</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" title="1"><span class="kw">dim</span>(X_test)</a></code></pre></div>
<pre><code>## [1] 150   3</code></pre>
<p>We will denote is as
<span class="math inline">\(\mathbf{X}&#39;\in\mathbb{R}^{150\times3}\)</span>
and call it a <em>test sample</em>.
Let’s invoke the <code>knn()</code> function from package <code>FNN</code>
to classify the points <span class="math inline">\(\mathbf{X}&#39;\)</span> with the 5-nearest neighbour rule
so as to obtain the corresponding predicted <span class="math inline">\(\hat{\mathbf{y}}&#39;\)</span>.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" title="1"><span class="kw">library</span>(<span class="st">&quot;FNN&quot;</span>) <span class="co"># load the package</span></a></code></pre></div>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" title="1">y_pred &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, y_train, <span class="dt">k=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb82-2" title="2"><span class="kw">head</span>(y_pred, <span class="dv">32</span>) <span class="co"># predicted outputs</span></a></code></pre></div>
<pre><code>##  [1] 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<p>Great, we can now enjoy the bottles no.
1, 2, 5, 6, 7, 10, 11, 12, 13, 14, …
because the classifier claims they’re not bad.</p>
</div>
<div id="classifier-assessment" class="section level2">
<h2><span class="header-section-number">3.4</span> Classifier Assessment</h2>
<!--

More metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics

-->
<p>But wait, are the consumers <em>really</em> going to enjoy the aforementioned wines?
The 5-NN classifier might as well be deceiving us or be generating random
predictions. It wouldn’t be responsible to put it in production without making
sure it’s doing its job reasonably well.</p>
<p>Luckily, we have gathered information on the <em>true</em> labels for the
observations in the test set, <span class="math inline">\(\mathbf{y}&#39;\)</span>:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" title="1">y_test &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">read.csv</span>(<span class="st">&quot;datasets/wine_test_y.csv&quot;</span>)[,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb84-2" title="2"><span class="kw">head</span>(y_test, <span class="dv">32</span>) <span class="co"># true outputs</span></a></code></pre></div>
<pre><code>##  [1] 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<p>Comparing the displayed fragments of <code>y_test</code> and <code>y_pred</code> vectors
(32 items), we see that most of the predictions are valid, but there are
4 mismatches.
As there are 150 observations
in the test set, it’ll be better if we come up with some
synthetic metrics that could summarise the <em>overall</em> performance of
our classifier.</p>
<p><em>Accuracy</em> is perhaps the most straightforward descriptor, being
the ratio of the correctly classified instances to all the instances.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" title="1"><span class="kw">mean</span>(y_test <span class="op">==</span><span class="st"> </span>y_pred) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.71333</code></pre>
<p>Could be worse. In 71% cases
we make a correct prediction – we classify a wine as “not bad” when
it’s actually decent and label it as “bad” when it’s indeed horrible.</p>
<div class="remark"><strong>Remark.</strong>
<p>Recall from Appendix <a href="appendix-rvector.html#appendix:rvector">C</a> that the <code>==</code> operator
<code>in R works in an elementwise manner. It outputs a logical vector whose $i$-th element is</code>TRUE<code>whenever</code>y_test[i]<code>is equal to</code>y_pred[i]<code>. Calling</code>mean()<code>on a logical vector converts it to a numeric one: each</code>TRUE<code>is replaced with</code>1<code>and</code>FALSE<code>becomes</code>0<code>. Arithmetic mean is nothing else that the sum of elements divided by their count. The sum of</code>1<code>s and</code>0<code>s is actually the number of</code>1s<code>, i.e., for how many $i$s it holds that</code>y_test[i]<code>is equal to</code>y_pred[i]`.</p>
</div>
<p>Recall that <span class="math inline">\(y_i&#39;\)</span> is the true label associated with the <span class="math inline">\(i\)</span>-th observation
in the test set. Let <span class="math inline">\(\hat{y}_i&#39;\)</span> denote the classifier’s output for
a given <span class="math inline">\(\mathbf{x}_{i,\cdot}&#39;\)</span>.
In our case, the outputs are binary, i.e., <span class="math inline">\(\hat{y}_i&#39;, y_i&#39;\in\{0,1\}\)</span>.
Table <a href="chap-knn.html#tab:true-vs-predicted">3.2</a> lists the 4 possible scenarios –
all the distinct pairs of <span class="math inline">\((\hat{y}_i&#39;, y_i&#39;)\)</span>.</p>
<table>
<caption><span id="tab:true-vs-predicted">Table 3.2: </span> True vs. predicted labels in a binary classification task; ideally, the number of false positives and false negatives should be kept to a minimum</caption>
<thead>
<tr class="header">
<th>.</th>
<th><span class="math inline">\(y_i&#39;=0\)</span></th>
<th><span class="math inline">\(y_i&#39;=1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\hat{y}_i&#39;=0\)</span></td>
<td><strong>True Negative</strong></td>
<td>False Negative (Type II error)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{y}_i&#39;=1\)</span></td>
<td>False Positive (Type I error)</td>
<td><strong>True Positive</strong></td>
</tr>
</tbody>
</table>
<p>We’d be happy with as many true positives and true negatives as possible.
Note that the terms <strong>positive</strong> and <strong>negative</strong> refer to
the classifier’s output, i.e., they indicate whether
<span class="math inline">\(\hat{y}_i\)</span> is equal to <span class="math inline">\(1\)</span> and <span class="math inline">\(0\)</span>, respectively.</p>
<p>Let’s take a deeper look at why the classifier’s accuracy is “only”
71%.
To summarise the correctness of predictions for the whole sample,
we can compute the <em>confusion matrix</em>:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" title="1">(C &lt;-<span class="st"> </span><span class="kw">table</span>(y_pred, y_test))</a></code></pre></div>
<pre><code>##       y_test
## y_pred  0  1
##      0 76 29
##      1 14 31</code></pre>
<p>Actually, the classifier labels 29 bad wines as not-bad
and 14 not-bad wines as bad.</p>
<p>In many applications we deal with <em>unbalanced problems</em>, where
the case <span class="math inline">\(y_i=1\)</span> is relatively rare(r),
yet predicting it correctly is much more important than being
accurate with respect to class <span class="math inline">\(0\)</span> – think of medical applications,
e.g., HIV testing or tumour diagnosis.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" title="1"><span class="kw">table</span>(y_train)</a></code></pre></div>
<pre><code>## y_train
##   0   1 
## 201  99</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" title="1"><span class="kw">table</span>(y_test)</a></code></pre></div>
<pre><code>## y_test
##  0  1 
## 90 60</code></pre>
<p>In our case, the “not-bad” class is much more populous than the “bad” one.
Moreover, most will agree that it’s better to be surprised with a vino
labelled as bad, than disappointed with a highly recommended product.
Therefore, <em>accuracy</em> as a metric may fail to quantify what we are aiming for.</p>
<div class="remark"><strong>Remark.</strong>
<p>If only 1% of the cases have true <span class="math inline">\(y_i&#39;=1\)</span>,
then a dummy classifier that always
outputs <span class="math inline">\(\hat{y}_i&#39;=0\)</span> has 99% accuracy.</p>
</div>
<div class="remark"><strong>Remark.</strong>
<p>Accuracy can be computed from a confusion matrix based on the formula:</p>
<p><span class="math display">\[
\text{Accuracy} = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}.
\]</span></p>
<!--
= \frac{1}{n} \sum_{i=1}^n \mathbb{I}\left(
y_i = \hat{y}_i
\right)
where $\mathbb{I}$ is the indicator function,
$\mathbb{I}(l)=1$ if logical condition $l$ is true and $0$ otherwise.
-->
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" title="1">(C[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span><span class="kw">sum</span>(C) <span class="co"># equivalent to the above</span></a></code></pre></div>
<pre><code>## [1] 0.71333</code></pre>
</div>
<p>Metrics such as precision and recall (and their aggregated version, F-measure)
aim to address the above problem problem:</p>
<ul>
<li><p><em>Precision</em> answers the question: If the classifier outputs <span class="math inline">\(1\)</span>,
what is the probability that this is indeed true?</p>
<p><span class="math display">\[
  \text{Precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}.
  \]</span></p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" title="1">C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">1</span>]) <span class="co"># Precision</span></a></code></pre></div>
<pre><code>## [1] 0.68889</code></pre></li>
<li><p><em>Recall</em> (a.k.a. sensitivity, hit rate or true positive rate) –
If the true class is <span class="math inline">\(1\)</span>, what is the probability that the classifier
will detect it?</p>
<p><span class="math display">\[
  \text{Recall} = \frac{\text{TP}}{\text{TP}+\text{FN}}.
  \]</span></p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" title="1">C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">1</span>,<span class="dv">2</span>]) <span class="co"># Recall</span></a></code></pre></div>
<pre><code>## [1] 0.51667</code></pre></li>
</ul>
<div class="exercise"><strong>Exercise.</strong>
<p>Precision or recall? It depends on an application.
In each of the following settings, which measure is more important?</p>
<ul>
<li>medical diagnosis,</li>
<li>medical screening,</li>
<li>suggestions of potential matches in a dating app,</li>
<li>plagiarism detection,</li>
<li>wine recommendation.</li>
</ul>
</div>
<p>As too many metrics may be daunting for some, as a compromise,
we can use the <em>F-measure</em> (a.k.a. <span class="math inline">\(F_1\)</span>-measure),
which is the harmonic mean of precision
and recall:</p>
<p><span class="math display">\[
\text{F} = \frac{1}{
    \frac{
        \frac{1}{\text{Precision}}+\frac{1}{\text{Recall}}
    }{2}
}
=
\left(
\frac{1}{2}
\left(
\text{Precision}^{-1}+\text{Recall}^{-1}
\right)
\right)^{-1}
=
\frac{\text{TP}}{\text{TP} + \frac{\text{FP} + \text{FN}}{2}}.
\]</span></p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" title="1">C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">2</span>,<span class="dv">1</span>]) <span class="co"># F</span></a></code></pre></div>
<pre><code>## [1] 0.59048</code></pre>
<div class="exercise"><strong>Exercise.</strong>
<p>Show that the above equality indeed holds.</p>
</div>
<div class="remark"><strong>Remark.</strong>
<p>The following function will come in handy in the future:</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" title="1">get_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(y_pred, y_test)</a>
<a class="sourceLine" id="cb102-2" title="2">{</a>
<a class="sourceLine" id="cb102-3" title="3">    <span class="co"># first, let&#39;s make sure that both inputs are encoded</span></a>
<a class="sourceLine" id="cb102-4" title="4">    <span class="co"># as factors with the same levels:</span></a>
<a class="sourceLine" id="cb102-5" title="5">    all_levels &lt;-<span class="st"> </span><span class="kw">unique</span>(<span class="kw">c</span>(<span class="kw">as.character</span>(y_pred), <span class="kw">as.character</span>(y_test)))</a>
<a class="sourceLine" id="cb102-6" title="6">    y_pred &lt;-<span class="st"> </span><span class="kw">factor</span>(y_pred, <span class="dt">levels=</span>all_levels)</a>
<a class="sourceLine" id="cb102-7" title="7">    y_test &lt;-<span class="st"> </span><span class="kw">factor</span>(y_test, <span class="dt">levels=</span>all_levels)</a>
<a class="sourceLine" id="cb102-8" title="8">    <span class="co"># compute the confusion matrix:</span></a>
<a class="sourceLine" id="cb102-9" title="9">    C &lt;-<span class="st"> </span><span class="kw">table</span>(y_pred, y_test)</a>
<a class="sourceLine" id="cb102-10" title="10">    <span class="kw">stopifnot</span>(<span class="kw">dim</span>(C) <span class="op">==</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb102-11" title="11">    <span class="co"># fetch the metrics:</span></a>
<a class="sourceLine" id="cb102-12" title="12">    <span class="kw">c</span>(<span class="dt">Acc=</span>(C[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span><span class="kw">sum</span>(C), <span class="co"># accuracy</span></a>
<a class="sourceLine" id="cb102-13" title="13">      <span class="dt">Prec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># precision</span></a>
<a class="sourceLine" id="cb102-14" title="14">      <span class="dt">Rec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">1</span>,<span class="dv">2</span>]), <span class="co"># recall</span></a>
<a class="sourceLine" id="cb102-15" title="15">      <span class="dt">F=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># F-measure</span></a>
<a class="sourceLine" id="cb102-16" title="16">      <span class="co"># Confusion matrix items:</span></a>
<a class="sourceLine" id="cb102-17" title="17">      <span class="dt">TN=</span>C[<span class="dv">1</span>,<span class="dv">1</span>], <span class="dt">FN=</span>C[<span class="dv">1</span>,<span class="dv">2</span>],</a>
<a class="sourceLine" id="cb102-18" title="18">      <span class="dt">FP=</span>C[<span class="dv">2</span>,<span class="dv">1</span>], <span class="dt">TP=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb102-19" title="19">    ) <span class="co"># return a named vector</span></a>
<a class="sourceLine" id="cb102-20" title="20">}</a></code></pre></div>
<p>For example:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" title="1"><span class="kw">get_metrics</span>(y_pred, y_test)</a></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.71333  0.68889  0.51667  0.59048 76.00000 29.00000 14.00000 31.00000</code></pre>
</div>
</div>
<div id="classifier-selection" class="section level2">
<h2><span class="header-section-number">3.5</span> Classifier Selection</h2>
<p>Note that the nearest neighbour scheme implies in fact a whole
family of classifiers. For each <span class="math inline">\(K\)</span>, the corresponding K-NN method
can be thought of as a different algorithm.
Therefore, it is wise to pose the question:
how to choose the best <span class="math inline">\(K\)</span> for K-NN classification?</p>
<p>Here, by the <em>best</em> we mean one that has the highest <em>predictive power</em>,
which we quantify by means of some chosen metric (such as accuracy, recall, precision, F-measure, etc.).</p>
<p>Let’s study how the performance metrics change when we vary
the number of nearest neighbours, <span class="math inline">\(K\)</span>.
Then, we’ll choose the parameter that corresponds to, say, the greatest
F-measure. However, we should not compute this metric on the test set!
There is always a risk that we can <em>overfit</em> to current data –
construct a classifier
that performs extremely well on the samples we have but
does not <em>generalise</em> well to the ones to come. The bottles we inspected
is just a drop in the wine ocean that our machine learning solution
will be filtering.</p>
<p>We are lucky though as we have one more wine sample available!</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" title="1">X_validate &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">read.csv</span>(<span class="st">&quot;datasets/wine_validate_X.csv&quot;</span>))</a>
<a class="sourceLine" id="cb105-2" title="2">y_validate &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">read.csv</span>(<span class="st">&quot;datasets/wine_validate_y.csv&quot;</span>)[,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb105-3" title="3"><span class="kw">dim</span>(X_validate)</a></code></pre></div>
<pre><code>## [1] 150   3</code></pre>
<p>We will call it a <em>validation</em> (or development) set and use it
for determining the optimal <span class="math inline">\(K\)</span>. Then the <em>test</em> set will be recalled
so as to perform the final evaluation (it’s going to mimic the wines
to come).</p>
<div style="margin-top: 1em">

</div>
<p>The following function computes the performance metrics
for the <span class="math inline">\(K\)</span>-NN classifier as a function of <span class="math inline">\(K\)</span>:</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb107-1" title="1">knn_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(K, X_train, X_validate, y_train, y_validate)</a>
<a class="sourceLine" id="cb107-2" title="2">{</a>
<a class="sourceLine" id="cb107-3" title="3">    y_pred &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_validate, y_train, <span class="dt">k=</span>K) <span class="co"># classify</span></a>
<a class="sourceLine" id="cb107-4" title="4">    <span class="kw">get_metrics</span>(y_pred, y_validate)</a>
<a class="sourceLine" id="cb107-5" title="5">}</a></code></pre></div>
<p>For example:</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" title="1"><span class="kw">knn_metrics</span>(<span class="dv">5</span>, X_train, X_validate, y_train, y_validate)</a></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.65333  0.40625  0.28261  0.33333 85.00000 33.00000 19.00000 13.00000</code></pre>
<p>Let’s evaluate the performance metrics as a function of different odd
(compare Section @(sec:mode)) <span class="math inline">\(K\)</span>s:</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb110-1" title="1">Ks &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">19</span>, <span class="dt">by=</span><span class="dv">2</span>) <span class="co"># 1, 3, 5, ...</span></a>
<a class="sourceLine" id="cb110-2" title="2">Ps &lt;-<span class="st"> </span><span class="kw">sapply</span>(Ks, <span class="co"># on each element in this vector</span></a>
<a class="sourceLine" id="cb110-3" title="3">        knn_metrics,     <span class="co"># apply this function</span></a>
<a class="sourceLine" id="cb110-4" title="4">        X_train, X_validate, y_train, y_validate <span class="co"># aux args</span></a>
<a class="sourceLine" id="cb110-5" title="5">    )</a>
<a class="sourceLine" id="cb110-6" title="6"><span class="co"># convert to &quot;vertical&quot; form - each K in separate row:</span></a>
<a class="sourceLine" id="cb110-7" title="7">Ps &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">t</span>(Ps))</a></code></pre></div>
<div class="remark"><strong>Remark.</strong>
<p>Note that <code>sapply(X, f, arg1, arg2, ...)</code>
outputs a matrix <code>Z</code> such that it’s <span class="math inline">\(i\)</span>-th column is
<code>f(X[i], arg1, arg2, ...)</code>. We transpose it by calling <code>t()</code> to
get a “vertical” (long) representation.</p>
</div>
<!--

We transpose this result, `t()`, in order to get each metric
corresponding to different columns in the result.
As usual, if you keep wondering, e.g.,  why `t()`, play with
the code yourself -- it's fun fun fun.

-->
<p>Example results:</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" title="1"><span class="kw">round</span>(<span class="kw">cbind</span>(<span class="dt">K=</span>Ks, Ps), <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##     K  Acc Prec  Rec    F TN FN FP TP
## 1   1 0.57 0.30 0.30 0.30 72 32 32 14
## 2   3 0.63 0.34 0.24 0.28 83 35 21 11
## 3   5 0.65 0.41 0.28 0.33 85 33 19 13
## 4   7 0.64 0.37 0.24 0.29 85 35 19 11
## 5   9 0.65 0.38 0.20 0.26 89 37 15  9
## 6  11 0.69 0.48 0.26 0.34 91 34 13 12
## 7  13 0.68 0.46 0.24 0.31 91 35 13 11
## 8  15 0.69 0.48 0.22 0.30 93 36 11 10
## 9  17 0.70 0.53 0.22 0.31 95 36  9 10
## 10 19 0.71 0.60 0.20 0.30 98 37  6  9</code></pre>
<p>Figure <a href="chap-knn.html#fig:whichK5">3.5</a> is worth a thousand tables though
(see <code>?matplot</code> in R).
It seems that precision tends to slightly increase, whereas recall – decrease
as <span class="math inline">\(K\)</span> increases (on this dataset, it’s not a general rule).</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" title="1"><span class="kw">matplot</span>(Ks, Ps[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dt">xlab=</span><span class="st">&quot;K&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Metric&quot;</span>,</a>
<a class="sourceLine" id="cb113-2" title="2">    <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">lty=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">type=</span><span class="st">&quot;b&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb113-3" title="3"><span class="kw">legend</span>(<span class="st">&quot;top&quot;</span>, <span class="dt">legend=</span><span class="kw">names</span>(Ps[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>]),</a>
<a class="sourceLine" id="cb113-4" title="4">    <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">lty=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">ncol=</span><span class="dv">4</span>, <span class="dt">bg=</span><span class="st">&quot;white&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:whichK5"></span>
<img src="030-knn-figures/whichK5-1.svg" alt=" Performance of K-NN classifiers on the validation set as a function of K" />
<p class="caption">Figure 3.5:  Performance of <span class="math inline">\(K\)</span>-NN classifiers on the validation set as a function of <span class="math inline">\(K\)</span></p>
</div>
<!--

(\*) **ROC** (Receiver Operating Characteristic) curve:


```r
TPR <- Ps$TP/(Ps$TP+Ps$FN) # True Positive Rate (recall)
FPR <- Ps$FP/(Ps$FP+Ps$TN) # False Positive Rate
plot(FPR, TPR, asp=1, xlim=c(0,1), ylim=c(0,1))
abline(a=0, b=1, lty=3)
```

![(\#fig:unnamed-chunk-1) plot of chunk unnamed-chunk-1](030-knn-figures/unnamed-chunk-1-1)

-->
<p>It’s very interesting that precision and recall (and hence F-measure)
are much lower on the validation set than on the test set.
This might be due to the small dataset sizes. As both of them have been
generated independently and at random, and they constitute a representative
sample of the population of all wines, the truth perhaps lies somewhere
“in-between” the reported metrics.</p>
<p>Anyway, we get the best F-measure for <span class="math inline">\(K=11\)</span>.
Let’s see how well it is likely to perform “in production”,
i.e., on the test set:</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" title="1"><span class="kw">knn_metrics</span>(<span class="dv">11</span>, X_train, X_test, y_train, y_test)</a></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.66000  0.65517  0.31667  0.42697 80.00000 41.00000 10.00000 19.00000</code></pre>
<p>This classifier is actually worse than the 5-NN one on the test set,
but there is nothing we can do about it.
It is an example of best practice to stick to the parameter we have
identified as the optimal one on the validation sample –
we have promised to treat the test set as an independent
judge in our case (unless new data and more evidence come and we will
be in a position to reevaluate/reconstruct our algorithm).
We should treat this model with caution (actually, this is true for every model).
No worries, we will encounter dilemma and disappointments of this
kind in our everyday data science activities (see the next Chapter for more).</p>
<!--

```r
#knn_metrics(5, X_train, X_test, y_train, y_test)
```
-->
</div>
<div id="implementing-a-k-nn-classifier" class="section level2">
<h2><span class="header-section-number">3.6</span> Implementing a K-NN Classifier (*)</h2>
<div id="main-routine" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Main Routine</h3>
<p>To show that machine learning methods are not magical creatures who
we should be terrified of, but rather the fruit of work of programmers
just like us (have you ever thought of volunteering in an open source
programming project for the good of the whole community?),
let’s implement a K-NN classifier ourselves, from scratch.</p>
<p>We’ll use a top-bottom approach,
starting with a general description of the admissible inputs
and the expected output. Then we’ll arrange the workflow for processing
of data into conveniently manageable chunks.</p>
<p>The function’s “declaration” will look like:</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" title="1">our_knn &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, y_train, <span class="dt">k=</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb116-2" title="2">    <span class="co"># k=1 denotes a parameter with a default value</span></a>
<a class="sourceLine" id="cb116-3" title="3">    <span class="co"># ... (see below) ...</span></a>
<a class="sourceLine" id="cb116-4" title="4">}</a></code></pre></div>
<p>It’s advisable to specify the type and form of the arguments
we’re expecting on input. The <code>stopifnot()</code> function verifies if a given
logical condition is true. If not, an error is raised.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" title="1"><span class="co"># this is the body of our_knn() - part 1</span></a>
<a class="sourceLine" id="cb117-2" title="2"><span class="kw">stopifnot</span>(<span class="kw">is.numeric</span>(X_train), <span class="kw">is.matrix</span>(X_train))</a>
<a class="sourceLine" id="cb117-3" title="3"><span class="kw">stopifnot</span>(<span class="kw">is.numeric</span>(X_test), <span class="kw">is.matrix</span>(X_test))</a>
<a class="sourceLine" id="cb117-4" title="4"><span class="kw">stopifnot</span>(<span class="kw">is.factor</span>(y_train))</a>
<a class="sourceLine" id="cb117-5" title="5"><span class="kw">stopifnot</span>(<span class="kw">ncol</span>(X_train) <span class="op">==</span><span class="st"> </span><span class="kw">ncol</span>(X_test))</a>
<a class="sourceLine" id="cb117-6" title="6"><span class="kw">stopifnot</span>(<span class="kw">nrow</span>(X_train) <span class="op">==</span><span class="st"> </span><span class="kw">length</span>(y_train))</a>
<a class="sourceLine" id="cb117-7" title="7"><span class="kw">stopifnot</span>(k <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>, k <span class="op">&lt;=</span><span class="st"> </span><span class="kw">length</span>(y_train))</a>
<a class="sourceLine" id="cb117-8" title="8">n_train &lt;-<span class="st"> </span><span class="kw">nrow</span>(X_train)</a>
<a class="sourceLine" id="cb117-9" title="9">n_test  &lt;-<span class="st"> </span><span class="kw">nrow</span>(X_test)</a>
<a class="sourceLine" id="cb117-10" title="10">p &lt;-<span class="st"> </span><span class="kw">ncol</span>(X_train)</a>
<a class="sourceLine" id="cb117-11" title="11">L &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">levels</span>(y_train))</a></code></pre></div>
<p>Therefore, we may assume from now on that
<span class="math inline">\(\mathtt{X\_train}\in\mathbb{R}^{\mathtt{n\_train}\times \mathtt{p}}\)</span>,
<span class="math inline">\(\mathtt{X\_test}\in\mathbb{R}^{\mathtt{n\_test}\times \mathtt{p}}\)</span> and
<span class="math inline">\(\mathtt{y\_train}\in\{1,\dots,L\}^{\mathtt{n\_train}}\)</span>.
Recall that R <code>factor</code> objects are internally encoded as integer vectors.</p>
<p>Next, we’ll call the (to-be-done) function <code>our_get_knnx()</code>,
which seeks nearest neighbours of all the points:</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" title="1"><span class="co"># our_get_knnx returns a matrix nn_indices of size n_test*k,</span></a>
<a class="sourceLine" id="cb118-2" title="2"><span class="co"># where nn_indices[i,j] denotes the index of</span></a>
<a class="sourceLine" id="cb118-3" title="3"><span class="co"># X_test[i,]&#39;s j-th nearest neighbour in X_train.</span></a>
<a class="sourceLine" id="cb118-4" title="4"><span class="co"># (It is the point X_train[nn_indices[i,j],]).</span></a>
<a class="sourceLine" id="cb118-5" title="5">nn_indices &lt;-<span class="st"> </span><span class="kw">our_get_knnx</span>(X_train, X_test, k)</a></code></pre></div>
<p>Then, for each point in <code>X_test</code>,
we fetch the labels corresponding to its nearest neighbours
and compute their mode (<code>our_mode()</code> function, wait for it):</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb119-1" title="1">y_pred &lt;-<span class="st"> </span><span class="kw">numeric</span>(n_test) <span class="co"># vector of length n_test</span></a>
<a class="sourceLine" id="cb119-2" title="2"><span class="co"># For now we will operate on the integer labels in {1,...,M}</span></a>
<a class="sourceLine" id="cb119-3" title="3">y_train_int &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(y_train)</a>
<a class="sourceLine" id="cb119-4" title="4"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n_test) {</a>
<a class="sourceLine" id="cb119-5" title="5">    <span class="co"># Get the labels of the NNs of the i-th point:</span></a>
<a class="sourceLine" id="cb119-6" title="6">    nn_labels_i &lt;-<span class="st"> </span>y_train_int[nn_indices[i,]]</a>
<a class="sourceLine" id="cb119-7" title="7">    <span class="co"># Compute the mode (majority vote):</span></a>
<a class="sourceLine" id="cb119-8" title="8">    y_pred[i] &lt;-<span class="st"> </span><span class="kw">our_mode</span>(nn_labels_i) <span class="co"># in {1,...,M}</span></a>
<a class="sourceLine" id="cb119-9" title="9">}</a></code></pre></div>
<p>Finally, we should convert the resulting integer vector
to an object of type <code>factor</code>:</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" title="1"><span class="co"># Convert y_pred to factor:</span></a>
<a class="sourceLine" id="cb120-2" title="2"><span class="kw">return</span>(<span class="kw">factor</span>(y_pred, <span class="dt">labels=</span><span class="kw">levels</span>(y_train)))</a></code></pre></div>
<!--
**Test-driven development** -- before writing


```r
test_our_knn <- function() {
    # ...
}
```



```r
test_our_mode <- function() {
    stopifnot(our_mode(c(1, 1, 1, 1)) == 1)
    stopifnot(our_mode(c(2, 2, 2, 2)) == 2)
    stopifnot(our_mode(c(3, 1, 3, 3)) == 3)
    stopifnot(our_mode(c(1, 1, 3, 3, 2)) %in% c(1, 3))
}
```


-->
</div>
<div id="sec:mode" class="section level3">
<h3><span class="header-section-number">3.6.2</span> Mode</h3>
<p>To implement the mode, we can use the <code>tabulate()</code> function.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Read the function’s documentation, see <code>?tabulate</code>.</p>
</div>
<p>For example:</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb121-1" title="1"><span class="kw">tabulate</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 4 2 0 0 1</code></pre>
<p>We should keep in mind that there might be multiple modes –
in such a case, it’s best to pick one at random (to avoid any bias).
For that, we can use the <code>sample()</code> function.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Read the function’s man page, see <code>?sample</code>.
Note that, dangerously, its behaviour is different when its first argument
is a vector of length 1.</p>
</div>
<p>An example implementation:</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb123-1" title="1">our_mode &lt;-<span class="st"> </span><span class="cf">function</span>(Y) {</a>
<a class="sourceLine" id="cb123-2" title="2">    <span class="co"># tabulate() will take care of</span></a>
<a class="sourceLine" id="cb123-3" title="3">    <span class="co"># checking the correctness of Y</span></a>
<a class="sourceLine" id="cb123-4" title="4">    t &lt;-<span class="st"> </span><span class="kw">tabulate</span>(Y) <span class="co"># factors == integer vectors here</span></a>
<a class="sourceLine" id="cb123-5" title="5">    <span class="co"># get indices corresponding to the maximal counts</span></a>
<a class="sourceLine" id="cb123-6" title="6">    mode_candidates &lt;-<span class="st"> </span><span class="kw">which</span>(t <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(t))</a>
<a class="sourceLine" id="cb123-7" title="7">    <span class="cf">if</span> (<span class="kw">length</span>(mode_candidates) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) <span class="kw">return</span>(mode_candidates)</a>
<a class="sourceLine" id="cb123-8" title="8">    <span class="cf">else</span> <span class="kw">return</span>(<span class="kw">sample</span>(mode_candidates, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb123-9" title="9">}</a></code></pre></div>
<p>Some tests:</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>))</a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 2</code></pre>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>))</a></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb130-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb132-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<!-- TODO: test-driven development, unit tests -- mention -->
</div>
<div id="nn-search-methods" class="section level3">
<h3><span class="header-section-number">3.6.3</span> NN Search Methods</h3>
<p>Last but not least, we get to implement the <code>our_get_knnx()</code> function.
It’s the function responsible for seeking the indices of nearest neighbours.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" title="1"><span class="co"># our_get_knnx returns a matrix nn_indices of size n_test*k,</span></a>
<a class="sourceLine" id="cb134-2" title="2"><span class="co"># where nn_indices[i,j] denotes the index of</span></a>
<a class="sourceLine" id="cb134-3" title="3"><span class="co"># X_test[i,]&#39;s j-th nearest neighbour in X_train.</span></a>
<a class="sourceLine" id="cb134-4" title="4"><span class="co"># (It is the point X_train[nn_indices[i,j],]).</span></a>
<a class="sourceLine" id="cb134-5" title="5">our_get_knnx &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, k) {</a>
<a class="sourceLine" id="cb134-6" title="6">    <span class="co"># ...</span></a>
<a class="sourceLine" id="cb134-7" title="7">}</a></code></pre></div>
<p>A naive approach to <code>our_get_knnx()</code> relies on computing
all pairwise distances,
and sorting them.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb135-1" title="1">our_get_knnx &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, k) {</a>
<a class="sourceLine" id="cb135-2" title="2">    n_test &lt;-<span class="st"> </span><span class="kw">nrow</span>(X_test)</a>
<a class="sourceLine" id="cb135-3" title="3">    nn_indices &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA_real_</span>, <span class="dt">nrow=</span>n_test, <span class="dt">ncol=</span>k)</a>
<a class="sourceLine" id="cb135-4" title="4">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n_test) {</a>
<a class="sourceLine" id="cb135-5" title="5">        d &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">1</span>, <span class="cf">function</span>(x)</a>
<a class="sourceLine" id="cb135-6" title="6">            <span class="kw">sqrt</span>(<span class="kw">sum</span>((x<span class="op">-</span>X_test[i,])<span class="op">^</span><span class="dv">2</span>)))</a>
<a class="sourceLine" id="cb135-7" title="7">        <span class="co"># now d[j] is the distance</span></a>
<a class="sourceLine" id="cb135-8" title="8">        <span class="co"># between X_train[j,] and X_test[i,]</span></a>
<a class="sourceLine" id="cb135-9" title="9">        nn_indices[i,] &lt;-<span class="st"> </span><span class="kw">order</span>(d)[<span class="dv">1</span><span class="op">:</span>k]</a>
<a class="sourceLine" id="cb135-10" title="10">    }</a>
<a class="sourceLine" id="cb135-11" title="11">    <span class="kw">return</span>(nn_indices)</a>
<a class="sourceLine" id="cb135-12" title="12">}</a></code></pre></div>
<p>A comparison with <code>FNN::knn()</code> with regards to time needed to run
the algorithms:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb136-1" title="1"><span class="kw">library</span>(<span class="st">&quot;microbenchmark&quot;</span>)</a>
<a class="sourceLine" id="cb136-2" title="2"><span class="kw">summary</span>(<span class="kw">microbenchmark</span>(</a>
<a class="sourceLine" id="cb136-3" title="3">    <span class="dt">our=</span>{A&lt;-<span class="kw">our_knn</span>(X_train, X_test, y_train, <span class="dt">k=</span><span class="dv">5</span>)},</a>
<a class="sourceLine" id="cb136-4" title="4">    <span class="dt">fnn=</span>{B&lt;-FNN<span class="op">::</span><span class="kw">knn</span>(X_train, X_test, y_train, <span class="dt">k=</span><span class="dv">5</span>)},</a>
<a class="sourceLine" id="cb136-5" title="5">    <span class="dt">unit=</span><span class="st">&quot;ms&quot;</span> <span class="co">#  # milliseconds</span></a>
<a class="sourceLine" id="cb136-6" title="6">))</a></code></pre></div>
<pre><code>##   expr    min    lq    mean  median     uq     max neval
## 1  our 101.70 107.4 112.859 110.641 114.03 200.462   100
## 2  fnn  10.09  10.6  11.638  11.124  12.33  17.205   100</code></pre>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" title="1"><span class="kw">mean</span>(A <span class="op">==</span><span class="st"> </span>B) <span class="co"># 1.0 on perfect match</span></a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>Both functions return identical results but our implementation is 10x slower.
It turns out that <code>our_get_knnx()</code> is the part that actually constitutes
the K-NN classifier’s performance bottleneck in case on big data samples.
<code>FNN::knn()</code> is efficiently written in C++, which is a compiled programming language.</p>
<p>R, on the other hand (just like Python and Matlab) is interpreted, therefore –
as a rule of thumb – we should consider it an order of magnitude slower
(see, however, the Julia language).</p>
<p>Let’s substitute our naive implementation of <code>our_get_knnx()</code>
with the equivalent one,
but written in C++ (available in the <code>FNN</code> package).</p>
<div class="remark"><strong>Remark.</strong>
<p>Note that we can write a C++ implementation ourselves,
see the <code>Rcpp</code> package <span class="citation">(Eddelbuettel <a href="references.html#ref-rcpp">2013</a>)</span> for convenient R and C++ integration.</p>
</div>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb140-1" title="1">our_get_knnx &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, k) {</a>
<a class="sourceLine" id="cb140-2" title="2">    <span class="co"># this is used by our_knn()</span></a>
<a class="sourceLine" id="cb140-3" title="3">    FNN<span class="op">::</span><span class="kw">get.knnx</span>(X_train, X_test, k, <span class="dt">algorithm=</span><span class="st">&quot;brute&quot;</span>)<span class="op">$</span>nn.index</a>
<a class="sourceLine" id="cb140-4" title="4">}</a>
<a class="sourceLine" id="cb140-5" title="5"></a>
<a class="sourceLine" id="cb140-6" title="6"><span class="kw">summary</span>(<span class="kw">microbenchmark</span>(</a>
<a class="sourceLine" id="cb140-7" title="7">    <span class="dt">our =</span>{A&lt;-<span class="kw">our_knn</span>(X_train, X_test, y_train, <span class="dt">k=</span><span class="dv">5</span>)},</a>
<a class="sourceLine" id="cb140-8" title="8">    <span class="dt">fnn1=</span>{B&lt;-FNN<span class="op">::</span><span class="kw">knn</span>(X_train, X_test, y_train, <span class="dt">k=</span><span class="dv">5</span>)}, <span class="co"># kd_tree, see below</span></a>
<a class="sourceLine" id="cb140-9" title="9">    <span class="dt">fnn2=</span>{C&lt;-FNN<span class="op">::</span><span class="kw">knn</span>(X_train, X_test, y_train, <span class="dt">k=</span><span class="dv">5</span>, <span class="dt">algorithm=</span><span class="st">&quot;brute&quot;</span>)},</a>
<a class="sourceLine" id="cb140-10" title="10">    <span class="dt">unit=</span><span class="st">&quot;ms&quot;</span> <span class="co"># milliseconds</span></a>
<a class="sourceLine" id="cb140-11" title="11">))</a></code></pre></div>
<pre><code>##   expr     min       lq     mean   median       uq     max neval
## 1  our 0.84474  0.90961  0.95866  0.93309  0.96688  2.1751   100
## 2 fnn1 9.76447 10.33771 10.98574 10.71992 11.23307 17.1421   100
## 3 fnn2 9.81180 10.40005 10.96215 10.66986 11.20543 13.7569   100</code></pre>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb142-1" title="1"><span class="kw">mean</span>(A <span class="op">==</span><span class="st"> </span>B) <span class="co"># 1.0 on perfect match</span></a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>The timings are really interesting, taking into account
that <code>FNN::knn()</code> uses <code>FNN::get.knnx()</code> as well
(the R package ecosystem is comprised of open source software,
we have the freedom to read the function’s source code
by calling <code>print(FNN::knn)</code>). Of course, before
drawing conclusions about the quality of our implementation, we should
test the aforementioned procedures on samples of different sizes and dimensionalities. This is left to the reader as an:</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Test the run-times of the aforementioned procedures on samples
of different sizes and dimensionalities, for example, on
datasets generated randomly.</p>
</div>
<div style="margin-top: 1em">

</div>
<p>Before we conclude, let us note that there are special
<em>spatial search data structures</em>
– such as metric trees – that aim to speed up searching for nearest
neighbours in <em>low-dimensional spaces</em> (for small <span class="math inline">\(p\)</span>).
For example, <code>FNN::get.knnx()</code> also implements the so-called
kd-trees.</p>
<!--
 vp-trees, gnats, kd-trees, FANN
-->
<p>Here is a function that generates <code>n</code> random points in a <code>p</code>
dimensional hypercube. The function fill report time taken
to look up <code>k</code> nearest neighbours based on a brute-force algorithm
(all pairs of points considered) vs. on the kd-tree.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb144-1" title="1">test_speed &lt;-<span class="st"> </span><span class="cf">function</span>(n, p, k) {</a>
<a class="sourceLine" id="cb144-2" title="2">    A &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n<span class="op">*</span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p)</a>
<a class="sourceLine" id="cb144-3" title="3">    s &lt;-<span class="st"> </span><span class="kw">summary</span>(microbenchmark<span class="op">::</span><span class="kw">microbenchmark</span>(</a>
<a class="sourceLine" id="cb144-4" title="4">        <span class="dt">brute_force=</span>FNN<span class="op">::</span><span class="kw">get.knnx</span>(A, A, k, <span class="dt">algorithm=</span><span class="st">&quot;brute&quot;</span>),</a>
<a class="sourceLine" id="cb144-5" title="5">        <span class="dt">kd_tree=</span>FNN<span class="op">::</span><span class="kw">get.knnx</span>(A, A, k, <span class="dt">algorithm=</span><span class="st">&quot;kd_tree&quot;</span>),</a>
<a class="sourceLine" id="cb144-6" title="6">        <span class="dt">times=</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb144-7" title="7">    ), <span class="dt">unit=</span><span class="st">&quot;s&quot;</span>)</a>
<a class="sourceLine" id="cb144-8" title="8">    <span class="co"># report minimum of each 3 time measurements:</span></a>
<a class="sourceLine" id="cb144-9" title="9">    <span class="kw">return</span>(<span class="kw">structure</span>(s<span class="op">$</span>min, <span class="dt">names=</span><span class="kw">as.character</span>(s<span class="op">$</span>expr)))</a>
<a class="sourceLine" id="cb144-10" title="10">}</a></code></pre></div>
<p>Example timings:</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" title="1"><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">2</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>## brute_force     kd_tree 
##    0.288927    0.012373</code></pre>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb147-1" title="1"><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">5</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>## brute_force     kd_tree 
##    0.406603    0.060394</code></pre>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb149-1" title="1"><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">10</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>## brute_force     kd_tree 
##     0.63985     0.63335</code></pre>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb151-1" title="1"><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">20</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>## brute_force     kd_tree 
##      1.2351      5.2283</code></pre>
<p>In spaces of higher dimensionality, the brute force algorithm
is actually faster.
It turns out that searching in high-dimensional spaces is hard due
to the various phenomena collectively referred to as the
<em>curse of dimensionality</em> (see, e.g., <span class="citation">(Blum et al. <a href="references.html#ref-foundds">2020</a>)</span>).
Yet, is low-dimensional data boring? Well, our physical
world is perceived as 3-dimensional, spatial data (on maps) is 2-dimensional,
therefore, there are interesting use cases of kd-trees anyway.</p>
<!--

## Exercises

TODO

another pre-fabricated train/test split?


-->
</div>
</div>
<div id="remarks-1" class="section level2">
<h2><span class="header-section-number">3.7</span> Remarks</h2>
<p><strong>TODO</strong> …..</p>
<p>Note that the K-nearest neighbour method is suitable
for any multiclass classification, i.e., for any number of levels <span class="math inline">\(L\)</span>
of <span class="math inline">\(\mathbf{y}\)</span>. However, precision and recall can only be computed
if <span class="math inline">\(L=2\)</span>.</p>
<p>In practice searching for nearest neighbours is time-consuming
for larger datasets – to
classify a single point we have to query the whole training set
(unless it’s a space of low dimensionality).</p>
<p>Note that our implementation
requires <span class="math inline">\(c\cdot n_\text{test}\cdot n_\text{train}\cdot p\)</span>
arithmetic operations for some <span class="math inline">\(c&gt;1\)</span>.
The overall cost of sorting is at least <span class="math inline">\(d\cdot n_\text{test}\cdot n_\text{train}\cdot\log n_\text{train}\)</span>
for some <span class="math inline">\(d&gt;1\)</span>.
This does not scale well with both <span class="math inline">\(n_\text{test}\)</span> and <span class="math inline">\(n_\text{train}\)</span>
(think – big data).</p>
<p>Moreover, the training set should be available at all times.
Other algorithms discussed in this book will try to come up with
a synthetic/compressed representation (a model) of the training set.</p>
<p>However, the advantage of K-NN is that it naturally adapts to new training
points – we don’t have to recompute any models to take them into account
(hence, we can say, that K-NN can also work in an *online mode).</p>
<p>K-NN is an influential concept – see Chapter <a href="chap-recommenders.html#chap:recommenders">14</a>
for how it is naturally employed in recommender systems.</p>
<!--TODO: exercise -- density, not NN-based classification
regression - consider the epsilon-neighbourhood.-->
<p>Instead of nearest neighbours, we could be also considering so-called
<span class="math inline">\(\epsilon\)</span>-neighbourhoods – all the points from <span class="math inline">\(\mathbf{X}\)</span>
whose distance to a given <span class="math inline">\(\boldsymbol{x}&#39;\)</span> is not greater than
<span class="math inline">\(\epsilon\)</span> (for example, DBSCAN <span class="citation">(Ling <a href="references.html#ref-pre_dbscan">1973</a>)</span> <span class="citation">(Ester et al. <a href="references.html#ref-dbscan">1996</a>)</span>
is a clustering method based on this concept).
However, in classification task we should be aware of the fact that
some test points might have empty <span class="math inline">\(\epsilon\)</span>-neighbourhoods and that
this issue should be resolved somehow.</p>
<p>There are also approximate nearest neighbours, e.g., nmslib,
faiss, etc., see also <a href="https://github.com/erikbern/ann-benchmarks" class="uri">https://github.com/erikbern/ann-benchmarks</a></p>
<div style="margin-top: 1em">

</div>
<p>(*) 1-NN classification is essentially based
on a dataset’s so-called Voronoi diagram.
Interestingly, in single linkage clustering, we also seek 1-nearest neighbours
(between clusters).</p>
<p>Recommended further reading: <span class="citation">(Hastie et al. <a href="references.html#ref-esl">2017</a>: Section 13.3)</span></p>
<p>Next Chapter….</p>
<p>Further we will discuss some other well-known classifiers:</p>
<ul>
<li><em>Decision trees</em></li>
<li><em>Logistic regression</em></li>
</ul>

<!--
kate: indent-width 4; word-wrap-column 74; default-dictionary en_AU
Copyright (C) 2020, Marek Gagolewski, https://www.gagolewski.com
This material is licensed under the Creative Commons BY-NC-ND 4.0 License.
-->
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap-hclust.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-feature-engineering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

</body>

</html>
