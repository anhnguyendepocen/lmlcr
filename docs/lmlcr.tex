\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[10pt,b5paper,krantz1]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Lightweight Machine Learning Classics with R},
            pdfauthor={Marek Gagolewski},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

% Copyright (C) 2020, Marek Gagolewski, https://www.gagolewski.com

\RequireXeTeX %Force XeTeX check

%\usepackage{xcolor}
%\definecolor{darkgray}{cmyk}{0.0,0.9,0.0,0.95}
\usepackage{amsmath}%
\usepackage{amssymb}
\usepackage{wasysym}%
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multicol}
%\usepackage{fixltx2e}
%\usepackage{fix-cm}
%\usepackage{bookmark}
%\usepackage{makeidx}

\frenchspacing
\tolerance=5000

% \newenvironment{exercise}{%
% \bigskip\noindent\textbf{Exercise. }%
% \it\ignorespaces%
% }{\ignorespaces%
% \hfill$\square$%
% }


\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}[chapter]
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{proof}{Proof}



\newenvironment{solution}{%
\bigskip\noindent\textbf{Solution. }%
\it\ignorespaces%
\ignorespaces%
}{\ignorespaces%
\hfill$\blacksquare$%
}

\setlength\paperheight{9in}
\setlength\paperwidth{6in}
\setlength\voffset{-0.5in}
\setlength\hoffset{-1.5in}


\setmainfont[
    UprightFeatures={SmallCapsFont=AlegreyaSC-Regular}
]{Alegreya}
\setsansfont{Alegreya Sans}
\setmonofont[Scale=0.9]{Ubuntu Mono} %

%\setmainfont{XITS}
% \setmathfont{XITS Math}
% \setmathfont[range={\mathcal,\mathbfcal},StylisticSet=1]{XITS Math}
% \setmathfont[range={\symbfup}]{XITS Math Bold}
\setmathfont{texgyrepagella-math.otf}
\renewcommand{\mathbf}[1]{\symbfup{#1}}
\renewcommand{\boldsymbol}[1]{\symbfup{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Settings below are based on
% bookdown: Authoring Books and Technical Documents with R Markdown
% By Yihui Xie
% https://bookdown.org/yihui/bookdown/publishers.html
% -- Thanks!
\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Workaround for \tableofcontents-related compilation errors:
%
% Author: Werner
% https://tex.stackexchange.com/questions/129778/hyperref-links-entire-line-like-word
\makeatletter
\def\contentsline#1#2#3#4{%
  \begingroup
    \Hy@safe@activestrue
  \edef\x{\endgroup
    \def\noexpand\Hy@tocdestname{#4}%
  }\x
  \csname l@#1\endcsname{%
    %\hyper@linkstart{link}{\Hy@tocdestname}{#2}\hyper@linkend
    #2%
  }{%
    %\hyper@linkstart{link}{\Hy@tocdestname}{#3}\hyper@linkend
    #3%
  }%
}
% Update ToC hyperlinks for Chapters
\patchcmd{\l@chapter}% <cmd>
  {{\cftchapfont #1}}% <search>
  {\hyper@linkstart{link}{\Hy@tocdestname}{}{\cftchapfont #1}}% <replace>
  {}{}% <success><failure>
\patchcmd{\cftchapfillnum}{\par}{\hyper@linkend\par}{}{}
% Update ToC hyperlinks for Sections
\patchcmd{\l@section}% <cmd>
  {{\cftsecfont #1}}% <search>
  {\hyper@linkstart{link}{\Hy@tocdestname}{}{\cftsecfont #1}}% <replace>
  {}{}% <success><failure>
\patchcmd{\cftsecfillnum}{\par}{\hyper@linkend\par}{}{}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\frontmatter

\title{Lightweight Machine Learning Classics with R}
\author{Marek Gagolewski}
\date{DRAFT v0.3 2020-05-16 21:49 (b3afcde)}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


\begin{quote}
\textbf{This is a draft version (distributed in the hope that it will be useful)
of the book \emph{Lightweight Machine Learning Classics with R}
by Marek Gagolewski.}
\end{quote}

\begin{quote}
\textbf{Please submit any feature requests, remarks and bug fixes
via the project site at \href{https://github.com/gagolews/lmlcr/issues}{github}. Thanks!}
\end{quote}

Copyright (C) 2020, \href{https://www.gagolewski.com}{Marek Gagolewski}.
This material is licensed under the Creative Commons
\href{https://creativecommons.org/licenses/by-nc-nd/4.0/}{Attribution-NonCommercial-NoDerivatives 4.0 International}
License (CC BY-NC-ND 4.0).

You can access this book at:

\begin{itemize}
\tightlist
\item
  \url{https://lmlcr.gagolewski.com/} (a browser-friendly version)
\item
  \url{https://lmlcr.gagolewski.com/lmlcr.pdf} (PDF)
\item
  \url{https://github.com/gagolews/lmlcr} (source code)
\end{itemize}

\hypertarget{aims-and-scope}{%
\subsubsection*{Aims and Scope}\label{aims-and-scope}}


Machine learning has numerous exciting real-world applications,
including stock market prediction, speech recognition,
computer-aided medical diagnosis, content and product recommendation,
anomaly detection in security camera footages, game playing,
autonomous vehicle operation and many others.

In this book we will take an unpretentious glance at the most fundamental
algorithms that have stood the test of time and which form
the basis for state-of-the-art solutions of modern AI,
which is principally (big) data-driven.
We will learn how to use the R language (R Development Core Team \protect\hyperlink{ref-rpoject}{2020})
for implementing various stages
of data processing and modelling activities.
For a more in-depth treatment of R, refer to this book's Appendices
and, for instance, (Wickham \& Grolemund \protect\hyperlink{ref-r4ds}{2017}, Peng \protect\hyperlink{ref-rprogdatascience}{2019}, Venables et al. \protect\hyperlink{ref-Rintro}{2020}).

These pages contain solid underpinnings for further studies
related to statistical learning, machine learning
data science, data analytics and artificial intelligence,
including (Bishop \protect\hyperlink{ref-bishop}{2006}, Hastie et al. \protect\hyperlink{ref-esl}{2017}, James et al. \protect\hyperlink{ref-islr}{2017}).
We will also appreciate the vital role of mathematics as a universal
language for formalising data-intense problems and communicating their
solutions. The book is aimed at readers who are yet to be fluent with
university-level linear algebra, calculus and probability theory,
such as 1st year undergrads or those who have forgotten
all the maths they have learned and need a gentle, non-invasive,
yet rigorous introduction to the topic.
For a nice, machine learning-focused introduction to mathematics alone,
see, e.g., (Deisenroth et al. \protect\hyperlink{ref-mml}{2020}).

\hypertarget{about-the-author}{%
\subsubsection*{About the Author}\label{about-the-author}}


I, Marek Gagolewski, am currently a Senior Lecturer (equivalent to Associate Professor in the US)
in Applied AI at Deakin University in Melbourne, VIC, Australia
and an Associate Professor in Data Science at Warsaw University of Technology,
Poland. My main passion is in research -- my primary interests include
machine learning and optimisation algorithms, data aggregation and clustering,
statistical modelling and scientific computing.
\emph{Explaining} of things matters to me more than merely tuning the knobs
so as to increase a chosen performance metric (with uncontrollable consequences
to other ones); the latter belongs to technology and wizardry,
not science.

I'm an author of more than 70 publications.
I've developed several open source R and Python packages, including
\href{http://www.gagolewski.com/software/stringi/}{stringi},
which is among the most often downloaded R extensions.

On top of that, I teach various courses related to
R and Python programming, algorithms,
data science and machine learning -- and I'm good at it.
This book was also influenced by my teaching experience at
\href{https://datascienceretreat.com}{Data Science Retreat} in Berlin, Germany.

\hypertarget{acknowledgements}{%
\subsubsection*{Acknowledgements}\label{acknowledgements}}


This book has been prepared with pandoc, Markdown and GitBook.
R code chunks have been processed with knitr.
A little help of bookdown, good ol' Makefiles and shell scripts
did the trick.

The following R packages are used or referred to in the text:
bookdown, Cairo, DEoptim, fastcluster, FNN, genie, gsl, hydroPSO, ISLR, keras, knitr, Matrix, microbenchmark, pdist, RColorBrewer, recommenderlab, rpart, rpart.plot, rworldmap, scatterplot3d, stringi, tensorflow, tidyr, titanic, vioplot.

During the writing of this book, I've been listening to the music
featuring John Coltrane, Krzysztof Komeda,
Henry Threadgill, Albert Ayler, Paco de Lucia and Tomatito.

\mainmatter

\hypertarget{simple-linear-regression}{%
\chapter{Simple Linear Regression}\label{simple-linear-regression}}

\hypertarget{machine-learning}{%
\section{Machine Learning}\label{machine-learning}}

\hypertarget{what-is-machine-learning}{%
\subsection{What is Machine Learning?}\label{what-is-machine-learning}}

An \textbf{algorithm} is a well-defined sequence of instructions that,
for a given sequence of input arguments,
yields some desired output.

In other words, it is a specific recipe for a \textbf{function}.

Developing algorithms is a tedious task.

In \textbf{machine learning}, we build and study computer algorithms
that make \emph{predictions} or \emph{decisions} but which are not
manually programmed.

\textbf{Learning} needs some material based upon which new knowledge is to be acquired.

In other words, we need \textbf{data}.

\hypertarget{main-types-of-machine-learning-problems}{%
\subsection{Main Types of Machine Learning Problems}\label{main-types-of-machine-learning-problems}}

Machine Learning Problems include, but are not limited to:

\begin{itemize}
\item
  \textbf{Supervised learning} -- for every input point (e.g., a photo)
  there is an associated desired output (e.g., whether it depicts a crosswalk
  or how many cars can be seen on it)
\item
  \textbf{Unsupervised learning} -- inputs are unlabelled, the aim is to discover
  the underlying structure in the data (e.g., automatically group customers
  w.r.t. common behavioural patterns)
\item
  \textbf{Semi-supervised learning} -- some inputs are labelled, the others
  are not (definitely a cheaper scenario)
\item
  \textbf{Reinforcement learning} -- learn to act based on a
  feedback given after the actual decision was made
  (e.g., learn to play The Witcher 7 by testing different hypotheses
  what to do to survive as long as possible)
\end{itemize}

\hypertarget{supervised-learning}{%
\section{Supervised Learning}\label{supervised-learning}}

\hypertarget{formalism}{%
\subsection{Formalism}\label{formalism}}

Let \(\mathbf{X}=\{\mathfrak{X}_1,\dots,\mathfrak{X}_n\}\)
be an input sample (``a database'')
that consists of \(n\) objects.

Most often we assume that each object \(\mathfrak{X}_i\)
is represented using \(p\) numbers for some \(p\).

We denote this fact as \(\mathfrak{X}_i\in \mathbb{R}^p\)
(it is \emph{a \(p\)-dimensional real vector} or
\emph{a sequence of \(p\) numbers} or
\emph{a point in a \(p\)-dimensional real space}
or \emph{an element of a real \(p\)-space} etc.).

If we have ``complex'' objects on input,
we can always try representing them as \textbf{feature vectors} (e.g.,
come up with numeric attributes that best describe them in a task at hand).

\begin{exercise}

Consider the following problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How would you represent a patient in a clinic?
\item
  How would you represent a car in an insurance company's database?
\item
  How would you represent a student in an university?
\end{enumerate}

\end{exercise}

Of course, our setting is \emph{abstract} in the sense that
there might be different realities \emph{hidden} behind these symbols.

This is what maths is for -- creating \emph{abstractions} or \emph{models}
of complex entities/phenomena so that they can be much more easily manipulated
or understood.
This is very powerful -- spend a moment
contemplating how many real-world situations fit into our framework.

This also includes image/video data, e.g., a 1920×1080 pixel image
can be ``unwound'' to a ``flat'' vector of length 2,073,600.

(*) There are some algorithms such as Multidimensional Scaling,
Locally Linear Embedding, IsoMap etc.
that can do that automagically.

\bigskip

In cases such as this we say that we deal with \emph{structured (tabular) data}\\
-- \(\mathbf{X}\) can be written as an (\(n\times p\))-matrix:
\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} \\
\end{array}
\right]
\]
Mathematically, we denote this as \(\mathbf{X}\in\mathbb{R}^{n\times d}\).

\begin{description}
\item[Remark.]
Structured data == think: Excel/Calc spreadsheets, SQL tables etc.
\end{description}

For an example, consider the famous Fisher's Iris flower dataset,
see \texttt{?iris} in R
and \url{https://en.wikipedia.org/wiki/Iris_flower_data_set}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X <-}\StringTok{ }\NormalTok{iris[}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{] }\CommentTok{# first 6 rows and 4 columns}
\NormalTok{X         }\CommentTok{# or: print(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1          5.1         3.5          1.4         0.2
## 2          4.9         3.0          1.4         0.2
## 3          4.7         3.2          1.3         0.2
## 4          4.6         3.1          1.5         0.2
## 5          5.0         3.6          1.4         0.2
## 6          5.4         3.9          1.7         0.4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(X)    }\CommentTok{# gives n and p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(iris) }\CommentTok{# for the full dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 150   5
\end{verbatim}

\(x_{i,j}\in\mathbb{R}\)
represents the \(j\)-th feature of the \(i\)-th observation,
\(j=1,\dots,p\), \(i=1,\dots,n\).

For instance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X[}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{] }\CommentTok{# 3rd row, 2nd column}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.2
\end{verbatim}

The third observation (data point, row in \(\mathbf{X}\))
consists of items \((x_{3,1}, \dots, x_{3,p})\) that can be extracted by calling:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X[}\DecValTok{3}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 3          4.7         3.2          1.3         0.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(X[}\DecValTok{3}\NormalTok{,]) }\CommentTok{# drops names}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.7 3.2 1.3 0.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(X[}\DecValTok{3}\NormalTok{,])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

Moreover, the second feature/variable/column
is comprised of
\((x_{1,2}, x_{2,2}, \dots, x_{n,2})\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X[,}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.5 3.0 3.2 3.1 3.6 3.9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(X[,}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

We will sometimes use the following notation to emphasise that
the \(\mathbf{X}\) matrix consists of \(n\) rows
or \(p\) columns:

\[
\mathbf{X}=\left[
\begin{array}{c}
\mathbf{x}_{1,\cdot} \\
\mathbf{x}_{2,\cdot} \\
\vdots\\
\mathbf{x}_{n,\cdot} \\
\end{array}
\right]
=
\left[
\begin{array}{cccc}
\mathbf{x}_{\cdot,1} &
\mathbf{x}_{\cdot,2} &
\cdots &
\mathbf{x}_{\cdot,p} \\
\end{array}
\right].
\]

Here, \(\mathbf{x}_{i,\cdot}\) is a \emph{row vector} of length \(p\),
i.e., a \((1\times p)\)-matrix:

\[
\mathbf{x}_{i,\cdot} = \left[
\begin{array}{cccc}
x_{i,1} &
x_{i,2} &
\cdots &
x_{i,p} \\
\end{array}
\right].
\]

Moreover, \(\mathbf{x}_{\cdot,j}\) is a \emph{column vector} of length \(n\),
i.e., an \((n\times 1)\)-matrix:

\[
\mathbf{x}_{\cdot,j} = \left[
\begin{array}{cccc}
x_{1,j} &
x_{2,j} &
\cdots &
x_{n,j} \\
\end{array}
\right]^T=\left[
\begin{array}{c}
{x}_{1,j} \\
{x}_{2,j} \\
\vdots\\
{x}_{n,j} \\
\end{array}
\right],
\]

where \(\cdot^T\) denotes the \emph{transpose} of a given matrix --
think of this as a kind of rotation; it allows us to introduce a set of
``vertically stacked'' objects using a single inline formula.

\hypertarget{desired-outputs}{%
\subsection{Desired Outputs}\label{desired-outputs}}

In supervised learning,
apart from the inputs we are also given the corresponding
reference/desired outputs.

The aim of supervised learning is to try to create an ``algorithm'' that,
given an input point, generates an output that is as \emph{close} as possible
to the desired one. The given data sample will be used to ``train'' this ``model''.

Usually the reference outputs are encoded as individual numbers (scalars)
or textual labels.

In other words, with each input \(\mathbf{x}_{i,\cdot}\) we associate
the desired output \(y_i\):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# in iris, iris[, 5] gives the Ys}
\NormalTok{iris[}\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(iris), }\DecValTok{3}\NormalTok{), ]  }\CommentTok{# three random rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species
## 14           4.3         3.0          1.1         0.1    setosa
## 50           5.0         3.3          1.4         0.2    setosa
## 118          7.7         3.8          6.7         2.2 virginica
\end{verbatim}

Hence, our dataset is \([\mathbf{X}\ \mathbf{y}]\) --
each object is represented as a row vector
\([\mathbf{x}_{i,\cdot}\ y_i]\), \(i=1,\dots,n\):

\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} & y_1\\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} & y_2\\
\vdots & \vdots & \ddots & \vdots    & \vdots\\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} & y_n\\
\end{array}
\right],
\]

where:

\[
\mathbf{y} = \left[
\begin{array}{cccc}
y_{1} &
y_{2} &
\cdots &
y_{n} \\
\end{array}
\right]^T=\left[
\begin{array}{c}
{y}_{1} \\
{y}_{2} \\
\vdots\\
{y}_{n} \\
\end{array}
\right].
\]

\hypertarget{types-of-supervised-learning-problems}{%
\subsection{Types of Supervised Learning Problems}\label{types-of-supervised-learning-problems}}

Depending on the type of the elements in \(\mathbf{y}\)
(the domain of \(\mathbf{y}\)),
supervised learning problems are usually
classified as:

\begin{itemize}
\item
  \textbf{regression} -- each \(y_i\) is a real number

  e.g., \(y_i=\) future market stock price
  with \(\mathbf{x}_{i,\cdot}=\) prices from \(p\) previous days
\item
  \textbf{classification} -- each \(y_i\) is a discrete label

  e.g., \(y_i=\) healthy (0) or ill (1)
  with \(\mathbf{x}_{i,\cdot}=\) a patient's health record
\item
  \textbf{ordinal regression} (a.k.a. ordinal classification) -- each \(y_i\) is a rank

  e.g., \(y_i=\) rating of a product on the scale 1--5
  with \(\mathbf{x}_{i,\cdot}=\) ratings of \(p\) most similar products
\end{itemize}

\begin{exercise}

Example Problems -- Discussion:

Which of the following are instances of classification problems? Which of them are regression tasks?

What kind of data should you gather in order to tackle them?

\begin{itemize}
\tightlist
\item
  Detect email spam
\item
  Predict a market stock price
\item
  Predict the likeability of a new ad
\item
  Assess credit risk
\item
  Detect tumour tissues in medical images
\item
  Predict time-to-recovery of cancer patients
\item
  Recognise smiling faces on photographs
\item
  Detect unattended luggage in airport security camera footage
\item
  Turn on emergency braking to avoid a collision with pedestrians
\end{itemize}

\end{exercise}

A single dataset can become an instance of many different ML problems.

Examples -- the \texttt{wines} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wines <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/winequality-all.csv"}\NormalTok{, }\DataTypeTok{comment=}\StringTok{"#"}\NormalTok{)}
\NormalTok{wines[}\DecValTok{1}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   fixed.acidity volatile.acidity citric.acid residual.sugar chlorides
## 1           7.4              0.7           0            1.9     0.076
##   free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates
## 1                  11                   34  0.9978 3.51      0.56
##   alcohol response color
## 1     9.4        5   red
\end{verbatim}

\texttt{alcohol} is a numeric (quantitative) variable (see Figure \ref{fig:wines_regression} for a histogram depicting its empirical distribution):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(wines}\OperatorTok{$}\NormalTok{alcohol) }\CommentTok{# continuous variable}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     8.0     9.5    10.3    10.5    11.3    14.9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(wines}\OperatorTok{$}\NormalTok{alcohol, }\DataTypeTok{main=}\StringTok{""}\NormalTok{, }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{); }\KeywordTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:wines_regression}{%
\centering
\includegraphics{01-regression-simple-figures/wines_regression-1.pdf}
\caption{Quantitative (numeric) outputs lead to regression problems}\label{fig:wines_regression}
}
\end{figure}

\texttt{color} is a quantitative variable with two possible outcomes (see Figure \ref{fig:wines_binary} for a bar plot):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(wines}\OperatorTok{$}\NormalTok{color) }\CommentTok{# binary variable}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   red white 
##  1599  4898
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{barplot}\NormalTok{(}\KeywordTok{table}\NormalTok{(wines}\OperatorTok{$}\NormalTok{color), }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{6000}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:wines_binary}{%
\centering
\includegraphics{01-regression-simple-figures/wines_binary-1.pdf}
\caption{Quantitative outputs lead to classification tasks}\label{fig:wines_binary}
}
\end{figure}

Moreover, \texttt{response} is an ordinal variable, representing
a wine's rating as assigned by a wine expert
(see Figure \ref{fig:wines_ordinal} for a barplot).
Note that although the ranks are represented with numbers,
we they are not continuous variables. Moreover,
these ranks are something more than just labels -- they are linearly
ordered, we know what's the smallest rank and whats the greatest one.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(wines}\OperatorTok{$}\NormalTok{response) }\CommentTok{# ordinal variable}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##    3    4    5    6    7    8    9 
##   30  216 2138 2836 1079  193    5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{barplot}\NormalTok{(}\KeywordTok{table}\NormalTok{(wines}\OperatorTok{$}\NormalTok{response), }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3000}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:wines_ordinal}{%
\centering
\includegraphics{01-regression-simple-figures/wines_ordinal-1.pdf}
\caption{Ordinal variables constitute ordinal regression tasks}\label{fig:wines_ordinal}
}
\end{figure}

\hypertarget{simple-regression}{%
\section{Simple Regression}\label{simple-regression}}

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

\textbf{Simple regression} is the easiest setting to start with -- let's assume
\(p=1\), i.e., all inputs are 1-dimensional.
Denote \(x_i=x_{i,1}\).

We will use it to build many intuitions, for example, it'll be easy
to illustrate all the concepts graphically.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"ISLR"}\NormalTok{) }\CommentTok{# Credit dataset}
\KeywordTok{plot}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Balance, Credit}\OperatorTok{$}\NormalTok{Rating) }\CommentTok{# scatter plot}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:credit_scatter}{%
\centering
\includegraphics{01-regression-simple-figures/credit_scatter-1.pdf}
\caption{A scatter plot of Rating vs.~Balance}\label{fig:credit_scatter}
}
\end{figure}

In what follows we will be modelling the Credit Rating (\(Y\))
as a function of the average Credit Card Balance (\(X\)) in USD
for customers \emph{with positive Balance only}.
It is because it is evident from Figure \ref{fig:credit_scatter}
that some customers with zero balance obtained a credit rating
based on some external data source that we don't have access to in
our very setting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Balance[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{]))}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Rating[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

Figure \ref{fig:credit_XY_plot} gives the updated scatter plot
with the zero-balance clients ``taken care of''.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(X, Y, }\DataTypeTok{xlab=}\StringTok{"X (Balance)"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Y (Rating)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:credit_XY_plot}{%
\centering
\includegraphics{01-regression-simple-figures/credit_XY_plot-1.pdf}
\caption{A scatter plot of Rating vs.~Balance with clients of Balance=0 removed}\label{fig:credit_XY_plot}
}
\end{figure}

Our aim is to construct a function \(f\) that
\textbf{models} Rating as a function of Balance,
\(f(X)=Y\).

We are equipped with \(n=310\) reference (observed) Ratings
\(\mathbf{y}=[y_1\ \cdots\ y_n]^T\)
for particular Balances \(\mathbf{x}=[x_1\ \cdots\ x_n]^T\).

Note the following naming conventions:

\begin{itemize}
\item
  Variable types:

  \begin{itemize}
  \item
    \(X\) -- independent/explanatory/predictor variable
  \item
    \(Y\) -- dependent/response/predicted variable
  \end{itemize}
\item
  Also note that:

  \begin{itemize}
  \item
    \(Y\) -- idealisation (any possible Rating)
  \item
    \(\mathbf{y}=[y_1\ \cdots\ y_n]^T\) -- values actually observed
  \end{itemize}
\end{itemize}

The model will not be ideal, but it might be usable:

\begin{itemize}
\item
  We will be able to \textbf{predict} the rating of any new client.

  What should be the rating of a client with Balance of \$1500?

  What should be the rating of a client with Balance of \$2500?
\item
  We will be able to \textbf{describe} (understand) this reality using a single mathematical formula
  so as to infer that there is an association between \(X\) and \(Y\)

  Think of ``data compression'' and laws of physics, e.g., \(E=mc^2\).
\end{itemize}

(*) Mathematically, we will assume that there is some ``true'' function that models the data
(true relationship between \(Y\) and \(X\)),
but the observed outputs are subject to \textbf{additive error}:
\[Y=f(X)+\varepsilon.\]

\(\varepsilon\) is a random term, classically we assume that
errors are independent of each other,
have expected value of \(0\) (there is no systematic error = unbiased)
and that they follow a normal distribution.

(*) We denote this as \(\varepsilon\sim\mathcal{N}(0, \sigma)\)
(read: random variable \(\varepsilon\) follows a normal distribution
with expected value of \(0\) and standard deviation of \(\sigma\) for some \(\sigma\ge 0\)).

\(\sigma\) controls the amount of noise (and hence, uncertainty).
Figure \ref{fig:normal_distribs} gives the plot of the probability
distribution function (PDFs, densities)
of \(\mathcal{N}(0, \sigma)\) for different \(\sigma\)s:

\begin{figure}
\hypertarget{fig:normal_distribs}{%
\centering
\includegraphics{01-regression-simple-figures/normal_distribs-1.pdf}
\caption{Probability density functions of normal distributions with different standard deviations \(\sigma\).}\label{fig:normal_distribs}
}
\end{figure}

\hypertarget{search-space-and-objective}{%
\subsection{Search Space and Objective}\label{search-space-and-objective}}

There are many different functions that can be \textbf{fitted} into
the observed \((\mathbf{x},\mathbf{y})\),
compare Figure \ref{fig:credit_different_models}.
Some of them are better than the other (with respect to
different aspects, such as fit quality, simplicity etc.).

\begin{figure}
\hypertarget{fig:credit_different_models}{%
\centering
\includegraphics{01-regression-simple-figures/credit_different_models-1.pdf}
\caption{Different polynomial models fitted to data}\label{fig:credit_different_models}
}
\end{figure}

Thus, we need a formal \textbf{model selection criterion}
that could enable as to tackle the model fitting
task on a computer.

Usually, we will be interested in a model
that minimises appropriately aggregated \textbf{residuals}
\(f(x_i)-y_i\), i.e.,
\textbf{predicted outputs minus observed outputs},
often denoted with \(\hat{y}_i-y_i\),
for \(i=1,\dots,n\).

\begin{figure}
\hypertarget{fig:credit_residuals}{%
\centering
\includegraphics{01-regression-simple-figures/credit_residuals-1.pdf}
\caption{Residuals are defined as the differences between the predicted and observed outputs \(\hat{y}_i-y_i\)}\label{fig:credit_residuals}
}
\end{figure}

In Figure \ref{fig:credit_residuals}, the residuals correspond to the
lengths of the dashed line segments -- they measure the discrepancy between
the outputs generated by the model (what we get)
and the true outputs (what we want).

Top choice: sum of squared residuals:
\[
\begin{array}{rl}
\mathrm{SSR}(f|\mathbf{x},\mathbf{y})
& = \left( f(x_1)-y_1 \right)^2 + \dots + \left( f(x_n)-y_n \right)^2 \\
& =
\displaystyle\sum_{i=1}^n \left( f(x_i)-y_i \right)^2
\end{array}
\]

\begin{description}
\item[Remark.]
Read ``\(\sum_{i=1}^n z_i\)'' as ``the sum of \(z_i\) for \(i\) from \(1\) to \(n\)'';
this is just a shorthand for \(z_1+z_2+\dots+z_n\).
\item[Remark.]
The notation \(\mathrm{SSR}(f|\mathbf{x},\mathbf{y})\) means that
it is the error measure
corresponding to the model \((f)\) \emph{given} our data.\\
We could've denoted it with \(\mathrm{SSR}_{\mathbf{x},\mathbf{y}}(f)\)
or even \(\mathrm{SSR}(f)\) to emphasise that \(\mathbf{x},\mathbf{y}\)
are just fixed values and we are not interested
in changing them at all (they are ``global variables'').
\end{description}

We enjoy SSR because (amongst others):

\begin{itemize}
\item
  larger errors are penalised much more than smaller ones

  \begin{quote}
  (this can be considered a drawback as well)
  \end{quote}
\item
  (**) statistically speaking, this has a clear underlying interpretation

  \begin{quote}
  (assuming errors are normally distributed,
  finding a model minimising the SSR is equivalent
  to maximum likelihood estimation)
  \end{quote}
\item
  the models minimising the SSR can often be found easily

  \begin{quote}
  (corresponding optimisation tasks have an analytic solution --
  studied already by Gauss in the late 18th century)
  \end{quote}
\end{itemize}

(**) Other choices:

\begin{itemize}
\tightlist
\item
  regularised SSR, e.g., lasso or ridge regression (in the case of multiple input variables)
\item
  sum or median of absolute values (robust regression)
\end{itemize}

Fitting a model to data can be written as an optimisation problem:

\[
\min_{f\in\mathcal{F}} \mathrm{SSR}(f|\mathbf{x},\mathbf{y}),
\]

i.e., find \(f\) minimising the SSR \textbf{(seek ``best'' \(f\))}\\
amongst the set of admissible models \(\mathcal{F}\).

Example \(\mathcal{F}\)s:

\begin{itemize}
\item
  \(\mathcal{F}=\{\text{All possible functions of one variable}\}\) -- if there are no repeated
  \(x_i\)'s, this corresponds to data \emph{interpolation}; note that there
  are many functions that give SSR of \(0\).
\item
  \(\mathcal{F}=\{ x\mapsto x^2, x\mapsto \cos(x), x\mapsto \exp(2x+7)-9 \}\) -- obviously
  an ad-hoc choice but you can easily choose the best amongst the 3 by computing 3 sums of squares.
\item
  \(\mathcal{F}=\{ x\mapsto a+bx\}\) -- the space of linear functions of one variable
\item
  etc.
\end{itemize}

(e.g., \(x\mapsto x^2\) is read ``\(x\) maps to \(x^2\)'' and is
an elegant way to define an inline function \(f\) such that \(f(x)=x^2\))

\hypertarget{simple-linear-regression-1}{%
\section{Simple Linear Regression}\label{simple-linear-regression-1}}

\hypertarget{introduction-1}{%
\subsection{Introduction}\label{introduction-1}}

If the family of admissible models \(\mathcal{F}\) consists only of all linear functions of one variable,
we deal with a \textbf{simple linear regression}.

Our problem becomes:

\[
\min_{a,b\in\mathbb{R}} \sum_{i=1}^n \left(
a+bx_i-y_i
\right)^2
\]

In other words, we seek best fitting line in terms of the squared residuals.

This is the \textbf{method of least squares}.

This is particularly nice, because our search space
is just \(\mathbb{R}^2\) -- easy to handle both analytically and numerically.

\begin{figure}
\hypertarget{fig:credit_different_lines_ssr}{%
\centering
\includegraphics{01-regression-simple-figures/credit_different_lines_ssr-1.pdf}
\caption{Three simple linear models together with the corresponding SSRs}\label{fig:credit_different_lines_ssr}
}
\end{figure}

\begin{exercise}

Which of lines in Figure \ref{fig:credit_different_lines_ssr} is the least squares solution?

\end{exercise}

\hypertarget{solution-in-r}{%
\subsection{Solution in R}\label{solution-in-r}}

Let's fit the linear model minimising the SSR in R.
The \texttt{lm()} function (\texttt{l}inear \texttt{m}odels) has a convenient \emph{formula}-based interface.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X)}
\end{Highlighting}
\end{Shaded}

In R, the expression ``\texttt{Y\textasciitilde{}X}'' denotes a formula, which we read as:
variable \texttt{Y} is a function of \texttt{X}.
Note that the dependent variable is on the left side of the formula.
Here, \texttt{X} and \texttt{Y} are two R numeric vectors of identical lengths.

Let's print the fitted model:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Y ~ X)
## 
## Coefficients:
## (Intercept)            X  
##     226.471        0.266
\end{verbatim}

Hence, the fitted model is:
\[
Y = f(X) = 226.47114+0.26615 X
\qquad (+ \varepsilon)
\]

Coefficient \(a\) (intercept):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f}\OperatorTok{$}\NormalTok{coefficient[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept) 
##      226.47
\end{verbatim}

Coefficient \(b\) (slope):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f}\OperatorTok{$}\NormalTok{coefficient[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       X 
## 0.26615
\end{verbatim}

Plotting, see Figure \ref{fig:credit_plot_lm}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(X, Y, }\DataTypeTok{col=}\StringTok{"#000000aa"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(f, }\DataTypeTok{col=}\DecValTok{2}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:credit_plot_lm}{%
\centering
\includegraphics{01-regression-simple-figures/credit_plot_lm-1.pdf}
\caption{Fitted regression line}\label{fig:credit_plot_lm}
}
\end{figure}

SSR:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(f}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2132108
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{((f}\OperatorTok{$}\NormalTok{coefficient[}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{f}\OperatorTok{$}\NormalTok{coefficient[}\DecValTok{2}\NormalTok{]}\OperatorTok{*}\NormalTok{X}\OperatorTok{-}\NormalTok{Y)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{# equivalent}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2132108
\end{verbatim}

We can predict the model's output for yet-unobserved inputs by
writing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X_new <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1500}\NormalTok{, }\DecValTok{2000}\NormalTok{, }\DecValTok{2500}\NormalTok{) }\CommentTok{# example inputs}
\NormalTok{f}\OperatorTok{$}\NormalTok{coefficient[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{f}\OperatorTok{$}\NormalTok{coefficient[}\DecValTok{2}\NormalTok{]}\OperatorTok{*}\NormalTok{X_new}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 625.69 758.76 891.84
\end{verbatim}

Note that linear models can also be fitted based on formulas that refer
to a data frame's columns. For example, let us wrap
both \(\mathbf{x}\) and \(\mathbf{y}\) inside a data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{XY <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Balance=}\NormalTok{X, }\DataTypeTok{Rating=}\NormalTok{Y)}
\KeywordTok{head}\NormalTok{(XY, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Balance Rating
## 1     333    283
## 2     903    483
## 3     580    514
\end{verbatim}

By writing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Rating}\OperatorTok{~}\NormalTok{Balance, }\DataTypeTok{data=}\NormalTok{XY)}
\end{Highlighting}
\end{Shaded}

now \texttt{Balance} and \texttt{Rating} refer to column names in the \texttt{XY} data frame,
and not the objects in R's ``workspace''.

Based on the above, we can make a prediction
using the \texttt{predict()} function"

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X_new <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Balance=}\KeywordTok{c}\NormalTok{(}\DecValTok{1500}\NormalTok{, }\DecValTok{2000}\NormalTok{, }\DecValTok{2500}\NormalTok{))}
\KeywordTok{predict}\NormalTok{(f, X_new)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      1      2      3 
## 625.69 758.76 891.84
\end{verbatim}

Interestingly:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(f, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Balance=}\KeywordTok{c}\NormalTok{(}\DecValTok{5000}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      1 
## 1557.2
\end{verbatim}

This is more than the highest possible rating -- we have extrapolated
way beyond the observable data range.

Note that our \(Y=a+bX\) model is \textbf{interpretable}
and \textbf{well-behaving}
(not all machine learning models will have this feature,
think: deep neural networks,
which we rather conceive as \emph{black boxes}):

\begin{itemize}
\item
  we know that by increasing \(X\) by a small amount,
  \(Y\) will also increase (positive correlation),
\item
  the model is continuous -- small change in \(X\)
  doesn't yield any drastic change in \(Y\),
\item
  we know what will happen if we increase or decrease \(X\) by, say, \(100\),
\item
  the function is invertible -- if we want Rating of \(500\),
  we can compute the associated preferred Balance that should yield it
  (provided that the model is valid).
\end{itemize}

\hypertarget{analytic-solution}{%
\subsection{Analytic Solution}\label{analytic-solution}}

It may be shown (which we actually do below)
that the solution is:

\[
\left\{
\begin{array}{rl}
b  = & \dfrac{
n \displaystyle\sum_{i=1}^n x_i y_i - \displaystyle\sum_{i=1}^n  y_i \displaystyle\sum_{i=1}^n x_i
}{
n \displaystyle\sum_{i=1}^n x_i x_i -   \displaystyle\sum_{i=1}^n x_i\displaystyle\sum_{i=1}^n x_i
}\\
a = & \dfrac{1}{n}\displaystyle\sum_{i=1}^n  y_i - b  \dfrac{1}{n} \displaystyle\sum_{i=1}^n x_i  \\
\end{array}
\right.
\]

Which can be implemented in R as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(X)}
\NormalTok{b <-}\StringTok{ }\NormalTok{(n}\OperatorTok{*}\KeywordTok{sum}\NormalTok{(X}\OperatorTok{*}\NormalTok{Y)}\OperatorTok{-}\KeywordTok{sum}\NormalTok{(X)}\OperatorTok{*}\KeywordTok{sum}\NormalTok{(Y))}\OperatorTok{/}\NormalTok{(n}\OperatorTok{*}\KeywordTok{sum}\NormalTok{(X}\OperatorTok{*}\NormalTok{X)}\OperatorTok{-}\KeywordTok{sum}\NormalTok{(X)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{a <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(Y)}\OperatorTok{-}\NormalTok{b}\OperatorTok{*}\KeywordTok{mean}\NormalTok{(X)}
\KeywordTok{c}\NormalTok{(a, b) }\CommentTok{# the same as f$coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 226.47114   0.26615
\end{verbatim}

\hypertarget{derivation-of-the-solution}{%
\subsection{Derivation of the Solution (**)}\label{derivation-of-the-solution}}

\begin{description}
\item[Remark.]
You can safely skip this part if you are yet to know
how to search for a minimum of a function of many variables
and what are partial derivatives.
\end{description}

Denote with:

\[
E(a,b)=\mathrm{SSR}(x\mapsto a+bx|\mathbf{x},\mathbf{y})
=\sum_{i=1}^n \left( a+bx_i - y_i \right) ^2.
\]

We seek the minimum of \(E\) w.r.t. both \(a,b\).

\begin{description}
\item[Theorem.]
If \(E\) has a (local) minimum at \((a^*,b^*)\),
then its partial derivatives vanish therein,
i.e., \(\partial E/\partial a(a^*, b^*) = 0\) and \(\partial E/\partial b(a^*, b^*)=0\).
\end{description}

We have:

\[
E(a,b)   = \displaystyle\sum_{i=1}^n \left( a+bx_i - y_i \right) ^2.
\]

We need to compute the partial derivatives \(\partial E/\partial a\) (derivative of \(E\)
w.r.t. variable \(a\) -- all other terms treated as constants)
and \(\partial E/\partial b\) (w.r.t. \(b\)).

Useful rules -- derivatives w.r.t. \(a\) (denote \(f'(a)=(f(a))'\)):

\begin{itemize}
\tightlist
\item
  \((f(a)+g(a))'=f'(a)+g'(a)\) (derivative of sum is sum of derivatives)
\item
  \((f(a) g(a))' = f'(a)g(a) + f(a)g'(a)\) (derivative of product)
\item
  \((f(g(a)))' = f'(g(a)) g'(a)\) (chain rule)
\item
  \((c)' = 0\) for any constant \(c\) (expression not involving \(a\))
\item
  \((a^p)' = pa^{p-1}\) for any \(p\)
\item
  in particular: \((c a^2+d)'=2ca\), \((ca)'=c\), \(((ca+d)^2)'=2(ca+d)c\) (application of the above rules)
\end{itemize}

We seek \(a,b\) such that \(\frac{\partial E}{\partial a}(a,b) = 0\)
and \(\frac{\partial E}{\partial b}(a,b)=0\).

\[
\left\{
\begin{array}{rl}
\frac{\partial E}{\partial a}(a,b)=&2\displaystyle\sum_{i=1}^n \left( a+bx_i - y_i \right) = 0\\
\frac{\partial E}{\partial b}(a,b)=&2\displaystyle\sum_{i=1}^n \left( a+bx_i - y_i \right) x_i = 0 \\
\end{array}
\right.
\]

This is a system of 2 linear equations. Easy.

Rearranging like back in the school days:

\[
\left\{
\begin{array}{rl}
b \displaystyle\sum_{i=1}^n x_i+ a n = & \displaystyle\sum_{i=1}^n  y_i  \\
b \displaystyle\sum_{i=1}^n x_i x_i + a \displaystyle\sum_{i=1}^n x_i = & \displaystyle\sum_{i=1}^n x_i y_i \\
\end{array}
\right.
\]

It is left as an exercise to show that the solution is:

\[
\left\{
\begin{array}{rl}
b^*  = & \dfrac{
n \displaystyle\sum_{i=1}^n x_i y_i - \displaystyle\sum_{i=1}^n  y_i \displaystyle\sum_{i=1}^n x_i
}{
n \displaystyle\sum_{i=1}^n x_i x_i -   \displaystyle\sum_{i=1}^n x_i\displaystyle\sum_{i=1}^n x_i
}\\
a^* = & \dfrac{1}{n}\displaystyle\sum_{i=1}^n  y_i - b^*  \dfrac{1}{n} \displaystyle\sum_{i=1}^n x_i  \\
\end{array}
\right.
\]

(we should additionally perform the second derivative test
to assure that this is the minimum of \(E\) -- which is exactly the case though)

(**) In the next chapter, we will introduce the notion of Pearson's
linear coefficient, \(r\) (see \texttt{cor()} in R). It might be shown that
\(a\) and \(b\) can also be rewritten as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(b <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(X,Y)}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(Y)}\OperatorTok{/}\KeywordTok{sd}\NormalTok{(X))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         [,1]
## [1,] 0.26615
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(a <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(Y)}\OperatorTok{-}\NormalTok{b}\OperatorTok{*}\KeywordTok{mean}\NormalTok{(X))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        [,1]
## [1,] 226.47
\end{verbatim}

\hypertarget{exercises-in-r}{%
\section{Exercises in R}\label{exercises-in-r}}

\hypertarget{the-anscombe-quartet}{%
\subsection{The Anscombe Quartet}\label{the-anscombe-quartet}}

Here is a famous illustrative example proposed by
the statistician Francis Anscombe in the early 1970s.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(anscombe) }\CommentTok{# `anscombe` is a built-in object}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    x1 x2 x3 x4    y1   y2    y3    y4
## 1  10 10 10  8  8.04 9.14  7.46  6.58
## 2   8  8  8  8  6.95 8.14  6.77  5.76
## 3  13 13 13  8  7.58 8.74 12.74  7.71
## 4   9  9  9  8  8.81 8.77  7.11  8.84
## 5  11 11 11  8  8.33 9.26  7.81  8.47
## 6  14 14 14  8  9.96 8.10  8.84  7.04
## 7   6  6  6  8  7.24 6.13  6.08  5.25
## 8   4  4  4 19  4.26 3.10  5.39 12.50
## 9  12 12 12  8 10.84 9.13  8.15  5.56
## 10  7  7  7  8  4.82 7.26  6.42  7.91
## 11  5  5  5  8  5.68 4.74  5.73  6.89
\end{verbatim}

What we see above is a single data frame
that encodes four separate datasets:
\texttt{anscombe\$x1} and \texttt{anscombe\$y1} define the first pair of variables,
\texttt{anscombe\$x2} and \texttt{anscombe\$y2} define the second pair and so forth.

\begin{exercise}

Split the above data (manually) into four data frames
\texttt{ans1}, \ldots{}, \texttt{ans4} with columns \texttt{x} and \texttt{y}.

For example, \texttt{ans1} should look like:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(ans1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     x     y
## 1  10  8.04
## 2   8  6.95
## 3  13  7.58
## 4   9  8.81
## 5  11  8.33
## 6  14  9.96
## 7   6  7.24
## 8   4  4.26
## 9  12 10.84
## 10  7  4.82
## 11  5  5.68
\end{verbatim}

\end{exercise}

\begin{solution}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ans1 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{x1, }\DataTypeTok{y=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{y1)}
\NormalTok{ans2 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{x2, }\DataTypeTok{y=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{y2)}
\NormalTok{ans3 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{x3, }\DataTypeTok{y=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{y3)}
\NormalTok{ans4 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{x4, }\DataTypeTok{y=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{y4)}
\KeywordTok{print}\NormalTok{(ans1)}
\end{Highlighting}
\end{Shaded}

\end{solution}

\begin{exercise}

Compute the mean of each \texttt{x} and \texttt{y} variable.

\end{exercise}

\begin{solution}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(ans1}\OperatorTok{$}\NormalTok{x) }\CommentTok{# individual column}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(ans1}\OperatorTok{$}\NormalTok{y) }\CommentTok{# individual column}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7.5009
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(ans2, mean) }\CommentTok{# all columns in ans2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      x      y 
## 9.0000 7.5009
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(anscombe, mean) }\CommentTok{# all columns in the full anscombe dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     x1     x2     x3     x4     y1     y2     y3     y4 
## 9.0000 9.0000 9.0000 9.0000 7.5009 7.5009 7.5000 7.5009
\end{verbatim}

\emph{Comment: This is really interesting, all the means of \texttt{x} columns
as well as the means of \texttt{y}s are almost identical.}

\end{solution}

\begin{exercise}

Compute the standard deviation of each \texttt{x} and \texttt{y} variable.

\end{exercise}

\begin{solution}

The solution is similar to the previous one, just replace \texttt{mean} with \texttt{sd}.
Here, to learn something new, we will use the \texttt{knitr::kable()} function
that pretty-prints a given matrix or data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(anscombe, sd)}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(results, }\DataTypeTok{col.names=}\StringTok{"standard deviation"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lr@{}}
\toprule
& standard deviation\tabularnewline
\midrule
\endhead
x1 & 3.3166\tabularnewline
x2 & 3.3166\tabularnewline
x3 & 3.3166\tabularnewline
x4 & 3.3166\tabularnewline
y1 & 2.0316\tabularnewline
y2 & 2.0317\tabularnewline
y3 & 2.0304\tabularnewline
y4 & 2.0306\tabularnewline
\bottomrule
\end{longtable}

\emph{Comment: This is even more interesting, because the numbers agree up to 2 decimal digits.}

\end{solution}

\begin{exercise}

Fit a simple linear regression model for each data set.
Draw the scatter plots again (\texttt{plot()})
and add the regression lines (\texttt{lines()} or \texttt{abline()}).

\end{exercise}

\begin{solution}

To recall, this can be done with the \texttt{lm()} function
explained in Lecture 2.

At this point we should already have become lazy -- the tasks are very
repetitious. Let's automate them by writing a single function
that does all the above for any data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_models <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(ans) \{}
    \CommentTok{# ans is a data frame with columns x and y}
\NormalTok{    f <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y}\OperatorTok{~}\NormalTok{x, }\DataTypeTok{data=}\NormalTok{ans) }\CommentTok{# fit linear model}
    \KeywordTok{print}\NormalTok{(f}\OperatorTok{$}\NormalTok{coefficients) }\CommentTok{# estimated coefficients}
    \KeywordTok{plot}\NormalTok{(ans}\OperatorTok{$}\NormalTok{x, ans}\OperatorTok{$}\NormalTok{y) }\CommentTok{# scatter plot}
    \KeywordTok{abline}\NormalTok{(f, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{) }\CommentTok{# regression line}
    \KeywordTok{return}\NormalTok{(f)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we can apply it on the four particular examples.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\CommentTok{# four plots on 1 figure (2x2 grid)}
\NormalTok{f1 <-}\StringTok{ }\KeywordTok{fit_models}\NormalTok{(ans1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)           x 
##     3.00009     0.50009
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f2 <-}\StringTok{ }\KeywordTok{fit_models}\NormalTok{(ans2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)           x 
##      3.0009      0.5000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f3 <-}\StringTok{ }\KeywordTok{fit_models}\NormalTok{(ans3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)           x 
##     3.00245     0.49973
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f4 <-}\StringTok{ }\KeywordTok{fit_models}\NormalTok{(ans4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)           x 
##     3.00173     0.49991
\end{verbatim}

\begin{figure}
\hypertarget{fig:anscombe_fit_apply}{%
\centering
\includegraphics{01-regression-simple-figures/anscombe_fit_apply-1.pdf}
\caption{Fitted regression lines for the Anscombe quartet}\label{fig:anscombe_fit_apply}
}
\end{figure}

\emph{Comment: All the estimated models are virtually the same,
the regression lines are
\(y=0.5x+3\), compare Figure \ref{fig:anscombe_fit_apply}.}

\end{solution}

\begin{exercise}

Create scatter plots of the residuals (predicted \(\hat{y}_i\) minus
true \(y_i\)) as a function of the predicted \(\hat{y}_i=f(x_i)\) for every
\(i=1,\dots,11\).

\end{exercise}

\begin{solution}

To recall, the model predictions can be generated by (amongst others)
calling the \texttt{predict()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y_pred1 <-}\StringTok{ }\NormalTok{f1}\OperatorTok{$}\NormalTok{fitted.values }\CommentTok{# predict(f1, ans1)}
\NormalTok{y_pred2 <-}\StringTok{ }\NormalTok{f2}\OperatorTok{$}\NormalTok{fitted.values }\CommentTok{# predict(f2, ans2)}
\NormalTok{y_pred3 <-}\StringTok{ }\NormalTok{f3}\OperatorTok{$}\NormalTok{fitted.values }\CommentTok{# predict(f3, ans3)}
\NormalTok{y_pred4 <-}\StringTok{ }\NormalTok{f4}\OperatorTok{$}\NormalTok{fitted.values }\CommentTok{# predict(f4, ans4)}
\end{Highlighting}
\end{Shaded}

Plots of residuals as a function of the predicted (fitted) values
are given in Figure \ref{fig:anscombe_resid_plot}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\CommentTok{# four plots on 1 figure (2x2 grid)}
\KeywordTok{plot}\NormalTok{(y_pred1, y_pred1}\OperatorTok{-}\NormalTok{ans1}\OperatorTok{$}\NormalTok{y)}
\KeywordTok{plot}\NormalTok{(y_pred2, y_pred2}\OperatorTok{-}\NormalTok{ans2}\OperatorTok{$}\NormalTok{y)}
\KeywordTok{plot}\NormalTok{(y_pred3, y_pred3}\OperatorTok{-}\NormalTok{ans3}\OperatorTok{$}\NormalTok{y)}
\KeywordTok{plot}\NormalTok{(y_pred4, y_pred4}\OperatorTok{-}\NormalTok{ans4}\OperatorTok{$}\NormalTok{y)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:anscombe_resid_plot}{%
\centering
\includegraphics{01-regression-simple-figures/anscombe_resid_plot-1.pdf}
\caption{Residuals vs.~fitted values for the regression lines fitted to the Anscombe quartet}\label{fig:anscombe_resid_plot}
}
\end{figure}

\emph{Comment: Ideally, the residuals shouldn't be correlated
with the predicted values -- they should ``oscillate'' randomly
around 0. This is only the case of the first dataset.
All the other cases are ``alarming'' in the sense that
they suggest that the obtained models are ``suspicious''
(perhaps data cleansing is needed or a linear model is not at all appropriate).}

\end{solution}

\begin{exercise}

Draw conclusions (in your own words).

\end{exercise}

\begin{solution}

We're being taught a lesson here: don't perform data analysis tasks
automatically, don't look at bare numbers only, visualise your data first!

\end{solution}

\begin{exercise}

Read more about Anscombe's quartet at \url{https://en.wikipedia.org/wiki/Anscombe\%27s_quartet}

\end{exercise}

\hypertarget{outro}{%
\section{Outro}\label{outro}}

\hypertarget{remarks}{%
\subsection{Remarks}\label{remarks}}

In supervised learning, with each input point,
there's an associated reference output value.

Learning a model = constructing a function that approximates
(minimising some error measure) the given data.

Regression = the output variable \(Y\) is continuous.

We studied linear models with a single independent variable based on
the least squares (SSR) fit.

In the next part we will extend this setting to the case
of many variables, i.e., \(p>1\), called multiple regression.

\hypertarget{further-reading}{%
\subsection{Further Reading}\label{further-reading}}

Recommended further reading: (James et al. \protect\hyperlink{ref-islr}{2017}: Chapters 1, 2 and 3)

Other: (Hastie et al. \protect\hyperlink{ref-esl}{2017}: Chapter 1, Sections 3.2 and 3.3)

\hypertarget{multiple-regression}{%
\chapter{Multiple Regression}\label{multiple-regression}}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

\hypertarget{formalism-1}{%
\subsection{Formalism}\label{formalism-1}}

Let \(\mathbf{X}\in\mathbb{R}^{n\times p}\) be an input matrix
that consists of \(n\) points in a \(p\)-dimensional space.

In other words, we have a database on \(n\) objects, each of which
being described by means of \(p\) numerical features.

\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} \\
\end{array}
\right]
\]

Recall that in supervised learning,
apart from \(\mathbf{X}\), we are also given the corresponding \(\mathbf{y}\);
with each input point \(\mathbf{x}_{i,\cdot}\) we associate the desired output \(y_i\).

In this chapter we are still interested in \textbf{regression} tasks;
hence, we assume that each \(y_i\)
it is a real number, i.e., \(y_i\in\mathbb{R}\).

Hence, our dataset is \([\mathbf{X}\ \mathbf{y}]\) --
where each object is represented as a row vector
\([\mathbf{x}_{i,\cdot}\ y_i]\), \(i=1,\dots,n\):

\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} & y_1\\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} & y_2\\
\vdots & \vdots & \ddots & \vdots    & \vdots\\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} & y_n\\
\end{array}
\right].
\]

\hypertarget{simple-linear-regression---recap}{%
\subsection{Simple Linear Regression - Recap}\label{simple-linear-regression---recap}}

In a simple regression task, we have assumed that \(p=1\) -- there is only
one independent variable,
denoted \(x_i=x_{i,1}\).

We restricted ourselves to linear models of the form \(Y=f(X)=a+bX\)
that minimised the sum of squared residuals (SSR), i.e.,

\[
\min_{a,b\in\mathbb{R}} \sum_{i=1}^n \left(
a+bx_i-y_i
\right)^2.
\]

The solution is:

\[
\left\{
\begin{array}{rl}
b  = & \dfrac{
n \displaystyle\sum_{i=1}^n x_i y_i - \displaystyle\sum_{i=1}^n  y_i \displaystyle\sum_{i=1}^n x_i
}{
n \displaystyle\sum_{i=1}^n x_i x_i -   \displaystyle\sum_{i=1}^n x_i\displaystyle\sum_{i=1}^n x_i
}\\
a = & \dfrac{1}{n}\displaystyle\sum_{i=1}^n  y_i - b  \dfrac{1}{n} \displaystyle\sum_{i=1}^n x_i  \\
\end{array}
\right.
\]

Fitting in R can be performed by calling the \texttt{lm()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"ISLR"}\NormalTok{) }\CommentTok{# Credit dataset}
\NormalTok{X <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Balance[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{])}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Rating[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{])}
\NormalTok{f <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X) }\CommentTok{# Y~X is a formula, read: Y is a function of X}
\KeywordTok{print}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Y ~ X)
## 
## Coefficients:
## (Intercept)            X  
##     226.471        0.266
\end{verbatim}

Figure \ref{fig:simple_recap2} gives the scatter plot
of Y vs.~X together with the fitted simple linear model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(X, Y, }\DataTypeTok{xlab=}\StringTok{"X (Balance)"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Y (Credit)"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(f, }\DataTypeTok{col=}\DecValTok{2}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:simple_recap2}{%
\centering
\includegraphics{02-regression-multiple-figures/simple_recap2-1.pdf}
\caption{Fitted regression line for the Credit dataset}\label{fig:simple_recap2}
}
\end{figure}

\hypertarget{multiple-linear-regression}{%
\section{Multiple Linear Regression}\label{multiple-linear-regression}}

\hypertarget{problem-formulation}{%
\subsection{Problem Formulation}\label{problem-formulation}}

Let's now generalise the above to the case of many variables
\(X_1, \dots, X_p\).

We wish to model the dependent variable as a function of \(p\) independent variables.
\[
Y = f(X_1,\dots,X_p)   \qquad (+\varepsilon)
\]

Restricting ourselves to the class of \textbf{linear models}, we have
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p.
\]

Above we studied the case where \(p=1\), i.e., \(Y=a+bX_1\) with \(\beta_0=a\) and \(\beta_1=b\).

The above equation defines:

\begin{itemize}
\tightlist
\item
  \(p=1\) --- a line (see Figure \ref{fig:simple_recap2}),
\item
  \(p=2\) --- a plane (see Figure \ref{fig:scatterplot3dexample}),
\item
  \(p\ge 3\) --- a hyperplane (well, most people find it difficult
  to imagine objects in high dimensions,
  but we are lucky to have this thing called maths).
\end{itemize}

\begin{figure}
\hypertarget{fig:scatterplot3dexample}{%
\centering
\includegraphics{02-regression-multiple-figures/scatterplot3dexample-1.pdf}
\caption{Fitted regression plane for the Credit dataset}\label{fig:scatterplot3dexample}
}
\end{figure}

\hypertarget{fitting-a-linear-model-in-r}{%
\subsection{Fitting a Linear Model in R}\label{fitting-a-linear-model-in-r}}

\texttt{lm()} accepts a formula of the form \texttt{Y\textasciitilde{}X1+X2+...+Xp}.

It finds the least squares fit, i.e., solves
\[
\min_{\beta_0, \beta_1,\dots, \beta_p\in\mathbb{R}}
\sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_i \right) ^2
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X1 <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Balance[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{])}
\NormalTok{X2 <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Income[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{])}
\NormalTok{Y  <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Rating[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{])}
\NormalTok{f <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X1}\OperatorTok{+}\NormalTok{X2)}
\NormalTok{f}\OperatorTok{$}\NormalTok{coefficients }\CommentTok{# ß0, ß1, ß2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)          X1          X2 
##    172.5587      0.1828      2.1976
\end{verbatim}

By the way, the 3D scatter plot in Figure \ref{fig:scatterplot3dexample}
was generated by calling:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"scatterplot3d"}\NormalTok{)}
\NormalTok{s3d <-}\StringTok{ }\KeywordTok{scatterplot3d}\NormalTok{(X1, X2, Y,}
    \DataTypeTok{angle=}\DecValTok{60}\NormalTok{, }\CommentTok{# change angle to reveal more}
    \DataTypeTok{highlight.3d=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"Balance"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Income"}\NormalTok{,}
    \DataTypeTok{zlab=}\StringTok{"Rating"}\NormalTok{)}
\NormalTok{s3d}\OperatorTok{$}\KeywordTok{plane3d}\NormalTok{(f, }\DataTypeTok{lty.box=}\StringTok{"solid"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

(\texttt{s3d} is an R list, one of its elements named \texttt{plane3d} is a function object -- this is legal)

\hypertarget{finding-the-best-model}{%
\section{Finding the Best Model}\label{finding-the-best-model}}

\hypertarget{model-diagnostics}{%
\subsection{Model Diagnostics}\label{model-diagnostics}}

Here is Rating (\(Y\)) as function of Balance (\(X_1\), lefthand side of Figure \ref{fig:x12_y})
and Income (\(X_2\), righthand side of Figure \ref{fig:x12_y}).

\begin{figure}
\hypertarget{fig:x12_y}{%
\centering
\includegraphics{02-regression-multiple-figures/x12_y-1.pdf}
\caption{Scatter plots of \(Y\) vs.~\(X_1\) and \(X_2\)}\label{fig:x12_y}
}
\end{figure}

Moreover, Figure \ref{fig:x12_ycolmap} depicts
(in a hopefully readable manner) both \(X_1\) and \(X_2\) with Rating \(Y\)
encoded with a colour (low ratings are green, high ratings are red;
some rating values are explicitly printed out within the plot).

\begin{figure}
\hypertarget{fig:x12_ycolmap}{%
\centering
\includegraphics{02-regression-multiple-figures/x12_ycolmap-1.pdf}
\caption{A heatmap for Rating as a function of Balance and Income; greens represent low credit ratings, whereas reds -- high ones}\label{fig:x12_ycolmap}
}
\end{figure}

Consider the three following models.

\begin{longtable}[]{@{}ll@{}}
\toprule
Formula & Equation\tabularnewline
\midrule
\endhead
Rating \textasciitilde{} Balance + Income & \(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2\)\tabularnewline
Rating \textasciitilde{} Balance & \(Y=a + b X_1\) (\(\beta_0=a, \beta_1=b, \beta_2=0\))\tabularnewline
Rating \textasciitilde{} Income & \(Y=a + b X_2\) (\(\beta_0=a, \beta_1=0, \beta_2=b\))\tabularnewline
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f12 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X1}\OperatorTok{+}\NormalTok{X2) }\CommentTok{# Rating ~ Balance + Income}
\NormalTok{f12}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)          X1          X2 
##    172.5587      0.1828      2.1976
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f1  <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X1)    }\CommentTok{# Rating ~ Balance}
\NormalTok{f1}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)          X1 
##   226.47114     0.26615
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f2  <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X2)    }\CommentTok{# Rating ~ Income}
\NormalTok{f2}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)          X2 
##    253.8514      3.0253
\end{verbatim}

Which of the three models is the best?
Of course, by using the word ``best'',
we need to answer the question ``best?\ldots{} but with respect to what kind of measure?''

So far we were fitting w.r.t. SSR,
as the multiple regression model generalises the two simple ones,
the former must yield a not-worse SSR.
This is because in the case of \(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2\),
setting \(\beta_1\) to 0 (just one of uncountably many possible \(\beta_1\)s,
if it happens to be the \emph{best} one, good for us)
gives \(Y=a + b X_2\)
whereas by setting \(\beta_2\) to 0 we obtain \(Y=a + b X_1\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(f12}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 358261
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(f1}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2132108
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(f2}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1823473
\end{verbatim}

We get that, in terms of SSRs, \(f_{12}\) is better than \(f_{2}\),
which in turn is better than \(f_{1}\).
However, these error values per se (sheer numbers)
are meaningless (not meaningful).

\begin{description}
\item[Remark.]
Interpretability in ML has always been an important issue, think the EU
General Data Protection Regulation (GDPR), amongst others.
\end{description}

\hypertarget{ssr-mse-rmse-and-mae}{%
\subsubsection{SSR, MSE, RMSE and MAE}\label{ssr-mse-rmse-and-mae}}

The quality of fit can be assessed by performing some \emph{descriptive
statistical analysis of the residuals}, \(\hat{y}_i-y_i\),
for \(i=1,\dots,n\).

I know how to summarise data on the residuals!
Of course I should compute their arithmetic mean and I'm done with that shtask!
Interestingly, the mean of residuals (this can be shown analytically)
in the least squared fit is always equal to \(0\):
\[
 \frac{1}{n} \sum_{i=1}^n (\hat{y}_i-y_i)=0.
\]
Therefore, we need a different metric.

\begin{exercise}

(*) A proof of this fact is left as an exercise to the curious;
assume \(p=1\) just as in the previous chapter and note that \(\hat{y}_i=a x_i+b\).

\end{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(f12}\OperatorTok{$}\NormalTok{residuals) }\CommentTok{# almost zero numerically}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -2.0867e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{all.equal}\NormalTok{(}\KeywordTok{mean}\NormalTok{(f12}\OperatorTok{$}\NormalTok{residuals), }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

We noted that sum of squared residuals (SSR) is not interpretable,
but the mean squared residuals
(MSR) -- also called mean squared error (MSE) regression loss -- is a little better.
Recall that mean is defined as the sum divided by number of samples.

\[
 \mathrm{MSE}(f) = \frac{1}{n} \sum_{i=1}^n (f(\mathbf{x}_{i,\cdot})-y_i)^2.
\]

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(f12}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1155.7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(f1}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6877.8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(f2}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5882.2
\end{verbatim}

This gives an information of how much do we err \emph{per sample},
so at least this measure does not depend on \(n\) anymore.
However, if the original \(Y\)s are, say, in metres \([\mathrm{m}]\),
MSE is expressed in metres squared \([\mathrm{m}^2]\).

To account for that, we may consider the root mean squared error (RMSE):
\[
 \mathrm{RMSE}(f) = \sqrt{\frac{1}{n} \sum_{i=1}^n (f(\mathbf{x}_{i,\cdot})-y_i)^2}.
\]
This is just like with the sample variance vs.~standard deviation --
recall the latter is defined as the square root of the former.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(f12}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 33.995
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(f1}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 82.932
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(f2}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 76.695
\end{verbatim}

The interpretation of the RMSE is rather quirky;
it is some-sort-of-averaged \emph{deviance} from the true rating
(which is on the scale 0--1000, hence we see that the first model is not
that bad). Recall that the square function is sensitive to large observations,
hence, it penalises notable deviations more heavily.

As still we have a problem with finding something easily interpretable
(your non-technical boss or client may ask you: but what do these numbers mean??),
we suggest here that the mean absolute error (MAE;
also called mean absolute deviations, MAD)
might be a better idea than the above:
\[
 \mathrm{MAE}(f) = \frac{1}{n} \sum_{i=1}^n |f(\mathbf{x}_{i,\cdot})-y_i|.
\]

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{abs}\NormalTok{(f12}\OperatorTok{$}\NormalTok{residuals))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 22.863
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{abs}\NormalTok{(f1}\OperatorTok{$}\NormalTok{residuals))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 61.489
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{abs}\NormalTok{(f2}\OperatorTok{$}\NormalTok{residuals))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 64.151
\end{verbatim}

With the above we may say ``On average, the predicted rating differs from the
observed one by\ldots{}''. That is good enough.

\begin{description}
\item[Remark.]
(*) You may ask why don't we fit models so as to minimise the MAE
and we minimise the RMSE instead (note that minimising RMSE is the same as
minimising the SSR, one is a strictly monotone transformation of the other
and do not affect the solution). Well, it is possible.
It turns out that, however, minimising MAE is more computationally expensive
and the solution may be numerically unstable.
So it's rarely an analyst's first choice (assuming they are well-educated
enough to know about the MAD regression task). However, it may be worth
trying it out sometimes.

Sometimes we might prefer MAD regression to the classic one
if our data is heavily contaminated by outliers. But
in such cases it is worth checking if proper data cleansing does
the trick.
\end{description}

\hypertarget{graphical-summaries-of-residuals}{%
\subsubsection{Graphical Summaries of Residuals}\label{graphical-summaries-of-residuals}}

If we are not happy with single numerical aggregated of the residuals
or their absolute values, we can (and should) always compute a whole
bunch of descriptive statistics:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(f12}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -108.10   -1.94    7.81    0.00   20.25   50.62
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(f1}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  -226.8   -48.3   -10.1     0.0    42.6   268.7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(f2}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -195.16  -57.34   -1.28    0.00   64.01  175.34
\end{verbatim}

The outputs generated by \texttt{summary()} include:

\begin{itemize}
\tightlist
\item
  \texttt{Min.} -- sample minimum
\item
  \texttt{1st\ Qu.} -- 1st quartile == 25th percentile == quantile of order 0.25
\item
  \texttt{Median} -- median == 50th percentile == quantile of order 0.5
\item
  \texttt{3rd\ Qu.} -- 3rd quartile = 75th percentile == quantile of order 0.75
\item
  \texttt{Max.} -- sample maximum
\end{itemize}

For example, 1st quartile is the observation \(q\) such that
25\% values are \(\le q\) and 75\% values are \(\ge q\),
see \texttt{?quantile} in R.

Graphically, it is nice to summarise the empirical distribution
of the residuals on a \textbf{box and whisker plot}.
Here is the key to decipher Figure \ref{fig:boxplot_explained}:

\begin{itemize}
\tightlist
\item
  IQR == Interquartile range == Q3\(-\)Q1 (box width)
\item
  The box contains 50\% of the ``most typical'' observations
\item
  Box and whiskers altogether have width \(\le\) 4 IQR
\item
  Outliers == observations potentially worth inspecting (is it a bug or a feature?)
\end{itemize}

\begin{figure}
\hypertarget{fig:boxplot_explained}{%
\centering
\includegraphics{02-regression-multiple-figures/boxplot_explained-1.pdf}
\caption{An example boxplot}\label{fig:boxplot_explained}
}
\end{figure}

Figure \ref{fig:boxplot_residuals} is worth a thousand words:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(}\DataTypeTok{horizontal=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"residuals"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{,}
  \KeywordTok{list}\NormalTok{(}\DataTypeTok{f12=}\NormalTok{f12}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{f1=}\NormalTok{f1}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{f2=}\NormalTok{f2}\OperatorTok{$}\NormalTok{residuals))}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\DecValTok{0}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:boxplot_residuals}{%
\centering
\includegraphics{02-regression-multiple-figures/boxplot_residuals-1.pdf}
\caption{Box plots of the residuals for the three models studied}\label{fig:boxplot_residuals}
}
\end{figure}

Figure \ref{fig:violinplot_residuals} gives a \emph{violin plot} -- a blend of a box plot and a (kernel) density estimator (histogram-like):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"vioplot"}\NormalTok{)}
\KeywordTok{vioplot}\NormalTok{(}\DataTypeTok{horizontal=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"residuals"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{,}
  \KeywordTok{list}\NormalTok{(}\DataTypeTok{f12=}\NormalTok{f12}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{f1=}\NormalTok{f1}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{f2=}\NormalTok{f2}\OperatorTok{$}\NormalTok{residuals))}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\DecValTok{0}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:violinplot_residuals}{%
\centering
\includegraphics{02-regression-multiple-figures/violinplot_residuals-1.pdf}
\caption{Violin plots of the residuals for the three models studied}\label{fig:violinplot_residuals}
}
\end{figure}

We can also take a look at the absolute values of the residuals.
Here are some descriptive statistics:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(}\KeywordTok{abs}\NormalTok{(f12}\OperatorTok{$}\NormalTok{residuals))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.065   6.464  14.071  22.863  26.418 108.100
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(}\KeywordTok{abs}\NormalTok{(f1}\OperatorTok{$}\NormalTok{residuals))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.506  19.664  45.072  61.489  80.124 268.738
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(}\KeywordTok{abs}\NormalTok{(f2}\OperatorTok{$}\NormalTok{residuals))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.655  29.854  59.676  64.151  95.738 195.156
\end{verbatim}

Figure \ref{fig:absresiduals_boxplot} is worth \$1000:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(}\DataTypeTok{horizontal=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"abs(residuals)"}\NormalTok{,}
  \KeywordTok{list}\NormalTok{(}\DataTypeTok{f12=}\KeywordTok{abs}\NormalTok{(f12}\OperatorTok{$}\NormalTok{residuals), }\DataTypeTok{f1=}\KeywordTok{abs}\NormalTok{(f1}\OperatorTok{$}\NormalTok{residuals),}
       \DataTypeTok{f2=}\KeywordTok{abs}\NormalTok{(f2}\OperatorTok{$}\NormalTok{residuals)))}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\DecValTok{0}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:absresiduals_boxplot}{%
\centering
\includegraphics{02-regression-multiple-figures/absresiduals_boxplot-1.pdf}
\caption{Box plots of the modules of the residuals for the three models studied}\label{fig:absresiduals_boxplot}
}
\end{figure}

\hypertarget{coefficient-of-determination-r-squared}{%
\subsubsection{Coefficient of Determination (R-squared)}\label{coefficient-of-determination-r-squared}}

If we didn't know the range of the dependent variable
(in our case we do know that the credit rating is on the scale 0--1000),
the RMSE or MAE would be hard to interpret.

It turns out that there is a popular \emph{normalised} (unit-less) measure
that is somehow easy to interpret with no domain-specific knowledge
of the modelled problem.
Namely, the (unadjusted) \textbf{\(R^2\) score} (the coefficient of determination)
is given by:

\[
R^2(f) = 1 - \frac{\sum_{i=1}^{n} \left(y_i-f(\mathbf{x}_{i,\cdot})\right)^2}{\sum_{i=1}^{n} \left(y_i-\bar{y}\right)^2},
\]
where \(\bar{y}\) is the arithmetic mean \(\frac{1}{n}\sum_{i=1}^n y_i\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(r12 <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(f12)}\OperatorTok{$}\NormalTok{r.squared)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.93909
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{sum}\NormalTok{(f12}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{((Y}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(Y))}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{# the same}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.93909
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(r1 <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(f1)}\OperatorTok{$}\NormalTok{r.squared)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.63751
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(r2 <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(f2)}\OperatorTok{$}\NormalTok{r.squared)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.68998
\end{verbatim}

The coefficient of determination gives the proportion of variance of the
dependent variable explained by independent variables in the model;
\(R^2(f)\simeq 1\) indicates a perfect fit.
The first model is a very good one, the simple models are
``more or less okay''.

Unfortunately, \(R^2\) tends to automatically increase as the number of independent variables
increase (recall that the more variables in the model,
the better the SSR must be).
To correct for this phenomenon, we sometimes consider the \textbf{adjusted \(R^2\)}:

\[
\bar{R}^2(f) = 1 - (1-{R}^2(f))\frac{n-1}{n-p-1}
\]

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(f12)}\OperatorTok{$}\NormalTok{adj.r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.93869
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(x); }\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{r12)}\OperatorTok{*}\NormalTok{(n}\DecValTok{-1}\NormalTok{)}\OperatorTok{/}\NormalTok{(n}\DecValTok{-3}\NormalTok{) }\CommentTok{# the same}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.93869
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(f1)}\OperatorTok{$}\NormalTok{adj.r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.63633
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(f2)}\OperatorTok{$}\NormalTok{adj.r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.68897
\end{verbatim}

In other words, the adjusted \(R^2\) penalises for more complex models.

\begin{description}
\item[Remark.]
(*) Side note -- results of some statistical tests (e.g., significance of coefficients)
are reported by calling \texttt{summary(f12)} etc. --- refer to a more advanced source to obtain more information.
These, however, require the verification of some assumptions regarding the input data
and the residuals.
\end{description}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(f12)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Y ~ X1 + X2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -108.10   -1.94    7.81   20.25   50.62 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 1.73e+02   3.95e+00    43.7   <2e-16 ***
## X1          1.83e-01   5.16e-03    35.4   <2e-16 ***
## X2          2.20e+00   5.64e-02    39.0   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 34.2 on 307 degrees of freedom
## Multiple R-squared:  0.939,  Adjusted R-squared:  0.939 
## F-statistic: 2.37e+03 on 2 and 307 DF,  p-value: <2e-16
\end{verbatim}

\hypertarget{residuals-vs.-fitted-plot}{%
\subsubsection{Residuals vs.~Fitted Plot}\label{residuals-vs.-fitted-plot}}

We can also create scatter plots of the residuals
(predicted \(\hat{y}_i\) minus
true \(y_i\)) as a function of the predicted
\(\hat{y}_i=f(\mathbf{x}_{i,\cdot})\), see Figure \ref{fig:resid_vs_fitted}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred12 <-}\StringTok{ }\NormalTok{f12}\OperatorTok{$}\NormalTok{fitted.values }\CommentTok{# predict(f12, data.frame(X1, X2))}
\NormalTok{Y_pred1  <-}\StringTok{ }\NormalTok{f1}\OperatorTok{$}\NormalTok{fitted.values  }\CommentTok{# predict(f1, data.frame(X1))}
\NormalTok{Y_pred2  <-}\StringTok{ }\NormalTok{f2}\OperatorTok{$}\NormalTok{fitted.values  }\CommentTok{# predict(f2, data.frame(X2))}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(Y_pred12, Y_pred12}\OperatorTok{-}\NormalTok{Y)}
\KeywordTok{plot}\NormalTok{(Y_pred1,  Y_pred1 }\OperatorTok{-}\NormalTok{Y)}
\KeywordTok{plot}\NormalTok{(Y_pred2,  Y_pred2 }\OperatorTok{-}\NormalTok{Y)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:resid_vs_fitted}{%
\centering
\includegraphics{02-regression-multiple-figures/resid_vs_fitted-1.pdf}
\caption{Residuals vs.~fitted outputs for the three regression models}\label{fig:resid_vs_fitted}
}
\end{figure}

Ideally (provided that the hypothesis that the dependent variable
is indeed a linear function of the dependent variable(s) is true),
we would expect to see a point cloud that spread around \(0\) in a
very much unorderly fashion.

\hypertarget{variable-selection}{%
\subsection{Variable Selection}\label{variable-selection}}

Okay, up to now we've been considering the problem of modelling
the \texttt{Rating} variable as a function of \texttt{Balance} and/or \texttt{Income}.
However, it the \texttt{Credit} data set there are other variables
possibly worth inspecting.

Consider all quantitative (numeric-continuous) variables in the \texttt{Credit} data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C <-}\StringTok{ }\NormalTok{Credit[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{,}
    \KeywordTok{c}\NormalTok{(}\StringTok{"Rating"}\NormalTok{, }\StringTok{"Limit"}\NormalTok{, }\StringTok{"Income"}\NormalTok{, }\StringTok{"Age"}\NormalTok{,}
      \StringTok{"Education"}\NormalTok{, }\StringTok{"Balance"}\NormalTok{)]}
\KeywordTok{head}\NormalTok{(C)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Rating Limit  Income Age Education Balance
## 1    283  3606  14.891  34        11     333
## 2    483  6645 106.025  82        15     903
## 3    514  7075 104.593  71        11     580
## 4    681  9504 148.924  36        11     964
## 5    357  4897  55.882  68        16     331
## 6    569  8047  80.180  77        10    1151
\end{verbatim}

Obviously there are many possible combinations of the variables
upon which regression models can be constructed
(precisely, for \(p\) variables there are \(2^p\) such models).
How do we choose the \emph{best} set of inputs?

\begin{description}
\item[Remark.]
We should already be suspicious at this point:
wait\ldots{} \emph{best} requires some sort of criterion, right?
\end{description}

First, however, let's draw a matrix of scatter plots for
every pair of variables
-- so as to get an impression of how individual variables
interact with each other, see Figure \ref{fig:pairs}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairs}\NormalTok{(C)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:pairs}{%
\centering
\includegraphics{02-regression-multiple-figures/pairs-1.pdf}
\caption{Scatter plot matrix for the Credit dataset}\label{fig:pairs}
}
\end{figure}

It seems like \texttt{Rating} depends on \texttt{Limit} almost linearly\ldots{}
We have a tool to actually quantify the degree of linear dependence
between a pair of variables --
Pearson's \(r\) -- the linear correlation coefficient:

\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y})
}{
    \sqrt{\sum_{i=1}^n (x_i-\bar{x})^2} \sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}
}.
\]

It holds \(r\in[-1,1]\), where:

\begin{itemize}
\tightlist
\item
  \(r=1\) -- positive linear dependence (\(y\) increases as \(x\) increases)
\item
  \(r=-1\) -- negative linear dependence (\(y\) decreases as \(x\) increases)
\item
  \(r\simeq 0\) -- uncorrelated or non-linearly dependent
\end{itemize}

Figure \ref{fig:pearson_interpret} gives an illustration of the above.

\begin{figure}
\hypertarget{fig:pearson_interpret}{%
\centering
\includegraphics{02-regression-multiple-figures/pearson_interpret-1.pdf}
\caption{Different datasets and the corresponding Pearson's \(r\) coefficients}\label{fig:pearson_interpret}
}
\end{figure}

To compute Pearson's \(r\) between all pairs of variables, we call:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\KeywordTok{cor}\NormalTok{(C), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Rating  Limit Income   Age Education Balance
## Rating     1.000  0.996  0.831 0.167    -0.040   0.798
## Limit      0.996  1.000  0.834 0.164    -0.032   0.796
## Income     0.831  0.834  1.000 0.227    -0.033   0.414
## Age        0.167  0.164  0.227 1.000     0.024   0.008
## Education -0.040 -0.032 -0.033 0.024     1.000   0.001
## Balance    0.798  0.796  0.414 0.008     0.001   1.000
\end{verbatim}

\texttt{Rating} and \texttt{Limit} are almost perfectly linearly correlated,
and both seem to describe the same thing.

For practical purposes, we'd rather model \texttt{Rating} as a function of the other variables.
For simple linear regression models, we'd choose either \texttt{Income} or \texttt{Balance}.
How about multiple regression though?

The best model:

\begin{itemize}
\tightlist
\item
  has high predictive power,
\item
  is simple.
\end{itemize}

These two criteria are often mutually exclusive.

Which variables should be included in the optimal model?

Again, the definition of the ``best'' object needs a \emph{fitness} function.

For fitting a single model to data, we use the SSR.

We need a metric that takes the number of dependent variables into account.

\begin{description}
\item[Remark.]
(*) Unfortunately, the adjusted \(R^2\), despite its interpretability,
is not really suitable for this task. It does not penalise complex models
heavily enough to be really useful.
\end{description}

Here we'll be using \textbf{the Akaike Information Criterion} (AIC).

For a model \(f\) with \(p'\) independent variables:
\[
\mathrm{AIC}(f) = 2(p'+1)+n\log(\mathrm{SSR}(f))-n\log n
\]

Our task is to find the combination of independent variables
that minimises the AIC.

\begin{description}
\item[Remark.]
(**) Note that this is a bi-level optimisation problem -- for every
considered combination of variables (which we look for),
we must solve another problem of finding the best model
involving these variables -- the one that minimises the SSR.
\[
\min_{s_1,s_2,\dots,s_p\in\{0, 1\}}
\left(
\begin{array}{l}
2\left(\displaystyle\sum_{j=1}^p s_j +1\right)+\\
n\log\left(
\displaystyle\min_{\beta_0,\beta_1,\dots,\beta_p\in\mathbb{R}}
\sum_{i=1}^n \left(
\beta_0 + s_1\beta_1 x_{i,1} + \dots + s_p\beta_p x_{i,p}
-y_i
\right)^2
\right)
\end{array}
\right)
\]
We dropped the \(n\log n\) term, because it is always constant
and hence doesn't affect the solution.
If \(s_j=0\), then the \(s_j\beta_j x_{i,j}\) term is equal to
\(0\), and hence is not considered in the model.
This plays the role of including \(s_j=1\) or omitting \(s_j=0\) the \(j\)-th
variable in the model building exercise.
\end{description}

For \(p\) variables, the number of their possible
combinations is equal to \(2^p\)
(grows exponentially with \(p\)).
For large \(p\) (think big data), an extensive search is impractical
(in our case we could get away with this though -- left as an exercise
to a slightly more advanced reader).
Therefore, to find the variable combination minimising the AIC,
we often rely on one of the two following greedy heuristics:

\begin{itemize}
\item
  forward selection:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    start with an empty model
  \item
    find an independent variable
    whose addition to the current model would yield the highest decrease in the AIC and add it to the model
  \item
    go to step 2 until AIC decreases
  \end{enumerate}
\item
  backward elimination:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    start with the full model
  \item
    find an independent variable
    whose removal from the current model would decrease the AIC the most and eliminate it from the model
  \item
    go to step 2 until AIC decreases
  \end{enumerate}
\end{itemize}

\begin{description}
\item[Remark.]
(**) The above bi-level optimisation problem
can be solved by implementing a genetic algorithm -- see further chapter for more details.
\item[Remark.]
(*) There are of course many other methods which also perform
some form of variable selection, e.g., lasso regression.
But these minimise a different objective.
\end{description}

First, a forward selection example.
We need a data sample to work with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C <-}\StringTok{ }\NormalTok{Credit[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{,}
    \KeywordTok{c}\NormalTok{(}\StringTok{"Rating"}\NormalTok{, }\StringTok{"Income"}\NormalTok{, }\StringTok{"Age"}\NormalTok{,}
      \StringTok{"Education"}\NormalTok{, }\StringTok{"Balance"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

Then, a formula that represents a model with no variables
(model from which we'll start our search):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(model_empty <-}\StringTok{ }\NormalTok{Rating}\OperatorTok{~}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rating ~ 1
\end{verbatim}

Last, we need a model that includes all the variables.
We're too lazy to list all of them manually, therefore,
we can use the \texttt{model.frame()} function to generate
a corresponding formula:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(model_full <-}\StringTok{ }\KeywordTok{formula}\NormalTok{(}\KeywordTok{model.frame}\NormalTok{(Rating}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{C))) }\CommentTok{# all variables}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rating ~ Income + Age + Education + Balance
\end{verbatim}

Now we are ready.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{step}\NormalTok{(}\KeywordTok{lm}\NormalTok{(model_empty, }\DataTypeTok{data=}\NormalTok{C), }\CommentTok{# starting model}
    \DataTypeTok{scope=}\NormalTok{model_full,         }\CommentTok{# gives variables to consider}
    \DataTypeTok{direction=}\StringTok{"forward"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Start:  AIC=3055.8
## Rating ~ 1
## 
##             Df Sum of Sq     RSS  AIC
## + Income     1   4058342 1823473 2695
## + Balance    1   3749707 2132108 2743
## + Age        1    164567 5717248 3049
## <none>                   5881815 3056
## + Education  1      9631 5872184 3057
## 
## Step:  AIC=2694.7
## Rating ~ Income
## 
##             Df Sum of Sq     RSS  AIC
## + Balance    1   1465212  358261 2192
## <none>                   1823473 2695
## + Age        1      2836 1820637 2696
## + Education  1      1063 1822410 2697
## 
## Step:  AIC=2192.3
## Rating ~ Income + Balance
## 
##             Df Sum of Sq    RSS  AIC
## + Age        1      4119 354141 2191
## + Education  1      2692 355568 2192
## <none>                   358261 2192
## 
## Step:  AIC=2190.7
## Rating ~ Income + Balance + Age
## 
##             Df Sum of Sq    RSS  AIC
## + Education  1      2926 351216 2190
## <none>                   354141 2191
## 
## Step:  AIC=2190.1
## Rating ~ Income + Balance + Age + Education
\end{verbatim}

\begin{verbatim}
## 
## Call:
## lm(formula = Rating ~ Income + Balance + Age + Education, data = C)
## 
## Coefficients:
## (Intercept)       Income      Balance          Age    Education  
##     173.830        2.167        0.184        0.223       -0.960
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{formula}\NormalTok{(}\KeywordTok{lm}\NormalTok{(Rating}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{C))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rating ~ Income + Age + Education + Balance
\end{verbatim}

The full model has been selected.

\bigskip

And now for something completely different --
a backward elimination example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{step}\NormalTok{(}\KeywordTok{lm}\NormalTok{(model_full, }\DataTypeTok{data=}\NormalTok{C), }\CommentTok{# from}
     \DataTypeTok{scope=}\NormalTok{model_empty,      }\CommentTok{# to}
     \DataTypeTok{direction=}\StringTok{"backward"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Start:  AIC=2190.1
## Rating ~ Income + Age + Education + Balance
## 
##             Df Sum of Sq     RSS  AIC
## <none>                    351216 2190
## - Education  1      2926  354141 2191
## - Age        1      4353  355568 2192
## - Balance    1   1468466 1819682 2698
## - Income     1   1617191 1968406 2722
\end{verbatim}

\begin{verbatim}
## 
## Call:
## lm(formula = Rating ~ Income + Age + Education + Balance, data = C)
## 
## Coefficients:
## (Intercept)       Income          Age    Education      Balance  
##     173.830        2.167        0.223       -0.960        0.184
\end{verbatim}

The full model is considered the best again.

\bigskip

Forward selection example -- full dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C <-}\StringTok{ }\NormalTok{Credit[,  }\CommentTok{# do not restrict to Credit$Balance>0}
    \KeywordTok{c}\NormalTok{(}\StringTok{"Rating"}\NormalTok{, }\StringTok{"Income"}\NormalTok{, }\StringTok{"Age"}\NormalTok{,}
      \StringTok{"Education"}\NormalTok{, }\StringTok{"Balance"}\NormalTok{)]}
\KeywordTok{step}\NormalTok{(}\KeywordTok{lm}\NormalTok{(model_empty, }\DataTypeTok{data=}\NormalTok{C),}
    \DataTypeTok{scope=}\NormalTok{model_full,}
    \DataTypeTok{direction=}\StringTok{"forward"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Start:  AIC=4034.3
## Rating ~ 1
## 
##             Df Sum of Sq     RSS  AIC
## + Balance    1   7124258 2427627 3488
## + Income     1   5982140 3569744 3643
## + Age        1    101661 9450224 4032
## <none>                   9551885 4034
## + Education  1      8675 9543210 4036
## 
## Step:  AIC=3488.4
## Rating ~ Balance
## 
##             Df Sum of Sq     RSS  AIC
## + Income     1   1859749  567878 2909
## + Age        1     98562 2329065 3474
## <none>                   2427627 3488
## + Education  1      5130 2422497 3490
## 
## Step:  AIC=2909.3
## Rating ~ Balance + Income
## 
##             Df Sum of Sq    RSS  AIC
## <none>                   567878 2909
## + Age        1      2142 565735 2910
## + Education  1      1209 566669 2910
\end{verbatim}

\begin{verbatim}
## 
## Call:
## lm(formula = Rating ~ Balance + Income, data = C)
## 
## Coefficients:
## (Intercept)      Balance       Income  
##     145.351        0.213        2.186
\end{verbatim}

This procedure suggests including only the \texttt{Balance} and \texttt{Income}
variables.

\bigskip

Backward elimination example -- full dataset:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{step}\NormalTok{(}\KeywordTok{lm}\NormalTok{(model_full, }\DataTypeTok{data=}\NormalTok{C), }\CommentTok{# full model}
     \DataTypeTok{scope=}\NormalTok{model_empty, }\CommentTok{# empty model}
     \DataTypeTok{direction=}\StringTok{"backward"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Start:  AIC=2910.9
## Rating ~ Income + Age + Education + Balance
## 
##             Df Sum of Sq     RSS  AIC
## - Education  1      1238  565735 2910
## - Age        1      2172  566669 2910
## <none>                    564497 2911
## - Income     1   1759273 2323770 3475
## - Balance    1   2992164 3556661 3645
## 
## Step:  AIC=2909.8
## Rating ~ Income + Age + Balance
## 
##           Df Sum of Sq     RSS  AIC
## - Age      1      2142  567878 2909
## <none>                  565735 2910
## - Income   1   1763329 2329065 3474
## - Balance  1   2991523 3557259 3643
## 
## Step:  AIC=2909.3
## Rating ~ Income + Balance
## 
##           Df Sum of Sq     RSS  AIC
## <none>                  567878 2909
## - Income   1   1859749 2427627 3488
## - Balance  1   3001866 3569744 3643
\end{verbatim}

\begin{verbatim}
## 
## Call:
## lm(formula = Rating ~ Income + Balance, data = C)
## 
## Coefficients:
## (Intercept)       Income      Balance  
##     145.351        2.186        0.213
\end{verbatim}

This procedure gives the same results as forward selection
(however, for other data sets this might not necessarily be the case).

\hypertarget{variable-transformation}{%
\subsection{Variable Transformation}\label{variable-transformation}}

So far we have been fitting linear models of the form:
\[
Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p.
\]

What about some non-linear models such as polynomials etc.? For example:
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_1^3 + \beta_4 X_2.
\]

Solution: pre-process inputs by setting
\(X_1' := X_1\), \(X_2' := X_1^2\), \(X_3' := X_1^3\), \(X_4' := X_2\)
and fit a linear model:

\[
Y = \beta_0 + \beta_1 X_1' + \beta_2 X_2' + \beta_3 X_3' + \beta_4 X_4'.
\]

This trick works for every model of the form
\(Y=\sum_{i=1}^k \sum_{j=1}^p \varphi_{i,j}(X_j)\) for any \(k\)
and any univariate functions \(\varphi_{i,j}\).

Also, with a little creativity (and maths), we might be able to transform
a few other models to a linear one, e.g.,

\[
Y = b e^{aX} \qquad \to \qquad \log Y = \log b + aX \qquad\to\qquad Y'=aX+b'
\]

This is an example of a model's \textbf{linearisation}.
However, not every model can be linearised.
In particular, one that involves functions that are not invertible.

For example, here's a series of simple (\(p=1\)) degree-\(d\)
polynomial regression models
of the form:
\[
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \dots + \beta_d X^d.
\]

Such models can be fitted with the \texttt{lm()} function based on the formula
of the form \texttt{Y\textasciitilde{}poly(X,\ d,\ raw=TRUE)} or \texttt{Y\textasciitilde{}X+I(X\^{}2)+I(X\^{}3)+...}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f1_}\DecValTok{1}\NormalTok{  <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X1)}
\NormalTok{f1_}\DecValTok{3}\NormalTok{  <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X1}\OperatorTok{+}\KeywordTok{I}\NormalTok{(X1}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{+}\KeywordTok{I}\NormalTok{(X1}\OperatorTok{^}\DecValTok{3}\NormalTok{)) }\CommentTok{# also: Y~poly(X1, 3, raw=TRUE)}
\NormalTok{f1_}\DecValTok{10}\NormalTok{ <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\KeywordTok{poly}\NormalTok{(X1, }\DecValTok{10}\NormalTok{, }\DataTypeTok{raw=}\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Above we have fitted the polynomials of degrees 1, 3 and 10.
Note that a polynomial of degree 1 is just a line.

Let us depict the three models:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(X1, Y, }\DataTypeTok{col=}\StringTok{"#000000aa"}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1100}\NormalTok{))}
\NormalTok{x <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(X1), }\KeywordTok{max}\NormalTok{(X1), }\DataTypeTok{length.out=}\DecValTok{101}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{predict}\NormalTok{(f1_}\DecValTok{1}\NormalTok{, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{X1=}\NormalTok{x)), }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{predict}\NormalTok{(f1_}\DecValTok{3}\NormalTok{, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{X1=}\NormalTok{x)), }\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{predict}\NormalTok{(f1_}\DecValTok{10}\NormalTok{, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{X1=}\NormalTok{x)), }\DataTypeTok{col=}\StringTok{"darkgreen"}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:poly2}{%
\centering
\includegraphics{02-regression-multiple-figures/poly2-1.pdf}
\caption{Polynomials of different degrees fitted to the Credit dataset}\label{fig:poly2}
}
\end{figure}

From Figure \ref{fig:poly2} we see that there's clearly a problem
with the degree-10 polynomial.

\hypertarget{predictive-vs.-descriptive-power}{%
\subsection{Predictive vs.~Descriptive Power}\label{predictive-vs.-descriptive-power}}

The above high-degree polynomial model (\texttt{f1\_10}) is a typical instance
of a phenomenon called an \textbf{overfit}.

Clearly (based on our expert knowledge), the \texttt{Rating} shouldn't
decrease as \texttt{Balance} increases.

In other words, \texttt{f1\_10} gives a better fit to data actually observed,
but fails to produce good results for the points that are yet to come.

We say that it \textbf{generalises} poorly to unseen data.

Assume our true model is of the form:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{true_model <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\DecValTok{3}\OperatorTok{*}\NormalTok{x}\OperatorTok{^}\DecValTok{3}\OperatorTok{+}\DecValTok{5}
\end{Highlighting}
\end{Shaded}

Let's generate the following random sample from this model (with \(Y\) subject
to error), see Figure \ref{fig:figBIASVARIANCE1}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{) }\CommentTok{# to assure reproducibility}
\NormalTok{n <-}\StringTok{ }\DecValTok{25}
\NormalTok{X <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(n, }\DataTypeTok{min=}\DecValTok{0}\NormalTok{, }\DataTypeTok{max=}\DecValTok{1}\NormalTok{)}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{true_model}\NormalTok{(X)}\OperatorTok{+}\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{sd=}\FloatTok{0.2}\NormalTok{) }\CommentTok{# add normally-distributed noise}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(X, Y)}
\NormalTok{x <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length.out=}\DecValTok{101}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{true_model}\NormalTok{(x), }\DataTypeTok{col=}\DecValTok{2}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:figBIASVARIANCE1}{%
\centering
\includegraphics{02-regression-multiple-figures/figBIASVARIANCE1-1.pdf}
\caption{Synthetic data generated by means of the formula \(Y=3x^3+5\) (\(+\) noise)}\label{fig:figBIASVARIANCE1}
}
\end{figure}

Let's fit polynomials of different degrees, see Figure \ref{fig:figBIASVARIANCE2}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(X, Y)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{true_model}\NormalTok{(x), }\DataTypeTok{col=}\DecValTok{2}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}

\NormalTok{dmax <-}\StringTok{ }\DecValTok{11} \CommentTok{# maximal polynomial degree}
\NormalTok{MSE_train <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(dmax)}
\NormalTok{MSE_test  <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(dmax)}
\ControlFlowTok{for}\NormalTok{ (d }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{dmax) \{ }\CommentTok{# for every polynomial degree}
\NormalTok{    f <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\KeywordTok{poly}\NormalTok{(X, d, }\DataTypeTok{raw=}\OtherTok{TRUE}\NormalTok{)) }\CommentTok{# fit a d-degree polynomial}
\NormalTok{    y <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(f, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{X=}\NormalTok{x))}
    \KeywordTok{lines}\NormalTok{(x, y, }\DataTypeTok{col=}\NormalTok{d)}
    \CommentTok{# MSE on given random X,Y:}
\NormalTok{    MSE_train[d] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(f}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
    \CommentTok{# MSE on many more points:}
\NormalTok{    MSE_test[d]  <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((y}\OperatorTok{-}\KeywordTok{true_model}\NormalTok{(x))}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:figBIASVARIANCE2}{%
\centering
\includegraphics{02-regression-multiple-figures/figBIASVARIANCE2-1.pdf}
\caption{Polynomials fitted to our synthetic dataset}\label{fig:figBIASVARIANCE2}
}
\end{figure}

Some of the polynomials are fitted too well!

\begin{description}
\item[Remark]
(*) The oscillation of the high-degree polynomials at the domain
boundaries is known as the Runge phenomenon.
\end{description}

Compare the mean squared error (MSE) for the observed vs.~future data points,
see Figure \ref{fig:figBIASVARIANCE3}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{matplot}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{dmax, }\KeywordTok{cbind}\NormalTok{(MSE_train, MSE_test), }\DataTypeTok{type=}\StringTok{"b"}\NormalTok{,}
    \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\FloatTok{1e-3}\NormalTok{, }\FloatTok{2e3}\NormalTok{), }\DataTypeTok{log=}\StringTok{"y"}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,}
    \DataTypeTok{xlab=}\StringTok{"Model complexity (polynomial degree)"}\NormalTok{,}
    \DataTypeTok{ylab=}\StringTok{"MSE"}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\DataTypeTok{legend=}\KeywordTok{c}\NormalTok{(}\StringTok{"MSE on original data"}\NormalTok{, }\StringTok{"MSE on the whole range"}\NormalTok{),}
    \DataTypeTok{lty=}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\DataTypeTok{col=}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\DataTypeTok{bg=}\StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:figBIASVARIANCE3}{%
\centering
\includegraphics{02-regression-multiple-figures/figBIASVARIANCE3-1.pdf}
\caption{MSE on the dataset used to construct the model vs.~MSE on a whole range of points as function of the polynomial degree}\label{fig:figBIASVARIANCE3}
}
\end{figure}

Note the logarithmic scale on the \(y\) axis.

This is a very typical behaviour!

\begin{itemize}
\item
  A model's fit to observed data improves as the model's complexity increases.
\item
  A model's generalisation to unseen data initially improves, but then becomes worse.
\item
  In the above example, the sweet spot is at a polynomial of degree 3, which is exactly
  our true underlying model.
\end{itemize}

Hence, most often \textbf{we should be interested in the accuracy of the predictions
made in the case of unobserved data.}

If we have a data set of a considerable size,
we can divide it (randomly) into two parts:

\begin{itemize}
\tightlist
\item
  \emph{training sample} (say, 60\% or 80\%) -- used to fit a model
\item
  \emph{test sample} (the remaining 40\% or 20\%) -- used to assess its quality
  (e.g., using MSE)
\end{itemize}

More on this issue in the chapter on Classification.

\begin{description}
\item[Remark.]
(*) We shall see that sometimes a train-test-validate
split will be necessary, e.g., 60-20-20\%.
\end{description}

\hypertarget{exercises-in-r-1}{%
\section{Exercises in R}\label{exercises-in-r-1}}

\hypertarget{anscombes-quartet-revisited}{%
\subsection{Anscombe's Quartet Revisited}\label{anscombes-quartet-revisited}}

Consider the \texttt{anscombe} database once again:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(anscombe) }\CommentTok{# `anscombe` is a built-in object}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    x1 x2 x3 x4    y1   y2    y3    y4
## 1  10 10 10  8  8.04 9.14  7.46  6.58
## 2   8  8  8  8  6.95 8.14  6.77  5.76
## 3  13 13 13  8  7.58 8.74 12.74  7.71
## 4   9  9  9  8  8.81 8.77  7.11  8.84
## 5  11 11 11  8  8.33 9.26  7.81  8.47
## 6  14 14 14  8  9.96 8.10  8.84  7.04
## 7   6  6  6  8  7.24 6.13  6.08  5.25
## 8   4  4  4 19  4.26 3.10  5.39 12.50
## 9  12 12 12  8 10.84 9.13  8.15  5.56
## 10  7  7  7  8  4.82 7.26  6.42  7.91
## 11  5  5  5  8  5.68 4.74  5.73  6.89
\end{verbatim}

Recall that in the previous Chapter we have
split the above data into four data frames
\texttt{ans1}, \ldots{}, \texttt{ans4} with columns \texttt{x} and \texttt{y}.

\begin{exercise}

In \texttt{ans1}, fit a regression line to the data set as-is.

\end{exercise}

\begin{solution}

We've done that already, see Figure \ref{fig:anscombe3}.
What a wonderful exercise, thank you -- effective
learning is often done by repeating stuff.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ans1 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{x1, }\DataTypeTok{y=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{y1)}
\NormalTok{f1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y}\OperatorTok{~}\NormalTok{x, }\DataTypeTok{data=}\NormalTok{ans1)}
\KeywordTok{plot}\NormalTok{(ans1}\OperatorTok{$}\NormalTok{x, ans1}\OperatorTok{$}\NormalTok{y)}
\KeywordTok{abline}\NormalTok{(f1, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:anscombe3}{%
\centering
\includegraphics{02-regression-multiple-figures/anscombe3-1.pdf}
\caption{Fitted regression line for \texttt{ans1}}\label{fig:anscombe3}
}
\end{figure}

\end{solution}

\begin{exercise}

In \texttt{ans2}, fit a quadratic model (\(y=a + bx + cx^2\)).

\end{exercise}

\begin{solution}

How to fit a polynomial model is explained above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ans2 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{x2, }\DataTypeTok{y=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{y2)}
\NormalTok{f2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y}\OperatorTok{~}\NormalTok{x}\OperatorTok{+}\KeywordTok{I}\NormalTok{(x}\OperatorTok{^}\DecValTok{2}\NormalTok{), }\DataTypeTok{data=}\NormalTok{ans2)}
\KeywordTok{plot}\NormalTok{(ans2}\OperatorTok{$}\NormalTok{x, ans2}\OperatorTok{$}\NormalTok{y)}
\NormalTok{x_plot <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DataTypeTok{by=}\FloatTok{0.1}\NormalTok{)}
\NormalTok{y_plot <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(f2, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x_plot))}
\KeywordTok{lines}\NormalTok{(x_plot, y_plot, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:anscombe4}{%
\centering
\includegraphics{02-regression-multiple-figures/anscombe4-1.pdf}
\caption{Fitted quadratic model for \texttt{ans2}}\label{fig:anscombe4}
}
\end{figure}

\emph{Comment: From Figure \ref{fig:anscombe4} we see that it's
an almost-perfect fit! Clearly,
the second Anscombe dataset isn't a case of linearly
dependent variables.}

\end{solution}

\begin{exercise}

In \texttt{ans3}, remove the obvious outlier from data
and fit a regression line.

\end{exercise}

\begin{solution}

Let's plot the data set first, see Figure \ref{fig:anscombe5}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ans3 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{x3, }\DataTypeTok{y=}\NormalTok{anscombe}\OperatorTok{$}\NormalTok{y3)}
\KeywordTok{plot}\NormalTok{(ans3}\OperatorTok{$}\NormalTok{x, ans3}\OperatorTok{$}\NormalTok{y)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:anscombe5}{%
\centering
\includegraphics{02-regression-multiple-figures/anscombe5-1.pdf}
\caption{Scatter plot for \texttt{ans3}}\label{fig:anscombe5}
}
\end{figure}

Indeed, the observation at \(x\simeq 13\) is an obvious outlier.
Perhaps the easiest way
to remove it is to call:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ans3b <-}\StringTok{ }\NormalTok{ans3[ans3}\OperatorTok{$}\NormalTok{y}\OperatorTok{<=}\DecValTok{12}\NormalTok{,] }\CommentTok{# the outlier is definitely at y>12}
\end{Highlighting}
\end{Shaded}

We could also use the condition \texttt{y\ \textless{}\ max(y)}, amongst others.

Now let's fit the linear model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f3b <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y}\OperatorTok{~}\NormalTok{x, }\DataTypeTok{data=}\NormalTok{ans3b)}
\KeywordTok{plot}\NormalTok{(ans3b}\OperatorTok{$}\NormalTok{x, ans3b}\OperatorTok{$}\NormalTok{y)}
\KeywordTok{abline}\NormalTok{(f3b, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:anscombe7}{%
\centering
\includegraphics{02-regression-multiple-figures/anscombe7-1.pdf}
\caption{Scatter plot for \texttt{ans3} with the outlier removed and the fitted linear model}\label{fig:anscombe7}
}
\end{figure}

\emph{Comment: Now Figure \ref{fig:anscombe7} is what we call linearly correlated data.
By the way, Pearson's coefficient now equals \texttt{1}.}

\end{solution}

\hypertarget{countries-of-the-world-simple-models-involving-the-gdp-per-capita}{%
\subsection{Countries of the World -- Simple models involving the GDP per capita}\label{countries-of-the-world-simple-models-involving-the-gdp-per-capita}}

Let's consider the World Factbook 2020 dataset
(see this book's \texttt{datasets} folder).
It consists of country names, their population,
area, GDP, mortality rates etc. We have scraped it from the CIA website
at \url{https://www.cia.gov/library/publications/the-world-factbook/docs/rankorderguide.html}
and compiled into a single file on 3 April 2020.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factbook <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/world_factbook_2020.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here is a preview of a few features for 3 selected countries (see \texttt{help("\%in\%")}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factbook[factbook}\OperatorTok{$}\NormalTok{country }\OperatorTok{%in%}
\StringTok{    }\KeywordTok{c}\NormalTok{(}\StringTok{"Australia"}\NormalTok{, }\StringTok{"New Zealand"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
    \KeywordTok{c}\NormalTok{(}\StringTok{"country"}\NormalTok{, }\StringTok{"area"}\NormalTok{, }\StringTok{"population"}\NormalTok{, }\StringTok{"gdp_per_capita_ppp"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           country    area population gdp_per_capita_ppp
## 15      Australia 7741220   25466459              50400
## 169   New Zealand  268838    4925477              39000
## 247 United States 9833517  332639102              59800
\end{verbatim}

\begin{exercise}

List the 10 countries with the highest GDP per capita.

\end{exercise}

\begin{solution}

To recall, to generate a list of indexes that produce an ordered
version of a numeric vector, we need to call the \texttt{order()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{which_top <-}\StringTok{ }\KeywordTok{tail}\NormalTok{(}\KeywordTok{order}\NormalTok{(factbook}\OperatorTok{$}\NormalTok{gdp_per_capita_ppp, }\DataTypeTok{na.last=}\OtherTok{FALSE}\NormalTok{), }\DecValTok{10}\NormalTok{)}
\NormalTok{factbook[which_top, }\KeywordTok{c}\NormalTok{(}\StringTok{"country"}\NormalTok{, }\StringTok{"gdp_per_capita_ppp"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           country gdp_per_capita_ppp
## 113       Ireland              73200
## 35         Brunei              78900
## 114   Isle of Man              84600
## 211     Singapore              94100
## 26        Bermuda              99400
## 141    Luxembourg             105100
## 157        Monaco             115700
## 142         Macau             122000
## 192         Qatar             124100
## 139 Liechtenstein             139100
\end{verbatim}

By the way, the reported values are in USD.

\emph{Question: Which of these countries are tax havens?}

\end{solution}

\begin{exercise}

Find the 5 most positively and the 5 most negatively
correlated variables with the \texttt{gdp\_per\_capita\_ppp} feature
(of course, with respect to the Pearson coefficient).

\end{exercise}

\begin{solution}

This can be solved via a call to \texttt{cor()}.
Note that we need to make sure that missing vales are omitted
from computations.
A quick glimpse at the manual page
(\texttt{?cor}) reveals that computing the correlation between a column
and all the other ones (of course, except \texttt{country}, which
is non-numeric) can be performed as follows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(factbook}\OperatorTok{$}\NormalTok{gdp_per_capita_ppp,}
\NormalTok{    factbook[,}\OperatorTok{!}\NormalTok{(}\KeywordTok{names}\NormalTok{(factbook) }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"country"}\NormalTok{, }\StringTok{"gdp_per_capita_ppp"}\NormalTok{))],}
    \DataTypeTok{use=}\StringTok{"complete.obs"}\NormalTok{)[}\DecValTok{1}\NormalTok{,]}
\NormalTok{or <-}\StringTok{ }\KeywordTok{order}\NormalTok{(r) }\CommentTok{# ordering permutation (indexes)}
\NormalTok{r[}\KeywordTok{head}\NormalTok{(or, }\DecValTok{5}\NormalTok{)] }\CommentTok{# first 5 ordered indexes}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   infant_mortality_rate maternal_mortality_rate              birth_rate 
##                -0.74658                -0.67005                -0.60822 
##              death_rate    total_fertility_rate 
##                -0.57216                -0.56725
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r[}\KeywordTok{tail}\NormalTok{(or, }\DecValTok{5}\NormalTok{)] }\CommentTok{# last 5 ordered indexes}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        natural_gas_production         gross_national_saving 
##                       0.56898                       0.61133 
##                    median_age obesity_adult_prevalence_rate 
##                       0.62090                       0.63681 
##      life_expectancy_at_birth 
##                       0.75461
\end{verbatim}

\emph{Comment: ``Live long and prosper'' just gained a new meaning.
Richer countries have lower infant and maternal mortality rates,
lower birth rates, but higher life expectancy and obesity prevalence.
Note, however, that correlation is not causation:
we are unlikely to increase the GDP by asking people to put on weight.}

\end{solution}

\begin{exercise}

Fit simple regression models where the per capita GDP explains
its four most correlated variables (four individual models).
Draw them on a scatter plot. Compute the root mean squared errors (RMSE),
mean absolute errors (MAE) and the coefficients of determination (\(R^2\)).

\end{exercise}

\begin{solution}

The four most correlated variables (we should look at the absolute
value of the correlation coefficient now -- recall that it
is the correlation of 0 that means no linear dependence; 1 and -1
show a strong association between a pair of variables) are:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(most_correlated <-}\StringTok{ }\KeywordTok{names}\NormalTok{(r)[}\KeywordTok{tail}\NormalTok{(}\KeywordTok{order}\NormalTok{(}\KeywordTok{abs}\NormalTok{(r)), }\DecValTok{4}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "obesity_adult_prevalence_rate" "maternal_mortality_rate"      
## [3] "infant_mortality_rate"         "life_expectancy_at_birth"
\end{verbatim}

We could take the above column names and construct four
formulas manually, e.g., by writing
\texttt{gdp\_per\_capita\_ppp\textasciitilde{}life\_expectancy\_at\_birth},
but we are lazy. Being lazy when it comes to computer
programming is often a virtue, not a flaw in one's character.

Instead, we will run a \texttt{for} loop that extracts the pairs of
interesting columns and constructs a formula based on two vectors
(\texttt{lm(Y\textasciitilde{}X)}), see Figure \ref{fig:factbookA6}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\CommentTok{# 4 plots on a 2x2 grid}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{) \{}
    \KeywordTok{print}\NormalTok{(most_correlated[i])}
\NormalTok{    X <-}\StringTok{ }\NormalTok{factbook[,}\StringTok{"gdp_per_capita_ppp"}\NormalTok{]}
\NormalTok{    Y <-}\StringTok{ }\NormalTok{factbook[,most_correlated[i]]}
\NormalTok{    f <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X)}
    \KeywordTok{print}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{RMSE=}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(f}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)),}
                \DataTypeTok{MAE=}\KeywordTok{mean}\NormalTok{(}\KeywordTok{abs}\NormalTok{(f}\OperatorTok{$}\NormalTok{residuals)),}
                \DataTypeTok{R2=}\KeywordTok{summary}\NormalTok{(f)}\OperatorTok{$}\NormalTok{r.squared))}
    \KeywordTok{plot}\NormalTok{(X, Y, }\DataTypeTok{xlab=}\StringTok{"gdp_per_capita_ppp"}\NormalTok{,}
               \DataTypeTok{ylab=}\NormalTok{most_correlated[i])}
    \KeywordTok{abline}\NormalTok{(f, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "obesity_adult_prevalence_rate"
##        RMSE    MAE       R2
## [1,] 11.041 8.1589 0.062196
\end{verbatim}

\begin{verbatim}
## [1] "maternal_mortality_rate"
##        RMSE    MAE      R2
## [1,] 204.93 146.53 0.21481
\end{verbatim}

\begin{verbatim}
## [1] "infant_mortality_rate"
##        RMSE    MAE     R2
## [1,] 15.746 12.166 0.3005
\end{verbatim}

\begin{verbatim}
## [1] "life_expectancy_at_birth"
##        RMSE    MAE      R2
## [1,] 5.4292 4.3727 0.43096
\end{verbatim}

\begin{figure}
\hypertarget{fig:factbookA6}{%
\centering
\includegraphics{02-regression-multiple-figures/factbookA6-1.pdf}
\caption{A scatter plot matrix and regression lines for the 4 variables most correlated with the per capita GDP}\label{fig:factbookA6}
}
\end{figure}

Recall that the root mean squared error is the square root of
the arithmetic mean of the squared residuals.
Mean absolute error is the average of the absolute values of the residuals.
The coefficient of determination
is given by: \(R^2(f) = 1 - \frac{\sum_{i=1}^{n} \left(y_i-f(\mathbf{x}_{i,\cdot})\right)^2}{\sum_{i=1}^{n} \left(y_i-\bar{y}\right)^2}\).

\emph{Comment: Unfortunately, we were misled by the high correlation coefficients
between the \(X\)s and \(Y\)s:
the low actual \(R^2\) scores indicate that these models should not
be deemed trustworthy. Note that 3 of the plots are evidently L-shaped.}

\emph{Fun fact: (*) Interestingly, it can be shown that \(R^2\)
(in the case of the linear models fitted by minimising
the SSR) is the square of the correlation
between the true \(Y\)s and the predicted \(Y\)s:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X <-}\StringTok{ }\NormalTok{factbook[,}\StringTok{"gdp_per_capita_ppp"}\NormalTok{]}
\NormalTok{Y <-}\StringTok{ }\NormalTok{factbook[,most_correlated[i]]}
\NormalTok{f <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X, }\DataTypeTok{y=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(}\KeywordTok{summary}\NormalTok{(f)}\OperatorTok{$}\NormalTok{r.squared)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.43096
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{cor}\NormalTok{(f}\OperatorTok{$}\NormalTok{fitted.values, f}\OperatorTok{$}\NormalTok{y)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.43096
\end{verbatim}

\emph{Side note: Do note that RMSE and MAE are interpretable: for instance,
average error of life expectancy prediction based on the GDP is
4-5 years. Recall that you can find the information on the variables' units
of measure at
\url{https://www.cia.gov/library/publications/the-world-factbook/docs/rankorderguide.html}.}

\end{solution}

\hypertarget{countries-of-the-world-most-correlated-variables}{%
\subsection{Countries of the World -- Most correlated variables (*)}\label{countries-of-the-world-most-correlated-variables}}

Let's get back to the World Factbook 2020 dataset (\texttt{world\_factbook\_2020.csv}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factbook <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/world_factbook_2020.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{exercise}

Create a data frame \texttt{C} with three columns named \texttt{col1}, \texttt{col2}
and \texttt{r} and \(p(p-1)/2\) rows,
where \(p\) is the number of numeric features in \texttt{factbook}.
Every row should represent a unique pair of column names in \texttt{factbook}
(we do not distinguish between \texttt{a,b} and \texttt{b,a})
of correlation coefficients between them.

\end{exercise}

\begin{solution}

First we will solve this exercise considering only
4 numeric features in our dataset, so that we can keep
track of how the R expressions we evaluate actually work.

Let us compute the Pearson coefficients between chosen pairs of variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{R <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(factbook[,}\KeywordTok{c}\NormalTok{(}\StringTok{"area"}\NormalTok{, }\StringTok{"median_age"}\NormalTok{, }\StringTok{"birth_rate"}\NormalTok{, }\StringTok{"exports"}\NormalTok{)],}
    \DataTypeTok{use=}\StringTok{"complete.obs"}\NormalTok{) }\CommentTok{# 4 selected columns}
\KeywordTok{print}\NormalTok{(R)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 area median_age birth_rate  exports
## area        1.000000   0.044524  -0.031995  0.49259
## median_age  0.044524   1.000000  -0.921592  0.29973
## birth_rate -0.031995  -0.921592   1.000000 -0.24296
## exports     0.492586   0.299727  -0.242955  1.00000
\end{verbatim}

Note that the \texttt{R} matrix has \texttt{1.0} on the diagonal (where each entry
represents a correlation between a variable and itself).
Moreover, it is symmetric around the diagonal -- \texttt{R{[}i,j{]}\ ==\ R{[}j,i{]}},
because it is the correlation between the same pair of variables.
Hence, from now on we may be interested in the elements
below the diagonal. We can get access to them by using \texttt{lower.tri()}
(``lower triangle'').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{R[}\KeywordTok{lower.tri}\NormalTok{(R)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  0.044524 -0.031995  0.492586 -0.921592  0.299727 -0.242955
\end{verbatim}

This is already the 3rd column of the data frame we are asked to generate,
which should look like:

\begin{verbatim}
##         col1       col2         r
## 1 median_age       area  0.044524
## 2 birth_rate       area -0.031995
## 3    exports       area  0.492586
## 4 birth_rate median_age -0.921592
## 5    exports median_age  0.299727
## 6    exports birth_rate -0.242955
\end{verbatim}

How the generate \texttt{col1} and \texttt{col2}?
One idea is to take the ``lower triangles'' of the following matrices:

\begin{verbatim}
##      [,1]         [,2]         [,3]         [,4]        
## [1,] "area"       "area"       "area"       "area"      
## [2,] "median_age" "median_age" "median_age" "median_age"
## [3,] "birth_rate" "birth_rate" "birth_rate" "birth_rate"
## [4,] "exports"    "exports"    "exports"    "exports"
\end{verbatim}

and:

\begin{verbatim}
##      [,1]   [,2]         [,3]         [,4]     
## [1,] "area" "median_age" "birth_rate" "exports"
## [2,] "area" "median_age" "birth_rate" "exports"
## [3,] "area" "median_age" "birth_rate" "exports"
## [4,] "area" "median_age" "birth_rate" "exports"
\end{verbatim}

Here is a complete solution for all the features is \texttt{factbook}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{R <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(factbook[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DataTypeTok{use=}\StringTok{"complete.obs"}\NormalTok{) }\CommentTok{# skip the `country` column}
\NormalTok{rrr <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{dimnames}\NormalTok{(R)[[}\DecValTok{1}\NormalTok{]], }\DataTypeTok{nrow=}\KeywordTok{nrow}\NormalTok{(R), }\DataTypeTok{ncol=}\KeywordTok{ncol}\NormalTok{(R))}
\NormalTok{ccc <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{dimnames}\NormalTok{(R)[[}\DecValTok{2}\NormalTok{]], }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{nrow=}\KeywordTok{nrow}\NormalTok{(R), }\DataTypeTok{ncol=}\KeywordTok{ncol}\NormalTok{(R))}
\NormalTok{C <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{col1=}\NormalTok{rrr[}\KeywordTok{lower.tri}\NormalTok{(rrr)],}
                \DataTypeTok{col2=}\NormalTok{ccc[}\KeywordTok{lower.tri}\NormalTok{(ccc)],}
                \DataTypeTok{r=}\NormalTok{R[}\KeywordTok{lower.tri}\NormalTok{(R)])}
\end{Highlighting}
\end{Shaded}

\emph{Comment: In ``classical'' programming languages we would perhaps
have used of a double (nested) \texttt{for} loop here (a less readable solution).}

\end{solution}

\begin{exercise}

Find the 5 most correlated pairs of variables.

\end{exercise}

\begin{solution}

This can be done by ordering the rows of \texttt{C} in decreasing
order of absolute values of \texttt{C\$r}, and then choosing the first 5 rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C_top <-}\StringTok{ }\KeywordTok{head}\NormalTok{(C[}\KeywordTok{order}\NormalTok{(}\KeywordTok{abs}\NormalTok{(C}\OperatorTok{$}\NormalTok{r), }\DataTypeTok{decreasing=}\OtherTok{TRUE}\NormalTok{),], }\DecValTok{5}\NormalTok{)}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(C_top)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllr@{}}
\toprule
\begin{minipage}[b]{0.06\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.47\columnwidth}\raggedright
col1\strut
\end{minipage} & \begin{minipage}[b]{0.27\columnwidth}\raggedright
col2\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\raggedleft
r\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.06\columnwidth}\raggedright
1687\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
electricity\_installed\_generating\_capacity\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
electricity\_production\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedleft
0.99942\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedright
1684\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
electricity\_consumption\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
electricity\_production\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedleft
0.99921\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedright
88\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
labor\_force\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
population\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedleft
0.99862\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedright
1718\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
electricity\_installed\_generating\_capacity\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
electricity\_consumption\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedleft
0.99815\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.06\columnwidth}\raggedright
1300\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
telephones\_mobile\_cellular\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
labor\_force\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedleft
0.99793\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\emph{Comment: The most correlated pairs of features are not really
``mind-blowing''\ldots{}}

\end{solution}

\begin{exercise}

Fit simple regression models for the most correlated pair of variables.

\end{exercise}

\begin{solution}

There is a degree of ambiguity here: should \texttt{col1} or rather \texttt{col2}
be treated as the dependent variable in our model?
Let's do it either way.

To learn something new, which is exactly why we are all here,
we will create the formulas programmatically, by first
concatenating (joining) appropriate strings
(note that in order to input a double quotes character,
we need to proceed in with a backslash), and then
calling the \texttt{formula()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{form <-}\StringTok{ }\KeywordTok{formula}\NormalTok{(}\KeywordTok{paste}\NormalTok{(C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\StringTok{"~"}\NormalTok{, C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]))}
\NormalTok{f <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{factbook)}
\KeywordTok{print}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = form, data = factbook)
## 
## Coefficients:
##                               (Intercept)  
##                                  7.95e+08  
## electricity_installed_generating_capacity  
##                                  3.63e+03
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(factbook[,C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]], factbook[,C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]],}
    \DataTypeTok{xlab=}\NormalTok{C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], }\DataTypeTok{ylab=}\NormalTok{C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{])}
\KeywordTok{abline}\NormalTok{(f, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:factbookB9}{%
\centering
\includegraphics{02-regression-multiple-figures/factbookB9-1.pdf}
\caption{Most correlated pair of variables and the invisible regression line}\label{fig:factbookB9}
}
\end{figure}

Figure \ref{fig:factbookB9} is just great.

Wait, hold on a second! The fitted line is not present on the plot!

\begin{exercise}

There is a serious, extremely hard-to-detect bug in the above code.
The reader is kindly asked to try to identify it.

\end{exercise}

\begin{solution}

The author wishes to apologise, but this where he will get slightly\ldots{}
emotional. We are all programmers here. So let's speak like a programmer
to a programmer.

Aaaaaaaaaaaaaaaargh!!!!!!!!!!!
I've spent 3 hours trying to locate it!!!

So in Appendix related to data frame wrangling I have
explicitly asked everyone (including myself, because I tend to
forget that!!!)
to globally set the \texttt{stringsAsFactors=FALSE} option.
But we didn't do so.
The problem is where we create the \texttt{C} matrix using the \texttt{data.frame()}
function -- \texttt{C\$col1} and \texttt{C\$col2} are objects of type:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(C}\OperatorTok{$}\NormalTok{col1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(C}\OperatorTok{$}\NormalTok{col2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}

Is this is a problem? Well it is, because
indexing an object \texttt{x} with a factor \texttt{f},
\texttt{x{[}f{]}} means \texttt{x{[}as.numeric(f){]}} and not \texttt{x{[}as.character(f){]}}.

Therefore, \texttt{factbook{[},C\_top{[}1,1{]}{]}} refers to:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(factbook)[C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "gdp_per_capita_ppp"
\end{verbatim}

and not:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.character}\NormalTok{(C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "electricity_installed_generating_capacity"
\end{verbatim}

Here is a corrected version, see \ref{fig:factbookB13}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
    \DataTypeTok{col1=}\NormalTok{rrr[}\KeywordTok{lower.tri}\NormalTok{(rrr)],}
    \DataTypeTok{col2=}\NormalTok{ccc[}\KeywordTok{lower.tri}\NormalTok{(ccc)],}
    \DataTypeTok{r=}\NormalTok{R[}\KeywordTok{lower.tri}\NormalTok{(R)],}
    \DataTypeTok{stringsAsFactors=}\OtherTok{FALSE} \CommentTok{######## !!!!!!!!!!!!!!!!!!!!!!!!}
\NormalTok{)}
\NormalTok{C_top <-}\StringTok{ }\KeywordTok{head}\NormalTok{(C[}\KeywordTok{order}\NormalTok{(}\KeywordTok{abs}\NormalTok{(C}\OperatorTok{$}\NormalTok{r), }\DataTypeTok{decreasing=}\OtherTok{TRUE}\NormalTok{),], }\DecValTok{5}\NormalTok{)}
\NormalTok{form <-}\StringTok{ }\KeywordTok{formula}\NormalTok{(}\KeywordTok{paste}\NormalTok{(C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{], }\StringTok{"~"}\NormalTok{, C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]))}
\NormalTok{f <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(form, }\DataTypeTok{data=}\NormalTok{factbook)}
\KeywordTok{print}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = form, data = factbook)
## 
## Coefficients:
##                               (Intercept)  
##                                  7.95e+08  
## electricity_installed_generating_capacity  
##                                  3.63e+03
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(factbook[,C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]], factbook[,C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]],}
    \DataTypeTok{xlab=}\NormalTok{C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], }\DataTypeTok{ylab=}\NormalTok{C_top[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{])}
\KeywordTok{abline}\NormalTok{(f, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:factbookB13}{%
\centering
\includegraphics{02-regression-multiple-figures/factbookB13-1.pdf}
\caption{A bugfixed version of Figure \ref{fig:factbookB9}}\label{fig:factbookB13}
}
\end{figure}

Seriously, I really like the R language, but this \texttt{stringsAsFactors}
thing is it's weakest spot. Just fire up \texttt{options(stringsAsFactors=FALSE)}
next time! (Which I of course won't). End of story.

\end{solution}

\end{solution}

\hypertarget{countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita}{%
\subsection{Countries of the World -- A non-linear model based on the GDP per capita}\label{countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita}}

Let's revisit the World Factbook 2020 dataset (\texttt{world\_factbook\_2020.csv}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factbook <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/world_factbook_2020.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{exercise}

Draw a histogram of the empirical distribution
of the GDP per capita. Moreover, draw a histogram of the logarithm
of the GDP/person.

\end{exercise}

\begin{solution}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{hist}\NormalTok{(factbook}\OperatorTok{$}\NormalTok{gdp_per_capita_ppp, }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{, }\DataTypeTok{main=}\OtherTok{NA}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(}\KeywordTok{log}\NormalTok{(factbook}\OperatorTok{$}\NormalTok{gdp_per_capita_ppp), }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{, }\DataTypeTok{main=}\OtherTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:factbookC2}{%
\centering
\includegraphics{02-regression-multiple-figures/factbookC2-1.pdf}
\caption{Histograms of the empirical distribution of the GDP per capita with linear (left) and log (right) scale on the X axis}\label{fig:factbookC2}
}
\end{figure}

\emph{Comment: In Figure \ref{fig:factbookC2} we see that
distribution of the GDP is right-skewed: most countries
have small GDP. However, few of them
(those in the ``right tail'' of the distribution)
are very very rich (hey, how about taxing the richest countries?!).
There is the famous observation made by V. Pareto
stating that most assets are in the hands of the ``wealthy minority''
(compare: power law, rich-get-richer rule, preferential attachment in complex networks).
Interestingly, many real-world-phenomena are distributed similarly
(e.g., the popularity of web pages, the number of followers of Instagram
profiles). It is frequently the case that the logarithm of the aforementioned
variable looks more ``normal'' (is bell-shaped).}

\emph{Side note: ``The'' logarithm most often refers to the logarithm base
\(e\), \(\log x = \log_e x\),
where \(e\simeq 2.72\) is the Euler constant, see \texttt{exp(1)} in R.
Note that you can only compute logarithms of positive real numbers.}

Non-technical audience might be confused when asked to contemplate
the distribution of the logarithm of a variable. Let's make it
more user-friendly (on the other hand, we could've asked them
to harden up\ldots{})
by nicely re-labelling the X axis,
see Figure \ref{fig:factbookC3}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(}\KeywordTok{log}\NormalTok{(factbook}\OperatorTok{$}\NormalTok{gdp_per_capita_ppp), }\DataTypeTok{axes=}\OtherTok{FALSE}\NormalTok{,}
    \DataTypeTok{xlab=}\StringTok{"GDP per capita (thousands USD)"}\NormalTok{, }\DataTypeTok{main=}\OtherTok{NA}\NormalTok{, }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{)}
\KeywordTok{box}\NormalTok{()}
\KeywordTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{) }\CommentTok{# Y axis}
\NormalTok{at <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{2000}\NormalTok{, }\DecValTok{5000}\NormalTok{, }\DecValTok{10000}\NormalTok{, }\DecValTok{20000}\NormalTok{, }\DecValTok{50000}\NormalTok{, }\DecValTok{100000}\NormalTok{, }\DecValTok{200000}\NormalTok{)}
\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{at=}\KeywordTok{log}\NormalTok{(at), }\DataTypeTok{labels=}\NormalTok{at}\OperatorTok{/}\DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:factbookC3}{%
\centering
\includegraphics{02-regression-multiple-figures/factbookC3-1.pdf}
\caption{Histogram of the empirical distribution of the GDP per capita now with human-readable X axis labels (not the logarithmic scale)}\label{fig:factbookC3}
}
\end{figure}

\emph{Comment: This is still a plot of the logarithm of the
distribution of the per capita GDP, but it's somehow ``hidden'' behind
the human-readable axis labels. Nice.}

\end{solution}

\begin{exercise}

Fit a simple linear model of \texttt{life\_expectancy\_at\_birth}
as a function of \texttt{gdp\_per\_capita\_ppp}.

\end{exercise}

\begin{solution}

Easy. We have already done than in one of the previous exercises.
Yet, to learn something new, let's note that the \texttt{plot()} function
accepts formulas as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(life_expectancy_at_birth}\OperatorTok{~}\NormalTok{gdp_per_capita_ppp, }\DataTypeTok{data=}\NormalTok{factbook)}
\KeywordTok{plot}\NormalTok{(life_expectancy_at_birth}\OperatorTok{~}\NormalTok{gdp_per_capita_ppp, }\DataTypeTok{data=}\NormalTok{factbook)}
\KeywordTok{abline}\NormalTok{(f, }\DataTypeTok{col=}\StringTok{"purple"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(f)}\OperatorTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.43096
\end{verbatim}

\begin{figure}
\hypertarget{fig:factbookC4}{%
\centering
\includegraphics{02-regression-multiple-figures/factbookC4-1.pdf}
\caption{Linear model fitted for life expectancy vs.~GDP/person}\label{fig:factbookC4}
}
\end{figure}

\emph{Comment: From Figure \ref{fig:factbookC4} we see that
this is not a good model.}

\end{solution}

\begin{exercise}

Draw a scatter plot of \texttt{life\_expectancy\_at\_birth} as a function
\texttt{gdp\_per\_capita\_ppp}, with the X axis being logarithmic.
Compute the correlation coefficient between
\texttt{log(gdp\_per\_capita\_ppp)} and \texttt{life\_expectancy\_at\_birth}.

\end{exercise}

\begin{solution}

We could apply the \texttt{log()}-transformation manually
and generate fancy X axis labels ourselves. However,
the \texttt{plot()} function has the \texttt{log} argument (see \texttt{?plot.default})
which provides us with all we need, see Figure \ref{fig:factbookC5}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(factbook}\OperatorTok{$}\NormalTok{gdp_per_capita_ppp,}
\NormalTok{    factbook}\OperatorTok{$}\NormalTok{life_expectancy_at_birth,}
    \DataTypeTok{log=}\StringTok{"x"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:factbookC5}{%
\centering
\includegraphics{02-regression-multiple-figures/factbookC5-1.pdf}
\caption{Scatter plot of life expectancy vs.~GDP/person with log scale on the X axis}\label{fig:factbookC5}
}
\end{figure}

Here is the \emph{linear} correlation coefficient between the logarithm
of the GDP/person and the life expectancy.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(}\KeywordTok{log}\NormalTok{(factbook}\OperatorTok{$}\NormalTok{gdp_per_capita_ppp), factbook}\OperatorTok{$}\NormalTok{life_expectancy_at_birth,}
    \DataTypeTok{use=}\StringTok{"complete.obs"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.80665
\end{verbatim}

The correlation is quite high, hence the following task.

\end{solution}

\begin{exercise}

Fit a model predicting \texttt{life\_expectancy\_at\_birth}
by means of \texttt{log(gdp\_per\_capita\_ppp)}.

\end{exercise}

\begin{solution}

We would like to fit a model of the form \(Y=a\log X+b\).
The formula \texttt{life\_expectancy\_at\_birth\textasciitilde{}log(gdp\_per\_capita\_ppp)}
should do the trick here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(life_expectancy_at_birth}\OperatorTok{~}\KeywordTok{log}\NormalTok{(gdp_per_capita_ppp), }\DataTypeTok{data=}\NormalTok{factbook)}
\KeywordTok{plot}\NormalTok{(life_expectancy_at_birth}\OperatorTok{~}\KeywordTok{log}\NormalTok{(gdp_per_capita_ppp), }\DataTypeTok{data=}\NormalTok{factbook)}
\KeywordTok{abline}\NormalTok{(f, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{3}\NormalTok{)}
\NormalTok{f}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             (Intercept) log(gdp_per_capita_ppp) 
##                 28.3064                  4.8178
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(f)}\OperatorTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.65069
\end{verbatim}

\begin{figure}
\hypertarget{fig:factbookC7}{%
\centering
\includegraphics{02-regression-multiple-figures/factbookC7-1.pdf}
\caption{Linear model fitted for life expectancy vs.~the logarithm of GDP/person}\label{fig:factbookC7}
}
\end{figure}

\emph{Comment: That is an okay model (in terms of the coefficient of determination), see Figure \ref{fig:factbookC7}.}

\end{solution}

\begin{exercise}

Draw the fitted logarithmic model on a scatter plot
with a standard, non-logarithmic X axis.

\end{exercise}

\begin{solution}

The model fitted above is of the form
\(Y\simeq4.82 \log X+28.31\).
To depict it on a plot with linear (non-logarithmic) axes,
we can compute this formula on multiple points by hand,
see Figure \ref{fig:factbookC8}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(factbook}\OperatorTok{$}\NormalTok{gdp_per_capita_ppp, factbook}\OperatorTok{$}\NormalTok{life_expectancy_at_birth)}

\CommentTok{# many points on the X axis:}
\NormalTok{xxx <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(factbook}\OperatorTok{$}\NormalTok{gdp_per_capita_ppp, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{),}
            \KeywordTok{max}\NormalTok{(factbook}\OperatorTok{$}\NormalTok{gdp_per_capita_ppp, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{),}
            \DataTypeTok{length.out=}\DecValTok{101}\NormalTok{)}
\NormalTok{yyy <-}\StringTok{ }\NormalTok{f}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{f}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{]}\OperatorTok{*}\KeywordTok{log}\NormalTok{(xxx)}
\KeywordTok{lines}\NormalTok{(xxx, yyy, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:factbookC8}{%
\centering
\includegraphics{02-regression-multiple-figures/factbookC8-1.pdf}
\caption{Logarithmic model fitted for life expectancy vs.~GDP/person}\label{fig:factbookC8}
}
\end{figure}

\emph{Comment: Well, people are not immortal\ldots{}
The original (linear) model didn't really take that into account.
Also, recall that correlation is not causation.
Moreover, there is a lot of variability at an individual level.
Being born in a less-wealthy country (e.g., not in a tax haven),
doesn't mean you don't have the whole life ahead of you.
Do the cool staff, do something for the others. Life's not about money.}

\end{solution}

\hypertarget{countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp}{%
\subsection{Countries of the World -- A multiple regression model for the per capita GDP}\label{countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp}}

Let's play with World Factbook 2020
(\texttt{world\_factbook\_2020.csv}) once again.
World is an interesting place, so we're far from being bored with this dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factbook <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/world_factbook_2020.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's restrict ourselves to the following columns, mostly
related to imports and exports:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factbookn <-}\StringTok{ }\NormalTok{factbook[}\KeywordTok{c}\NormalTok{(}\StringTok{"gdp_purchasing_power_parity"}\NormalTok{,}
    \StringTok{"imports"}\NormalTok{, }\StringTok{"exports"}\NormalTok{, }\StringTok{"electricity_exports"}\NormalTok{,}
    \StringTok{"electricity_imports"}\NormalTok{, }\StringTok{"military_expenditures"}\NormalTok{,}
    \StringTok{"crude_oil_exports"}\NormalTok{, }\StringTok{"crude_oil_imports"}\NormalTok{,}
    \StringTok{"natural_gas_exports"}\NormalTok{, }\StringTok{"natural_gas_imports"}\NormalTok{,}
    \StringTok{"reserves_of_foreign_exchange_and_gold"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

Let's compute the per capita versions of the above, by dividing
all values by each country's population:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(factbookn))}
\NormalTok{    factbookn[[i]] <-}\StringTok{ }\NormalTok{factbookn[[i]]}\OperatorTok{/}\NormalTok{factbook}\OperatorTok{$}\NormalTok{population}
\end{Highlighting}
\end{Shaded}

We are going to build a few multiple regression models using the
\texttt{step()} function, which is not too fond of missing values, therefore
they should be removed first:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factbookn <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(factbookn)}
\KeywordTok{c}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(factbook), }\KeywordTok{nrow}\NormalTok{(factbookn)) }\CommentTok{# how many countries were omitted?}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 261 157
\end{verbatim}

\begin{exercise}

Build a model for \texttt{gdp\_purchasing\_power\_parity} as a function
of \texttt{imports} and \texttt{exports} (all per capita).

\end{exercise}

\begin{solution}

Let's first take a look at how the aforementioned variables
are related to each other, see Figure \ref{fig:factbookD5}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairs}\NormalTok{(factbookn[}\KeywordTok{c}\NormalTok{(}\StringTok{"gdp_purchasing_power_parity"}\NormalTok{, }\StringTok{"imports"}\NormalTok{, }\StringTok{"exports"}\NormalTok{)])}
\KeywordTok{cor}\NormalTok{(factbookn[}\KeywordTok{c}\NormalTok{(}\StringTok{"gdp_purchasing_power_parity"}\NormalTok{, }\StringTok{"imports"}\NormalTok{, }\StringTok{"exports"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                             gdp_purchasing_power_parity imports exports
## gdp_purchasing_power_parity                     1.00000 0.82891 0.81899
## imports                                         0.82891 1.00000 0.94241
## exports                                         0.81899 0.94241 1.00000
\end{verbatim}

\begin{figure}
\hypertarget{fig:factbookD5}{%
\centering
\includegraphics{02-regression-multiple-figures/factbookD5-1.pdf}
\caption{Scatter plot matrix for GDP, imports and exports}\label{fig:factbookD5}
}
\end{figure}

They are nicely correlated. Moreover, they are on a similar scale
(``tens of thousands of USD per capita'').

Fitting the requested model yields:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{options}\NormalTok{(}\DataTypeTok{scipen=}\DecValTok{10}\NormalTok{) }\CommentTok{# prefer "decimal" over "scientific" notation}
\NormalTok{f1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(gdp_purchasing_power_parity}\OperatorTok{~}\NormalTok{imports}\OperatorTok{+}\NormalTok{exports, }\DataTypeTok{data=}\NormalTok{factbookn)}
\NormalTok{f1}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)     imports     exports 
##  9852.53813     1.44194     0.78067
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(f1)}\OperatorTok{$}\NormalTok{adj.r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.69598
\end{verbatim}

\end{solution}

\begin{exercise}

Use forward selection to come up with
a model for \texttt{gdp\_purchasing\_power\_parity} per capita.

\end{exercise}

\begin{solution}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(model_empty <-}\StringTok{ }\NormalTok{gdp_purchasing_power_parity}\OperatorTok{~}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## gdp_purchasing_power_parity ~ 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(model_full <-}\StringTok{ }\KeywordTok{formula}\NormalTok{(}\KeywordTok{model.frame}\NormalTok{(gdp_purchasing_power_parity}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{factbookn)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## gdp_purchasing_power_parity ~ imports + exports + electricity_exports + 
##     electricity_imports + military_expenditures + crude_oil_exports + 
##     crude_oil_imports + natural_gas_exports + natural_gas_imports + 
##     reserves_of_foreign_exchange_and_gold
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f2 <-}\StringTok{ }\KeywordTok{step}\NormalTok{(}\KeywordTok{lm}\NormalTok{(model_empty, }\DataTypeTok{data=}\NormalTok{factbookn),}
    \DataTypeTok{scope=}\NormalTok{model_full,}
    \DataTypeTok{direction=}\StringTok{"forward"}\NormalTok{, }\DataTypeTok{trace=}\DecValTok{0}\NormalTok{)}
\NormalTok{f2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = gdp_purchasing_power_parity ~ imports + crude_oil_exports + 
##     crude_oil_imports + electricity_imports + natural_gas_imports, 
##     data = factbookn)
## 
## Coefficients:
##         (Intercept)              imports    crude_oil_exports  
##             7603.24                 1.77            128472.22  
##   crude_oil_imports  electricity_imports  natural_gas_imports  
##           100781.64                 1.62                 3.13
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(f2)}\OperatorTok{$}\NormalTok{adj.r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7865
\end{verbatim}

\emph{Comment: Interestingly, it's mostly the import-related variables
that contribute to the GDP per capita. However, the model
is not perfect, so we should refrain ourselves from building a brand new
economic theory around this ``discovery''. On the other hand,
you know what they say: all models are wrong, but some might be useful.
Note that we used the adjusted \(R^2\) coefficient to correct
for the number of variables in the model
so as to make it more comparable with the coefficient corresponding
to the \texttt{f1} model.}

\end{solution}

\begin{exercise}

Use backward elimination to construct a model
for \texttt{gdp\_purchasing\_power\_parity} per capita.

\end{exercise}

\begin{solution}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f3 <-}\StringTok{ }\KeywordTok{step}\NormalTok{(}\KeywordTok{lm}\NormalTok{(model_full, }\DataTypeTok{data=}\NormalTok{factbookn),}
    \DataTypeTok{scope=}\NormalTok{model_empty,}
    \DataTypeTok{direction=}\StringTok{"backward"}\NormalTok{, }\DataTypeTok{trace=}\DecValTok{0}\NormalTok{)}
\NormalTok{f3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = gdp_purchasing_power_parity ~ imports + electricity_imports + 
##     crude_oil_exports + crude_oil_imports + natural_gas_imports, 
##     data = factbookn)
## 
## Coefficients:
##         (Intercept)              imports  electricity_imports  
##             7603.24                 1.77                 1.62  
##   crude_oil_exports    crude_oil_imports  natural_gas_imports  
##           128472.22            100781.64                 3.13
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(f3)}\OperatorTok{$}\NormalTok{adj.r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7865
\end{verbatim}

\emph{Comment: This is the same model as the one
found by forward selection, i.e., \texttt{f2}.}

\end{solution}

\hypertarget{outro-1}{%
\section{Outro}\label{outro-1}}

\hypertarget{remarks-1}{%
\subsection{Remarks}\label{remarks-1}}

Multiple regression is simple, fast to apply and interpretable.

Linear models go beyond fitting of straight lines and other hyperplanes!

A complex model may overfit and hence generalise poorly to unobserved inputs.

Note that the SSR criterion makes the models sensitive to outliers.

\textbf{Remember:}

good models
\[=\]
better understanding of the modelled reality \(+\) better predictions
\[=\]
more revenue, your boss' happiness, your startup's growth etc.

\hypertarget{other-methods-for-regression}{%
\subsection{Other Methods for Regression}\label{other-methods-for-regression}}

Other example approaches to regression:

\begin{itemize}
\tightlist
\item
  ridge regression,
\item
  lasso regression,
\item
  least absolute deviations (LAD) regression,
\item
  multiadaptive regression splines (MARS),
\item
  K-nearest neighbour (K-NN) regression, see \texttt{FNN::knn.reg()} in R,
\item
  regression trees,
\item
  support-vector regression (SVR),
\item
  neural networks (also deep) for regression.
\end{itemize}

\hypertarget{derivation-of-the-solution-1}{%
\subsection{Derivation of the Solution (**)}\label{derivation-of-the-solution-1}}

We would like to find an analytical solution
to the problem of minimising of the sum of squared residuals:

\[
\min_{\beta_0, \beta_1,\dots, \beta_p\in\mathbb{R}} E(\beta_0, \beta_1, \dots, \beta_p)=
\sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)^2
\]

This requires computing the \(p+1\) partial derivatives
\({\partial E}/{\partial \beta_j}\) for \(j=0,\dots,p\).

The partial derivatives are very similar to each other;
\(\frac{\partial E}{\partial \beta_0}\) is given by:
\[
\frac{\partial E}{\partial \beta_0}(\beta_0,\beta_1,\dots,\beta_p)=
2 \sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)
\]
and \(\frac{\partial E}{\partial \beta_j}\) for \(j>0\) is equal to:
\[
\frac{\partial E}{\partial \beta_j}(\beta_0,\beta_1,\dots,\beta_p)=
2 \sum_{i=1}^n x_{i,j} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)
\]

Then all we need to do is to solve the system of linear equations:

\[
\left\{
\begin{array}{rcl}
\frac{\partial E}{\partial \beta_0}(\beta_0,\beta_1,\dots,\beta_p)&=&0 \\
\frac{\partial E}{\partial \beta_1}(\beta_0,\beta_1,\dots,\beta_p)&=&0 \\
\vdots\\
\frac{\partial E}{\partial \beta_p}(\beta_0,\beta_1,\dots,\beta_p)&=&0 \\
\end{array}
\right.
\]

The above system of \(p+1\) linear equations, which we are supposed to solve
for \(\beta_0,\beta_1,\dots,\beta_p\):
\[
\left\{
\begin{array}{rcl}
2 \sum_{i=1}^n \phantom{x_{i,0}}\left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)&=&0 \\
2 \sum_{i=1}^n x_{i,1} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)&=&0 \\
\vdots\\
2 \sum_{i=1}^n x_{i,p} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)&=&0 \\
\end{array}
\right.
\]
can be rewritten as:
\[
\left\{
\begin{array}{rcl}
\sum_{i=1}^n \phantom{x_{i,0}}\left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}\right)&=& \sum_{i=1}^n \phantom{x_{i,0}} y_i \\
\sum_{i=1}^n x_{i,1} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}\right)&=&\sum_{i=1}^n x_{i,1} y_i \\
\vdots\\
\sum_{i=1}^n x_{i,p} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}\right)&=&\sum_{i=1}^n x_{i,p} y_i \\
\end{array}
\right.
\]

and further as:
\[
\left\{
\begin{array}{rcl}
\beta_0\ n\phantom{\sum_{i=1}^n x} + \beta_1\sum_{i=1}^n \phantom{x_{i,0}}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n \phantom{x_{i,0}}  x_{i,p} &=&\sum_{i=1}^n\phantom{x_{i,0}} y_i \\
\beta_0 \sum_{i=1}^n x_{i,1} + \beta_1\sum_{i=1}^n x_{i,1}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n x_{i,1}  x_{i,p} &=&\sum_{i=1}^n x_{i,1} y_i \\
\vdots\\
\beta_0 \sum_{i=1}^n x_{i,p} + \beta_1\sum_{i=1}^n x_{i,p}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n x_{i,p}  x_{i,p} &=&\sum_{i=1}^n x_{i,p} y_i \\
\end{array}
\right.
\]
Note that the terms involving \(x_{i,j}\) and \(y_i\) (the sums) are all constant
-- these are some fixed real numbers. We have learned how to solve such
problems in high school.

\begin{exercise}

Try deriving the analytical solution and implementing it for \(p=2\).
Recall that in the previous chapter we solved the special case of \(p=1\).

\end{exercise}

\hypertarget{solution-in-matrix-form}{%
\subsection{Solution in Matrix Form (***)}\label{solution-in-matrix-form}}

Assume that \(\mathbf{X}\in\mathbb{R}^{n\times p}\) (a matrix with inputs),
\(\mathbf{y}\in\mathbb{R}^{n\times 1}\) (a column vector of reference outputs)
and
\(\boldsymbol{\beta}\in\mathbb{R}^{(p+1)\times 1}\) (a column vector of parameters).

Firstly, note that a linear model of the form:
\[
f_{\boldsymbol\beta}(\mathbf{x})=\beta_0+\beta_1 x_1+\dots+\beta_p x_p
\]
can be rewritten as:
\[
f_{\boldsymbol\beta}(\mathbf{x})=\beta_0 1+\beta_1 x_1+\dots+\beta_p x_p
=\dot{\mathbf{x}}\boldsymbol\beta,
\]
where \(\dot{\mathbf{x}}=[1\ x_1\ x_2\ \cdots\ x_p]\).

Similarly, if we assume that \(\dot{\mathbf{X}}=[\boldsymbol{1}\ \mathbf{X}]\in\mathbb{R}^{n\times (p+1)}\)
is the input matrix with a prepended column of \(1\)s, i.e.,
\(\boldsymbol{1}=[1\ 1\ \cdots\ 1]^T\) and \(\dot{x}_{i,0}=1\) (for brevity of notation
the columns added will have index \(0\)),
\(\dot{x}_{i,j}=x_{i,j}\) for all \(j\ge 1\) and all \(i\),
then:
\[
\hat{\mathbf{y}} = \dot{\mathbf{X}} \boldsymbol\beta
\]
gives the vector of predicted outputs for every input point.

This way, the sum of squared residuals
\[
E(\beta_0, \beta_1, \dots, \beta_p)=
\sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)^2
\]
can be rewritten as:
\[
E(\boldsymbol\beta)=\| \dot{\mathbf{X}} \boldsymbol\beta - \mathbf{y} \|^2,
\]
where as usual \(\|\cdot\|^2\) denotes the squared Euclidean norm.

Recall that this can be re-expressed as:
\[
E(\boldsymbol\beta)= (\dot{\mathbf{X}} \boldsymbol\beta - \mathbf{y})^T (\dot{\mathbf{X}} \boldsymbol\beta - \mathbf{y}).
\]

In order to find the minimum of \(E\) w.r.t. \(\boldsymbol\beta\),
we need to find the parameters that make the partial derivatives vanish, i.e.:

\[
\left\{
\begin{array}{rcl}
\frac{\partial E}{\partial \beta_0}(\boldsymbol\beta)&=&0 \\
\frac{\partial E}{\partial \beta_1}(\boldsymbol\beta)&=&0 \\
\vdots\\
\frac{\partial E}{\partial \beta_p}(\boldsymbol\beta)&=&0 \\
\end{array}
\right.
\]

\begin{description}
\item[Remark.]
(***) Interestingly, the above can also be expressed in matrix form,
using the special notation:
\[
\nabla E(\boldsymbol\beta) = \boldsymbol{0}
\]
Here, \(\nabla E\) (nabla symbol = differential operator)
denotes the function gradient, i.e., the vector of all partial derivatives.
This is nothing more than syntactic sugar for this quite commonly applied operator.
\end{description}

Anyway, the system of linear equations we have derived above:
\[
\left\{
\begin{array}{rcl}
\beta_0\ n\phantom{\sum_{i=1}^n x} + \beta_1\sum_{i=1}^n \phantom{x_{i,0}}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n \phantom{x_{i,0}}  x_{i,p} &=&\sum_{i=1}^n\phantom{x_{i,0}} y_i \\
\beta_0 \sum_{i=1}^n x_{i,1} + \beta_1\sum_{i=1}^n x_{i,1}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n x_{i,1}  x_{i,p} &=&\sum_{i=1}^n x_{i,1} y_i \\
\vdots\\
\beta_0 \sum_{i=1}^n x_{i,p} + \beta_1\sum_{i=1}^n x_{i,p}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n x_{i,p}  x_{i,p} &=&\sum_{i=1}^n x_{i,p} y_i \\
\end{array}
\right.
\]
can be rewritten in matrix terms as:
\[
\left\{
\begin{array}{rcl}
\beta_0 \dot{\mathbf{x}}_{\cdot,0}^T \dot{\mathbf{x}}_{\cdot,0} + \beta_1 \dot{\mathbf{x}}_{\cdot,0}^T \dot{\mathbf{x}}_{\cdot,1}+\dots+\beta_p \dot{\mathbf{x}}_{\cdot,0}^T \dot{\mathbf{x}}_{\cdot,p} &=& \dot{\mathbf{x}}_{\cdot,0}^T \mathbf{y} \\
\beta_0 \dot{\mathbf{x}}_{\cdot,1}^T \dot{\mathbf{x}}_{\cdot,0} + \beta_1 \dot{\mathbf{x}}_{\cdot,1}^T \dot{\mathbf{x}}_{\cdot,1}+\dots+\beta_p \dot{\mathbf{x}}_{\cdot,1}^T \dot{\mathbf{x}}_{\cdot,p} &=& \dot{\mathbf{x}}_{\cdot,1}^T \mathbf{y} \\
\vdots\\
\beta_0 \dot{\mathbf{x}}_{\cdot,p}^T \dot{\mathbf{x}}_{\cdot,0} + \beta_1 \dot{\mathbf{x}}_{\cdot,p}^T \dot{\mathbf{x}}_{\cdot,1}+\dots+\beta_p \dot{\mathbf{x}}_{\cdot,p}^T \dot{\mathbf{x}}_{\cdot,p} &=& \dot{\mathbf{x}}_{\cdot,p}^T \mathbf{y}\\
\end{array}
\right.
\]

This can be restated as:
\[
\left\{
\begin{array}{rcl}
\left(\dot{\mathbf{x}}_{\cdot,0}^T \dot{\mathbf{X}}\right)\, \boldsymbol\beta &=& \dot{\mathbf{x}}_{\cdot,0}^T \mathbf{y} \\
\left(\dot{\mathbf{x}}_{\cdot,1}^T \dot{\mathbf{X}}\right)\, \boldsymbol\beta  &=& \dot{\mathbf{x}}_{\cdot,1}^T \mathbf{y} \\
\vdots\\
\left(\dot{\mathbf{x}}_{\cdot,p}^T \dot{\mathbf{X}}\right)\, \boldsymbol\beta  &=& \dot{\mathbf{x}}_{\cdot,p}^T \mathbf{y}\\
\end{array}
\right.
\]
which in turn is equivalent to:
\[
\left(\dot{\mathbf{X}}^T\mathbf{X}\right)\,\boldsymbol\beta = \dot{\mathbf{X}}^T\mathbf{y}.
\]

Such a system of linear equations in matrix form can be solved numerically using,
amongst others, the \texttt{solve()} function.

\begin{description}
\item[Remark.]
(***) In practice, we'd rather rely on QR or SVD decompositions
of matrices for efficiency and numerical accuracy reasons.
\end{description}

Numeric example -- solution via \texttt{lm()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X1 <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Balance[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{])}
\NormalTok{X2 <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Income[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{])}
\NormalTok{Y  <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Rating[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{])}
\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X1}\OperatorTok{+}\NormalTok{X2)}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)          X1          X2 
##    172.5587      0.1828      2.1976
\end{verbatim}

Recalling that \(\mathbf{A}^T \mathbf{B}\) can be computed
by calling \texttt{t(A)\ \%*\%\ B} or -- even faster -- by calling \texttt{crossprod(A,\ B)},
we can also use \texttt{solve()} to obtain the same result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X_dot <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DecValTok{1}\NormalTok{, X1, X2)}
\KeywordTok{solve}\NormalTok{( }\KeywordTok{crossprod}\NormalTok{(X_dot, X_dot), }\KeywordTok{crossprod}\NormalTok{(X_dot, Y) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        [,1]
##    172.5587
## X1   0.1828
## X2   2.1976
\end{verbatim}

\hypertarget{pearsons-r-in-matrix-form}{%
\subsection{Pearson's r in Matrix Form (**)}\label{pearsons-r-in-matrix-form}}

Recall the Pearson linear correlation coefficient:
\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y})
}{
    \sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}\ \sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}
}
\]

Denote with \(\boldsymbol{x}^\circ\) and \(\boldsymbol{y}^\circ\) the centred versions
of \(\boldsymbol{x}\) and \(\boldsymbol{y}\), respectively,
i.e., \(x_i^\circ=x_i-\bar{x}\) and \(y_i^\circ=y_i-\bar{y}\).

Rewriting the above yields:
\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \sum_{i=1}^n x_i^\circ y_i^\circ
}{
    \sqrt{\sum_{i=1}^n ({x_i^\circ})^2}\  \sqrt{\sum_{i=1}^n ({y_i^\circ})^2}
}
\]
which is exactly:
\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \boldsymbol{x}^\circ\cdot \boldsymbol{y}^\circ
}{
    \| \boldsymbol{x}^\circ \|\    \| \boldsymbol{y}^\circ \|
}
\]
i.e., the normalised dot product of the centred versions of the two vectors.

This is the cosine of the angle between the two vectors
(in \(n\)-dimensional spaces)!

(**) Recalling from the previous chapter that \(\mathbf{A}^T \mathbf{A}\)
gives the dot product between all the pairs of columns in a matrix \(\mathbf{A}\),
we can implement an equivalent version of \texttt{cor(C)} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C <-}\StringTok{ }\NormalTok{Credit[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{,}
    \KeywordTok{c}\NormalTok{(}\StringTok{"Rating"}\NormalTok{, }\StringTok{"Limit"}\NormalTok{, }\StringTok{"Income"}\NormalTok{, }\StringTok{"Age"}\NormalTok{,}
    \StringTok{"Education"}\NormalTok{, }\StringTok{"Balance"}\NormalTok{)]}
\NormalTok{C_centred <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(C, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(c) c}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(c))}
\NormalTok{C_normalised <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(C_centred, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(c)}
\NormalTok{    c}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{(c}\OperatorTok{^}\DecValTok{2}\NormalTok{)))}
\KeywordTok{round}\NormalTok{(}\KeywordTok{t}\NormalTok{(C_normalised) }\OperatorTok{%*%}\StringTok{ }\NormalTok{C_normalised, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Rating  Limit Income   Age Education Balance
## Rating     1.000  0.996  0.831 0.167    -0.040   0.798
## Limit      0.996  1.000  0.834 0.164    -0.032   0.796
## Income     0.831  0.834  1.000 0.227    -0.033   0.414
## Age        0.167  0.164  0.227 1.000     0.024   0.008
## Education -0.040 -0.032 -0.033 0.024     1.000   0.001
## Balance    0.798  0.796  0.414 0.008     0.001   1.000
\end{verbatim}

\hypertarget{further-reading-1}{%
\subsection{Further Reading}\label{further-reading-1}}

Recommended further reading: (James et al. \protect\hyperlink{ref-islr}{2017}: Chapters 1, 2 and 3)

Other: (Hastie et al. \protect\hyperlink{ref-esl}{2017}: Chapter 1, Sections 3.2 and 3.3)

\hypertarget{classification-with-k-nearest-neighbours}{%
\chapter{Classification with K-Nearest Neighbours}\label{classification-with-k-nearest-neighbours}}

\hypertarget{introduction-3}{%
\section{Introduction}\label{introduction-3}}

\hypertarget{classification-task}{%
\subsection{Classification Task}\label{classification-task}}

Let \(\mathbf{X}\in\mathbb{R}^{n\times p}\) be an input matrix
that consists of \(n\) points in a \(p\)-dimensional space (each of the \(n\) objects
is described by means of \(p\) numerical features).

Recall that in supervised learning, with each
\(\mathbf{x}_{i,\cdot}\) we associate the desired output \(y_i\).

\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} \\
\end{array}
\right]
\qquad
\mathbf{y} = \left[
\begin{array}{c}
{y}_{1} \\
{y}_{2} \\
\vdots\\
{y}_{n} \\
\end{array}
\right].
\]

\bigskip

In this chapter we are interested in \textbf{classification} tasks;
we assume that each \(y_i\) is a \emph{label} (e.g., a character string) --
it is of quantitative/categorical type.

Most commonly, we are faced with \textbf{binary classification} tasks
where there are only two possible distinct labels.

We traditionally denote them with \(0\)s and \(1\)s.

For example:

\begin{longtable}[]{@{}ll@{}}
\toprule
0 & 1\tabularnewline
\midrule
\endhead
no & yes\tabularnewline
false & true\tabularnewline
failure & success\tabularnewline
healthy & ill\tabularnewline
\bottomrule
\end{longtable}

On the other hand, in \textbf{multiclass classification},
we assume that each \(y_i\) takes more than two possible values.

Example plot of a synthetic dataset
with the reference binary \(y\)s is given in Figure \ref{fig:classify_intro}.
The ``true'' decision boundary is at \(X_1=0\) but the classes
slightly overlap (the dataset is a bit noisy).

\begin{figure}
\hypertarget{fig:classify_intro}{%
\centering
\includegraphics{03-classification-neighbours-figures/classify_intro-1.pdf}
\caption{A synthetic 2D dataset with the true decision boundary at \(X_1=0\)}\label{fig:classify_intro}
}
\end{figure}

\hypertarget{data}{%
\subsection{Data}\label{data}}

For illustration, let's consider the Wine Quality dataset (Cortez et al. \protect\hyperlink{ref-wines}{2009})
that can be downloaded from the UCI Machine Learning Repository
(\url{https://archive.ics.uci.edu/ml/datasets/Wine+Quality}) --
white wines only.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wines <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/winequality-all.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{wines <-}\StringTok{ }\NormalTok{wines[wines}\OperatorTok{$}\NormalTok{color }\OperatorTok{==}\StringTok{ "white"}\NormalTok{,]}
\NormalTok{(n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(wines)) }\CommentTok{# number of samples}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4898
\end{verbatim}

These are Vinho Verde wine samples from the north of Portugal,
see \url{https://www.vinhoverde.pt/en/homepage}.

There are 11 physicochemical features reported.
Moreover, there is a wine rating (which we won't consider here)
on the scale 0 (bad) to 10 (excellent)
given by wine experts.

The input matrix \(\mathbf{X}\in\mathbb{R}^{n\times p}\)
consists of the first 10 numeric variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(wines[,}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{])}
\KeywordTok{dim}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4898   10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(X, }\DecValTok{2}\NormalTok{) }\CommentTok{# first two rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      fixed.acidity volatile.acidity citric.acid residual.sugar
## 1600           7.0             0.27        0.36           20.7
## 1601           6.3             0.30        0.34            1.6
##      chlorides free.sulfur.dioxide total.sulfur.dioxide density  pH
## 1600     0.045                  45                  170   1.001 3.0
## 1601     0.049                  14                  132   0.994 3.3
##      sulphates
## 1600      0.45
## 1601      0.49
\end{verbatim}

The 11th variable measures the amount of alcohol (in \%).

We will convert this dependent variable to a binary one:

\begin{itemize}
\tightlist
\item
  0 == (\texttt{alcohol\ \ \textless{}\ 12}) == lower-alcohol wines1
\item
  1 == (\texttt{alcohol\ \textgreater{}=\ 12}) == higher-alcohol wines
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# recall that TRUE == 1}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(wines}\OperatorTok{$}\NormalTok{alcohol }\OperatorTok{>=}\StringTok{ }\DecValTok{12}\NormalTok{)))}
\KeywordTok{table}\NormalTok{(Y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Y
##    0    1 
## 4085  813
\end{verbatim}

Now \((\mathbf{X},\mathbf{y})\) is a basis for an interesting (yet challenging)
binary classification task.

\hypertarget{training-and-test-sets}{%
\subsection{Training and Test Sets}\label{training-and-test-sets}}

Recall that we are genuinely interested in the construction of supervised learning models for the two following purposes:

\begin{itemize}
\tightlist
\item
  \textbf{description} -- to explain a given dataset in simpler terms,
\item
  \textbf{prediction} -- to forecast the values of the dependent variable
  for inputs that are yet to be observed.
\end{itemize}

In the latter case:

\begin{itemize}
\tightlist
\item
  we don't want our models to \emph{overfit} to current data,
\item
  we want our models to \emph{generalise} well
  to new data.
\end{itemize}

\bigskip

One way to assess if a model has sufficient predictive power is based
on a random \textbf{train-test split} of the original dataset:

\begin{itemize}
\tightlist
\item
  \emph{training sample} (usually 60-80\% of the observations) -- used to construct a model,
\item
  \emph{test sample} (remaining 40-20\%) -- used to assess the goodness of fit.
\end{itemize}

\begin{description}
\item[Remark.]
\textbf{Test sample must not be used in the training phase!} (No cheating!)
\end{description}

60/40\% train-test split in R:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{# reproducibility matters}
\NormalTok{random_indices <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(n)}
\KeywordTok{head}\NormalTok{(random_indices) }\CommentTok{# preview}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2463 2511 2227  526 4291 2986
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# first 60% of the indices (they are arranged randomly)}
\CommentTok{# will constitute the train sample:}
\NormalTok{train_indices <-}\StringTok{ }\NormalTok{random_indices[}\DecValTok{1}\OperatorTok{:}\KeywordTok{floor}\NormalTok{(n}\OperatorTok{*}\FloatTok{0.6}\NormalTok{)]}
\NormalTok{X_train <-}\StringTok{ }\NormalTok{X[train_indices,]}
\NormalTok{Y_train <-}\StringTok{ }\NormalTok{Y[train_indices]}
\CommentTok{# the remaining indices (40%) go to the test sample:}
\NormalTok{X_test  <-}\StringTok{ }\NormalTok{X[}\OperatorTok{-}\NormalTok{train_indices,]}
\NormalTok{Y_test  <-}\StringTok{ }\NormalTok{Y[}\OperatorTok{-}\NormalTok{train_indices]}
\end{Highlighting}
\end{Shaded}

\hypertarget{discussed-methods}{%
\subsection{Discussed Methods}\label{discussed-methods}}

Our aim is to build a classifier that takes 10 wine physicochemical
features and determines whether it's a ``strong'' wine.

We will discuss 3 simple and educational (yet practically useful)
classification algorithms:

\begin{itemize}
\tightlist
\item
  \emph{K-nearest neighbour scheme} -- this chapter,
\item
  \emph{Decision trees} -- the next chapter,
\item
  \emph{Logistic regression} -- the next chapter.
\end{itemize}

\hypertarget{k-nearest-neighbour-classifier}{%
\section{K-nearest Neighbour Classifier}\label{k-nearest-neighbour-classifier}}

\hypertarget{introduction-4}{%
\subsection{Introduction}\label{introduction-4}}

\begin{description}
\item[Rule.]
``If you don't know what to do in a situation, just act like the people around you''
\end{description}

For some integer \(K\ge 1\), the \textbf{K-Nearest Neighbour (\emph{K-NN}) Classifier}
proceeds as follows.

To classify a new point \(\mathbf{x}'\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  find the \(K\) nearest neighbours of a given point \(\mathbf{x}'\) amongst the points in the train set,
  denoted \(\mathbf{x}_{i_1,\cdot}, \dots, \mathbf{x}_{i_K,\cdot}\):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    compute the Euclidean distances between \(\mathbf{x}'\) and each \(\mathbf{x}_{i,\cdot}\) from the train set,
    \[d_i = \|\mathbf{x}'-\mathbf{x}_{i,\cdot}\|\]
  \item
    order \(d_i\)s in increasing order,
    \(d_{i_1} \le d_{i_2} \le \dots \le d_{i_K}\)
  \item
    pick first \(K\) indices (these are the \emph{nearest} neighbours)
  \end{enumerate}
\item
  fetch the corresponding reference labels \(y_{i_1}, \dots, y_{i_K}\)
\item
  return their \emph{mode} as a result, i.e., the most frequently occurring label (a.k.a. \emph{majority vote})
\end{enumerate}

Here is how \(K\)-NN classifier works on a synthetic 2D dataset.
Firstly let's consider \(K=1\), see Figure \ref{fig:fig_plot_knn1}.
Gray and pink regions depict how new points would be classified.
In particular 1-NN is ``greedy'' in the sense that we just
locate the nearest point.

\begin{figure}
\hypertarget{fig:fig_plot_knn1}{%
\centering
\includegraphics{03-classification-neighbours-figures/fig_plot_knn1-1.pdf}
\caption{1-NN class bounds for our 2D synthetic dataset}\label{fig:fig_plot_knn1}
}
\end{figure}

\begin{description}
\item[Remark.]
(*) 1-NN classification is essentially based
on a dataset's so-called Voronoi diagram.
\end{description}

Increasing \(K\) somehow smoothens the decision boundary (this makes it
less ``local'' and more ``global'').
Figure \ref{fig:fig_plot_knn3} depicts the \(K=3\) case.

\begin{figure}
\hypertarget{fig:fig_plot_knn3}{%
\centering
\includegraphics{03-classification-neighbours-figures/fig_plot_knn3-1.pdf}
\caption{3-NN class bounds for our 2D synthetic dataset}\label{fig:fig_plot_knn3}
}
\end{figure}

\begin{figure}
\hypertarget{fig:fig_plot_knn25}{%
\centering
\includegraphics{03-classification-neighbours-figures/fig_plot_knn25-1.pdf}
\caption{25-NN class bounds for our 2D synthetic dataset}\label{fig:fig_plot_knn25}
}
\end{figure}

Recall that the ``true'' decision boundary for this synthetic dataset
is at \(X_1=0\). The 25-NN classifier did quite a good job, see Figure \ref{fig:fig_plot_knn25}.

\hypertarget{example-in-r}{%
\subsection{Example in R}\label{example-in-r}}

We shall be calling the \texttt{knn()} function from package \texttt{FNN}
to classify the points from the test sample
extracted from the \texttt{wines} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"FNN"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's make prediction using the 5-nn classifier:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_knn5 <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(X_train, X_test, Y_train, }\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\KeywordTok{head}\NormalTok{(Y_test, }\DecValTok{28}\NormalTok{) }\CommentTok{# True Ys}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(Y_knn5, }\DecValTok{28}\NormalTok{) }\CommentTok{# Predicted Ys}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(Y_test }\OperatorTok{==}\StringTok{ }\NormalTok{Y_knn5) }\CommentTok{# accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.81735
\end{verbatim}

9-nn classifier:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_knn9 <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(X_train, X_test, Y_train, }\DataTypeTok{k=}\DecValTok{9}\NormalTok{)}
\KeywordTok{head}\NormalTok{(Y_test, }\DecValTok{28}\NormalTok{) }\CommentTok{# True Ys}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(Y_knn9, }\DecValTok{28}\NormalTok{) }\CommentTok{# Predicted Ys}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(Y_test }\OperatorTok{==}\StringTok{ }\NormalTok{Y_knn9) }\CommentTok{# accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.81939
\end{verbatim}

\hypertarget{feature-engineering}{%
\subsection{Feature Engineering}\label{feature-engineering}}

Note that the Euclidean distance that we used above
implicitly assumes that every feature (independent variable)
is on the same scale.

However, when dealing with, e.g., physical quantities,
we often perform conversions of units of measurement (kg → g, feet → m etc.).

Transforming a single feature may drastically change the metric
structure of the dataset
and therefore highly affect the obtained predictions.

To ``bring data to the same scale'', we often apply a trick called \textbf{standardisation}.

Computing the so-called \textbf{Z-scores} of the \(j\)-th feature, \(\mathbf{x}_{\cdot,j}\),
is done by subtracting from each observation the sample mean and dividing the result by the sample
standard deviation:

\[z_{i,j} = \frac{x_{i,j}-\bar{x}_{\cdot,j}}{s_{x_{\cdot,j}}}\]

This a new feature \(\mathbf{z}_{\cdot,j}\) that always has mean 0 and standard deviation of 1.

Moreover, it is \emph{unit-less} (e.g., we divide a value in kgs by a value in kgs,
the units are cancelled out).
This, amongst others, prevents one of the features from dominating
the other ones.

Z-scores are easy to interpret, e.g., 0.5 denotes an observation
that is 0.5 standard deviations above the mean
and -3 informs us that a value is 3 standard deviations below the mean.

\begin{description}
\item[Remark.]
(*) If data are normally distributed (bell-shaped histogram),
with very high probability, most (expected value is 99.74\%) observations
should have Z-scores between -3 and 3. Those that don't, are
``suspicious'', maybe they are outliers? We should inspect them manually.
\end{description}

Let's compute \texttt{Z\_train} and \texttt{Z\_test},
being the standardised versions of \texttt{X\_train}
and \texttt{X\_test}, respectively.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{means <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(X_train, }\DecValTok{2}\NormalTok{, mean) }\CommentTok{# column means}
\NormalTok{sds   <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(X_train, }\DecValTok{2}\NormalTok{, sd)   }\CommentTok{# column standard deviations}
\NormalTok{Z_train <-}\StringTok{ }\NormalTok{X_train }\CommentTok{# copy}
\NormalTok{Z_test  <-}\StringTok{ }\NormalTok{X_test  }\CommentTok{# copy}
\ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(X)) \{}
\NormalTok{    Z_train[,j] <-}\StringTok{ }\NormalTok{(Z_train[,j]}\OperatorTok{-}\NormalTok{means[j])}\OperatorTok{/}\NormalTok{sds[j]}
\NormalTok{    Z_test[,j]  <-}\StringTok{ }\NormalTok{(Z_test[,j] }\OperatorTok{-}\NormalTok{means[j])}\OperatorTok{/}\NormalTok{sds[j]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Note that we have transformed the training and test sample in the very same
way. Computing means and standard deviations separately for these two datasets
is a common error -- it is the training set that we use in the course of the
learning process.
The above can be re-written as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Z_train <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{apply}\NormalTok{(X_train, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(r) (r}\OperatorTok{-}\NormalTok{means)}\OperatorTok{/}\NormalTok{sds))}
\NormalTok{Z_test  <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{apply}\NormalTok{(X_test,  }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(r) (r}\OperatorTok{-}\NormalTok{means)}\OperatorTok{/}\NormalTok{sds))}
\end{Highlighting}
\end{Shaded}

See Figure \ref{fig:standardise_depict_hist} for an illustration.
Note that the righthand figures (histograms of standardised variables)
are on the same scale now.

\begin{figure}
\hypertarget{fig:standardise_depict_hist}{%
\centering
\includegraphics{03-classification-neighbours-figures/standardise_depict_hist-1.pdf}
\caption{Empirical distribution of two variables (pH on the top, fixed.acidity on the bottom) before (left) and after (right) standardising}\label{fig:standardise_depict_hist}
}
\end{figure}

\begin{description}
\item[Remark.]
Of course, standardisation is only about shifting and scaling, it
preserves the shape of the distribution. If the original variable
is right skewed or bimodal, its standardised version will remain as such.
\end{description}

Let's compute the accuracy of K-NN classifiers acting on standardised data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_knn5s <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Z_train, Z_test, Y_train, }\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(Y_test }\OperatorTok{==}\StringTok{ }\NormalTok{Y_knn5s) }\CommentTok{# accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.91429
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_knn9s <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Z_train, Z_test, Y_train, }\DataTypeTok{k=}\DecValTok{9}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(Y_test }\OperatorTok{==}\StringTok{ }\NormalTok{Y_knn9s) }\CommentTok{# accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.91378
\end{verbatim}

The accuracy is much better.

Standardisation is an example of \emph{feature engineering}.

Good models rarely work well ``straight out of the box'' -- if that was the case,
we wouldn't need data scientists and machine learning engineers!

To increase models' accuracy, we often spend a lot of time:

\begin{itemize}
\tightlist
\item
  cleansing data (e.g., removing outliers)
\item
  extracting new features
\item
  transforming existing features
\item
  trying to find a set of features that are relevant
\end{itemize}

This is the ``more art than science'' part of data science (sic!), and
hence most textbooks are not really eager for discussing such topics
(including this one).

Sorry, this is sad but true. The solutions that work well in the case of dataset
A may fail in the B case and vice versa. However, the more exercises you solve,
the greater the arsenal of ideas/possible approaches you will have at hand
when dealing with real-world problems.

Feature selection -- example (manually selected columns):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{features <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"density"}\NormalTok{, }\StringTok{"residual.sugar"}\NormalTok{)}
\NormalTok{Y_knn5s <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Z_train[,features], Z_test[,features],}
\NormalTok{    Y_train, }\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(Y_test }\OperatorTok{==}\StringTok{ }\NormalTok{Y_knn5s) }\CommentTok{# accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.91633
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_knn9s <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Z_train[,features], Z_test[,features],}
\NormalTok{    Y_train, }\DataTypeTok{k=}\DecValTok{9}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(Y_test }\OperatorTok{==}\StringTok{ }\NormalTok{Y_knn9s) }\CommentTok{# accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.925
\end{verbatim}

\begin{exercise}

Try to find a combination of 2-4 features (by guessing or applying magic tricks)
that increases the accuracy of a \(K\)-NN classifier on this dataset.

\end{exercise}

\hypertarget{model-assessment-and-selection}{%
\section{Model Assessment and Selection}\label{model-assessment-and-selection}}

\hypertarget{performance-metrics}{%
\subsection{Performance Metrics}\label{performance-metrics}}

Recall that \(y_i\) denotes the true label associated with the \(i\)-th observation.

Let \(\hat{y}_i\) denote the classifier's output for a given \(\mathbf{x}_{i,\cdot}\).

Ideally, we'd wish that \(\hat{y}_i=y_i\).

Sadly, in practice we will make errors.

Here are the 4 possible situations (true vs.~predicted label):

\begin{longtable}[]{@{}lll@{}}
\toprule
. & \(y_i=0\) & \(y_i=1\)\tabularnewline
\midrule
\endhead
\(\hat{y}_i=0\) & \textbf{True Negative} & False Negative (Type II error)\tabularnewline
\(\hat{y}_i=1\) & False Positive (Type I error) & \textbf{True Positive}\tabularnewline
\bottomrule
\end{longtable}

Note that the terms \textbf{positive} and \textbf{negative} refer to
the classifier's output, i.e., occur when \(\hat{y}_i\) is equal to \(1\) and \(0\), respectively.

A \textbf{confusion matrix} is used to summarise
the correctness of predictions for the whole sample:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Z_train, Z_test, Y_train, }\DataTypeTok{k=}\DecValTok{9}\NormalTok{)}
\NormalTok{(C <-}\StringTok{ }\KeywordTok{table}\NormalTok{(Y_pred, Y_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Y_test
## Y_pred    0    1
##      0 1607  133
##      1   36  184
\end{verbatim}

For example,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\CommentTok{# number of TNs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1607
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\CommentTok{# number of FPs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 36
\end{verbatim}

\textbf{Accuracy} is the ratio of the correctly classified instances
to all the instances.

In other words, it is the probability of making a correct prediction.

\[
\text{Accuracy} = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
= \frac{1}{n} \sum_{i=1}^n \mathbb{I}\left(
y_i = \hat{y}_i
\right)
\]
where \(\mathbb{I}\) is the indicator function,
\(\mathbb{I}(l)=1\) if logical condition \(l\) is true and \(0\) otherwise.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(Y_test }\OperatorTok{==}\StringTok{ }\NormalTok{Y_pred) }\CommentTok{# accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.91378
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(C[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(C) }\CommentTok{# equivalently}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.91378
\end{verbatim}

In many applications we are dealing with \textbf{unbalanced problems}, where
the case \(y_i=1\) is relatively rare,
yet predicting it correctly is much more important than being
accurate with respect to class \(0\).

\begin{description}
\item[Remark.]
Think of medical applications, e.g., HIV testing
or tumour diagnosis.
\end{description}

In such a case, \emph{accuracy} as a metric fails to quantify what we are aiming for.

\begin{description}
\item[Remark.]
If only 1\% of the cases have true \(y_i=1\),
then a dummy classifier that always
outputs \(\hat{y}_i=0\) has 99\% accuracy.
\end{description}

Metrics such as precision and recall (and their aggregated version, F-measure)
aim to address this problem.

\textbf{Precision}

\[
\text{Precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}
\]

If the classifier outputs \(1\),
what is the probability that this is indeed true?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{(C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]) }\CommentTok{# Precision}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.83636
\end{verbatim}

\textbf{Recall} (a.k.a. sensitivity, hit rate or true positive rate)

\[
\text{Recall} = \frac{\text{TP}}{\text{TP}+\text{FN}}
\]

If the true class is \(1\), what is the probability that the classifier
will detect it?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{(C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]) }\CommentTok{# Recall}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.58044
\end{verbatim}

\begin{description}
\item[Remark.]
Precision or recall? It depends on an application.
Think of medical diagnosis, medical screening, plagiarism detection,
etc. --- which measure is more important in each of the settings listed?
\end{description}

As a compromise, we can use the \textbf{F-measure}
(a.k.a. \(F_1\)-measure),
which is the harmonic mean of precision
and recall:

\[
\text{F} = \frac{1}{
    \frac{
        \frac{1}{\text{Precision}}+\frac{1}{\text{Recall}}
    }{2}
}
=
\left(
\frac{1}{2}
\left(
\text{Precision}^{-1}+\text{Recall}^{-1}
\right)
\right)^{-1}
=
\frac{\text{TP}}{\text{TP} + \frac{\text{FP} + \text{FN}}{2}}
\]

\begin{exercise}

Show that the above equality holds.

\end{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{(C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\FloatTok{0.5}\OperatorTok{*}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\FloatTok{0.5}\OperatorTok{*}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]) }\CommentTok{# F}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.68529
\end{verbatim}

The following function can come in handy in the future:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get_metrics <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(Y_pred, Y_test)}
\NormalTok{\{}
\NormalTok{    C <-}\StringTok{ }\KeywordTok{table}\NormalTok{(Y_pred, Y_test) }\CommentTok{# confusion matrix}
    \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{dim}\NormalTok{(C) }\OperatorTok{==}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
    \KeywordTok{c}\NormalTok{(}\DataTypeTok{Acc=}\NormalTok{(C[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(C), }\CommentTok{# accuracy}
      \DataTypeTok{Prec=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{(C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]), }\CommentTok{# precision}
      \DataTypeTok{Rec=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{(C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]), }\CommentTok{# recall}
      \DataTypeTok{F=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{(C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\FloatTok{0.5}\OperatorTok{*}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\FloatTok{0.5}\OperatorTok{*}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]), }\CommentTok{# F-measure}
      \CommentTok{# Confusion matrix items:}
      \DataTypeTok{TN=}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], }\DataTypeTok{FN=}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{],}
      \DataTypeTok{FP=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{], }\DataTypeTok{TP=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{    ) }\CommentTok{# return a named vector}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_metrics}\NormalTok{(Y_pred, Y_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Acc       Prec        Rec          F         TN         FN 
##    0.91378    0.83636    0.58044    0.68529 1607.00000  133.00000 
##         FP         TP 
##   36.00000  184.00000
\end{verbatim}

\hypertarget{how-to-choose-k-for-k-nn-classification}{%
\subsection{How to Choose K for K-NN Classification?}\label{how-to-choose-k-for-k-nn-classification}}

We haven't yet considered the question which \(K\) yields \emph{the best}
classifier.

Best == one that has the highest \emph{predictive power}.

Best == with respect to some chosen metric (accuracy, recall, precision, F-measure, \ldots{})

Let's study how the metrics on the test set change as functions of the number of nearest neighbours considered, \(K\).

Auxiliary function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knn_metrics <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(k, X_train, X_test, Y_train, Y_test)}
\NormalTok{\{}
\NormalTok{    Y_pred <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(X_train, X_test, Y_train, }\DataTypeTok{k=}\NormalTok{k) }\CommentTok{# classify}
    \KeywordTok{get_metrics}\NormalTok{(Y_pred, Y_test)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

For example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{knn_metrics}\NormalTok{(}\DecValTok{5}\NormalTok{, Z_train, Z_test, Y_train, Y_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Acc       Prec        Rec          F         TN         FN 
##    0.91429    0.82251    0.59937    0.69343 1602.00000  127.00000 
##         FP         TP 
##   41.00000  190.00000
\end{verbatim}

Example call to evaluate metrics as a function of different \(K\)s:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ks <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DataTypeTok{by=}\DecValTok{2}\NormalTok{)}
\NormalTok{Ps <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{t}\NormalTok{(}
    \KeywordTok{sapply}\NormalTok{(Ks, }\CommentTok{# on each element in this vector}
\NormalTok{        knn_metrics,     }\CommentTok{# apply this function}
\NormalTok{        Z_train, Z_test, Y_train, Y_test }\CommentTok{# aux args}
\NormalTok{    )))}
\end{Highlighting}
\end{Shaded}

\begin{description}
\item[Remark.]
Note that \texttt{sapply(X,\ f,\ arg1,\ arg2,\ ...)}
outputs a list \texttt{Y} such that
\texttt{Y{[}{[}i{]}{]}\ =\ f(X{[}i{]},\ arg1,\ arg2,\ ...)}
which is then simplified to a matrix.
\item[Remark.]
We transpose this result, \texttt{t()}, in order to get each metric
corresponding to different columns in the result.
As usual, if you keep wondering, e.g., why \texttt{t()}, play with
the code yourself -- it's fun fun fun.
\end{description}

Example results:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{K=}\NormalTok{Ks, Ps), }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     K  Acc Prec  Rec    F   TN  FN FP  TP
## 1   1 0.92 0.77 0.72 0.74 1574  90 69 227
## 2   3 0.92 0.79 0.66 0.72 1587 108 56 209
## 3   5 0.91 0.82 0.60 0.69 1602 127 41 190
## 4   7 0.91 0.82 0.56 0.67 1604 138 39 179
## 5   9 0.91 0.84 0.58 0.69 1607 133 36 184
## 6  11 0.91 0.85 0.56 0.68 1611 138 32 179
## 7  13 0.91 0.83 0.57 0.68 1606 136 37 181
## 8  15 0.91 0.83 0.55 0.66 1607 144 36 173
## 9  17 0.91 0.82 0.53 0.64 1607 149 36 168
## 10 19 0.90 0.81 0.52 0.63 1603 151 40 166
\end{verbatim}

Figure \ref{fig:whichK5} is worth a thousand tables though (see \texttt{?matplot} in R). The reader is kindly asked to draw conclusions themself.

\begin{figure}
\hypertarget{fig:whichK5}{%
\centering
\includegraphics{03-classification-neighbours-figures/whichK5-1.pdf}
\caption{Performance of \(K\)-nn classifiers as a function of \(K\) for standardised and raw data}\label{fig:whichK5}
}
\end{figure}

\hypertarget{training-validation-and-test-sets}{%
\subsection{Training, Validation and Test sets}\label{training-validation-and-test-sets}}

In the \(K\)-NN classification task, there are many hyperparameters to tune up:

\begin{itemize}
\item
  Which \(K\) should we choose?
\item
  Should we standardise the dataset?
\item
  Which variables should be taken into account when computing the Euclidean distance?
\end{itemize}

\begin{description}
\item[Remark.]
\textbf{If we select the best hyperparameter set based on test
sample error, we will run into the trap of overfitting again}.
This time we'll be overfitting to the test set --- the model that is optimal
for a given test sample doesn't have to generalise well to other test samples (!).
\end{description}

In order to overcome this problem,
we can perform a random \textbf{train-validation-test split} of the original dataset:

\begin{itemize}
\tightlist
\item
  \emph{training sample} (e.g., 60\%) -- used to construct the models
\item
  \emph{validation sample} (e.g., 20\%) -- used to tune the hyperparameters of the classifier
\item
  \emph{test sample} (e.g., 20\%) -- used to assess the goodness of fit
\end{itemize}

An example way to perform a 60/20/20\% train-validation-test split:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{# reproducibility matters}
\NormalTok{random_indices <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(n)}
\NormalTok{n1 <-}\StringTok{ }\KeywordTok{floor}\NormalTok{(n}\OperatorTok{*}\FloatTok{0.6}\NormalTok{)}
\NormalTok{n2 <-}\StringTok{ }\KeywordTok{floor}\NormalTok{(n}\OperatorTok{*}\FloatTok{0.8}\NormalTok{)}
\NormalTok{X2_train <-}\StringTok{ }\NormalTok{X[random_indices[}\DecValTok{1}     \OperatorTok{:}\NormalTok{n1], ]}
\NormalTok{Y2_train <-}\StringTok{ }\NormalTok{Y[random_indices[}\DecValTok{1}     \OperatorTok{:}\NormalTok{n1]  ]}
\NormalTok{X2_valid <-}\StringTok{ }\NormalTok{X[random_indices[(n1}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{:}\NormalTok{n2], ]}
\NormalTok{Y2_valid <-}\StringTok{ }\NormalTok{Y[random_indices[(n1}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{:}\NormalTok{n2]  ]}
\NormalTok{X2_test  <-}\StringTok{ }\NormalTok{X[random_indices[(n2}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{:}\NormalTok{n ], ]}
\NormalTok{Y2_test  <-}\StringTok{ }\NormalTok{Y[random_indices[(n2}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{:}\NormalTok{n ]  ]}
\KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(X2_train)}\OperatorTok{+}\KeywordTok{nrow}\NormalTok{(X2_valid)}\OperatorTok{+}\KeywordTok{nrow}\NormalTok{(X2_test)}
    \OperatorTok{==}\StringTok{ }\KeywordTok{nrow}\NormalTok{(X))}
\end{Highlighting}
\end{Shaded}

\begin{exercise}

Find the best \(K\) on the validation set and compute the error metrics
on the test set.

\end{exercise}

\begin{description}
\item[Remark.]
(*) If our dataset is too small,
we can use various \emph{cross-validation} techniques
instead of a train-validate-test split.
\end{description}

\hypertarget{implementing-a-k-nn-classifier}{%
\section{Implementing a K-NN Classifier (*)}\label{implementing-a-k-nn-classifier}}

\hypertarget{factor-data-type}{%
\subsection{Factor Data Type}\label{factor-data-type}}

Recall that (see Appendix B for more details)
\texttt{factor} type in R is a very convenient means to encode categorical data
(such as \(\mathbf{y}\)):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{, }\StringTok{"no"}\NormalTok{, }\StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{)}
\NormalTok{f <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(x, }\DataTypeTok{levels=}\KeywordTok{c}\NormalTok{(}\StringTok{"no"}\NormalTok{, }\StringTok{"yes"}\NormalTok{))}
\NormalTok{f}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] yes no  no  yes no 
## Levels: no yes
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(f) }\CommentTok{# counts}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## f
##  no yes 
##   3   2
\end{verbatim}

Internally, objects of type \texttt{factor} are represented as integer vectors
with elements in \(\{1,\dots,M\}\), where \(M\) is the number of possible levels.

Labels, used to ``decipher'' the numeric codes, are stored separately.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(f) }\CommentTok{# 2nd label, 1st label, 1st label etc.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 1 1 2 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{levels}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "no"  "yes"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{levels}\NormalTok{(f) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"failure"}\NormalTok{, }\StringTok{"success"}\NormalTok{) }\CommentTok{# re-encode}
\NormalTok{f}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] success failure failure success failure
## Levels: failure success
\end{verbatim}

\hypertarget{main-routine}{%
\subsection{Main Routine (*)}\label{main-routine}}

Let's implement a K-NN classifier ourselves
by using a top-bottom approach.

We will start with a general description of the admissible inputs
and the expected output.

Then we will arrange the processing of data into
conveniently manageable chunks.

The function's declaration will look like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{our_knn <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(X_train, X_test, Y_train, }\DataTypeTok{k=}\DecValTok{1}\NormalTok{) \{}
    \CommentTok{# k=1 denotes a parameter with a default value}
    \CommentTok{# ...}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Load an example dataset on which we will test our algorithm:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wines <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/winequality-all.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{wines <-}\StringTok{ }\NormalTok{wines[wines}\OperatorTok{$}\NormalTok{color }\OperatorTok{==}\StringTok{ "white"}\NormalTok{,]}
\NormalTok{X <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(wines[,}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{])}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(wines}\OperatorTok{$}\NormalTok{alcohol }\OperatorTok{>=}\StringTok{ }\DecValTok{12}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Note that \texttt{Y} is now a factor object.

Train-test split:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{random_indices <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(n)}
\NormalTok{train_indices <-}\StringTok{ }\NormalTok{random_indices[}\DecValTok{1}\OperatorTok{:}\KeywordTok{floor}\NormalTok{(n}\OperatorTok{*}\FloatTok{0.6}\NormalTok{)]}
\NormalTok{X_train <-}\StringTok{ }\NormalTok{X[train_indices,]}
\NormalTok{Y_train <-}\StringTok{ }\NormalTok{Y[train_indices]}
\NormalTok{X_test  <-}\StringTok{ }\NormalTok{X[}\OperatorTok{-}\NormalTok{train_indices,]}
\NormalTok{Y_test  <-}\StringTok{ }\NormalTok{Y[}\OperatorTok{-}\NormalTok{train_indices]}
\end{Highlighting}
\end{Shaded}

First, we should specify the type and form of the arguments
we're expecting:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is the body of our_knn() - part 1}
\KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{is.numeric}\NormalTok{(X_train), }\KeywordTok{is.matrix}\NormalTok{(X_train))}
\KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{is.numeric}\NormalTok{(X_test), }\KeywordTok{is.matrix}\NormalTok{(X_test))}
\KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{is.factor}\NormalTok{(Y_train))}
\KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{ncol}\NormalTok{(X_train) }\OperatorTok{==}\StringTok{ }\KeywordTok{ncol}\NormalTok{(X_test))}
\KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(X_train) }\OperatorTok{==}\StringTok{ }\KeywordTok{length}\NormalTok{(Y_train))}
\KeywordTok{stopifnot}\NormalTok{(k }\OperatorTok{>=}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{n_train <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(X_train)}
\NormalTok{n_test  <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(X_test)}
\NormalTok{p <-}\StringTok{ }\KeywordTok{ncol}\NormalTok{(X_train)}
\NormalTok{M <-}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{levels}\NormalTok{(Y_train))}
\end{Highlighting}
\end{Shaded}

Therefore,

\(\mathtt{X\_train}\in\mathbb{R}^{\mathtt{n\_train}\times \mathtt{p}}\),
\(\mathtt{X\_test}\in\mathbb{R}^{\mathtt{n\_test}\times \mathtt{p}}\) and
\(\mathtt{Y\_train}\in\{1,\dots,M\}^{\mathtt{n\_train}}\)

\begin{description}
\item[Remark.]
Recall that R \texttt{factor} objects are internally encoded as integer vectors.
\end{description}

Next, we will call the (to-be-done) function \texttt{our\_get\_knnx()},
which seeks nearest neighbours of all the points:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# our_get_knnx returns a matrix nn_indices of size n_test*k,}
\CommentTok{# where nn_indices[i,j] denotes the index of}
\CommentTok{# X_test[i,]'s j-th nearest neighbour in X_train.}
\CommentTok{# (It is the point X_train[nn_indices[i,j],]).}
\NormalTok{nn_indices <-}\StringTok{ }\KeywordTok{our_get_knnx}\NormalTok{(X_train, X_test, k)}
\end{Highlighting}
\end{Shaded}

Then, for each point in \texttt{X\_test},
we fetch the labels corresponding to its nearest neighbours
and compute their mode:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n_test) }\CommentTok{# vector of length n_test}
\CommentTok{# For now we will operate on the integer labels in \{1,...,M\}}
\NormalTok{Y_train_int <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(Y_train)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_test) \{}
    \CommentTok{# Get the labels of the NNs of the i-th point:}
\NormalTok{    nn_labels_i <-}\StringTok{ }\NormalTok{Y_train_int[nn_indices[i,]]}
    \CommentTok{# Compute the mode (majority vote):}
\NormalTok{    Y_pred[i] <-}\StringTok{ }\KeywordTok{our_mode}\NormalTok{(nn_labels_i) }\CommentTok{# in \{1,...,M\}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Finally, we should convert the resulting integer vector
to an object of type \texttt{factor}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Convert Y_pred to factor:}
\KeywordTok{return}\NormalTok{(}\KeywordTok{factor}\NormalTok{(Y_pred, }\DataTypeTok{labels=}\KeywordTok{levels}\NormalTok{(Y_train)))}
\end{Highlighting}
\end{Shaded}

\hypertarget{mode}{%
\subsection{Mode}\label{mode}}

To implement the mode, we can use the \texttt{tabulate()} function.

\begin{exercise}

Read the function's man page, see \texttt{?tabulate}.

\end{exercise}

For example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tabulate}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4 2 0 0 1
\end{verbatim}

There might be multiple modes -- in such a case, we should pick one at random.

For that, we can use the \texttt{sample()} function.

\begin{exercise}

Read the function's man page, see \texttt{?sample}.
Note that its behaviour is different when it's first argument is a vector of length 1.

\end{exercise}

An example implementation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{our_mode <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(Y) \{}
    \CommentTok{# tabulate() will take care of}
    \CommentTok{# checking the correctness of Y}
\NormalTok{    t <-}\StringTok{ }\KeywordTok{tabulate}\NormalTok{(Y)}
\NormalTok{    mode_candidates <-}\StringTok{ }\KeywordTok{which}\NormalTok{(t }\OperatorTok{==}\StringTok{ }\KeywordTok{max}\NormalTok{(t))}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{length}\NormalTok{(mode_candidates) }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\KeywordTok{return}\NormalTok{(mode_candidates)}
    \ControlFlowTok{else} \KeywordTok{return}\NormalTok{(}\KeywordTok{sample}\NormalTok{(mode_candidates, }\DecValTok{1}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{our_mode}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{our_mode}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{our_mode}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{our_mode}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{our_mode}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\hypertarget{nn-search-routines}{%
\subsection{NN Search Routines (*)}\label{nn-search-routines}}

Last but not least, we should implement the \texttt{our\_get\_knnx()} function.

It is the function responsible for seeking the indices of nearest neighbours.

It turns out this function will actually constitute the K-NN classifier's performance
bottleneck in case of big data samples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# our_get_knnx returns a matrix nn_indices of size n_test*k,}
\CommentTok{# where nn_indices[i,j] denotes the index of}
\CommentTok{# X_test[i,]'s j-th nearest neighbour in X_train.}
\CommentTok{# (It is the point X_train[nn_indices[i,j],]).}
\NormalTok{our_get_knnx <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(X_train, X_test, k) \{}
    \CommentTok{# ...}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

A naive approach to \texttt{our\_get\_knnx()} relies on computing all pairwise distances,
and sorting them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{our_get_knnx <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(X_train, X_test, k) \{}
\NormalTok{    n_test <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(X_test)}
\NormalTok{    nn_indices <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA_real_}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{n_test, }\DataTypeTok{ncol=}\NormalTok{k)}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_test) \{}
\NormalTok{        d <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(X_train, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x)}
            \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{((x}\OperatorTok{-}\NormalTok{X_test[i,])}\OperatorTok{^}\DecValTok{2}\NormalTok{)))}
        \CommentTok{# now d[j] is the distance}
        \CommentTok{# between X_train[j,] and X_test[i,]}
\NormalTok{        nn_indices[i,] <-}\StringTok{ }\KeywordTok{order}\NormalTok{(d)[}\DecValTok{1}\OperatorTok{:}\NormalTok{k]}
\NormalTok{    \}}
\NormalTok{    nn_indices}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

A comparison with \texttt{FNN:knn()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(Ya <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(X_train, X_test, Y_train, }\DataTypeTok{k=}\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##   0.176   0.000   0.175
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(Yb <-}\StringTok{ }\KeywordTok{our_knn}\NormalTok{(X_train, X_test, Y_train, }\DataTypeTok{k=}\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##  20.472   0.000  20.467
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(Ya }\OperatorTok{==}\StringTok{ }\NormalTok{Yb) }\CommentTok{# 1.0 on perfect match}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

Both functions return identical results but our implementation is ``slightly'' slower.

\texttt{FNN:knn()} is efficiently written in C++, which is a compiled programming language.

R, on the other hand (just like Python and Matlab) is interpreted, therefore
as a rule of thumb we should consider it an order of magnitude slower (see, however, the Julia language).

Let's substitute our naive implementation with the equivalent one,
but written in C++ (available in the \texttt{FNN} package).

\begin{description}
\item[Remark.]
(*) Note that we can write a C++ implementation ourselves,
see the Rcpp package for seamless R and C++ integration.
\end{description}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{our_get_knnx <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(X_train, X_test, k) \{}
    \CommentTok{# this is used by our_knn()}
\NormalTok{    FNN}\OperatorTok{::}\KeywordTok{get.knnx}\NormalTok{(X_train, X_test, k, }\DataTypeTok{algorithm=}\StringTok{"brute"}\NormalTok{)}\OperatorTok{$}\NormalTok{nn.index}
\NormalTok{\}}
\KeywordTok{system.time}\NormalTok{(Ya <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(X_train, X_test, Y_train, }\DataTypeTok{k=}\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##   0.176   0.000   0.175
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(Yb <-}\StringTok{ }\KeywordTok{our_knn}\NormalTok{(X_train, X_test, Y_train, }\DataTypeTok{k=}\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##   0.058   0.000   0.058
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(Ya }\OperatorTok{==}\StringTok{ }\NormalTok{Yb) }\CommentTok{# 1.0 on perfect match}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

Note that our solution requires \(c\cdot n_\text{test}\cdot n_\text{train}\cdot p\)
arithmetic operations for some \(c>1\).
The overall cost of sorting is at least \(d\cdot n_\text{test}\cdot n_\text{train}\cdot\log n_\text{train}\)
for some \(d>1\).

This does not scale well with both \(n_\text{test}\) and \(n_\text{train}\)
(think -- big data).

\bigskip

It turns out that there are special \textbf{spatial data structures}
-- such as \emph{metric trees} -- that aim to speed up searching for nearest
neighbours in \emph{low-dimensional spaces} (for small \(p\)).

\begin{description}
\item[Remark.]
(*) Searching in high-dimensional spaces is hard due to the so-called
curse of dimensionality.
\end{description}

For example, \texttt{FNN::get.knnx()} also implements the so-called
kd-trees.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"microbenchmark"}\NormalTok{)}
\NormalTok{test_speed <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(n, p, k) \{}
\NormalTok{    A <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{runif}\NormalTok{(n}\OperatorTok{*}\NormalTok{p), }\DataTypeTok{nrow=}\NormalTok{n, }\DataTypeTok{ncol=}\NormalTok{p)}
\NormalTok{    s <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(microbenchmark}\OperatorTok{::}\KeywordTok{microbenchmark}\NormalTok{(}
        \DataTypeTok{brute=}\NormalTok{FNN}\OperatorTok{::}\KeywordTok{get.knnx}\NormalTok{(A, A, k, }\DataTypeTok{algorithm=}\StringTok{"brute"}\NormalTok{),}
        \DataTypeTok{kd_tree=}\NormalTok{FNN}\OperatorTok{::}\KeywordTok{get.knnx}\NormalTok{(A, A, k, }\DataTypeTok{algorithm=}\StringTok{"kd_tree"}\NormalTok{),}
        \DataTypeTok{times=}\DecValTok{3}
\NormalTok{    ), }\DataTypeTok{unit=}\StringTok{"s"}\NormalTok{)}
    \CommentTok{# minima of 3 time measurements:}
    \KeywordTok{structure}\NormalTok{(s}\OperatorTok{$}\NormalTok{min, }\DataTypeTok{names=}\KeywordTok{as.character}\NormalTok{(s}\OperatorTok{$}\NormalTok{expr))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{test_speed}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    brute  kd_tree 
## 0.383825 0.017505
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{test_speed}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    brute  kd_tree 
## 0.551996 0.083214
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{test_speed}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   brute kd_tree 
## 0.83981 0.85611
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{test_speed}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   brute kd_tree 
##  1.6258  7.2967
\end{verbatim}

\hypertarget{different-metrics}{%
\subsection{Different Metrics (*)}\label{different-metrics}}

The Euclidean distance is just one particular example
of many possible \textbf{metrics} (metric == a mathematical term,
above we have used this term in a more relaxed fashion, when referring
to accuracy etc.).

Mathematically, we say that \(d\) is a metric on a set \(X\)
(e.g., \(\mathbb{R}^p\)), whenever
it is a function \(d:X\times X\to [0,\infty]\) such that for all \(x,x',x''\in X\):

\begin{itemize}
\tightlist
\item
  \(d(x, x') = 0\) if and only if \(x=x'\),
\item
  \(d(x, x') = d(x', x)\) (it is symmetric)
\item
  \(d(x, x'') \le d(x, x') + d(x', x'')\) (it fulfils the triangle inequality)
\end{itemize}

\begin{description}
\item[Remark.]
(*) Not all the properties are required in all the applications;
sometimes we might need a few additional ones.
\end{description}

We can easily generalise the way we introduced the K-NN method
to have a classifier that is based on a point's neighbourhood
with respect to any metric.

Example metrics on \(\mathbb{R}^p\):

\begin{itemize}
\tightlist
\item
  \textbf{Euclidean}
  \[
  d_2(\mathbf{x}, \mathbf{x}') = \| \mathbf{x}-\mathbf{x}' \| = \| \mathbf{x}-\mathbf{x}' \|_2 = \sqrt{ \sum_{i=1}^p (x_i-x_i')^2 }
  \]
\item
  \textbf{Manhattan} (taxicab)
  \[
  d_1(\mathbf{x}, \mathbf{x}') = \| \mathbf{x}-\mathbf{x}' \|_1 = { \sum_{i=1}^p |x_i-x_i'| }
  \]
\item
  \textbf{Chebyshev} (maximum)
  \[
  d_\infty(\mathbf{x}, \mathbf{x}') = \| \mathbf{x}-\mathbf{x}' \|_\infty = \max_{i=1,\dots,p} |x_i-x_i'|
  \]
\end{itemize}

We can define metrics on different spaces too.

For example, the \textbf{Levenshtein distance} is a popular choice
for comparing character strings (also DNA sequences etc.)

It is an \emph{edit distance} -- it measures the minimal number of
single-character insertions, deletions or substitutions to change
one string into another.

For instance:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{adist}\NormalTok{(}\StringTok{"happy"}\NormalTok{, }\StringTok{"nap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## [1,]    3
\end{verbatim}

This is because we need 1 substitution and 2 deletions,

happy → nappy → napp → nap.

See also:

\begin{itemize}
\tightlist
\item
  the Hamming distance for categorical vectors (or strings of equal lengths),
\item
  the Jaccard distance for sets,
\item
  the Kendall tau rank distance for rankings.
\end{itemize}

Moreover, R package \texttt{stringdist} includes implementations
of numerous string metrics.

\hypertarget{outro-2}{%
\section{Outro}\label{outro-2}}

\hypertarget{remarks-2}{%
\subsection{Remarks}\label{remarks-2}}

Note that K-NN is suitable for any kind of multiclass classification.

However, in practice it's pretty slow for larger datasets -- to
classify a single point we have to query the whole training set (which
should be available at all times).

In the next part we will discuss some other well-known classifiers:

\begin{itemize}
\tightlist
\item
  \emph{Decision trees}
\item
  \emph{Logistic regression}
\end{itemize}

\hypertarget{side-note-k-nn-regression}{%
\subsection{Side Note: K-NN Regression}\label{side-note-k-nn-regression}}

The K-Nearest Neighbour scheme is intuitively pleasing.

No wonder it has inspired a similar approach for solving a regression task.

In order to make a prediction for a new point \(\mathbf{x}'\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  find the K-nearest neighbours of \(\mathbf{x}'\) amongst the points in the train set,
  denoted \(\mathbf{x}_{i_1,\cdot}, \dots, \mathbf{x}_{i_K,\cdot}\),
\item
  fetch the corresponding reference outputs \(y_{i_1}, \dots, y_{i_K}\),
\item
  return their arithmetic mean as a result,
  \[\hat{y}=\frac{1}{K} \sum_{j=1}^K y_{i_j}.\]
\end{enumerate}

Recall our modelling of the Credit Rating (\(Y\))
as a function of the average Credit Card Balance (\(X\))
based on the \texttt{ISLR::Credit} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"ISLR"}\NormalTok{) }\CommentTok{# Credit dataset}
\NormalTok{Xc <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Balance[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{]))}
\NormalTok{Yc <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(Credit}\OperatorTok{$}\NormalTok{Rating[Credit}\OperatorTok{$}\NormalTok{Balance}\OperatorTok{>}\DecValTok{0}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"FNN"}\NormalTok{) }\CommentTok{# knn.reg function}
\NormalTok{x <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\KeywordTok{min}\NormalTok{(Xc), }\KeywordTok{max}\NormalTok{(Xc), }\DataTypeTok{length.out=}\DecValTok{101}\NormalTok{))}
\NormalTok{y1  <-}\StringTok{ }\KeywordTok{knn.reg}\NormalTok{(Xc, x, Yc, }\DataTypeTok{k=}\DecValTok{1}\NormalTok{)}\OperatorTok{$}\NormalTok{pred}
\NormalTok{y5  <-}\StringTok{ }\KeywordTok{knn.reg}\NormalTok{(Xc, x, Yc, }\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}\OperatorTok{$}\NormalTok{pred}
\NormalTok{y25 <-}\StringTok{ }\KeywordTok{knn.reg}\NormalTok{(Xc, x, Yc, }\DataTypeTok{k=}\DecValTok{25}\NormalTok{)}\OperatorTok{$}\NormalTok{pred}
\end{Highlighting}
\end{Shaded}

The three models are depicted in Figure \ref{fig:knnreg3}.
Again, the higher the \(K\), the smoother the curve. On the other hand, for
small \(K\) we adapt better to what's in a point's neighbourhood.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(Xc, Yc, }\DataTypeTok{col=}\StringTok{"#666666c0"}\NormalTok{,}
    \DataTypeTok{xlab=}\StringTok{"Balance"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Rating"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, y1,  }\DataTypeTok{col=}\DecValTok{2}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, y5,  }\DataTypeTok{col=}\DecValTok{3}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, y25, }\DataTypeTok{col=}\DecValTok{4}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\DataTypeTok{legend=}\KeywordTok{c}\NormalTok{(}\StringTok{"K=1"}\NormalTok{, }\StringTok{"K=5"}\NormalTok{, }\StringTok{"K=25"}\NormalTok{),}
    \DataTypeTok{col=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\DataTypeTok{lwd=}\DecValTok{3}\NormalTok{, }\DataTypeTok{bg=}\StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:knnreg3}{%
\centering
\includegraphics{03-classification-neighbours-figures/knnreg3-1.pdf}
\caption{K-NN regression example}\label{fig:knnreg3}
}
\end{figure}

\hypertarget{further-reading-2}{%
\subsection{Further Reading}\label{further-reading-2}}

Recommended further reading: (Hastie et al. \protect\hyperlink{ref-esl}{2017}: Section 13.3)

\hypertarget{classification-with-trees-and-linear-models}{%
\chapter{Classification with Trees and Linear Models}\label{classification-with-trees-and-linear-models}}

\hypertarget{introduction-5}{%
\section{Introduction}\label{introduction-5}}

\hypertarget{classification-task-1}{%
\subsection{Classification Task}\label{classification-task-1}}

Let \(\mathbf{X}\in\mathbb{R}^{n\times p}\) be an input matrix
that consists of \(n\) points in a \(p\)-dimensional space (each of the \(n\) objects
is described by means of \(p\) numerical features)

Recall that in supervised learning, with each
\(\mathbf{x}_{i,\cdot}\) we associate the desired output \(y_i\).

Hence, our dataset is \([\mathbf{X}\ \mathbf{y}]\) --
where each object is represented as a row vector
\([\mathbf{x}_{i,\cdot}\ y_i]\), \(i=1,\dots,n\):

\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} & y_1\\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} & y_2\\
\vdots & \vdots & \ddots & \vdots    & \vdots\\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} & y_n\\
\end{array}
\right].
\]

\bigskip

In this chapter we are still interested in \textbf{classification} tasks;
we assume that each \(y_i\) is a descriptive label.

Let's assume that we are faced with \textbf{binary classification} tasks.

Hence, there are only two possible labels that we traditionally denote with \(0\)s and \(1\)s.

For example:

\begin{longtable}[]{@{}ll@{}}
\toprule
0 & 1\tabularnewline
\midrule
\endhead
no & yes\tabularnewline
false & true\tabularnewline
failure & success\tabularnewline
healthy & ill\tabularnewline
\bottomrule
\end{longtable}

Let's recall the synthetic 2D dataset from the previous chapter
(true decision boundary is at \(X_1=0\)), see Figure \ref{fig:synthetic}.

\begin{figure}
\hypertarget{fig:synthetic}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/synthetic-1.pdf}
\caption{A synthetic 2D dataset with the true decision boundary at \(X_1=0\)}\label{fig:synthetic}
}
\end{figure}

\hypertarget{data-1}{%
\subsection{Data}\label{data-1}}

For illustration, we'll be considering the Wine Quality dataset
(white wines only):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wines <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/winequality-all.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{wines <-}\StringTok{ }\NormalTok{wines[wines}\OperatorTok{$}\NormalTok{color }\OperatorTok{==}\StringTok{ "white"}\NormalTok{,]}
\NormalTok{(n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(wines)) }\CommentTok{# number of samples}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4898
\end{verbatim}

The input matrix \(\mathbf{X}\in\mathbb{R}^{n\times p}\)
consists of the first 10 numeric variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(wines[,}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{])}
\KeywordTok{dim}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4898   10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(X, }\DecValTok{2}\NormalTok{) }\CommentTok{# first two rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      fixed.acidity volatile.acidity citric.acid residual.sugar
## 1600           7.0             0.27        0.36           20.7
## 1601           6.3             0.30        0.34            1.6
##      chlorides free.sulfur.dioxide total.sulfur.dioxide density  pH
## 1600     0.045                  45                  170   1.001 3.0
## 1601     0.049                  14                  132   0.994 3.3
##      sulphates
## 1600      0.45
## 1601      0.49
\end{verbatim}

The 11th variable measures the amount of alcohol (in \%).

We will convert this dependent variable to a binary one:

\begin{itemize}
\tightlist
\item
  0 == (\texttt{alcohol\ \ \textless{}\ 12}) == lower-alcohol wines
\item
  1 == (\texttt{alcohol\ \textgreater{}=\ 12}) == higher-alcohol wines
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# recall that TRUE == 1}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(wines}\OperatorTok{$}\NormalTok{alcohol }\OperatorTok{>=}\StringTok{ }\DecValTok{12}\NormalTok{)))}
\KeywordTok{table}\NormalTok{(Y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Y
##    0    1 
## 4085  813
\end{verbatim}

60/40\% train-test split:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{# reproducibility matters}
\NormalTok{random_indices <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(n)}
\KeywordTok{head}\NormalTok{(random_indices) }\CommentTok{# preview}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2463 2511 2227  526 4291 2986
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# first 60% of the indices (they are arranged randomly)}
\CommentTok{# will constitute the train sample:}
\NormalTok{train_indices <-}\StringTok{ }\NormalTok{random_indices[}\DecValTok{1}\OperatorTok{:}\KeywordTok{floor}\NormalTok{(n}\OperatorTok{*}\FloatTok{0.6}\NormalTok{)]}
\NormalTok{X_train <-}\StringTok{ }\NormalTok{X[train_indices,]}
\NormalTok{Y_train <-}\StringTok{ }\NormalTok{Y[train_indices]}
\CommentTok{# the remaining indices (40%) go to the test sample:}
\NormalTok{X_test  <-}\StringTok{ }\NormalTok{X[}\OperatorTok{-}\NormalTok{train_indices,]}
\NormalTok{Y_test  <-}\StringTok{ }\NormalTok{Y[}\OperatorTok{-}\NormalTok{train_indices]}
\end{Highlighting}
\end{Shaded}

Let's also compute \texttt{Z\_train} and \texttt{Z\_test}, being the standardised versions of \texttt{X\_train}
and \texttt{X\_test}, respectively.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{means <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(X_train, }\DecValTok{2}\NormalTok{, mean) }\CommentTok{# column means}
\NormalTok{sds   <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(X_train, }\DecValTok{2}\NormalTok{, sd)   }\CommentTok{# column standard deviations}
\NormalTok{Z_train <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{apply}\NormalTok{(X_train, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(r) (r}\OperatorTok{-}\NormalTok{means)}\OperatorTok{/}\NormalTok{sds))}
\NormalTok{Z_test  <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{apply}\NormalTok{(X_test,  }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(r) (r}\OperatorTok{-}\NormalTok{means)}\OperatorTok{/}\NormalTok{sds))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get_metrics <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(Y_pred, Y_test)}
\NormalTok{\{}
\NormalTok{    C <-}\StringTok{ }\KeywordTok{table}\NormalTok{(Y_pred, Y_test) }\CommentTok{# confusion matrix}
    \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{dim}\NormalTok{(C) }\OperatorTok{==}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
    \KeywordTok{c}\NormalTok{(}\DataTypeTok{Acc=}\NormalTok{(C[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(C), }\CommentTok{# accuracy}
      \DataTypeTok{Prec=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{(C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]), }\CommentTok{# precision}
      \DataTypeTok{Rec=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{(C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]), }\CommentTok{# recall}
      \DataTypeTok{F=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{(C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\FloatTok{0.5}\OperatorTok{*}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\FloatTok{0.5}\OperatorTok{*}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]), }\CommentTok{# F-measure}
      \CommentTok{# Confusion matrix items:}
      \DataTypeTok{TN=}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], }\DataTypeTok{FN=}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{],}
      \DataTypeTok{FP=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{], }\DataTypeTok{TP=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{    ) }\CommentTok{# return a named vector}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let's go back to the K-NN algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"FNN"}\NormalTok{)}
\NormalTok{Y_knn5   <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(X_train, X_test, Y_train, }\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\NormalTok{Y_knn9   <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(X_train, X_test, Y_train, }\DataTypeTok{k=}\DecValTok{9}\NormalTok{)}
\NormalTok{Y_knn5s  <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Z_train, Z_test, Y_train, }\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\NormalTok{Y_knn9s  <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(Z_train, Z_test, Y_train, }\DataTypeTok{k=}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Recall the quality metrics we have obtained previously (as a point of reference):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cbind}\NormalTok{(}
    \DataTypeTok{Knn5=}\KeywordTok{get_metrics}\NormalTok{(Y_knn5, Y_test),}
    \DataTypeTok{Knn9=}\KeywordTok{get_metrics}\NormalTok{(Y_knn9, Y_test),}
    \DataTypeTok{Knn5s=}\KeywordTok{get_metrics}\NormalTok{(Y_knn5s, Y_test),}
    \DataTypeTok{Knn9s=}\KeywordTok{get_metrics}\NormalTok{(Y_knn9s, Y_test)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            Knn5       Knn9      Knn5s      Knn9s
## Acc     0.81735    0.81939    0.91429    0.91378
## Prec    0.38674    0.34959    0.82251    0.83636
## Rec     0.22082    0.13565    0.59937    0.58044
## F       0.28112    0.19545    0.69343    0.68529
## TN   1532.00000 1563.00000 1602.00000 1607.00000
## FN    247.00000  274.00000  127.00000  133.00000
## FP    111.00000   80.00000   41.00000   36.00000
## TP     70.00000   43.00000  190.00000  184.00000
\end{verbatim}

In this chapter we discuss the following simple and educational
(yet practically useful)
classification algorithms:

\begin{itemize}
\tightlist
\item
  \emph{decision trees},
\item
  \emph{binary logistic regression}.
\end{itemize}

\hypertarget{decision-trees}{%
\section{Decision Trees}\label{decision-trees}}

\hypertarget{introduction-6}{%
\subsection{Introduction}\label{introduction-6}}

Note that a K-NN classifier discussed in the previous chapter
is \textbf{model-free}.
The whole training set must be stored and referred to at all times.

Therefore, it doesn't \emph{explain} the data we have -- we may use it solely
for the purpose of \emph{prediction}.

Perhaps one of the most interpretable (and hence human-friendly) models
consist of decision rules of the form:

\textbf{IF \(x_{i,j_1}\le v_1\) AND \ldots{} AND \(x_{i,j_r}\le v_r\) THEN \(\hat{y}_i=1\).}

These can be organised into a \textbf{hierarchy} for greater readability.

This idea inspired the notion of \textbf{decision trees} (Breiman et al. \protect\hyperlink{ref-cart}{1984}).

\begin{figure}
\hypertarget{fig:plot_rpart}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/plot_rpart-1.pdf}
\caption{The simplest decision tree for the synthetic 2D dataset and the corresponding decision boundaries}\label{fig:plot_rpart}
}
\end{figure}

Figure \ref{fig:plot_rpart2} depicts a very simple decision tree
for the aforementioned synthetic dataset.
There is only one decision boundary (based on \(X_1\)) that splits
data into the ``left'' and ``right'' sides.
Each tree node reports 3 pieces of information:

\begin{itemize}
\tightlist
\item
  dominating class (0 or 1)
\item
  (relative) proportion of 1s represented in a node
\item
  (absolute) proportion of all observations in a node
\end{itemize}

Figures \ref{fig:plot_rpart2} and \ref{fig:plot_rpart3} depict
trees with more decision rules.
Take a moment to contemplate how the corresponding decision boundaries
changed with the introduction of new decision rules.

\begin{figure}
\hypertarget{fig:plot_rpart2}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/plot_rpart2-1.pdf}
\caption{A more complicated decision tree for the synthetic 2D dataset and the corresponding decision boundaries}\label{fig:plot_rpart2}
}
\end{figure}

\begin{figure}
\hypertarget{fig:plot_rpart3}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/plot_rpart3-1.pdf}
\caption{An even more complicated decision tree for the synthetic 2D dataset and the corresponding decision boundaries}\label{fig:plot_rpart3}
}
\end{figure}

\hypertarget{example-in-r-1}{%
\subsection{Example in R}\label{example-in-r-1}}

We will use the \texttt{rpart()} function from the \texttt{rpart} package
to build a classification tree.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"rpart"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"rpart.plot"}\NormalTok{)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{rpart()} uses a formula (\texttt{\textasciitilde{}}) interface, hence it will be easier
to feed it with data in a data.frame form.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{XY_train <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(X_train), }\DataTypeTok{Y=}\NormalTok{Y_train)}
\NormalTok{XY_test <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(X_test), }\DataTypeTok{Y=}\NormalTok{Y_test)}
\end{Highlighting}
\end{Shaded}

Fit and plot a decision tree, see Figure \ref{fig:plot_rpart1}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t1 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(Y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{XY_train, }\DataTypeTok{method=}\StringTok{"class"}\NormalTok{)}
\KeywordTok{rpart.plot}\NormalTok{(t1, }\DataTypeTok{tweak=}\FloatTok{1.1}\NormalTok{, }\DataTypeTok{fallen.leaves=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{digits=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:plot_rpart1}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/plot_rpart1-1.pdf}
\caption{A decision tree for the \texttt{wines} dataset}\label{fig:plot_rpart1}
}
\end{figure}

We can build less or more complex trees by playing
with the \texttt{cp} parameter, see Figures \ref{fig:plot_rpart222}
and \ref{fig:tree333}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# cp = complexity parameter, smaller → more complex tree}
\NormalTok{t2 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(Y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{XY_train, }\DataTypeTok{method=}\StringTok{"class"}\NormalTok{, }\DataTypeTok{cp=}\FloatTok{0.1}\NormalTok{)}
\KeywordTok{rpart.plot}\NormalTok{(t2, }\DataTypeTok{tweak=}\FloatTok{1.1}\NormalTok{, }\DataTypeTok{fallen.leaves=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{digits=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:plot_rpart222}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/plot_rpart222-1.pdf}
\caption{A (simpler) decision tree for the \texttt{wines} dataset}\label{fig:plot_rpart222}
}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# cp = complexity parameter, smaller → more complex tree}
\NormalTok{t3 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(Y}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{XY_train, }\DataTypeTok{method=}\StringTok{"class"}\NormalTok{, }\DataTypeTok{cp=}\FloatTok{0.00001}\NormalTok{)}
\KeywordTok{rpart.plot}\NormalTok{(t3, }\DataTypeTok{tweak=}\FloatTok{1.1}\NormalTok{, }\DataTypeTok{fallen.leaves=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{digits=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:tree333}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/tree333-1.pdf}
\caption{A (more complex) decision tree for the \texttt{wines} dataset}\label{fig:tree333}
}
\end{figure}

Trees with few decision rules actually are very nicely interpretable.
On the other hand, plotting of the complex ones is just hopeless;
we should treat them as ``black boxes'' instead.

Let's make some predictions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(t1, XY_test, }\DataTypeTok{type=}\StringTok{"class"}\NormalTok{)}
\KeywordTok{get_metrics}\NormalTok{(Y_pred, Y_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Acc       Prec        Rec          F         TN         FN 
##    0.92857    0.80623    0.73502    0.76898 1587.00000   84.00000 
##         FP         TP 
##   56.00000  233.00000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(t2, XY_test, }\DataTypeTok{type=}\StringTok{"class"}\NormalTok{)}
\KeywordTok{get_metrics}\NormalTok{(Y_pred, Y_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Acc       Prec        Rec          F         TN         FN 
##    0.90255    0.83871    0.49211    0.62028 1613.00000  161.00000 
##         FP         TP 
##   30.00000  156.00000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(t3, XY_test, }\DataTypeTok{type=}\StringTok{"class"}\NormalTok{)}
\KeywordTok{get_metrics}\NormalTok{(Y_pred, Y_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Acc       Prec        Rec          F         TN         FN 
##    0.91837    0.73433    0.77603    0.75460 1554.00000   71.00000 
##         FP         TP 
##   89.00000  246.00000
\end{verbatim}

\begin{description}
\item[Remark.]
(*) Interestingly, \texttt{rpart()} also provides us with information
about the importance degrees of each independent variable.
\end{description}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t1}\OperatorTok{$}\NormalTok{variable.importance}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(t1}\OperatorTok{$}\NormalTok{variable.importance)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              density       residual.sugar        fixed.acidity 
##            0.6562490            0.1984221            0.0305167 
##            chlorides                   pH     volatile.acidity 
##            0.0215008            0.0209678            0.0192880 
##            sulphates total.sulfur.dioxide          citric.acid 
##            0.0184293            0.0140482            0.0119201 
##  free.sulfur.dioxide 
##            0.0086579
\end{verbatim}

\hypertarget{a-note-on-decision-tree-learning}{%
\subsection{A Note on Decision Tree Learning}\label{a-note-on-decision-tree-learning}}

Learning an optimal decision tree is a computationally hard problem
-- we need some heuristics.

Examples:

\begin{itemize}
\tightlist
\item
  ID3 (Iterative Dichotomiser 3) (Quinlan \protect\hyperlink{ref-id3}{1986})
\item
  C4.5 algorithm (Quinlan \protect\hyperlink{ref-c45}{1993})
\item
  CART by Leo Breiman et al., (Breiman et al. \protect\hyperlink{ref-cart}{1984})
\end{itemize}

(**) Decision trees are most often constructed by a \emph{greedy}, \emph{top-down}
\emph{recursive partitioning}, see., e.g., (Therneau \& Atkinson \protect\hyperlink{ref-rpart}{2019}).

\hypertarget{binary-logistic-regression}{%
\section{Binary Logistic Regression}\label{binary-logistic-regression}}

\hypertarget{motivation}{%
\subsection{Motivation}\label{motivation}}

Recall that for a regression task, we fitted a very simple family of models
-- the linear ones -- by minimising the sum of squared residuals.

This approach was pretty effective.

(Very) theoretically, we could treat the class labels as numeric \(0\)s and \(1\)s
and apply regression models in a binary classification task.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{XY_train_r <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(X_train),}
    \DataTypeTok{Y=}\KeywordTok{as.numeric}\NormalTok{(Y_train)}\OperatorTok{-}\DecValTok{1} \CommentTok{# 0.0 or 1.0}
\NormalTok{)}
\NormalTok{XY_test_r <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(X_test),}
    \DataTypeTok{Y=}\KeywordTok{as.numeric}\NormalTok{(Y_test)}\OperatorTok{-}\DecValTok{1} \CommentTok{# 0.0 or 1.0}
\NormalTok{)}
\NormalTok{f_r <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{density}\OperatorTok{+}\NormalTok{residual.sugar}\OperatorTok{+}\NormalTok{pH, }\DataTypeTok{data=}\NormalTok{XY_train_r)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred_r <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(f_r, XY_test_r)}
\KeywordTok{summary}\NormalTok{(Y_pred_r)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -3.0468 -0.0211  0.1192  0.1645  0.3491  0.8892
\end{verbatim}

The predicted outputs, \(\hat{Y}\), are arbitrary real numbers,
but we can convert them to binary ones by checking if, e.g., \(\hat{Y}>0.5\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(Y_pred_r}\OperatorTok{>}\FloatTok{0.5}\NormalTok{)}
\KeywordTok{round}\NormalTok{(}\KeywordTok{get_metrics}\NormalTok{(Y_pred, XY_test_r}\OperatorTok{$}\NormalTok{Y), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##    0.927    0.865    0.647    0.740 1611.000  112.000   32.000  205.000
\end{verbatim}

\begin{description}
\item[Remark.]
(*) The threshold \(T=0.5\) could even be treated as a free parameter
we optimise for (w.r.t. different metrics over the validation sample),
see Figure \ref{fig:lm3b}.
\end{description}

\begin{figure}
\hypertarget{fig:lm3b}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/lm3b-1.pdf}
\caption{Quality metrics for a binary classifier ``Classify X as 1 if \(f(X)>T\) and as 0 if \(f(X)\le T\)''}\label{fig:lm3b}
}
\end{figure}

Despite we can, we shouldn't use linear regression for classification.
Treating class labels ``0'' and ``1'' as ordinary real numbers just doesn't
cut it -- we intuitively feel that we are doing something \emph{ugly}.
Luckily, there is a better, more meaningful approach that
still relies on a linear model, but has the \emph{right} semantics.

\hypertarget{logistic-model}{%
\subsection{Logistic Model}\label{logistic-model}}

Inspired by this idea, we could try modelling
the \textbf{\emph{probability} that a given point belongs to class \(1\)}.

This could also provide us with the \emph{confidence} in our prediction.

Probability is a number in \([0,1]\), but the outputs of a linear model are arbitrary real numbers.

However, we could transform those real-valued outputs by means
of some function \(\phi:\mathbb{R}\to[0,1]\) (preferably S-shaped == sigmoid),
so as to get:

\[
\Pr(Y=1|\mathbf{X},\boldsymbol\beta)=\phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p).
\]

\begin{description}
\item[Remark.]
The above reads as ``Probability that \(Y\) is from class 1 given \(\mathbf{X}\)
and \(\boldsymbol\beta\)''.
\end{description}

A popular choice is the \textbf{logistic sigmoid function},
see Figure \ref{fig:sigmoid}:

\[
\phi(t) = \frac{1}{1+e^{-t}} = \frac{e^t}{1+e^t}.
\]

\begin{figure}
\hypertarget{fig:sigmoid}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/sigmoid-1.pdf}
\caption{The logistic sigmoid function, \(\varphi\)}\label{fig:sigmoid}
}
\end{figure}

Hence our model becomes:

\[
Y=\frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)}}
\]

It is an instance of a \textbf{generalised linear model} (glm)
(there are of course many other possible generalisations).

\hypertarget{example-in-r-2}{%
\subsection{Example in R}\label{example-in-r-2}}

Let us first fit a simple (i.e., \(p=1\)) logistic regression model
using the \texttt{density} variable. The goodness-of-fit measure used in this
problem will be discussed a bit later.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(f <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{density, }\DataTypeTok{data=}\NormalTok{XY_train, }\DataTypeTok{family=}\KeywordTok{binomial}\NormalTok{(}\StringTok{"logit"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:  glm(formula = Y ~ density, family = binomial("logit"), data = XY_train)
## 
## Coefficients:
## (Intercept)      density  
##        1173        -1184  
## 
## Degrees of Freedom: 2937 Total (i.e. Null);  2936 Residual
## Null Deviance:       2670 
## Residual Deviance: 1420  AIC: 1420
\end{verbatim}

``logit'' above denotes the inverse of the logistic sigmoid function.
The fitted coefficients are equal to:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)     density 
##      1173.2     -1184.2
\end{verbatim}

Figure \ref{fig:glm2} depicts the obtained model, which can be written
as:
\[
\Pr(Y=1|x)=\displaystyle\frac{1}{1+e^{-\left(
1173.21-1184.21x
\right)}
}
\]
with \(x=\text{density}\).

\begin{figure}
\hypertarget{fig:glm2}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/glm2-1.pdf}
\caption{The probability that a given wine is a high-alcohol one given its density; black and red points denote the actual observed data points from the class 0 and 1, respectively}\label{fig:glm2}
}
\end{figure}

Some predicted probabilities:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\KeywordTok{head}\NormalTok{(}\KeywordTok{predict}\NormalTok{(f, XY_test, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{), }\DecValTok{12}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 1602 1605 1607 1608 1609 1613 1614 1615 1621 1622 1623 1627 
## 0.01 0.01 0.00 0.02 0.03 0.36 0.00 0.31 0.36 0.06 0.03 0.00
\end{verbatim}

We classify \(Y\) as 1 if the corresponding membership probability
is greater than \(0.5\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{predict}\NormalTok{(f, XY_test, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}\OperatorTok{>}\FloatTok{0.5}\NormalTok{)}
\KeywordTok{get_metrics}\NormalTok{(Y_pred, Y_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Acc       Prec        Rec          F         TN         FN 
##    0.89796    0.72763    0.58991    0.65157 1573.00000  130.00000 
##         FP         TP 
##   70.00000  187.00000
\end{verbatim}

And now a fit based on some other input variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(f <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Y}\OperatorTok{~}\NormalTok{density}\OperatorTok{+}\NormalTok{residual.sugar}\OperatorTok{+}\NormalTok{total.sulfur.dioxide,}
    \DataTypeTok{data=}\NormalTok{XY_train, }\DataTypeTok{family=}\KeywordTok{binomial}\NormalTok{(}\StringTok{"logit"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:  glm(formula = Y ~ density + residual.sugar + total.sulfur.dioxide, 
##     family = binomial("logit"), data = XY_train)
## 
## Coefficients:
##          (Intercept)               density        residual.sugar  
##             2.50e+03             -2.53e+03              8.58e-01  
## total.sulfur.dioxide  
##             9.74e-03  
## 
## Degrees of Freedom: 2937 Total (i.e. Null);  2934 Residual
## Null Deviance:       2670 
## Residual Deviance: 920   AIC: 928
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{predict}\NormalTok{(f, XY_test, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}\OperatorTok{>}\FloatTok{0.5}\NormalTok{)}
\KeywordTok{get_metrics}\NormalTok{(Y_pred, Y_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Acc       Prec        Rec          F         TN         FN 
##    0.93214    0.82394    0.73817    0.77870 1593.00000   83.00000 
##         FP         TP 
##   50.00000  234.00000
\end{verbatim}

\begin{exercise}

Try fitting different models based on other sets of features.

\end{exercise}

\hypertarget{loss-function-cross-entropy}{%
\subsection{Loss Function: Cross-entropy}\label{loss-function-cross-entropy}}

The fitting of the model can be written as an optimisation task:

\[
\min_{\beta_0, \beta_1,\dots, \beta_p\in\mathbb{R}}
\frac{1}{n} \sum_{i=1}^n
\epsilon\left(\hat{y}_i, y_i \right)
\]

where \(\epsilon(\hat{y}_i, y_i)\) denotes the penalty that measures the
``difference'' between the true \(y_i\) and its predicted version
\(\hat{y}_i=\Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta)\).

In the ordinary regression, we used the squared residual
\(\epsilon(\hat{y}_i, y_i) = (\hat{y}_i-y_i)^2\).
In \textbf{logistic regression} (the kind of a classifier we are
interested in right now), we use
the \textbf{cross-entropy} (a.k.a. \textbf{log-loss}, binary cross-entropy),

\[
\epsilon(\hat{y}_i,y_i) = - \left(y_i \log \hat{y}_i + (1-y_i) \log(1-\hat{y}_i)\right)
\]

The corresponding loss function has not only
many nice statistical properties (** related to maximum likelihood
estimation etc.)
but also an intuitive interpretation.

Note that the predicted \(\hat{y}_i\) is in \((0,1)\) and the true \(y_i\)
equals to either 0 or 1.
Recall also that \(\log t\in(-\infty, 0)\) for \(t\in (0,1)\).
Therefore, the formula for \(\epsilon(\hat{y}_i,y_i)\)
has a very intuitive behaviour:

\begin{itemize}
\item
  if true \(y_i=1\), then the penalty becomes \(\epsilon(\hat{y}_i, 1) = -\log(\hat{y}_i)\)

  \begin{itemize}
  \tightlist
  \item
    \(\hat{y}_i\) is the probability that the classified input is indeed from class \(1\)
  \item
    we'd be happy if the classifier outputted \(\hat{y}_i\simeq 1\) in this case;
    this is not penalised as \(-\log(t)\to 0\) as \(t\to 1\)
  \item
    however, if the classifier is totally wrong, i.e., it thinks that
    \(\hat{y}_i\simeq 0\), then the penalty will be very high, as \(-\log(t)\to+\infty\)
    as \(t\to 0\)
  \end{itemize}
\item
  if true \(y_i=0\), then the penalty becomes \(\epsilon(\hat{y}_i, 0) = -\log(1-\hat{y}_i)\)

  \begin{itemize}
  \tightlist
  \item
    \(1-\hat{y}_i\) is the predicted probability that the input is from class \(0\)
  \item
    we penalise heavily the case where \(1-\hat{y}_i\) is small (we'd be happy
    if the classifier was sure that \(1-\hat{y}_i\simeq 1\), because this is the ground-truth)
  \end{itemize}
\end{itemize}

\bigskip

(*) Having said that, let's expand the above formulae.
The task of minimising cross-entropy in the binary logistic regression
can be written as \(\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}} E(\boldsymbol\beta)\)
with:

\[
E(\boldsymbol\beta)=
-\frac{1}{n} \sum_{i=1}^n
y_i \log \Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta)
+ (1-y_i) \log(1-\Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta))
\]

Taking into account that:

\[
\Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta)=
\frac{1}{1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}},
\]

we get:

\[
E(\boldsymbol\beta)=
-\frac{1}{n}
\sum_{i=1}^n \left(
\begin{array}{r}
y_i \log \displaystyle\frac{1}{1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}}\\
+
(1-y_i) \log \displaystyle\frac{e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}}{1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}}
\end{array}
\right).
\]

Logarithms are really practitioner-friendly functions,
it holds:

\begin{itemize}
\tightlist
\item
  \(\log 1=0\),
\item
  \(\log e=1\) (where \(e \simeq 2.71828\) is the Euler constant;
  note that by writing \(\log\) we mean the natural a.k.a. base-\(e\) logarithm),
\item
  \(\log xy = \log x + \log y\),
\item
  \(\log x^p = p\log x\) (this is \(\log (x^p)\), not \((\log x)^p\)).
\end{itemize}

These facts imply, amongst others that:

\begin{itemize}
\tightlist
\item
  \(\log e^x = x \log e = x\),
\item
  \(\log \frac{x}{y} = \log x y^{-1} = \log x+\log y^{-1} = \log x - \log y\)
  (of course for \(y\neq 0\)),
\item
  \(\log \frac{1}{y} = -\log y\)
\end{itemize}

and so forth. Therefore,
based on the fact that
\(1/(1+e^{-x})=e^x/(1+e^x)\),
the above optimisation problem can be rewritten as:

\[
E(\boldsymbol\beta)=
\frac{1}{n}
\sum_{i=1}^n \left(
\begin{array}{r}
y_i \log \left(1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}\right)\\
+
(1-y_i) \log \left(1+e^{+(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}\right)
\end{array}
\right)
\]

or, if someone prefers:

\[
E(\boldsymbol\beta)=
\frac{1}{n}
\sum_{i=1}^n \left(
(1-y_i)\left(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}\right)
+\log \left(1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}\right)
\right).
\]

It turns out that there is no analytical formula
for the optimal set of parameters (\(\beta_0,\beta_1,\dots,\beta_p\)
minimising the log-loss).
In the chapter on optimisation, we shall see that
the solution to the logistic regression can be solved numerically
by means of quite simple iterative algorithms.
The two expanded formulae have lost the appealing interpretation
of the original one, however, it's more numerically well-behaving,
see, e.g., the \texttt{log1p()} function in base R or, even better,
\texttt{fermi\_dirac\_0()} in the \texttt{gsl} package.

\hypertarget{exercises-in-r-2}{%
\section{Exercises in R}\label{exercises-in-r-2}}

\hypertarget{edstats-preparing-data}{%
\subsection{EdStats -- Preparing Data}\label{edstats-preparing-data}}

In this exercise, we will prepare the EdStats dataset
for further analysis.
The file \texttt{edstats\_2019.csv} provides us with many
country-level Education Statistics extracted from the World Bank's Databank,
see \url{https://databank.worldbank.org/}.
Databank aggregates information from such sources as
the UNESCO Institute for Statistics,
OECD Programme for International Student Assessment (PISA)
etc. The official description reads:

\begin{quote}
``The World Bank EdStats Query holds around 2,500 internationally comparable
education indicators for access, progression, completion, literacy, teachers,
population, and expenditures. The indicators cover the education cycle from
pre-primary to tertiary education. The query also holds learning outcome data
from international learning assessments (PISA, TIMSS, etc.), equity data
from household surveys, and projection data to 2050.''
\end{quote}

\texttt{edstats\_2019.csv} was compiled on 24 April 2020 and lists
indicators reported between 2010 and 2019.
First, let's load the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edstats_}\DecValTok{2019}\NormalTok{ <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/edstats_2019.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{head}\NormalTok{(edstats_}\DecValTok{2019}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   CountryName CountryCode
## 1 Afghanistan         AFG
## 2 Afghanistan         AFG
## 3 Afghanistan         AFG
## 4 Afghanistan         AFG
## 5 Afghanistan         AFG
## 6 Afghanistan         AFG
##                                                    Series
## 1     Government expenditure on education as % of GDP (%)
## 2              Gross enrolment ratio, primary, female (%)
## 3                 Net enrolment rate, primary, female (%)
## 4                 Primary completion rate, both sexes (%)
## 5         PISA: Mean performance on the mathematics scale
## 6 PISA: Mean performance on the mathematics scale. Female
##                Code   Y2010  Y2011   Y2012   Y2013   Y2014   Y2015
## 1 SE.XPD.TOTL.GD.ZS  3.4794  3.462  2.6042  3.4545  3.6952  3.2558
## 2    SE.PRM.ENRR.FE 80.6355 80.937 86.3288 85.9021 86.7296 83.5044
## 3    SE.PRM.NENR.FE      NA     NA      NA      NA      NA      NA
## 4    SE.PRM.CMPT.ZS      NA     NA      NA      NA      NA      NA
## 5       LO.PISA.MAT      NA     NA      NA      NA      NA      NA
## 6    LO.PISA.MAT.FE      NA     NA      NA      NA      NA      NA
##     Y2016   Y2017  Y2018 Y2019
## 1  4.2284  4.0589     NA    NA
## 2 82.5584 82.0803 82.850    NA
## 3      NA      NA     NA    NA
## 4 79.9346 84.4150 85.625    NA
## 5      NA      NA     NA    NA
## 6      NA      NA     NA    NA
\end{verbatim}

This data frame is in a ``long'' format, where each indicator
for each country is given in its own row. Note that some indicators
are not surveyed/updated every year.

\begin{exercise}

Convert \texttt{edstats\_2019} to a ``wide'' format (one row per country,
each indicator in its own column) based on the most recent observed indicators.

\end{exercise}

\begin{solution}

First we need a function that returns the last non-missing value in a
given numeric vector.
To recall, \texttt{na.omit()}, removes all missing values and \texttt{tail()} can be
used to access the last observation easily. Unfortunately, if the vector
is consists of missing values only, the removal of \texttt{NA}s leads
to an empty sequence. However, the trick we can use is that
by extracting the first element from an empty vector by using
\texttt{{[}...{]}}, we get a \texttt{NA}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{last_non_na <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{tail}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x), }\DecValTok{1}\NormalTok{)[}\DecValTok{1}\NormalTok{]}
\KeywordTok{last_non_na}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{3}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\OtherTok{NA}\NormalTok{)) }\CommentTok{# example 1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{last_non_na}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\OtherTok{NA}\NormalTok{)) }\CommentTok{# example 2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

Let's extract the most recent indicator from each row in \texttt{edstats\_2019}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{values <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(edstats_}\DecValTok{2019}\NormalTok{[}\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{)], }\DecValTok{1}\NormalTok{, last_non_na)}
\KeywordTok{head}\NormalTok{(values)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  4.0589 82.8503      NA 85.6253      NA      NA
\end{verbatim}

Now, we shall create a data frame with 3 columns:
name of the country, indicator code, indicator value.
Let's order it with respect to the first two columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edstats_}\DecValTok{2019}\NormalTok{ <-}\StringTok{ }\NormalTok{edstats_}\DecValTok{2019}\NormalTok{[}\KeywordTok{c}\NormalTok{(}\StringTok{"CountryName"}\NormalTok{, }\StringTok{"Code"}\NormalTok{)]}
\CommentTok{# add a new column at the righthand end:}
\NormalTok{edstats_}\DecValTok{2019}\NormalTok{[}\StringTok{"Value"}\NormalTok{] <-}\StringTok{ }\NormalTok{values}
\NormalTok{edstats_}\DecValTok{2019}\NormalTok{ <-}\StringTok{ }\NormalTok{edstats_}\DecValTok{2019}\NormalTok{[}
    \KeywordTok{order}\NormalTok{(edstats_}\DecValTok{2019}\OperatorTok{$}\NormalTok{CountryName, edstats_}\DecValTok{2019}\OperatorTok{$}\NormalTok{Code), ]}
\KeywordTok{head}\NormalTok{(edstats_}\DecValTok{2019}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    CountryName           Code  Value
## 59 Afghanistan    HD.HCI.AMRT 0.7797
## 57 Afghanistan HD.HCI.AMRT.FE 0.8018
## 58 Afghanistan HD.HCI.AMRT.MA 0.7597
## 53 Afghanistan    HD.HCI.EYRS 8.5800
## 51 Afghanistan HD.HCI.EYRS.FE 6.7300
## 52 Afghanistan HD.HCI.EYRS.MA 9.2100
\end{verbatim}

To convert the data frame to a ``wide'' format, many readers would choose
the \texttt{pivot\_wider()} function from the \texttt{tidyr} package
(amongst others).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"tidyr"}\NormalTok{)}
\NormalTok{edstats <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}
    \KeywordTok{pivot_wider}\NormalTok{(edstats_}\DecValTok{2019}\NormalTok{, }\DataTypeTok{names_from=}\StringTok{"Code"}\NormalTok{, }\DataTypeTok{values_from=}\StringTok{"Value"}\NormalTok{),}
    \DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}
\NormalTok{)}
\NormalTok{edstats[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   CountryName HD.HCI.AMRT HD.HCI.AMRT.FE HD.HCI.AMRT.MA HD.HCI.EYRS
## 1 Afghanistan      0.7797         0.8018         0.7597        8.58
##   HD.HCI.EYRS.FE HD.HCI.EYRS.MA
## 1           6.73           9.21
\end{verbatim}

\bigskip

On a side note (*), the above solution is of course perfectly fine
and we can now live long and prosper.
Nevertheless, we are here to learn new skills, so let's note
that it has the drawback that it required
us to search for the answer on the internet (and go through many ``answers''
that actually don't work). If we are not converting between the long and the
wide formats on a daily basis, this might not be worth the hassle
(moreover, there's no guarantee that this function will work the same
way in the future, that the package we relied on will provide the same API etc.).

Instead, by relaying on a bit deeper knowledge of R programming
(which we already have, see Appendices A-D of our book),
we could implement the relevant procedure manually. The downside is that
this requires us to get out of our comfort zone and\ldots{} think.

First, let's generate the list of all countries and indicators:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{countries  <-}\StringTok{ }\KeywordTok{unique}\NormalTok{(edstats_}\DecValTok{2019}\OperatorTok{$}\NormalTok{CountryName)}
\KeywordTok{head}\NormalTok{(countries)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Afghanistan"    "Albania"        "Algeria"        "American Samoa"
## [5] "Andorra"        "Angola"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{indicators <-}\StringTok{ }\KeywordTok{unique}\NormalTok{(edstats_}\DecValTok{2019}\OperatorTok{$}\NormalTok{Code)}
\KeywordTok{head}\NormalTok{(indicators)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "HD.HCI.AMRT"    "HD.HCI.AMRT.FE" "HD.HCI.AMRT.MA" "HD.HCI.EYRS"   
## [5] "HD.HCI.EYRS.FE" "HD.HCI.EYRS.MA"
\end{verbatim}

Second, note that \texttt{edstats\_2019} gives all the possible combinations (pairs)
of the indexes and countries:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nrow}\NormalTok{(edstats_}\DecValTok{2019}\NormalTok{) }\CommentTok{# number of rows in edstats_2019}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 23852
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(countries)}\OperatorTok{*}\KeywordTok{length}\NormalTok{(indicators) }\CommentTok{# number of pairs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 23852
\end{verbatim}

Looking at the numbers in the \texttt{Value} column of \texttt{edstats\_2019},
this will exactly provide us with our desired ``wide'' data matrix,
if we read it in a rowwise manner. Hence, we can use
\texttt{matrix(...,\ byrow=TRUE)} to generate it:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# edstats_2019 is already sorted w.r.t. CountryName and Code}
\NormalTok{edstats2 <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}
    \DataTypeTok{CountryName=}\NormalTok{countries, }\CommentTok{# first column}
    \KeywordTok{as.data.frame}\NormalTok{(}
        \KeywordTok{matrix}\NormalTok{(edstats_}\DecValTok{2019}\OperatorTok{$}\NormalTok{Value,}
            \DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{,}
            \DataTypeTok{ncol=}\KeywordTok{length}\NormalTok{(indicators),}
            \DataTypeTok{dimnames=}\KeywordTok{list}\NormalTok{(}\OtherTok{NULL}\NormalTok{, indicators)}
\NormalTok{    )), }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{identical}\NormalTok{(edstats, edstats2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\end{solution}

\begin{exercise}

Export \texttt{edstats} to a CSV file.

\end{exercise}

\begin{solution}

This can be done as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.csv}\NormalTok{(edstats, }\StringTok{"edstats_2019_wide.csv"}\NormalTok{, }\DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We didn't export the row names, because they're useless in our case.

\end{solution}

\begin{exercise}

Explore \texttt{edstats\_meta.csv} to understand the meaning of the
EdStats indicators.

\end{exercise}

\begin{solution}

First, let's load the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meta <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/edstats_meta.csv"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{names}\NormalTok{(meta) }\CommentTok{# column names}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Code"       "Series"     "Definition" "Source"     "Topic"
\end{verbatim}

The \texttt{Series} column deciphers each indicator's meaning.
For instance, \texttt{LO.PISA.MAT} gives:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meta[meta}\OperatorTok{$}\NormalTok{Code}\OperatorTok{==}\StringTok{"LO.PISA.MAT"}\NormalTok{, }\StringTok{"Series"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "PISA: Mean performance on the mathematics scale"
\end{verbatim}

To get more information, we can take a look at the \texttt{Definition}
column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meta[meta}\OperatorTok{$}\NormalTok{Code}\OperatorTok{==}\StringTok{"LO.PISA.MAT"}\NormalTok{, }\StringTok{"Definition"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

which reads:
\emph{Average score of 15-year-old students on the PISA mathematics scale. The metric for the overall mathematics scale is based on a mean for OECD countries of 500 points and a standard deviation of 100 points. Data reflects country performance in the stated year according to PISA reports, but may not be comparable across years or countries. Consult the PISA website for more detailed information: \url{http://www.oecd.org/pisa/}}.

\end{solution}

\hypertarget{edstats-where-girls-are-better-at-maths-than-boys}{%
\subsection{EdStats -- Where Girls Are Better at Maths Than Boys?}\label{edstats-where-girls-are-better-at-maths-than-boys}}

In this task we will consider the ``wide'' version of the EdStats
dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edstats <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/edstats_2019_wide.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{edstats[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   CountryName HD.HCI.AMRT HD.HCI.AMRT.FE HD.HCI.AMRT.MA HD.HCI.EYRS
## 1 Afghanistan      0.7797         0.8018         0.7597        8.58
##   HD.HCI.EYRS.FE
## 1           6.73
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meta <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/edstats_meta.csv"}\NormalTok{,}
    \DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This dataset is small, moreover, we'll be more interested in the description
(understanding) of data, not prediction of the response variable
to unobserved samples. Note that we have the \emph{population} of the World
countries at hand here (new countries do not arise on a daily basis).
Therefore, a train-test split won't be performed.

\begin{exercise}

Add a 0/1 factor-type variable \texttt{girls\_rule\_maths} that is equal to 1
if and only if a country's average score of 15-year-old female students on the PISA
mathematics scale is greater than the corresponding indicator for the male ones.

\end{exercise}

\begin{solution}

Recall that a conversion of a logical value to a number
yields 1 for \texttt{TRUE} and \texttt{0} for \texttt{FALSE}. Hence:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edstats}\OperatorTok{$}\NormalTok{girls_rule_maths <-}
\StringTok{    }\KeywordTok{factor}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(}
\NormalTok{        edstats}\OperatorTok{$}\NormalTok{LO.PISA.MAT.FE}\OperatorTok{>}\NormalTok{edstats}\OperatorTok{$}\NormalTok{LO.PISA.MAT.MA}
\NormalTok{    ))}
\KeywordTok{head}\NormalTok{(edstats}\OperatorTok{$}\NormalTok{girls_rule_maths, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] <NA> 1    1    <NA> <NA> <NA> <NA> <NA> 0    <NA>
## Levels: 0 1
\end{verbatim}

Unfortunately, there are many missing values in the dataset.
More precisely:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(edstats}\OperatorTok{$}\NormalTok{girls_rule_maths)) }\CommentTok{# count}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 187
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(edstats}\OperatorTok{$}\NormalTok{girls_rule_maths)) }\CommentTok{# proportion}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.69776
\end{verbatim}

Countries such as Egypt, India, Iran or Venezuela
are not amongst the 79 members of the Programme for International
Student Assessment. Thus, we'll have to deal with the data we have.

The percentage of counties where ``girls rule'' is equal to:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(edstats}\OperatorTok{$}\NormalTok{girls_rule_maths}\OperatorTok{==}\DecValTok{1}\NormalTok{, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.33333
\end{verbatim}

Here is the list of those counties:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.character}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(}
\NormalTok{    edstats[edstats}\OperatorTok{$}\NormalTok{girls_rule_maths}\OperatorTok{==}\DecValTok{1}\NormalTok{, }\StringTok{"CountryName"}\NormalTok{]}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "Albania"              "Algeria"             
##  [3] "Brunei Darussalam"    "Bulgaria"            
##  [5] "Cyprus"               "Dominican Republic"  
##  [7] "Finland"              "Georgia"             
##  [9] "Hong Kong SAR, China" "Iceland"             
## [11] "Indonesia"            "Israel"              
## [13] "Jordan"               "Lithuania"           
## [15] "Malaysia"             "Malta"               
## [17] "Moldova"              "North Macedonia"     
## [19] "Norway"               "Philippines"         
## [21] "Qatar"                "Saudi Arabia"        
## [23] "Sweden"               "Thailand"            
## [25] "Trinidad and Tobago"  "United Arab Emirates"
## [27] "Vietnam"
\end{verbatim}

\end{solution}

\begin{exercise}

Learn a decision tree that distinguishes between the countries where
girls are better at maths than boys and assess the quality of this classifier.

\end{exercise}

\begin{solution}

Let's first create a subset of \texttt{edstats} that doesn't include
the country names as well as the boys' and girls' math scores.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edstats_subset <-}\StringTok{ }\NormalTok{edstats[}\OperatorTok{!}\NormalTok{(}\KeywordTok{names}\NormalTok{(edstats) }\OperatorTok{%in%}
\StringTok{    }\KeywordTok{c}\NormalTok{(}\StringTok{"CountryName"}\NormalTok{, }\StringTok{"LO.PISA.MAT.FE"}\NormalTok{, }\StringTok{"LO.PISA.MAT.MA"}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

Fitting and plotting (see Figure \ref{fig:girlsboys7tree})
of the tree can be performed as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"rpart"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"rpart.plot"}\NormalTok{)}
\NormalTok{tree <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(girls_rule_maths}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{edstats_subset,}
    \DataTypeTok{method=}\StringTok{"class"}\NormalTok{, }\DataTypeTok{model=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{rpart.plot}\NormalTok{(tree)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:girlsboys7tree}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/girlsboys7tree-1.pdf}
\caption{A decision tree explaining the \texttt{girls\_rule\_maths} variable}\label{fig:girlsboys7tree}
}
\end{figure}

The variables included in the model are:

\begin{itemize}
\tightlist
\item
  LO.PISA.REA.MA: \emph{PISA: Mean performance on the reading scale. Male}
\item
  LO.PISA.SCI: \emph{PISA: Mean performance on the science scale}
\end{itemize}

Note that the decision rules are well-interpretable, we can make a whole
story around it. Whether or not it is actually true -- is a different\ldots{} story.

To compute the basic classifier performance scores,
let's recall the \texttt{get\_metrics()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get_metrics <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(Y_pred, Y_test)}
\NormalTok{\{}
\NormalTok{    C <-}\StringTok{ }\KeywordTok{table}\NormalTok{(Y_pred, Y_test) }\CommentTok{# confusion matrix}
    \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{dim}\NormalTok{(C) }\OperatorTok{==}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
    \KeywordTok{c}\NormalTok{(}\DataTypeTok{Acc=}\NormalTok{(C[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{])}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(C), }\CommentTok{# accuracy}
      \DataTypeTok{Prec=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{(C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]), }\CommentTok{# precision}
      \DataTypeTok{Rec=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{(C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]), }\CommentTok{# recall}
      \DataTypeTok{F=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\NormalTok{(C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\FloatTok{0.5}\OperatorTok{*}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{+}\FloatTok{0.5}\OperatorTok{*}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]), }\CommentTok{# F-measure}
      \CommentTok{# Confusion matrix items:}
      \DataTypeTok{TN=}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{], }\DataTypeTok{FN=}\NormalTok{C[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{],}
      \DataTypeTok{FP=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{], }\DataTypeTok{TP=}\NormalTok{C[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{    ) }\CommentTok{# return a named vector}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we can judge the tree's character:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(tree, edstats_subset, }\DataTypeTok{type=}\StringTok{"class"}\NormalTok{)}
\KeywordTok{get_metrics}\NormalTok{(Y_pred, edstats_subset}\OperatorTok{$}\NormalTok{girls_rule_maths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.81481  1.00000  0.44444  0.61538 54.00000 15.00000  0.00000 12.00000
\end{verbatim}

\end{solution}

\begin{exercise}

Learn a decision tree that this time doesn't rely on any of the PISA indicators.

\end{exercise}

\begin{solution}

Let's remove the unwanted variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edstats_subset <-}\StringTok{ }\NormalTok{edstats[}\OperatorTok{!}\NormalTok{(}\KeywordTok{names}\NormalTok{(edstats) }\OperatorTok{%in%}
\StringTok{    }\KeywordTok{c}\NormalTok{(}\StringTok{"LO.PISA.MAT"}\NormalTok{, }\StringTok{"LO.PISA.MAT.FE"}\NormalTok{, }\StringTok{"LO.PISA.MAT.MA"}\NormalTok{,}
      \StringTok{"LO.PISA.REA"}\NormalTok{, }\StringTok{"LO.PISA.REA.FE"}\NormalTok{, }\StringTok{"LO.PISA.REA.MA"}\NormalTok{,}
      \StringTok{"LO.PISA.SCI"}\NormalTok{, }\StringTok{"LO.PISA.SCI.FE"}\NormalTok{, }\StringTok{"LO.PISA.SCI.MA"}\NormalTok{,}
      \StringTok{"CountryName"}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

On a side note, this could be done more easily by calling, e.g.,
\texttt{stri\_startswith\_fixed(names(edstats),\ "LO.PISA")} from the \texttt{stringi} package.

Fitting and plotting (see Figure \ref{fig:girlsboys11tree}) of the tree:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(girls_rule_maths}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{edstats_subset,}
    \DataTypeTok{method=}\StringTok{"class"}\NormalTok{, }\DataTypeTok{model=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{rpart.plot}\NormalTok{(tree)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:girlsboys11tree}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/girlsboys11tree-1.pdf}
\caption{Another decision tree explaining the \texttt{girls\_rule\_maths} variable}\label{fig:girlsboys11tree}
}
\end{figure}

Performance metrics:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(tree, edstats, }\DataTypeTok{type=}\StringTok{"class"}\NormalTok{)}
\KeywordTok{get_metrics}\NormalTok{(Y_pred, edstats_subset}\OperatorTok{$}\NormalTok{girls_rule_maths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.79012  0.69231  0.66667  0.67925 46.00000  9.00000  8.00000 18.00000
\end{verbatim}

It's interesting to note that some of the
goodness-of-fit measures are actually higher now.

The variables included in the model are:

\begin{itemize}
\tightlist
\item
  HD.HCI.AMRT.FE: \emph{Human Capital Index (HCI): Survival Rate from Age 15-60, Female}
\item
  SE.SEC.NENR.MA: \emph{Net enrolment rate, secondary, male (\%)}
\item
  SE.TER.ENRR.MA: \emph{Gross enrolment ratio, tertiary, male (\%)}
\end{itemize}

\end{solution}

\hypertarget{edstats-and-world-factbook-joining-forces}{%
\subsection{EdStats and World Factbook -- Joining Forces}\label{edstats-and-world-factbook-joining-forces}}

In the course of our data science journey, we have considered two datasets
dealing with country-level indicators: the World Factbook and
World Bank's EdStats.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factbook <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/world_factbook_2020.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{edstats <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/edstats_2019_wide.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's combine the information they provide
and see if we come up with a better model of where
girls' math scores are higher.

\begin{exercise}

Some country names in one dataset don't match those in the other
one, for instance: Czech Republic vs.~Czechia,
Myanmar vs.~Burma, etc. Resolve these conflicts as best you can.

\end{exercise}

\begin{solution}

To get a list of the mismatched country names, we can call either:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factbook}\OperatorTok{$}\NormalTok{country[}\OperatorTok{!}\NormalTok{(factbook}\OperatorTok{$}\NormalTok{country }\OperatorTok{%in%}\StringTok{ }\NormalTok{edstats}\OperatorTok{$}\NormalTok{CountryName)]}
\end{Highlighting}
\end{Shaded}

or:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edstats}\OperatorTok{$}\NormalTok{CountryName[}\OperatorTok{!}\NormalTok{(edstats}\OperatorTok{$}\NormalTok{CountryName }\OperatorTok{%in%}\StringTok{ }\NormalTok{factbook}\OperatorTok{$}\NormalTok{country)]}
\end{Highlighting}
\end{Shaded}

Unfortunately, the data need to be cleaned manually -- it's a tedious task.
The following consists of what we hope are the best matches
between the two datasets (yet, the list is not perfect;
in particular, the Republic of North Macedonia
is completely missing in one of the datasets):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{from_to <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{, }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{, }\KeywordTok{c}\NormalTok{(}
\CommentTok{# FROM (edstats)                  # TO (factbook)}
\StringTok{"Brunei Darussalam"}\NormalTok{             , }\StringTok{"Brunei"}\NormalTok{                            ,}
\StringTok{"Congo, Dem. Rep."}\NormalTok{              , }\StringTok{"Congo, Democratic Republic of the"}\NormalTok{ ,}
\StringTok{"Congo, Rep."}\NormalTok{                   , }\StringTok{"Congo, Republic of the"}\NormalTok{            ,}
\StringTok{"Czech Republic"}\NormalTok{                , }\StringTok{"Czechia"}\NormalTok{                           ,}
\StringTok{"Egypt, Arab Rep."}\NormalTok{              , }\StringTok{"Egypt"}\NormalTok{                             ,}
\StringTok{"Hong Kong SAR, China"}\NormalTok{          , }\StringTok{"Hong Kong"}\NormalTok{                         ,}
\StringTok{"Iran, Islamic Rep."}\NormalTok{            , }\StringTok{"Iran"}\NormalTok{                              ,}
\StringTok{"Korea, Dem. People’s Rep."}\NormalTok{     , }\StringTok{"Korea, North"}\NormalTok{                      ,}
\StringTok{"Korea, Rep."}\NormalTok{                   , }\StringTok{"Korea, South"}\NormalTok{                      ,}
\StringTok{"Kyrgyz Republic"}\NormalTok{               , }\StringTok{"Kyrgyzstan"}\NormalTok{                        ,}
\StringTok{"Lao PDR"}\NormalTok{                       , }\StringTok{"Laos"}\NormalTok{                              ,}
\StringTok{"Macao SAR, China"}\NormalTok{              , }\StringTok{"Macau"}\NormalTok{                             ,}
\StringTok{"Micronesia, Fed. Sts."}\NormalTok{         , }\StringTok{"Micronesia, Federated States of"}\NormalTok{   ,}
\StringTok{"Myanmar"}\NormalTok{                       , }\StringTok{"Burma"}\NormalTok{                             ,}
\StringTok{"Russian Federation"}\NormalTok{            , }\StringTok{"Russia"}\NormalTok{                            ,}
\StringTok{"Slovak Republic"}\NormalTok{               , }\StringTok{"Slovakia"}\NormalTok{                          ,}
\StringTok{"St. Kitts and Nevis"}\NormalTok{           , }\StringTok{"Saint Kitts and Nevis"}\NormalTok{             ,}
\StringTok{"St. Lucia"}\NormalTok{                     , }\StringTok{"Saint Lucia"}\NormalTok{                       ,}
\StringTok{"St. Martin (French part)"}\NormalTok{      , }\StringTok{"Saint Martin"}\NormalTok{                      ,}
\StringTok{"St. Vincent and the Grenadines"}\NormalTok{, }\StringTok{"Saint Vincent and the Grenadines"}\NormalTok{  ,}
\StringTok{"Syrian Arab Republic"}\NormalTok{          , }\StringTok{"Syria"}\NormalTok{                             ,}
\StringTok{"Venezuela, RB"}\NormalTok{                 , }\StringTok{"Venezuela"}\NormalTok{                         ,}
\StringTok{"Virgin Islands (U.S.)"}\NormalTok{         , }\StringTok{"Virgin Islands"}\NormalTok{                    ,}
\StringTok{"Yemen, Rep."}\NormalTok{                   , }\StringTok{"Yemen"}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Conversion of the names:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(from_to)) \{}
\NormalTok{    edstats}\OperatorTok{$}\NormalTok{CountryName[edstats}\OperatorTok{$}\NormalTok{CountryName}\OperatorTok{==}\NormalTok{from_to[i,}\DecValTok{1}\NormalTok{]] <-}\StringTok{ }\NormalTok{from_to[i,}\DecValTok{2}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

On a side note (*), this could be done with a single call to
a function in the \texttt{stringi} package:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"stringi"}\NormalTok{)}
\NormalTok{edstats}\OperatorTok{$}\NormalTok{CountryName <-}\StringTok{ }\KeywordTok{stri_replace_all_fixed}\NormalTok{(edstats}\OperatorTok{$}\NormalTok{CountryName,}
\NormalTok{    from_to[,}\DecValTok{1}\NormalTok{], from_to[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{vectorize_all=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{solution}

\begin{exercise}

Merge (join) the two datasets based on the country names.

\end{exercise}

\begin{solution}

This can be done by means of the \texttt{merge()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edbook <-}\StringTok{ }\KeywordTok{merge}\NormalTok{(edstats, factbook, }\DataTypeTok{by.x=}\StringTok{"CountryName"}\NormalTok{, }\DataTypeTok{by.y=}\StringTok{"country"}\NormalTok{)}
\KeywordTok{ncol}\NormalTok{(edbook) }\CommentTok{# how many columns we have now}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 157
\end{verbatim}

\end{solution}

\begin{exercise}

Learn a decision tree that distinguishes between the countries where
girls are better at maths than boys and assess the quality of this classifier.

\end{exercise}

\begin{solution}

We proceed as in one of the previous exercises:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edbook}\OperatorTok{$}\NormalTok{girls_rule_maths <-}
\StringTok{    }\KeywordTok{factor}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(}
\NormalTok{        edbook}\OperatorTok{$}\NormalTok{LO.PISA.MAT.FE}\OperatorTok{>}\NormalTok{edbook}\OperatorTok{$}\NormalTok{LO.PISA.MAT.MA}
\NormalTok{    ))}
\NormalTok{edbook_subset <-}\StringTok{ }\NormalTok{edbook[}\OperatorTok{!}\NormalTok{(}\KeywordTok{names}\NormalTok{(edbook) }\OperatorTok{%in%}
\StringTok{    }\KeywordTok{c}\NormalTok{(}\StringTok{"CountryName"}\NormalTok{, }\StringTok{"LO.PISA.MAT.FE"}\NormalTok{, }\StringTok{"LO.PISA.MAT.MA"}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

Fitting and plotting (see Figure \ref{fig:joinedstats9}):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"rpart"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"rpart.plot"}\NormalTok{)}
\NormalTok{tree <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(girls_rule_maths}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{edbook_subset,}
    \DataTypeTok{method=}\StringTok{"class"}\NormalTok{, }\DataTypeTok{model=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{rpart.plot}\NormalTok{(tree)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:joinedstats9}{%
\centering
\includegraphics{04-classification-trees_and_logistic-figures/joinedstats9-1.pdf}
\caption{Yet another decision tree explaining the \texttt{girls\_rule\_maths} variable}\label{fig:joinedstats9}
}
\end{figure}

Performance metrics:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(tree, edbook_subset, }\DataTypeTok{type=}\StringTok{"class"}\NormalTok{)}
\KeywordTok{get_metrics}\NormalTok{(Y_pred, edbook_subset}\OperatorTok{$}\NormalTok{girls_rule_maths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.82716  0.78261  0.66667  0.72000 49.00000  9.00000  5.00000 18.00000
\end{verbatim}

The variables included in the model are:

\begin{itemize}
\tightlist
\item
  education\_expenditures
\item
  LO.PISA.REA.MA: \emph{PISA: Mean performance on the reading scale. Male}
\item
  LO.PISA.SCI: \emph{PISA: Mean performance on the science scale}
\end{itemize}

This is\ldots{} not at all enlightening.
Rest assured that experts in education
or econometrics for whom we work in this (imaginary) project
would raise many questions at this very point. Merely applying
some computational procedure on a dataset doesn't cut it;
it's too early to ask for a paycheque.
Classifiers are just blind \emph{tools} in our gentle yet firm hands;
new questions are risen, new answers must be sought. Further
explorations are of course left as an exercise to the kind reader.

\end{solution}

\hypertarget{edstats-fitting-of-binary-logistic-regression-models}{%
\subsection{EdStats -- Fitting of Binary Logistic Regression Models}\label{edstats-fitting-of-binary-logistic-regression-models}}

In this task we're going to
consider the ``wide'' version of the EdStats
dataset again:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edstats <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/edstats_2019_wide.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's re-add the \texttt{girls\_rule\_maths} column
just as in the previous exercise.
Then, let's create a subset of \texttt{edstats} that doesn't include
the country names as well as the boys' and girls' math scores.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edstats}\OperatorTok{$}\NormalTok{girls_rule_maths <-}
\StringTok{    }\KeywordTok{factor}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(}
\NormalTok{        edstats}\OperatorTok{$}\NormalTok{LO.PISA.MAT.FE}\OperatorTok{>}\NormalTok{edstats}\OperatorTok{$}\NormalTok{LO.PISA.MAT.MA}
\NormalTok{    ))}
\NormalTok{edstats_subset <-}\StringTok{ }\NormalTok{edstats[}\OperatorTok{!}\NormalTok{(}\KeywordTok{names}\NormalTok{(edstats) }\OperatorTok{%in%}
\StringTok{    }\KeywordTok{c}\NormalTok{(}\StringTok{"CountryName"}\NormalTok{, }\StringTok{"LO.PISA.MAT.FE"}\NormalTok{, }\StringTok{"LO.PISA.MAT.MA"}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

\begin{exercise}

Fit and assess a logistic regression model for \texttt{girls\_rule\_maths} as a function
of \texttt{LO.PISA.REA.MA}+\texttt{LO.PISA.SCI}.

\end{exercise}

\begin{solution}

Fitting of the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(f1 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(girls_rule_maths}\OperatorTok{~}\NormalTok{LO.PISA.REA.MA}\OperatorTok{+}\NormalTok{LO.PISA.SCI,}
    \DataTypeTok{data=}\NormalTok{edstats_subset, }\DataTypeTok{family=}\KeywordTok{binomial}\NormalTok{(}\StringTok{"logit"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:  glm(formula = girls_rule_maths ~ LO.PISA.REA.MA + LO.PISA.SCI, 
##     family = binomial("logit"), data = edstats_subset)
## 
## Coefficients:
##    (Intercept)  LO.PISA.REA.MA     LO.PISA.SCI  
##         3.0927         -0.0882          0.0755  
## 
## Degrees of Freedom: 80 Total (i.e. Null);  78 Residual
##   (187 observations deleted due to missingness)
## Null Deviance:       103 
## Residual Deviance: 77.9  AIC: 83.9
\end{verbatim}

Performance metrics:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{predict}\NormalTok{(f1, edstats_subset, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}\OperatorTok{>}\FloatTok{0.5}\NormalTok{)}
\KeywordTok{get_metrics}\NormalTok{(Y_pred, edstats_subset}\OperatorTok{$}\NormalTok{girls_rule_maths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.79012  0.75000  0.55556  0.63830 49.00000 12.00000  5.00000 15.00000
\end{verbatim}

Relate the above numbers to those reported for the fitted decision trees.

Note that the fitted model is nicely interpretable:
the lower the boys' average result on the Reading Scale
or the higher the country's result on the Science Scale,
the higher the probability for \texttt{girls\_rule\_maths}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_X <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
    \DataTypeTok{LO.PISA.REA.MA=}\KeywordTok{c}\NormalTok{(}\DecValTok{475}\NormalTok{, }\DecValTok{450}\NormalTok{, }\DecValTok{475}\NormalTok{, }\DecValTok{500}\NormalTok{),}
    \DataTypeTok{LO.PISA.SCI=}   \KeywordTok{c}\NormalTok{(}\DecValTok{525}\NormalTok{, }\DecValTok{525}\NormalTok{, }\DecValTok{550}\NormalTok{, }\DecValTok{500}\NormalTok{)}
\NormalTok{)}
\KeywordTok{cbind}\NormalTok{(example_X,}
    \StringTok{`}\DataTypeTok{Pr(Y=1)}\StringTok{`}\NormalTok{=}\KeywordTok{predict}\NormalTok{(f1, example_X, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   LO.PISA.REA.MA LO.PISA.SCI  Pr(Y=1)
## 1            475         525 0.703342
## 2            450         525 0.955526
## 3            475         550 0.939986
## 4            500         500 0.038094
\end{verbatim}

\end{solution}

\begin{exercise}

(*) Fit and assess a logistic regression model for \texttt{girls\_rule\_maths}
featuring all \texttt{LO.PISA.REA*} and \texttt{LO.PISA.SCI*} as independent variables.

\end{exercise}

\begin{solution}

Model fitting:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(f2 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(girls_rule_maths}\OperatorTok{~}\NormalTok{LO.PISA.REA}\OperatorTok{+}\NormalTok{LO.PISA.REA.FE}\OperatorTok{+}\NormalTok{LO.PISA.REA.MA}\OperatorTok{+}
\StringTok{                            }\NormalTok{LO.PISA.SCI}\OperatorTok{+}\NormalTok{LO.PISA.SCI.FE}\OperatorTok{+}\NormalTok{LO.PISA.SCI.MA,}
    \DataTypeTok{data=}\NormalTok{edstats_subset, }\DataTypeTok{family=}\KeywordTok{binomial}\NormalTok{(}\StringTok{"logit"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
\end{verbatim}

\begin{verbatim}
## 
## Call:  glm(formula = girls_rule_maths ~ LO.PISA.REA + LO.PISA.REA.FE + 
##     LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + LO.PISA.SCI.MA, 
##     family = binomial("logit"), data = edstats_subset)
## 
## Coefficients:
##    (Intercept)     LO.PISA.REA  LO.PISA.REA.FE  LO.PISA.REA.MA  
##         -2.265           1.268          -0.544          -0.734  
##    LO.PISA.SCI  LO.PISA.SCI.FE  LO.PISA.SCI.MA  
##          1.269          -0.157          -1.112  
## 
## Degrees of Freedom: 80 Total (i.e. Null);  74 Residual
##   (187 observations deleted due to missingness)
## Null Deviance:       103 
## Residual Deviance: 33    AIC: 47
\end{verbatim}

The mysterious \texttt{fitted\ probabilities\ numerically\ 0\ or\ 1\ occurred} warning
denotes convergence problems of the underlying optimisation (fitting) procedure:
at least one of the model coefficients has had a fairly large order
of magnitude and hence the fitted probabilities has come very close to 0 or 1.
Recall that the probabilities are modelled by means of the logistic sigmoid
function applied on the output of a linear combination of the dependent variables.
Moreover, cross-entropy features a logarithm, and \(\log 0 = -\infty\).

This can be due to the fact that all the variables in the model are very
correlated with each other (multicollinearity; an ill-conditioned problem).
The obtained solution might be unstable -- there might be many local optima
and hence, different parameter vectors might be equally good.
Moreover, it is likely that a small change in one of the inputs might
lead to large change in the estimated model
(* normally, we would attack this problem by employing
some regularisation techniques).

Of course, the model's performance metrics can still be computed,
but then it's better if we treat it as a black box. Or, even better,
reduce the number of independent variables and come up with a simpler
model that serves its purpose better than this one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{predict}\NormalTok{(f2, edstats_subset, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}\OperatorTok{>}\FloatTok{0.5}\NormalTok{)}
\KeywordTok{get_metrics}\NormalTok{(Y_pred, edstats_subset}\OperatorTok{$}\NormalTok{girls_rule_maths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.86420  0.83333  0.74074  0.78431 50.00000  7.00000  4.00000 20.00000
\end{verbatim}

\end{solution}

\hypertarget{edstats-variable-selection-in-binary-logistic-regression}{%
\subsection{EdStats -- Variable Selection in Binary Logistic Regression (*)}\label{edstats-variable-selection-in-binary-logistic-regression}}

Back to our \texttt{girls\_rule\_maths} example, we still have so much to learn!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edstats <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/edstats_2019_wide.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{edstats}\OperatorTok{$}\NormalTok{girls_rule_maths <-}
\StringTok{    }\KeywordTok{factor}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(}
\NormalTok{        edstats}\OperatorTok{$}\NormalTok{LO.PISA.MAT.FE}\OperatorTok{>}\NormalTok{edstats}\OperatorTok{$}\NormalTok{LO.PISA.MAT.MA}
\NormalTok{    ))}
\NormalTok{edstats_subset <-}\StringTok{ }\NormalTok{edstats[}\OperatorTok{!}\NormalTok{(}\KeywordTok{names}\NormalTok{(edstats) }\OperatorTok{%in%}
\StringTok{    }\KeywordTok{c}\NormalTok{(}\StringTok{"CountryName"}\NormalTok{, }\StringTok{"LO.PISA.MAT.FE"}\NormalTok{, }\StringTok{"LO.PISA.MAT.MA"}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

\begin{exercise}

Construct a binary logistic regression model via forward selection
of variables.

\end{exercise}

\begin{solution}

Just as in the linear regression case, we can rely on the \texttt{step()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_empty <-}\StringTok{ }\NormalTok{girls_rule_maths}\OperatorTok{~}\DecValTok{1}
\NormalTok{(model_full <-}\StringTok{ }\KeywordTok{formula}\NormalTok{(}\KeywordTok{model.frame}\NormalTok{(girls_rule_maths}\OperatorTok{~}\NormalTok{.,}
    \DataTypeTok{data=}\NormalTok{edstats_subset)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## girls_rule_maths ~ HD.HCI.AMRT + HD.HCI.AMRT.FE + HD.HCI.AMRT.MA + 
##     HD.HCI.EYRS + HD.HCI.EYRS.FE + HD.HCI.EYRS.MA + HD.HCI.HLOS + 
##     HD.HCI.HLOS.FE + HD.HCI.HLOS.MA + HD.HCI.MORT + HD.HCI.MORT.FE + 
##     HD.HCI.MORT.MA + HD.HCI.OVRL + HD.HCI.OVRL.FE + HD.HCI.OVRL.MA + 
##     IT.CMP.PCMP.P2 + IT.NET.USER.P2 + LO.PISA.MAT + LO.PISA.REA + 
##     LO.PISA.REA.FE + LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + 
##     LO.PISA.SCI.MA + NY.GDP.MKTP.CD + NY.GDP.PCAP.CD + NY.GDP.PCAP.PP.CD + 
##     NY.GNP.PCAP.CD + NY.GNP.PCAP.PP.CD + SE.COM.DURS + SE.PRM.CMPT.FE.ZS + 
##     SE.PRM.CMPT.MA.ZS + SE.PRM.CMPT.ZS + SE.PRM.ENRL.TC.ZS + 
##     SE.PRM.ENRR + SE.PRM.ENRR.FE + SE.PRM.ENRR.MA + SE.PRM.NENR + 
##     SE.PRM.NENR.FE + SE.PRM.NENR.MA + SE.PRM.PRIV.ZS + SE.SEC.ENRL.TC.ZS + 
##     SE.SEC.ENRR + SE.SEC.ENRR.FE + SE.SEC.ENRR.MA + SE.SEC.NENR + 
##     SE.SEC.NENR.MA + SE.SEC.PRIV.ZS + SE.TER.ENRR + SE.TER.ENRR.FE + 
##     SE.TER.ENRR.MA + SE.TER.PRIV.ZS + SE.XPD.TOTL.GD.ZS + SL.TLF.ADVN.FE.ZS + 
##     SL.TLF.ADVN.MA.ZS + SL.TLF.ADVN.ZS + SP.POP.TOTL + SP.POP.TOTL.FE.IN + 
##     SP.POP.TOTL.MA.IN + SP.PRM.TOTL.FE.IN + SP.PRM.TOTL.IN + 
##     SP.PRM.TOTL.MA.IN + SP.SEC.TOTL.FE.IN + SP.SEC.TOTL.IN + 
##     SP.SEC.TOTL.MA.IN + UIS.PTRHC.56 + UIS.SAP.CE + UIS.SAP.CE.F + 
##     UIS.SAP.CE.M + UIS.X.PPP.1.FSGOV + UIS.X.PPP.2T3.FSGOV + 
##     UIS.X.PPP.5T8.FSGOV + UIS.X.US.1.FSGOV + UIS.X.US.2T3.FSGOV + 
##     UIS.X.US.5T8.FSGOV + UIS.XGDP.1.FSGOV + UIS.XGDP.23.FSGOV + 
##     UIS.XGDP.56.FSGOV + UIS.XUNIT.GDPCAP.1.FSGOV + UIS.XUNIT.GDPCAP.23.FSGOV + 
##     UIS.XUNIT.GDPCAP.5T8.FSGOV + UIS.XUNIT.PPP.1.FSGOV.FFNTR + 
##     UIS.XUNIT.PPP.2T3.FSGOV.FFNTR + UIS.XUNIT.PPP.5T8.FSGOV.FFNTR + 
##     UIS.XUNIT.US.1.FSGOV.FFNTR + UIS.XUNIT.US.23.FSGOV.FFNTR + 
##     UIS.XUNIT.US.5T8.FSGOV.FFNTR
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\KeywordTok{step}\NormalTok{(}\KeywordTok{glm}\NormalTok{(model_empty, }\DataTypeTok{data=}\NormalTok{edstats_subset, }\DataTypeTok{family=}\KeywordTok{binomial}\NormalTok{(}\StringTok{"logit"}\NormalTok{)),}
    \DataTypeTok{scope=}\NormalTok{model_full, }\DataTypeTok{direction=}\StringTok{"forward"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Start:  AIC=105.12
## girls_rule_maths ~ 1
\end{verbatim}

\begin{verbatim}
## Error in model.matrix.default(Terms, m, contrasts.arg = object$contrasts): variable 1 has no levels
\end{verbatim}

Melbourne, we have a problem!
Our dataset has too many missing values, and those cannot be present
in a logistic regression model (it's based on a linear combination of variables,
and sums/products involving \texttt{NA}s yield \texttt{NA}s\ldots{}).

Looking at the manual of \texttt{?step}, we see that the default
\texttt{NA} handling is via \texttt{na.omit()}, and that, when applied on a data frame,
results in the removal of all the \emph{rows}, where there is at least one \texttt{NA}.
Sadly, it's too invasive.

We should get rid of the data blanks manually.
First, definitely, we should remove all the rows where
\texttt{girls\_rule\_maths} is unknown:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edstats_subset <-}
\StringTok{    }\NormalTok{edstats_subset[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(edstats_subset}\OperatorTok{$}\NormalTok{girls_rule_maths),]}
\end{Highlighting}
\end{Shaded}

We are about to apply the forward selection process, whose purpose is
to choose variables for a model. Therefore, instead of removing any more
rows, we should remove the\ldots{} columns with missing data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{edstats_subset <-}
\StringTok{    }\NormalTok{edstats_subset[,}\KeywordTok{colSums}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(edstats_subset, is.na))}\OperatorTok{==}\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

(*) Alternatively, we could apply some techniques of missing data imputation;
this is beyond the scope of this book. For instance, \texttt{NA}s could be
replaced by the averages of their respective columns.

We are ready now to make use of \texttt{step()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_empty <-}\StringTok{ }\NormalTok{girls_rule_maths}\OperatorTok{~}\DecValTok{1}
\NormalTok{(model_full <-}\StringTok{ }\KeywordTok{formula}\NormalTok{(}\KeywordTok{model.frame}\NormalTok{(girls_rule_maths}\OperatorTok{~}\NormalTok{.,}
    \DataTypeTok{data=}\NormalTok{edstats_subset)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## girls_rule_maths ~ IT.NET.USER.P2 + LO.PISA.MAT + LO.PISA.REA + 
##     LO.PISA.REA.FE + LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + 
##     LO.PISA.SCI.MA + NY.GDP.MKTP.CD + NY.GDP.PCAP.CD + SP.POP.TOTL
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\KeywordTok{step}\NormalTok{(}\KeywordTok{glm}\NormalTok{(model_empty, }\DataTypeTok{data=}\NormalTok{edstats_subset, }\DataTypeTok{family=}\KeywordTok{binomial}\NormalTok{(}\StringTok{"logit"}\NormalTok{)),}
    \DataTypeTok{scope=}\NormalTok{model_full, }\DataTypeTok{direction=}\StringTok{"forward"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Start:  AIC=105.12
## girls_rule_maths ~ 1
## 
##                  Df Deviance   AIC
## + LO.PISA.REA.MA  1     90.9  94.9
## + LO.PISA.SCI.MA  1     93.3  97.3
## + NY.GDP.MKTP.CD  1     94.2  98.2
## + LO.PISA.REA     1     95.0  99.0
## + LO.PISA.SCI     1     96.9 100.9
## + LO.PISA.MAT     1     97.2 101.2
## + LO.PISA.REA.FE  1     97.9 101.9
## + LO.PISA.SCI.FE  1     99.4 103.4
## <none>                 103.1 105.1
## + SP.POP.TOTL     1    101.9 105.9
## + NY.GDP.PCAP.CD  1    102.3 106.3
## + IT.NET.USER.P2  1    103.1 107.1
## 
## Step:  AIC=94.93
## girls_rule_maths ~ LO.PISA.REA.MA
## 
##                  Df Deviance  AIC
## + LO.PISA.REA     1     42.8 48.8
## + LO.PISA.REA.FE  1     50.5 56.5
## + LO.PISA.SCI.FE  1     65.4 71.4
## + LO.PISA.SCI     1     77.9 83.9
## + LO.PISA.MAT     1     83.5 89.5
## + NY.GDP.MKTP.CD  1     87.4 93.4
## + IT.NET.USER.P2  1     87.5 93.5
## <none>                  90.9 94.9
## + NY.GDP.PCAP.CD  1     89.2 95.2
## + LO.PISA.SCI.MA  1     89.2 95.2
## + SP.POP.TOTL     1     90.2 96.2
## 
## Step:  AIC=48.83
## girls_rule_maths ~ LO.PISA.REA.MA + LO.PISA.REA
## 
##                  Df Deviance  AIC
## <none>                  42.8 48.8
## + LO.PISA.SCI.FE  1     40.9 48.9
## + SP.POP.TOTL     1     41.2 49.2
## + NY.GDP.PCAP.CD  1     41.3 49.3
## + LO.PISA.SCI     1     42.0 50.0
## + LO.PISA.MAT     1     42.4 50.4
## + IT.NET.USER.P2  1     42.7 50.7
## + LO.PISA.SCI.MA  1     42.7 50.7
## + NY.GDP.MKTP.CD  1     42.7 50.7
## + LO.PISA.REA.FE  1     42.8 50.8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:  glm(formula = girls_rule_maths ~ LO.PISA.REA.MA + LO.PISA.REA, 
##     family = binomial("logit"), data = edstats_subset)
## 
## Coefficients:
##    (Intercept)  LO.PISA.REA.MA     LO.PISA.REA  
##         -0.176          -0.600           0.577  
## 
## Degrees of Freedom: 80 Total (i.e. Null);  78 Residual
## Null Deviance:       103 
## Residual Deviance: 42.8  AIC: 48.8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{predict}\NormalTok{(f, edstats_subset, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}\OperatorTok{>}\FloatTok{0.5}\NormalTok{)}
\KeywordTok{get_metrics}\NormalTok{(Y_pred, edstats_subset}\OperatorTok{$}\NormalTok{girls_rule_maths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.88889  0.84615  0.81481  0.83019 50.00000  5.00000  4.00000 22.00000
\end{verbatim}

\end{solution}

\begin{exercise}

Choose a model via backward elimination.

\end{exercise}

\begin{solution}

Having a dataset with missing values removed, this is easy now:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\KeywordTok{suppressWarnings}\NormalTok{( }\CommentTok{# yeah, yeah, yeah...}
        \CommentTok{# fitted probabilities numerically 0 or 1 occurred}
    \KeywordTok{step}\NormalTok{(}\KeywordTok{glm}\NormalTok{(model_full, }\DataTypeTok{data=}\NormalTok{edstats_subset, }\DataTypeTok{family=}\KeywordTok{binomial}\NormalTok{(}\StringTok{"logit"}\NormalTok{)),}
        \DataTypeTok{scope=}\NormalTok{model_empty, }\DataTypeTok{direction=}\StringTok{"backward"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Start:  AIC=50.83
## girls_rule_maths ~ IT.NET.USER.P2 + LO.PISA.MAT + LO.PISA.REA + 
##     LO.PISA.REA.FE + LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + 
##     LO.PISA.SCI.MA + NY.GDP.MKTP.CD + NY.GDP.PCAP.CD + SP.POP.TOTL
## 
##                  Df Deviance  AIC
## - LO.PISA.MAT     1     26.8 48.8
## - LO.PISA.SCI.MA  1     26.8 48.8
## - NY.GDP.PCAP.CD  1     26.9 48.9
## - LO.PISA.SCI     1     26.9 48.9
## - LO.PISA.SCI.FE  1     27.1 49.1
## - LO.PISA.REA.FE  1     27.4 49.4
## - LO.PISA.REA     1     27.5 49.5
## - LO.PISA.REA.MA  1     27.6 49.6
## <none>                  26.8 50.8
## - IT.NET.USER.P2  1     29.3 51.3
## - NY.GDP.MKTP.CD  1     29.9 51.9
## - SP.POP.TOTL     1     31.7 53.7
## 
## Step:  AIC=48.84
## girls_rule_maths ~ IT.NET.USER.P2 + LO.PISA.REA + LO.PISA.REA.FE + 
##     LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + LO.PISA.SCI.MA + 
##     NY.GDP.MKTP.CD + NY.GDP.PCAP.CD + SP.POP.TOTL
## 
##                  Df Deviance  AIC
## - LO.PISA.SCI.MA  1     26.8 46.8
## - NY.GDP.PCAP.CD  1     26.9 46.9
## - LO.PISA.SCI     1     27.0 47.0
## - LO.PISA.SCI.FE  1     27.1 47.1
## - LO.PISA.REA.FE  1     27.4 47.4
## - LO.PISA.REA     1     27.5 47.5
## - LO.PISA.REA.MA  1     27.6 47.6
## <none>                  26.8 48.8
## - IT.NET.USER.P2  1     29.3 49.3
## - NY.GDP.MKTP.CD  1     29.9 49.9
## - SP.POP.TOTL     1     31.7 51.7
## 
## Step:  AIC=46.84
## girls_rule_maths ~ IT.NET.USER.P2 + LO.PISA.REA + LO.PISA.REA.FE + 
##     LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + NY.GDP.MKTP.CD + 
##     NY.GDP.PCAP.CD + SP.POP.TOTL
## 
##                  Df Deviance  AIC
## - NY.GDP.PCAP.CD  1     26.9 44.9
## <none>                  26.8 46.8
## - IT.NET.USER.P2  1     29.3 47.3
## - NY.GDP.MKTP.CD  1     29.9 47.9
## - LO.PISA.REA.FE  1     31.0 49.0
## - SP.POP.TOTL     1     31.8 49.8
## - LO.PISA.SCI     1     35.6 53.6
## - LO.PISA.SCI.FE  1     36.1 54.1
## - LO.PISA.REA     1     37.5 55.5
## - LO.PISA.REA.MA  1     50.9 68.9
## 
## Step:  AIC=44.87
## girls_rule_maths ~ IT.NET.USER.P2 + LO.PISA.REA + LO.PISA.REA.FE + 
##     LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + NY.GDP.MKTP.CD + 
##     SP.POP.TOTL
## 
##                  Df Deviance  AIC
## <none>                  26.9 44.9
## - NY.GDP.MKTP.CD  1     30.5 46.5
## - IT.NET.USER.P2  1     31.0 47.0
## - LO.PISA.REA.FE  1     31.1 47.1
## - SP.POP.TOTL     1     33.0 49.0
## - LO.PISA.SCI     1     35.9 51.9
## - LO.PISA.SCI.FE  1     36.4 52.4
## - LO.PISA.REA     1     37.5 53.5
## - LO.PISA.REA.MA  1     50.9 66.9
\end{verbatim}

The obtained model and its quality metrics:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:  glm(formula = girls_rule_maths ~ IT.NET.USER.P2 + LO.PISA.REA + 
##     LO.PISA.REA.FE + LO.PISA.REA.MA + LO.PISA.SCI + LO.PISA.SCI.FE + 
##     NY.GDP.MKTP.CD + SP.POP.TOTL, family = binomial("logit"), 
##     data = edstats_subset)
## 
## Coefficients:
##    (Intercept)  IT.NET.USER.P2     LO.PISA.REA  LO.PISA.REA.FE  
##      -1.66e+01        1.61e-01        1.85e+00       -8.00e-01  
## LO.PISA.REA.MA     LO.PISA.SCI  LO.PISA.SCI.FE  NY.GDP.MKTP.CD  
##      -1.03e+00       -1.35e+00        1.32e+00       -4.95e-12  
##    SP.POP.TOTL  
##       6.20e-08  
## 
## Degrees of Freedom: 80 Total (i.e. Null);  72 Residual
## Null Deviance:       103 
## Residual Deviance: 26.9  AIC: 44.9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{predict}\NormalTok{(f, edstats_subset, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}\OperatorTok{>}\FloatTok{0.5}\NormalTok{)}
\KeywordTok{get_metrics}\NormalTok{(Y_pred, edstats_subset}\OperatorTok{$}\NormalTok{girls_rule_maths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.91358  0.88462  0.85185  0.86792 51.00000  4.00000  3.00000 23.00000
\end{verbatim}

Note that we got a better (lower) AIC than in the forward selection
case, which means that backward elimination was better this time.
On the other hand, we needed to suppress the
\texttt{fitted\ probabilities\ numerically\ 0\ or\ 1\ occurred} warnings.
The returned model is perhaps \emph{unstable} as well and consists of too
many variables.

\end{solution}

\hypertarget{outro-3}{%
\section{Outro}\label{outro-3}}

\hypertarget{remarks-3}{%
\subsection{Remarks}\label{remarks-3}}

Other prominent classification algorithms:

\begin{itemize}
\tightlist
\item
  Naive Bayes and other probabilistic approaches,
\item
  Support Vector Machines (SVMs) and other kernel methods,
\item
  (Artificial) (Deep) Neural Networks.
\end{itemize}

Interestingly, in the next chapter we will note that the logistic regression model
is a special case of a \emph{feed-forward single layer neural network}.

We will also generalise the binary logistic regression to the case of
a multiclass classification.

The state-of-the art classifiers called
\emph{Random Forests} and \emph{XGBoost} (see also: \emph{AdaBoost}) are based on decision trees.
They tend to be more accurate but -- at the same time -- they fail to
exhibit the decision trees' important feature: interpretability.

Trees can also be used for regression tasks, see R package \texttt{rpart}.

\hypertarget{further-reading-3}{%
\subsection{Further Reading}\label{further-reading-3}}

Recommended further reading: (James et al. \protect\hyperlink{ref-islr}{2017}: Chapters 4 and 8)

Other: (Hastie et al. \protect\hyperlink{ref-esl}{2017}: Chapters 4 and 7 as well as (*) Chapters 9, 10, 13, 15)

\hypertarget{shallow-and-deep-neural-networks}{%
\chapter{Shallow and Deep Neural Networks}\label{shallow-and-deep-neural-networks}}

\hypertarget{introduction-7}{%
\section{Introduction}\label{introduction-7}}

\hypertarget{binary-logistic-regression-recap}{%
\subsection{Binary Logistic Regression: Recap}\label{binary-logistic-regression-recap}}

Let \(\mathbf{X}\in\mathbb{R}^{n\times p}\) be an input matrix
that consists of \(n\) points in a \(p\)-dimensional space.

\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} \\
\end{array}
\right]
\]

In other words, we have a database on \(n\) objects.
Each object is described by means of \(p\) numerical features.

With each input \(\mathbf{x}_{i,\cdot}\) we associate the desired output
\(y_i\) which is a categorical label -- hence we
will be dealing with \textbf{classification} tasks again.

To recall, in \textbf{binary logistic regression} we model
the probabilities that
a given input belongs to either of the two classes:

\[
\begin{array}{lr}
\Pr(Y=1|\mathbf{X},\boldsymbol\beta)=& \phi(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)\\
\Pr(Y=0|\mathbf{X},\boldsymbol\beta)=& 1-\phi(\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p)\\
\end{array}
\]
where \(\phi(t) = \frac{1}{1+e^{-t}}=\frac{e^t}{1+e^t}\) is the logistic sigmoid function.

It holds:
\[
\begin{array}{ll}
\Pr(Y=1|\mathbf{X},\boldsymbol\beta)=&\displaystyle\frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}},\\
\Pr(Y=0|\mathbf{X},\boldsymbol\beta)=&
\displaystyle\frac{1}{1+e^{+(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}=\displaystyle\frac{e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}{1+e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}.\\
\end{array}
\]

The fitting of the model was performed by minimising the cross-entropy (log-loss):
\[
\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}
-\frac{1}{n} \sum_{i=1}^n
\Bigg(y_i\log \hat{y}_i + (1-y_i)\log (1-\hat{y}_i)\Bigg).
\]
where \(\hat{y}_i=\Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta)\).

This is equivalent to:
\[
\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}
-\frac{1}{n} \sum_{i=1}^n
\Bigg(y_i\log \Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta) + (1-y_i)\log \Pr(Y=0|\mathbf{x}_{i,\cdot},\boldsymbol\beta)\Bigg).
\]

Note that for each \(i\),
either the left or the right term (in the bracketed expression) vanishes.

Hence, we may also write the above as:
\[
\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}
-\frac{1}{n} \sum_{i=1}^n
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\boldsymbol\beta).
\]

In this chapter we will generalise the binary logistic regression model:

\begin{itemize}
\item
  First we will consider the case of many classes
  (multiclass classification).
  This will lead to the multinomial logistic regression model.
\item
  Then we will note that the multinomial logistic regression is a special
  case of a feed-forward neural network.
\end{itemize}

\hypertarget{data-2}{%
\subsection{Data}\label{data-2}}

We will study the famous classic -- the MNIST image classification dataset
(Modified National Institute of Standards and Technology database),
see \url{http://yann.lecun.com/exdb/mnist/}

It consists of 28×28 pixel images of handwritten digits:

\begin{itemize}
\tightlist
\item
  \texttt{train}: 60,000 training images,
\item
  \texttt{t10k}: 10,000 testing images.
\end{itemize}

A few image instances from each class are depicted in
Figure \ref{fig:mnist_demo}.

\begin{figure}
\hypertarget{fig:mnist_demo}{%
\centering
\includegraphics{05-classification-nnets-figures/mnist_demo-1.pdf}
\caption{Example images in the MNIST database}\label{fig:mnist_demo}
}
\end{figure}

There are 10 unique digits, so this is a multiclass classification problem.

\begin{description}
\item[Remark.]
The dataset is already ``too easy'' for testing of the state-of-the-art
classifiers (see the notes below), but it's a great educational example.
\end{description}

Accessing MNIST via the \texttt{keras} package
(which we will use throughout this chapter anyway) is easy:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"keras"}\NormalTok{)}
\NormalTok{mnist <-}\StringTok{ }\KeywordTok{dataset_mnist}\NormalTok{()}
\NormalTok{X_train <-}\StringTok{ }\NormalTok{mnist}\OperatorTok{$}\NormalTok{train}\OperatorTok{$}\NormalTok{x}
\NormalTok{Y_train <-}\StringTok{ }\NormalTok{mnist}\OperatorTok{$}\NormalTok{train}\OperatorTok{$}\NormalTok{y}
\NormalTok{X_test  <-}\StringTok{ }\NormalTok{mnist}\OperatorTok{$}\NormalTok{test}\OperatorTok{$}\NormalTok{x}
\NormalTok{Y_test  <-}\StringTok{ }\NormalTok{mnist}\OperatorTok{$}\NormalTok{test}\OperatorTok{$}\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\texttt{X\_train} and \texttt{X\_test} consist of 28×28 pixel images.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(X_train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 60000    28    28
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(X_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10000    28    28
\end{verbatim}

\texttt{X\_train} and \texttt{X\_test} are 3-dimensional arrays, think
of them as vectors of 60000 and 10000 matrices of size 28×28, respectively.

These are grey-scale images, with 0 = black, \ldots{}, 255 = white:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{range}\NormalTok{(X_train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]   0 255
\end{verbatim}

Numerically, it's more convenient to work with colour values
converted to 0.0 = black, \ldots{}, 1.0 = white:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X_train <-}\StringTok{ }\NormalTok{X_train}\OperatorTok{/}\DecValTok{255}
\NormalTok{X_test  <-}\StringTok{ }\NormalTok{X_test}\OperatorTok{/}\DecValTok{255}
\end{Highlighting}
\end{Shaded}

\texttt{Y\_train} and \texttt{Y\_test} are the corresponding integer labels:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(Y_train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 60000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(Y_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(Y_train) }\CommentTok{# label distribution in the training sample}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Y_train
##    0    1    2    3    4    5    6    7    8    9 
## 5923 6742 5958 6131 5842 5421 5918 6265 5851 5949
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(Y_test)  }\CommentTok{# label distribution in the test sample}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Y_test
##    0    1    2    3    4    5    6    7    8    9 
##  980 1135 1032 1010  982  892  958 1028  974 1009
\end{verbatim}

Here is how we can plot one of the digits (see Figure \ref{fig:mnist_info2b}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{id <-}\StringTok{ }\DecValTok{123} \CommentTok{# image ID to show}
\KeywordTok{image}\NormalTok{(}\DataTypeTok{z=}\KeywordTok{t}\NormalTok{(X_train[id,,]), }\DataTypeTok{col=}\KeywordTok{grey.colors}\NormalTok{(}\DecValTok{256}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \DataTypeTok{axes=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\KeywordTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }\DataTypeTok{bg=}\StringTok{"white"}\NormalTok{,}
    \DataTypeTok{legend=}\KeywordTok{sprintf}\NormalTok{(}\StringTok{"True label=%d"}\NormalTok{, Y_train[id]))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:mnist_info2b}{%
\centering
\includegraphics{05-classification-nnets-figures/mnist_info2b-1.pdf}
\caption{Example image from the MNIST dataset}\label{fig:mnist_info2b}
}
\end{figure}

\hypertarget{multinomial-logistic-regression}{%
\section{Multinomial Logistic Regression}\label{multinomial-logistic-regression}}

\hypertarget{a-note-on-data-representation}{%
\subsection{A Note on Data Representation}\label{a-note-on-data-representation}}

So\ldots{} you may now be wondering ``how do we construct an image classifier,
this seems so complicated!''.

For a computer, (almost) everything is just numbers.

Instead of playing with \(n\) matrices, each of size 28×28,
we may ``flatten'' the images so as to get
\(n\) ``long'' vectors of length \(p=784\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X_train2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(X_train, }\DataTypeTok{ncol=}\DecValTok{28}\OperatorTok{*}\DecValTok{28}\NormalTok{)}
\NormalTok{X_test2  <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(X_test, }\DataTypeTok{ncol=}\DecValTok{28}\OperatorTok{*}\DecValTok{28}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The classifiers studied here do not take the ``spatial'' positioning of
the pixels into account anyway. Hence, now we're back to our ``comfort zone''.

\begin{description}
\item[Remark.]
(*) See, however, convolutional neural networks (CNNs),
e.g., in (Goodfellow et al. \protect\hyperlink{ref-deeplearn}{2016}).
\end{description}

\hypertarget{extending-logistic-regression}{%
\subsection{Extending Logistic Regression}\label{extending-logistic-regression}}

Let us generalise the binary logistic regression model
to a 10-class one (or, more generally, \(K\)-class one).

This time we will be modelling ten probabilities,
with
\(\Pr(Y=k|\mathbf{X},\mathbf{B})\) denoting the \emph{confidence} that a given image \(\mathbf{X}\)
is in fact the \(k\)-th digit:

\[
\begin{array}{lcl}
\Pr(Y=0|\mathbf{X},\mathbf{B})&=&\dots\\
\Pr(Y=1|\mathbf{X},\mathbf{B})&=&\dots\\
&\vdots&\\
\Pr(Y=9|\mathbf{X},\mathbf{B})&=&\dots\\
\end{array}
\]

where \(\mathbf{B}\) is the set of underlying model parameters
(to be determined soon).

In binary logistic regression,
the class probabilities are obtained by ``cleverly normalising'' (by means of the logistic sigmoid)
the outputs of a linear model (so that we obtain a value in \([0,1]\)).

\[
\Pr(Y=1|\mathbf{X},\boldsymbol\beta)=\phi(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)=\displaystyle\frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}
\]

In the multinomial case, we can use a separate linear model for each digit
so that each \(\Pr(Y=k|\mathbf{X},\mathbf{B})\), \(k=0,1,\dots,9\),
is given as a function of:
\[\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p}.\]

Therefore, instead of a parameter vector of length \((p+1)\),
we will need a parameter matrix of size \((p+1)\times 10\)
representing the model's definition.

\begin{description}
\item[Side note.]
The upper case of \(\beta\) is \(B\).
\end{description}

Then, these 10 numbers will have to be normalised
so as to they are all greater than \(0\) and sum to \(1\).

To maintain the spirit of the original model,
we can apply \(e^{-(\beta_{0,k} + \beta_{1,k} X_{1} + \dots + \beta_{p,k} X_{p})}\)
to get a positive value,
because the co-domain of the exponential function \(t\mapsto e^t\)
is \((0,\infty)\).

Then, dividing each output by the sum of all the outputs will guarantee that
the total sum equals 1.

This leads to:
\[
\begin{array}{lcl}
\Pr(Y=0|\mathbf{X},\mathbf{B})&=&\displaystyle\frac{e^{-(\beta_{0,0} + \beta_{1,0} X_{1} +  \dots + \beta_{p,0} X_{p})}}{\sum_{k=0}^9 e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_p)}},\\
\Pr(Y=1|\mathbf{X},\mathbf{B})&=&\displaystyle\frac{e^{-(\beta_{0,1} + \beta_{1,1} X_{1} +  \dots + \beta_{p,1} X_{p})}}{\sum_{k=0}^9 e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p})}},\\
&\vdots&\\
\Pr(Y=9|\mathbf{X},\mathbf{B})&=&\displaystyle\frac{e^{-(\beta_{0,9} + \beta_{1,9} X_{1} +  \dots + \beta_{p,9} X_{p})}}{\sum_{k=0}^9 e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p})}}.\\
\end{array}
\]

This reduces to the binary logistic regression
if we consider only the classes \(0\) and \(1\) and
fix \(\beta_{0,0}=\beta_{1,0}=\dots=\beta_{p,0}=0\) (as \(e^0=1\)).

\hypertarget{softmax-function}{%
\subsection{Softmax Function}\label{softmax-function}}

The above transformation (that maps 10 arbitrary real numbers
to positive ones that sum to 1)
is called the \textbf{softmax} function (or \emph{softargmax}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{softmax <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(T) \{}
\NormalTok{    T2 <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(T) }\CommentTok{# ignore the minus sign above}
\NormalTok{    T2}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(T2)}
\NormalTok{\}}
\KeywordTok{round}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(}
    \KeywordTok{softmax}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,  }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)),}
    \KeywordTok{softmax}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)),}
    \KeywordTok{softmax}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{9}\NormalTok{,  }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)),}
    \KeywordTok{softmax}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{9}\NormalTok{,  }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{8}\NormalTok{))), }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    0 1.00    0    0    0 0.00    0    0  0.00
## [2,]    0    0 0.50    0    0    0 0.50    0    0  0.00
## [3,]    0    0 0.73    0    0    0 0.27    0    0  0.00
## [4,]    0    0 0.67    0    0    0 0.24    0    0  0.09
\end{verbatim}

\hypertarget{one-hot-encoding-and-decoding}{%
\subsection{One-Hot Encoding and Decoding}\label{one-hot-encoding-and-decoding}}

The ten class-belongingness-degrees can be decoded
to obtain a single label by simply choosing
the class that is assigned the highest probability.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y_pred <-}\StringTok{ }\KeywordTok{softmax}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{9}\NormalTok{,  }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{8}\NormalTok{))}
\KeywordTok{round}\NormalTok{(y_pred, }\DecValTok{2}\NormalTok{) }\CommentTok{# probabilities of Y=0, 1, 2, ..., 9}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.00 0.00 0.67 0.00 0.00 0.00 0.24 0.00 0.00 0.09
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{which.max}\NormalTok{(y_pred)}\OperatorTok{-}\DecValTok{1} \CommentTok{# 1..10 -> 0..9}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\begin{description}
\item[Remark.]
\texttt{which.max(y)} returns an index \texttt{k} such that
\texttt{y{[}k{]}==max(y)} (recall that in R the first element in
a vector is at index \texttt{1}).
Mathematically, we denote this operation
as \(\mathrm{arg}\max_{k=1,\dots,K} y_k\).
\end{description}

To make processing the outputs of a logistic regression model more convenient,
we will apply the so-called \textbf{one-hot-encoding} of the labels.

Here, each label will be represented as a 0-1 vector
of 10 probabilities -- with probability 1 corresponding
to the true class only.

For instance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\DecValTok{2} \CommentTok{# true class (this is just an example)}
\NormalTok{y2 <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{y2[y}\OperatorTok{+}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{1} \CommentTok{# +1 because we need 0..9 -> 1..10}
\NormalTok{y2  }\CommentTok{# one-hot-encoded y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0 0 1 0 0 0 0 0 0 0
\end{verbatim}

To one-hot encode \emph{all} the reference outputs in R,
we start with a matrix of size \(n\times 10\) populated with ``0''s:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_train2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{nrow=}\KeywordTok{length}\NormalTok{(Y_train), }\DataTypeTok{ncol=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, for every \(i\), we insert a ``1'' in the \(i\)-th row
and the (\texttt{Y\_train{[}}\(i\)\texttt{{]}+1})-th column:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Note the "+1" 0..9 -> 1..10}
\NormalTok{Y_train2[}\KeywordTok{cbind}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(Y_train), Y_train}\OperatorTok{+}\DecValTok{1}\NormalTok{)] <-}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{description}
\item[Remark.]
In R, indexing a matrix \texttt{A} with a 2-column matrix \texttt{B}, i.e., \texttt{A{[}B{]}},
allows for an easy access to
\texttt{A{[}B{[}1,1{]},\ B{[}1,2{]}{]}}, \texttt{A{[}B{[}2,1{]},\ B{[}2,2{]}{]}}, \texttt{A{[}B{[}3,1{]},\ B{[}3,2{]}{]}}, \ldots{}
\end{description}

Sanity check:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(Y_train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5 0 4 1 9 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(Y_train2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    0    0    0    0    1    0    0    0     0
## [2,]    1    0    0    0    0    0    0    0    0     0
## [3,]    0    0    0    0    1    0    0    0    0     0
## [4,]    0    1    0    0    0    0    0    0    0     0
## [5,]    0    0    0    0    0    0    0    0    0     1
## [6,]    0    0    1    0    0    0    0    0    0     0
\end{verbatim}

Let us generalise the above idea and write a function
that can one-hot-encode any vector of integer labels:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one_hot_encode <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(Y) \{}
    \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{is.numeric}\NormalTok{(Y))}
\NormalTok{    c1 <-}\StringTok{ }\KeywordTok{min}\NormalTok{(Y) }\CommentTok{# first class label}
\NormalTok{    cK <-}\StringTok{ }\KeywordTok{max}\NormalTok{(Y) }\CommentTok{# last class label}
\NormalTok{    K <-}\StringTok{ }\NormalTok{cK}\OperatorTok{-}\NormalTok{c1}\OperatorTok{+}\DecValTok{1} \CommentTok{# number of classes}

\NormalTok{    Y2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{nrow=}\KeywordTok{length}\NormalTok{(Y), }\DataTypeTok{ncol=}\NormalTok{K)}
\NormalTok{    Y2[}\KeywordTok{cbind}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(Y), Y}\OperatorTok{-}\NormalTok{c1}\OperatorTok{+}\DecValTok{1}\NormalTok{)] <-}\StringTok{ }\DecValTok{1}
\NormalTok{    Y2}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Encode \texttt{Y\_train} and \texttt{Y\_test}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_train2 <-}\StringTok{ }\KeywordTok{one_hot_encode}\NormalTok{(Y_train)}
\NormalTok{Y_test2 <-}\StringTok{ }\KeywordTok{one_hot_encode}\NormalTok{(Y_test)}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-entropy-revisited}{%
\subsection{Cross-entropy Revisited}\label{cross-entropy-revisited}}

Our classifier will be outputting \(K=10\) probabilities.

The true class labels are not one-hot-encoded so that they
are represented as vectors of \(K-1\) zeros and a single one.

How to measure the ``agreement'' between these two?

\bigskip

In essence, we will be comparing
the probability vectors as generated by a classifier, \(\hat{Y}\):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(y_pred, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.00 0.00 0.67 0.00 0.00 0.00 0.24 0.00 0.00 0.09
\end{verbatim}

with the one-hot-encoded true probabilities, \(Y\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0 0 1 0 0 0 0 0 0 0
\end{verbatim}

It turns out that one of the definitions of cross-entropy introduced above
already handles the case of multiclass classification:
\[
E(\mathbf{B}) =
-\frac{1}{n} \sum_{i=1}^n
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\mathbf{B}).
\]
The smaller the probability corresponding to the ground-truth class
outputted by the classifier, the higher the penalty, see
Figure \ref{fig:cross_entropy_revisited_example3}.

\begin{figure}
\hypertarget{fig:cross_entropy_revisited_example3}{%
\centering
\includegraphics{05-classification-nnets-figures/cross_entropy_revisited_example3-1.pdf}
\caption{The less the classifier is confident about the prediction of the actually true label, the greater the penalty}\label{fig:cross_entropy_revisited_example3}
}
\end{figure}

To sum up, we will be solving the optimisation problem:
\[
\min_{\mathbf{B}\in\mathbb{R}^{(p+1)\times 10}}
-\frac{1}{n} \sum_{i=1}^n
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\mathbf{B}).
\]
This has no analytical solution,
but can be solved using iterative methods
(see the chapter on optimisation).

(*) Side note: A single term in the above formula,
\[
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\mathbf{B})
\]
given:

\begin{itemize}
\tightlist
\item
  \texttt{y\_pred} -- a vector of 10 probabilities
  generated by the model:
  \[
  \left[\Pr(Y=0|\mathbf{x}_{i,\cdot},\mathbf{B})\  \Pr(Y=1|\mathbf{x}_{i,\cdot},\mathbf{B})\ \cdots\ \Pr(Y=9|\mathbf{x}_{i,\cdot},\mathbf{B})\right]
  \]
\item
  \texttt{y2} -- a one-hot-encoded version of the true label, \(y_i\), of the form:
  \[
  \left[0\ 0\ \cdots\ 0\ 1\ 0\ \cdots\ 0\right]
  \]
\end{itemize}

can be computed as:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(y2}\OperatorTok{*}\KeywordTok{log}\NormalTok{(y_pred))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.40782
\end{verbatim}

\hypertarget{problem-formulation-in-matrix-form}{%
\subsection{Problem Formulation in Matrix Form (**)}\label{problem-formulation-in-matrix-form}}

The definition of a multinomial logistic regression
model for a multiclass classification task involving
classes \(\{1,2,\dots,K\}\)
is slightly bloated.

Assuming that \(\mathbf{X}\in\mathbb{R}^{n\times p}\) is the input matrix,
to compute the \(K\) predicted probabilities for the \(i\)-th input,
\[
\left[
\hat{y}_{i,1}\ \hat{y}_{i,2}\ \cdots\ \hat{y}_{i,K}
\right],
\]
given a parameter matrix \(\mathbf{B}^{(p+1)\times K}\), we apply:
\[
\begin{array}{lcl}
\hat{y}_{i,1}=\Pr(Y=1|\mathbf{x}_{i,\cdot},\mathbf{B})&=&\displaystyle\frac{e^{\beta_{0,1} + \beta_{1,1} x_{i,1} +  \dots + \beta_{p,1} x_{i,p}}}{\sum_{k=1}^K e^{\beta_{0,k} + \beta_{1,k} x_{i,1} +  \dots + \beta_{p,k} x_{i,p}}},\\
&\vdots&\\
\hat{y}_{i,K}=\Pr(Y=K|\mathbf{x}_{i,\cdot},\mathbf{B})&=&\displaystyle\frac{e^{\beta_{0,K} + \beta_{1,K} x_{i,1} +  \dots + \beta_{p,K} x_{i,p}}}{\sum_{k=1}^K e^{\beta_{0,k} + \beta_{1,k} x_{i,1} +  \dots + \beta_{p,k} x_{i,p}}}.\\
\end{array}
\]

\begin{description}
\item[Remark.]
We have dropped the minus sign in the exponentiation for
brevity of notation.
Note that we can always map \(b_{j,k}'=-b_{j,k}\).
\end{description}

It turns out we can make use of matrix notation
to tidy the above formulas.

Denote the linear combinations prior to computing the softmax function with:
\[
\begin{array}{lcl}
t_{i,1}&=&\beta_{0,1} + \beta_{1,1} x_{i,1} +  \dots + \beta_{p,1} x_{i,p},\\
&\vdots&\\
t_{i,K}&=&\beta_{0,K} + \beta_{1,K} x_{i,1} +  \dots + \beta_{p,K} x_{i,p}.\\
\end{array}
\]

We have:

\begin{itemize}
\tightlist
\item
  \(x_{i,j}\) -- the \(i\)-th observation, the \(j\)-th feature;
\item
  \(\hat{y}_{i,k}\) -- the \(i\)-th observation, the \(k\)-th class probability;
\item
  \(\beta_{j,k}\) -- the coefficient for the \(j\)-th feature when computing the \(k\)-th class.
\end{itemize}

Note that by augmenting \(\dot{\mathbf{X}}=[\boldsymbol{1}\ \mathbf{X}]\in\mathbb{R}^{n\times (p+1)}\) by adding a column of 1s, i.e.,
where \(\dot{x}_{i,0}=1\) and \(\dot{x}_{i,j}=x_{i,j}\) for all \(j\ge 1\) and all \(i\), we can write the above as:
\[
\begin{array}{lclcl}
t_{i,1}&=&\sum_{j=0}^p \dot{x}_{i,j}\, \beta_{j,1} &=& \dot{\mathbf{x}}_{i,\cdot}\, \boldsymbol\beta_{\cdot,1},\\
&\vdots&\\
t_{i,K}&=&\sum_{j=0}^p \dot{x}_{i,j}\, \beta_{j,K} &=& \dot{\mathbf{x}}_{i,\cdot}\, \boldsymbol\beta_{\cdot,K}.\\
\end{array}
\]

We can get the \(K\) linear combinations all at once
in the form of a row vector by writing:
\[
\left[
t_{i,1}\ t_{i,2}\ \cdots\ t_{i,K}
\right]
=
{\mathbf{x}_{i,\cdot}}\, \mathbf{B}.
\]

Moreover, we can do that for all the \(n\) inputs by writing:
\[
\mathbf{T}=\dot{\mathbf{X}}\,\mathbf{B}.
\]
Yes yes yes! This is a single matrix multiplication,
we have \(\mathbf{T}\in\mathbb{R}^{n\times K}\).

To obtain \(\hat{\mathbf{Y}}\), we have to apply the softmax function
on every row of \(\mathbf{T}\):
\[
\hat{\mathbf{Y}}=\mathrm{softmax}\left(
\dot{\mathbf{X}}\,\mathbf{B}
\right).
\]

That's it. Take some time to appreciate the elegance of this notation.

Methods for minimising cross-entropy expressed in matrix form
will be discussed in the next chapter.

\hypertarget{artificial-neural-networks}{%
\section{Artificial Neural Networks}\label{artificial-neural-networks}}

\hypertarget{artificial-neuron}{%
\subsection{Artificial Neuron}\label{artificial-neuron}}

A neuron can be thought of as a mathematical function,
see Figure \ref{fig:neuron}, which has its specific inputs
and an output.

\begin{figure}
\hypertarget{fig:neuron}{%
\centering
\includegraphics[width=0.5\textwidth,height=\textheight]{figures/neuron.pdf}
\caption{Neuron as a mathematical (black box) function; image based on: \url{https://en.wikipedia.org/wiki/File:Neuron3.png} by Egm4313.s12 at English Wikipedia, licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license}\label{fig:neuron}
}
\end{figure}

The Linear Threshold Unit (McCulloch and Pitts, 1940s),
the Perceptron (Rosenblatt, 1958) and the Adaptive Linear Neuron
(Widrow and Hoff, 1960) were amongst the first
models of an artificial neuron that could be used
for the purpose of pattern recognition, see Figure \ref{fig:perceptron}.
They can be thought of as processing units that compute
a weighted sum of the inputs,
which is then transformed by means of a nonlinear ``activation'' function.

\begin{figure}
\hypertarget{fig:perceptron}{%
\centering
\includegraphics[width=0.72\textwidth,height=\textheight]{figures/perceptron.pdf}
\caption{A simple model of an artificial neuron}\label{fig:perceptron}
}
\end{figure}

\hypertarget{logistic-regression-as-a-neural-network}{%
\subsection{Logistic Regression as a Neural Network}\label{logistic-regression-as-a-neural-network}}

The above resembles our binary logistic regression model,
where we determine a linear combination (a weighted sum) of \(p\) inputs
and then transform it using the logistic sigmoid ``activation'' function.
We can easily depict it in the Figure \ref{fig:neuron}-style,
see Figure \ref{fig:logistic_regression_binary}.

\begin{figure}
\hypertarget{fig:logistic_regression_binary}{%
\centering
\includegraphics[width=0.75\textwidth,height=\textheight]{figures/logistic_regression_binary.pdf}
\caption{Binary logistic regression}\label{fig:logistic_regression_binary}
}
\end{figure}

On the other hand, a multiclass logistic regression can be depicted as
in Figure \ref{fig:logistic_regression_multiclass}.
In fact, we can consider it as an instance of a:

\begin{itemize}
\tightlist
\item
  \textbf{single layer} (there is only one processing step that consists of 10 units),
\item
  \textbf{densely connected} (all the inputs are connected to all the components below),
\item
  \textbf{feed-forward} (the outputs are generated by processing the inputs from ``top'' to ``bottom'', there are no loops in the graph etc.)
\end{itemize}

\emph{artificial} \textbf{neural network}
that uses the softmax as the activation function.

\begin{figure}
\hypertarget{fig:logistic_regression_multiclass}{%
\centering
\includegraphics[width=0.75\textwidth,height=\textheight]{figures/logistic_regression_multiclass.pdf}
\caption{Multinomial logistic regression}\label{fig:logistic_regression_multiclass}
}
\end{figure}

\hypertarget{example-in-r-3}{%
\subsection{Example in R}\label{example-in-r-3}}

To train such a neural network (i.e., fit a multinomial
logistic regression model),
we will use the \texttt{keras} package,
a wrapper around the (GPU-enabled) TensorFlow library.

The training of the model takes a few minutes (for more complex
models and bigger datasets -- it could take hours/days).
Thus, it is wise to store the computed model (the \(\mathbf{B}\)
coefficient matrix and the accompanying \texttt{keras}'s auxiliary data)
for further reference:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file_name <-}\StringTok{ "datasets/mnist_keras_model1.h5"}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{file.exists}\NormalTok{(file_name)) \{ }\CommentTok{# File doesn't exist -> compute}
    \KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
    \CommentTok{# Start with an empty model}
\NormalTok{    model1 <-}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{()}
    \CommentTok{# Add a single layer with 10 units and softmax activation}
    \KeywordTok{layer_dense}\NormalTok{(model1, }\DataTypeTok{units=}\DecValTok{10}\NormalTok{, }\DataTypeTok{activation=}\StringTok{"softmax"}\NormalTok{)}
    \CommentTok{# We will be minimising the cross-entropy,}
    \CommentTok{# sgd == stochastic gradient descent, see the next chapter}
    \KeywordTok{compile}\NormalTok{(model1, }\DataTypeTok{optimizer=}\StringTok{"sgd"}\NormalTok{,}
            \DataTypeTok{loss=}\StringTok{"categorical_crossentropy"}\NormalTok{)}
    \CommentTok{# Fit the model (slooooow!)}
    \KeywordTok{fit}\NormalTok{(model1, X_train2, Y_train2, }\DataTypeTok{epochs=}\DecValTok{10}\NormalTok{)}
    \CommentTok{# Save the model for future reference}
    \KeywordTok{save_model_hdf5}\NormalTok{(model1, file_name)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{ }\CommentTok{# File exists -> reload the model}
\NormalTok{    model1 <-}\StringTok{ }\KeywordTok{load_model_hdf5}\NormalTok{(file_name)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let's make predictions over the test set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model1, X_test2)}
\KeywordTok{round}\NormalTok{(}\KeywordTok{head}\NormalTok{(Y_pred2), }\DecValTok{2}\NormalTok{) }\CommentTok{# predicted class probabilities}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00  0.00
## [2,] 0.01 0.00 0.93 0.01 0.00 0.01 0.04 0.00 0.00  0.00
## [3,] 0.00 0.96 0.02 0.00 0.00 0.00 0.00 0.00 0.01  0.00
## [4,] 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00
## [5,] 0.00 0.00 0.01 0.00 0.91 0.00 0.01 0.01 0.01  0.05
## [6,] 0.00 0.98 0.00 0.00 0.00 0.00 0.00 0.00 0.01  0.00
\end{verbatim}

Then, we can one-hot-decode the output probabilities:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(Y_pred2, }\DecValTok{1}\NormalTok{, which.max)}\OperatorTok{-}\DecValTok{1} \CommentTok{# 1..10 -> 0..9}
\KeywordTok{head}\NormalTok{(Y_pred, }\DecValTok{20}\NormalTok{) }\CommentTok{# predicted outputs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(Y_test, }\DecValTok{20}\NormalTok{) }\CommentTok{# true outputs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4
\end{verbatim}

Accuracy on the test set:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(Y_test }\OperatorTok{==}\StringTok{ }\NormalTok{Y_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9169
\end{verbatim}

Performance metrics for each digit separately (see also Figure \ref{fig:logistic6}):

\begin{longtable}[]{@{}rrrrrrrrr@{}}
\toprule
i & Acc & Prec & Rec & F & TN & FN & FP & TP\tabularnewline
\midrule
\endhead
0 & 0.9924 & 0.94664 & 0.97755 & 0.96185 & 8966 & 22 & 54 & 958\tabularnewline
1 & 0.9923 & 0.95920 & 0.97357 & 0.96633 & 8818 & 30 & 47 & 1105\tabularnewline
2 & 0.9803 & 0.92214 & 0.88372 & 0.90252 & 8891 & 120 & 77 & 912\tabularnewline
3 & 0.9802 & 0.89417 & 0.91188 & 0.90294 & 8881 & 89 & 109 & 921\tabularnewline
4 & 0.9833 & 0.90148 & 0.93177 & 0.91637 & 8918 & 67 & 100 & 915\tabularnewline
5 & 0.9793 & 0.91415 & 0.84753 & 0.87958 & 9037 & 136 & 71 & 756\tabularnewline
6 & 0.9885 & 0.93142 & 0.94990 & 0.94057 & 8975 & 48 & 67 & 910\tabularnewline
7 & 0.9834 & 0.92843 & 0.90856 & 0.91839 & 8900 & 94 & 72 & 934\tabularnewline
8 & 0.9754 & 0.86473 & 0.88604 & 0.87525 & 8891 & 111 & 135 & 863\tabularnewline
9 & 0.9787 & 0.90040 & 0.88702 & 0.89366 & 8892 & 114 & 99 & 895\tabularnewline
\bottomrule
\end{longtable}

Note how misleading the individual accuracies are! Averaging over
the above table's columns gives:

\begin{verbatim}
##     Acc    Prec     Rec       F 
## 0.98338 0.91628 0.91575 0.91575
\end{verbatim}

\begin{figure}
\hypertarget{fig:logistic6}{%
\centering
\includegraphics{05-classification-nnets-figures/logistic6-1.pdf}
\caption{Performance metrics for multinomial logistic regression on MNIST}\label{fig:logistic6}
}
\end{figure}

\hypertarget{deep-neural-networks}{%
\section{Deep Neural Networks}\label{deep-neural-networks}}

\hypertarget{introduction-8}{%
\subsection{Introduction}\label{introduction-8}}

In a brain, a neuron's output is an input a bunch of other neurons.
We could try aligning neurons into many interconnected layers.
This leads to a structure like the one in Figure \ref{fig:nnet}.

\begin{figure}
\hypertarget{fig:nnet}{%
\centering
\includegraphics[width=0.75\textwidth,height=\textheight]{figures/nnet.pdf}
\caption{A multi-layer neural network}\label{fig:nnet}
}
\end{figure}

\hypertarget{activation-functions}{%
\subsection{Activation Functions}\label{activation-functions}}

Each layer's outputs should be transformed by some non-linear
activation function. Otherwise, we'd end up with linear combinations of linear combinations,
which are linear combinations themselves.

Example activation functions
that can be used in hidden (inner) layers:

\begin{itemize}
\tightlist
\item
  \texttt{relu} -- The rectified linear unit:
  \[\psi(t)=\max(t, 0),\]
\item
  \texttt{sigmoid} -- The logistic sigmoid:
  \[\phi(t)= \frac{1}{1 + \exp(-t)},\]
\item
  \texttt{tanh} -- The hyperbolic function:
  \[\mathrm{tanh}(t) = \frac{\exp(t) - \exp(-t)}{\exp(t) + \exp(-t)}.\]
\end{itemize}

There is not much difference between them, but some might be more convenient
to handle numerically than the others, depending on the implementation.

\hypertarget{example-in-r---2-layers}{%
\subsection{Example in R - 2 Layers}\label{example-in-r---2-layers}}

Let's construct a 2-layer Neural Network of the type 784-800-10:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file_name <-}\StringTok{ "datasets/mnist_keras_model2.h5"}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{file.exists}\NormalTok{(file_name)) \{}
    \KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{    model2 <-}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{()}
    \KeywordTok{layer_dense}\NormalTok{(model2, }\DataTypeTok{units=}\DecValTok{800}\NormalTok{, }\DataTypeTok{activation=}\StringTok{"relu"}\NormalTok{)}
    \KeywordTok{layer_dense}\NormalTok{(model2, }\DataTypeTok{units=}\DecValTok{10}\NormalTok{,  }\DataTypeTok{activation=}\StringTok{"softmax"}\NormalTok{)}
    \KeywordTok{compile}\NormalTok{(model2, }\DataTypeTok{optimizer=}\StringTok{"sgd"}\NormalTok{,}
            \DataTypeTok{loss=}\StringTok{"categorical_crossentropy"}\NormalTok{)}
    \KeywordTok{fit}\NormalTok{(model2, X_train2, Y_train2, }\DataTypeTok{epochs=}\DecValTok{10}\NormalTok{)}
    \KeywordTok{save_model_hdf5}\NormalTok{(model2, file_name)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{    model2 <-}\StringTok{ }\KeywordTok{load_model_hdf5}\NormalTok{(file_name)}
\NormalTok{\}}

\NormalTok{Y_pred2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model2, X_test2)}
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(Y_pred2, }\DecValTok{1}\NormalTok{, which.max)}\OperatorTok{-}\DecValTok{1} \CommentTok{# 1..10 -> 0..9}
\KeywordTok{mean}\NormalTok{(Y_test }\OperatorTok{==}\StringTok{ }\NormalTok{Y_pred) }\CommentTok{# accuracy on the test set}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9583
\end{verbatim}

Performance metrics for each digit separately,
see also Figure \ref{fig:deep23}:

\begin{longtable}[]{@{}rrrrrrrrr@{}}
\toprule
i & Acc & Prec & Rec & F & TN & FN & FP & TP\tabularnewline
\midrule
\endhead
0 & 0.9948 & 0.96215 & 0.98571 & 0.97379 & 8982 & 14 & 38 & 966\tabularnewline
1 & 0.9962 & 0.98156 & 0.98502 & 0.98329 & 8844 & 17 & 21 & 1118\tabularnewline
2 & 0.9911 & 0.96000 & 0.95349 & 0.95673 & 8927 & 48 & 41 & 984\tabularnewline
3 & 0.9898 & 0.94773 & 0.95149 & 0.94960 & 8937 & 49 & 53 & 961\tabularnewline
4 & 0.9919 & 0.95829 & 0.95927 & 0.95878 & 8977 & 40 & 41 & 942\tabularnewline
5 & 0.9911 & 0.95470 & 0.94507 & 0.94986 & 9068 & 49 & 40 & 843\tabularnewline
6 & 0.9920 & 0.94888 & 0.96868 & 0.95868 & 8992 & 30 & 50 & 928\tabularnewline
7 & 0.9906 & 0.95517 & 0.95331 & 0.95424 & 8926 & 48 & 46 & 980\tabularnewline
8 & 0.9899 & 0.95421 & 0.94148 & 0.94780 & 8982 & 57 & 44 & 917\tabularnewline
9 & 0.9892 & 0.95643 & 0.93558 & 0.94589 & 8948 & 65 & 43 & 944\tabularnewline
\bottomrule
\end{longtable}

\begin{figure}
\hypertarget{fig:deep23}{%
\centering
\includegraphics{05-classification-nnets-figures/deep23-1.pdf}
\caption{Performance metrics for a 2-layer net 784-800-10 {[}relu{]} on MNIST}\label{fig:deep23}
}
\end{figure}

\hypertarget{example-in-r---6-layers}{%
\subsection{Example in R - 6 Layers}\label{example-in-r---6-layers}}

How about a 6-layer \emph{Deep} Neural Network
like 784-2500-2000-1500-1000-500-10? Here you are:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file_name <-}\StringTok{ "datasets/mnist_keras_model3.h5"}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{file.exists}\NormalTok{(file_name)) \{}
    \KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{    model3 <-}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{()}
    \KeywordTok{layer_dense}\NormalTok{(model3, }\DataTypeTok{units=}\DecValTok{2500}\NormalTok{, }\DataTypeTok{activation=}\StringTok{"relu"}\NormalTok{)}
    \KeywordTok{layer_dense}\NormalTok{(model3, }\DataTypeTok{units=}\DecValTok{2000}\NormalTok{, }\DataTypeTok{activation=}\StringTok{"relu"}\NormalTok{)}
    \KeywordTok{layer_dense}\NormalTok{(model3, }\DataTypeTok{units=}\DecValTok{1500}\NormalTok{, }\DataTypeTok{activation=}\StringTok{"relu"}\NormalTok{)}
    \KeywordTok{layer_dense}\NormalTok{(model3, }\DataTypeTok{units=}\DecValTok{1000}\NormalTok{, }\DataTypeTok{activation=}\StringTok{"relu"}\NormalTok{)}
    \KeywordTok{layer_dense}\NormalTok{(model3, }\DataTypeTok{units=}\DecValTok{500}\NormalTok{,  }\DataTypeTok{activation=}\StringTok{"relu"}\NormalTok{)}
    \KeywordTok{layer_dense}\NormalTok{(model3, }\DataTypeTok{units=}\DecValTok{10}\NormalTok{,   }\DataTypeTok{activation=}\StringTok{"softmax"}\NormalTok{)}
    \KeywordTok{compile}\NormalTok{(model3, }\DataTypeTok{optimizer=}\StringTok{"sgd"}\NormalTok{,}
            \DataTypeTok{loss=}\StringTok{"categorical_crossentropy"}\NormalTok{)}
    \KeywordTok{fit}\NormalTok{(model3, X_train2, Y_train2, }\DataTypeTok{epochs=}\DecValTok{10}\NormalTok{)}
    \KeywordTok{save_model_hdf5}\NormalTok{(model3, file_name)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{    model3 <-}\StringTok{ }\KeywordTok{load_model_hdf5}\NormalTok{(file_name)}
\NormalTok{\}}

\NormalTok{Y_pred2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model3, X_test2)}
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(Y_pred2, }\DecValTok{1}\NormalTok{, which.max)}\OperatorTok{-}\DecValTok{1} \CommentTok{# 1..10 -> 0..9}
\KeywordTok{mean}\NormalTok{(Y_test }\OperatorTok{==}\StringTok{ }\NormalTok{Y_pred) }\CommentTok{# accuracy on the test set}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9797
\end{verbatim}

Performance metrics for each digit separately,
see also Figure \ref{fig:deep63}.

\begin{longtable}[]{@{}rrrrrrrrr@{}}
\toprule
i & Acc & Prec & Rec & F & TN & FN & FP & TP\tabularnewline
\midrule
\endhead
0 & 0.9966 & 0.97395 & 0.99184 & 0.98281 & 8994 & 8 & 26 & 972\tabularnewline
1 & 0.9975 & 0.98856 & 0.98943 & 0.98899 & 8852 & 12 & 13 & 1123\tabularnewline
2 & 0.9951 & 0.98615 & 0.96609 & 0.97602 & 8954 & 35 & 14 & 997\tabularnewline
3 & 0.9964 & 0.99093 & 0.97327 & 0.98202 & 8981 & 27 & 9 & 983\tabularnewline
4 & 0.9960 & 0.97006 & 0.98982 & 0.97984 & 8988 & 10 & 30 & 972\tabularnewline
5 & 0.9962 & 0.98856 & 0.96861 & 0.97848 & 9098 & 28 & 10 & 864\tabularnewline
6 & 0.9963 & 0.98019 & 0.98121 & 0.98070 & 9023 & 18 & 19 & 940\tabularnewline
7 & 0.9961 & 0.98338 & 0.97860 & 0.98098 & 8955 & 22 & 17 & 1006\tabularnewline
8 & 0.9939 & 0.96065 & 0.97741 & 0.96896 & 8987 & 22 & 39 & 952\tabularnewline
9 & 0.9953 & 0.97436 & 0.97919 & 0.97677 & 8965 & 21 & 26 & 988\tabularnewline
\bottomrule
\end{longtable}

\begin{figure}
\hypertarget{fig:deep63}{%
\centering
\includegraphics{05-classification-nnets-figures/deep63-1.pdf}
\caption{Performance metrics for a 6-layer net 784-2500-2000-1500-1000-500-10 {[}relu{]} on MNIST}\label{fig:deep63}
}
\end{figure}

\begin{exercise}

Test the performance of different neural network
architectures (different number of layers, different number
of neurons in each layer etc.). Yes, it's more art than science!
Many tried to come up with various ``rules of thumb'',
see, for example, the \texttt{comp.ai.neural-nets} FAQ (Sarle \& others \protect\hyperlink{ref-aifaq}{2002}) at
\url{http://www.faqs.org/faqs/ai-faq/neural-nets/part3/preamble.html},
but what works well in one problem might not be generalisable
to another one.

\end{exercise}

\hypertarget{preprocessing-of-data}{%
\section{Preprocessing of Data}\label{preprocessing-of-data}}

\hypertarget{introduction-9}{%
\subsection{Introduction}\label{introduction-9}}

Do not underestimate the power of appropriate data preprocessing ---
deep neural networks are not a universal replacement for a data engineer's hard work!

On top of that, they are not interpretable -- these are merely black-boxes.

Among the typical transformations of the input images we can find:

\begin{itemize}
\tightlist
\item
  normalisation of colours (setting brightness, stretching contrast, etc.),
\item
  repositioning of the image (centring),
\item
  deskewing (see below),
\item
  denoising (e.g., by blurring).
\end{itemize}

Another frequently applied technique concerns an expansion of the training data
--- we can add ``artificially contaminated'' images to the training
set (e.g., slightly rotated digits) so as to be more ready to whatever
will be provided in the test test.

\hypertarget{image-deskewing}{%
\subsection{Image Deskewing}\label{image-deskewing}}

Deskewing of images (``straightening'' of the digits)
is amongst the most typical transformations
that can be applied on MNIST.

Unfortunately, we don't have (yet) the necessary
mathematical background to discuss this operation
in very detail.

Luckily, we can apply it on each image anyway.

See the GitHub repository at \url{https://github.com/gagolews/Playground.R}
for an example notebook and the \texttt{deskew.R} script.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# See https://github.com/gagolews/Playground.R}
\KeywordTok{source}\NormalTok{(}\StringTok{"~/R/Playground.R/deskew.R"}\NormalTok{)}
\CommentTok{# new_image <- deskew(old_image)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:deskew2}{%
\centering
\includegraphics{05-classification-nnets-figures/deskew2-1.pdf}
\caption{Deskewing of the MNIST digits}\label{fig:deskew2}
}
\end{figure}

Let's take a look at Figure \ref{fig:deskew2}.
In each pair, the left image (black background) is the original one,
and the right image (palette inverted for purely dramatic effects)
is its deskewed version.

Below we deskew each image in the training as well as in the test sample.
This also takes a long time, so let's store the resulting objects
for further reference:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file_name <-}\StringTok{ "datasets/mnist_deskewed_train.rds"}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{file.exists}\NormalTok{(file_name)) \{}
\NormalTok{    Z_train <-}\StringTok{ }\NormalTok{X_train}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(Z_train)[}\DecValTok{1}\NormalTok{]) \{}
\NormalTok{        Z_train[i,,] <-}\StringTok{ }\KeywordTok{deskew}\NormalTok{(Z_train[i,,])}
\NormalTok{    \}}
\NormalTok{    Z_train2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(Z_train, }\DataTypeTok{ncol=}\DecValTok{28}\OperatorTok{*}\DecValTok{28}\NormalTok{)}
    \KeywordTok{saveRDS}\NormalTok{(Z_train2, file_name)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{    Z_train2 <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(file_name)}
\NormalTok{\}}

\NormalTok{file_name <-}\StringTok{ "datasets/mnist_deskewed_test.rds"}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{file.exists}\NormalTok{(file_name)) \{}
\NormalTok{    Z_test <-}\StringTok{ }\NormalTok{X_test}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(Z_test)[}\DecValTok{1}\NormalTok{]) \{}
\NormalTok{        Z_test[i,,] <-}\StringTok{ }\KeywordTok{deskew}\NormalTok{(Z_test[i,,])}
\NormalTok{    \}}
\NormalTok{    Z_test2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(Z_test, }\DataTypeTok{ncol=}\DecValTok{28}\OperatorTok{*}\DecValTok{28}\NormalTok{)}
    \KeywordTok{saveRDS}\NormalTok{(Z_test2, file_name)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{    Z_test2 <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(file_name)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\item[Remark.]
RDS in a compressed file format used by R for object serialisation
(quickly storing its verbatim copies so that they can be reloaded
at any time).
\end{description}

Multinomial logistic regression model (1-layer NN):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file_name <-}\StringTok{ "datasets/mnist_keras_model1d.h5"}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{file.exists}\NormalTok{(file_name)) \{}
    \KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{    model1d <-}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{()}
    \KeywordTok{layer_dense}\NormalTok{(model1d, }\DataTypeTok{units=}\DecValTok{10}\NormalTok{, }\DataTypeTok{activation=}\StringTok{"softmax"}\NormalTok{)}
    \KeywordTok{compile}\NormalTok{(model1d, }\DataTypeTok{optimizer=}\StringTok{"sgd"}\NormalTok{,}
            \DataTypeTok{loss=}\StringTok{"categorical_crossentropy"}\NormalTok{)}
    \KeywordTok{fit}\NormalTok{(model1d, Z_train2, Y_train2, }\DataTypeTok{epochs=}\DecValTok{10}\NormalTok{)}
    \KeywordTok{save_model_hdf5}\NormalTok{(model1d, file_name)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{    model1d <-}\StringTok{ }\KeywordTok{load_model_hdf5}\NormalTok{(file_name)}
\NormalTok{\}}

\NormalTok{Y_pred2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model1d, Z_test2)}
\NormalTok{Y_pred <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(Y_pred2, }\DecValTok{1}\NormalTok{, which.max)}\OperatorTok{-}\DecValTok{1} \CommentTok{# 1..10 -> 0..9}
\KeywordTok{mean}\NormalTok{(Y_test }\OperatorTok{==}\StringTok{ }\NormalTok{Y_pred) }\CommentTok{# accuracy on the test set}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9488
\end{verbatim}

Performance metrics for each digit separately,
see also Figure \ref{fig:deskew6}.

\begin{longtable}[]{@{}rrrrrrrrr@{}}
\toprule
i & Acc & Prec & Rec & F & TN & FN & FP & TP\tabularnewline
\midrule
\endhead
0 & 0.9939 & 0.95450 & 0.98469 & 0.96936 & 8974 & 15 & 46 & 965\tabularnewline
1 & 0.9959 & 0.98236 & 0.98150 & 0.98193 & 8845 & 21 & 20 & 1114\tabularnewline
2 & 0.9878 & 0.95409 & 0.92636 & 0.94002 & 8922 & 76 & 46 & 956\tabularnewline
3 & 0.9904 & 0.95069 & 0.95446 & 0.95257 & 8940 & 46 & 50 & 964\tabularnewline
4 & 0.9888 & 0.94118 & 0.94501 & 0.94309 & 8960 & 54 & 58 & 928\tabularnewline
5 & 0.9905 & 0.94426 & 0.94955 & 0.94690 & 9058 & 45 & 50 & 847\tabularnewline
6 & 0.9905 & 0.95565 & 0.94468 & 0.95013 & 9000 & 53 & 42 & 905\tabularnewline
7 & 0.9892 & 0.96000 & 0.93385 & 0.94675 & 8932 & 68 & 40 & 960\tabularnewline
8 & 0.9855 & 0.91162 & 0.94251 & 0.92680 & 8937 & 56 & 89 & 918\tabularnewline
9 & 0.9851 & 0.92914 & 0.92270 & 0.92591 & 8920 & 78 & 71 & 931\tabularnewline
\bottomrule
\end{longtable}

\begin{figure}
\hypertarget{fig:deskew6}{%
\centering
\includegraphics{05-classification-nnets-figures/deskew6-1.pdf}
\caption{Performance of Multinomial Logistic Regression on the deskewed MNIST}\label{fig:deskew6}
}
\end{figure}

\hypertarget{summary-of-all-the-models-considered}{%
\subsection{Summary of All the Models Considered}\label{summary-of-all-the-models-considered}}

Let's summarise the quality of all the considered classifiers.
Figure \ref{fig:globalsum} gives the F-measures, for each digit
separately.

\begin{figure}
\hypertarget{fig:globalsum}{%
\centering
\includegraphics{05-classification-nnets-figures/globalsum-1.pdf}
\caption{Summary of F-measures for each classified digit and every method}\label{fig:globalsum}
}
\end{figure}

Note that the applied preprocessing of data increased
the prediction accuracy.

The same information can also be included on a heat map
which is depicted in Figure \ref{fig:globalsum2}
(see the \texttt{image()} function in R).

\begin{figure}
\hypertarget{fig:globalsum2}{%
\centering
\includegraphics{05-classification-nnets-figures/globalsum2-1.pdf}
\caption{A heat map of F-measures for each classified digit and each method}\label{fig:globalsum2}
}
\end{figure}

\hypertarget{outro-4}{%
\section{Outro}\label{outro-4}}

\hypertarget{remarks-4}{%
\subsection{Remarks}\label{remarks-4}}

We have discussed a multinomial logistic regression model
as a generalisation of the binary one.

This in turn is a special case of feed-forward neural networks.

There's a lot of hype (again\ldots{}) for deep neural networks in many applications,
including vision, self-driving cars, natural language processing,
speech recognition etc.

Many different architectures of neural networks and types of units
are being considered in theory and in practice, e.g.:

\begin{itemize}
\tightlist
\item
  convolutional neural networks apply a series of signal (e.g., image)
  transformations in first layers, they might actually ``discover''
  deskewing automatically etc.;
\item
  recurrent neural networks can imitate long/short-term memory
  that can be used for speech synthesis and time series prediction.
\end{itemize}

Main drawbacks of deep neural networks:

\begin{itemize}
\tightlist
\item
  learning is very slow, especially with very deep architectures (days, weeks);
\item
  models are not explainable (black boxes) and hard to debug;
\item
  finding good architectures is more art than science
  (maybe: more of a craftsmanship even);
\item
  sometimes using deep neural network is just an excuse for being too lazy
  to do proper data cleansing and pre-processing.
\end{itemize}

There are many issues and challenges that are tackled in more advanced
AI/ML courses and books, such as (Goodfellow et al. \protect\hyperlink{ref-deeplearn}{2016}).

\hypertarget{beyond-mnist}{%
\subsection{Beyond MNIST}\label{beyond-mnist}}

The MNIST dataset is a classic, although its use in deep learning
research is nowadays discouraged
-- the dataset is not considered challenging anymore -- state of the art classifiers
can reach \(99.8\%\) accuracy.

See Zalando's Fashion-MNIST (by Kashif Rasul \& Han Xiao) at
\url{https://github.com/zalandoresearch/fashion-mnist} for a modern replacement.

Alternatively, take a look at CIFAR-10 and CIFAR-100 (\url{https://www.cs.toronto.edu/~kriz/cifar.html})
by A. Krizhevsky et al.
or at ImageNet (\url{http://image-net.org/index}) for an even greater challenge.

\hypertarget{further-reading-4}{%
\subsection{Further Reading}\label{further-reading-4}}

Recommended further reading: (James et al. \protect\hyperlink{ref-islr}{2017}: Chapter 11), (Sarle \& others \protect\hyperlink{ref-aifaq}{2002}) and (Goodfellow et al. \protect\hyperlink{ref-deeplearn}{2016})

See also the \texttt{keras} package tutorials available at:
\url{https://cran.r-project.org/web/packages/keras/index.html}
and \url{https://keras.rstudio.com}.

\hypertarget{continuous-optimisation-with-iterative-algorithms}{%
\chapter{Continuous Optimisation with Iterative Algorithms}\label{continuous-optimisation-with-iterative-algorithms}}

\hypertarget{introduction-10}{%
\section{Introduction}\label{introduction-10}}

\hypertarget{optimisation-problems}{%
\subsection{Optimisation Problems}\label{optimisation-problems}}

\textbf{Mathematical optimisation} (a.k.a. mathematical programming)
deals with the study of algorithms to solve problems related
to selecting the \emph{best} element amongst the set of available alternatives.

Most frequently ``best'' is expressed in terms
of an \emph{error} or \emph{goodness of fit} measure:
\[
f:\mathbb{D}\to\mathbb{R}
\]
called an \textbf{objective function}, where
\(\mathbb{D}\) is the \textbf{search space} (problem domain, feasible set).

An \textbf{optimisation task} deals with finding an element \(\mathbf{x}\in \mathbb{D}\)
amongst the set of possible candidate solutions,
that minimises or maximises \(f\):

\[
\min_{\mathbf{x}\in \mathbb{D}} f(\mathbf{x})
\quad\text{or}\quad\max_{\mathbf{x}\in \mathbb{D}} f(\mathbf{x}),
\]

In this chapter we will deal with \textbf{unconstrained continuous optimisation},
i.e., we will assume the search space is \(\mathbb{D}=\mathbb{R}^p\) for some \(p\) --
we'll be optimising over \(p\) real-valued parameters.

\hypertarget{types-of-minima-and-maxima}{%
\subsection{Types of Minima and Maxima}\label{types-of-minima-and-maxima}}

Note that minimising \(f\) is the same as maximising \(\bar{f}=-f\).

In other words, \(\min_{\mathbf{x}\in \mathbb{D}} f(\mathbf{x})\)
and \(\max_{\mathbf{x}\in \mathbb{D}} -f(\mathbf{x})\)
represent the same optimisation problems
(and hence have identical solutions).

\begin{description}
\item[Definition.]
A \textbf{minimum} of \(f\) is a point \(\mathbf{x}^*\) such that
\(f(\mathbf{x}^*)\le f(\mathbf{x})\) for all \(\mathbf{x}\in \mathbb{D}\).
On the other hand, a \textbf{maximum} of \(f\) is a point \(\mathbf{x}^*\) such that
\(f(\mathbf{x}^*)\ge f(\mathbf{x})\) for all \(\mathbf{x}\in \mathbb{D}\).
\end{description}

Assuming that \(\mathbb{D}=\mathbb{R}\), Figure \ref{fig:f_global_minimum}
shows an example objective function, \(f:\mathbb{D}\to\mathbb{R}\),
that has a minimum at \({x}^*=1\)
with \(f(x^*)=-2\).

\begin{figure}
\hypertarget{fig:f_global_minimum}{%
\centering
\includegraphics{06-optimisation-iterative-figures/f_global_minimum-1.pdf}
\caption{A function with the global minimum at \({x}^*=1\)}\label{fig:f_global_minimum}
}
\end{figure}

\begin{description}
\item[Remark.]
We can denote these two facts as follows:

\begin{itemize}
\tightlist
\item
  \((\min_{x\in \mathbb{R}} f(x))=-2\) (value of \(f\) at the minimum is \(-2\)),
\item
  \((\mathrm{arg}\min_{x\in \mathbb{R}} f(x))=1\) (location of the minimum,
  i.e.,~\emph{argument minimum}, is \(1\)).
\end{itemize}
\end{description}

By definition, a minimum/maximum \emph{might not necessarily be unique}.
This depends on a problem.

Assuming that \(\mathbb{D}=\mathbb{R}\), Figure \ref{fig:f_global_minimum_not_unique}
gives an example objective function, \(f:\mathbb{D}\to\mathbb{R}\),
that has multiple minima; every \({x}^*\in[1-\sqrt{2},1+\sqrt{2}]\)
yields \(f(x^*)=0\).

\begin{figure}
\hypertarget{fig:f_global_minimum_not_unique}{%
\centering
\includegraphics{06-optimisation-iterative-figures/f_global_minimum_not_unique-1.pdf}
\caption{A function that has multiple minima}\label{fig:f_global_minimum_not_unique}
}
\end{figure}

\begin{description}
\item[Remark.]
If this was the case of some machine learning problem, it'd mean
that we could have many equally well-performing models,
and hence many equivalent explanations of the same phenomenon.
\end{description}

Moreover, it may happen that a function has \emph{multiple local minima},
compare Figure \ref{fig:f_global_local_minima}.

\begin{figure}
\hypertarget{fig:f_global_local_minima}{%
\centering
\includegraphics{06-optimisation-iterative-figures/f_global_local_minima-1.pdf}
\caption{A function with two local minima}\label{fig:f_global_local_minima}
}
\end{figure}

\begin{description}
\item[Definition.]
We say that \(f\) has a \textbf{local minimum}
at \(\mathbf{x}^+\in \mathbb{D}\),
if for some neighbourhood \(B(\mathbf{x}^+)\) of \(\mathbf{x}^+\)
it holds \(f(\mathbf{x}^+) \le f(\mathbf{x})\) for each
\(\mathbf{x}\in B(\mathbf{x}^+)\).

If \(\mathbb{D}=\mathbb{R}\), by neighbourhood \(B(x)\) of \(x\)
we mean an open interval centred at \(x\) of width \(2r\)
for some small \(r>0\), i.e., \((x-r, x+r)\)
\item[Definition.]
(*) If \(\mathbb{D}=\mathbb{R}^p\) (for any \(p\ge 1\)), by neighbourhood \(B(\mathbf{x})\) of \(\mathbf{x}\)
we mean an \emph{open ball} centred at \(\mathbf{x}^+\) of some small radius \(r>0\),
i.e., \(\{\mathbf{y}: \|\mathbf{x}-\mathbf{y}\|<r\}\)
(read: the set of all the points with Euclidean distances
to \(\mathbf{x}\) less than \(r\)).
\end{description}

To avoid ambiguity, the ``true'' minimum (a point \(\mathbf{x}^*\) such that
\(f(\mathbf{x}^*)\le f({x})\) for all \(\mathbf{x}\in \mathbb{D}\))
is sometimes also referred to as
a \textbf{global} minimum.

\begin{description}
\item[Remark.]
Of course, the global minimum is also a function's local minimum.
\end{description}

The existence of local minima is problematic
as most of the optimisation methods might get stuck there
and fail to return the global one.

Moreover, we cannot often be sure if the result returned by an algorithm
is indeed a global minimum. Maybe there exists a better solution
that hasn't been considered yet? Or maybe the function
is very noisy (see Figure \ref{fig:smooth_vs_nonsmooth})?

\begin{figure}
\hypertarget{fig:smooth_vs_nonsmooth}{%
\centering
\includegraphics{06-optimisation-iterative-figures/smooth_vs_nonsmooth-1.pdf}
\caption{Smooth vs.~non-smooth vs.~noisy objective functions}\label{fig:smooth_vs_nonsmooth}
}
\end{figure}

\clearpage

\hypertarget{example-objective-over-a-2d-domain}{%
\subsection{Example Objective over a 2D Domain}\label{example-objective-over-a-2d-domain}}

Of course, our objective function does not necessarily have to be defined
over a one-dimensional domain.

For example, consider the following function:
\[
g(x_1,x_2)=\log\left((x_1^{2}+x_2-5)^{2}+(x_1+x_2^{2}-3)^{2}+x_1^2-1.60644\dots\right)
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g  <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x1, x2)}
    \KeywordTok{log}\NormalTok{((x1}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{x2}\DecValTok{-5}\NormalTok{)}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{(x1}\OperatorTok{+}\NormalTok{x2}\OperatorTok{^}\DecValTok{2-3}\NormalTok{)}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{x1}\OperatorTok{^}\DecValTok{2}\FloatTok{-1.60644366086443841}\NormalTok{)}
\NormalTok{x1 <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DataTypeTok{length.out=}\DecValTok{100}\NormalTok{)}
\NormalTok{x2 <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DataTypeTok{length.out=}\DecValTok{100}\NormalTok{)}
\CommentTok{# outer() expands two vectors to form a 2D grid}
\CommentTok{# and applies a given function on each point}
\NormalTok{y  <-}\StringTok{ }\KeywordTok{outer}\NormalTok{(x1, x2, g)}
\end{Highlighting}
\end{Shaded}

There are four local minima:

\begin{longtable}[]{@{}rrr@{}}
\toprule
x1 & x2 & f(x1,x2)\tabularnewline
\midrule
\endhead
2.2780 & -0.61343 & 1.3564\tabularnewline
-2.6123 & -2.34546 & 1.7051\tabularnewline
1.7988 & 1.19879 & 0.6955\tabularnewline
-1.5423 & 2.15641 & 0.0000\tabularnewline
\bottomrule
\end{longtable}

The global minimum is at \(\mathbf{x}^*=(x_1^*, x_2^*)\) as below:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{g}\NormalTok{(}\OperatorTok{-}\FloatTok{1.542255693195422641930153}\NormalTok{, }\FloatTok{2.156405289793087261832605}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

Let's explore various ways of depicting \(f\) first.
A contour plot and a heat map
are given in Figure \ref{fig:contour_g}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{# 2 in 1}
\CommentTok{# lefthand plot:}
\KeywordTok{contour}\NormalTok{(x1, x2, y, }\DataTypeTok{nlevels=}\DecValTok{25}\NormalTok{)}
\KeywordTok{points}\NormalTok{(}\OperatorTok{-}\FloatTok{1.54226}\NormalTok{, }\FloatTok{2.15641}\NormalTok{, }\DataTypeTok{col=}\DecValTok{2}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{3}\NormalTok{)}
\CommentTok{# righthand plot:}
\KeywordTok{image}\NormalTok{(x1, x2, y)}
\KeywordTok{contour}\NormalTok{(x1, x2, y, }\DataTypeTok{add=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:contour_g}{%
\centering
\includegraphics{06-optimisation-iterative-figures/contour_g-1.pdf}
\caption{A contour plot and a heat map of \(g(x_1,x_2)\)}\label{fig:contour_g}
}
\end{figure}

Two perspective plots (views from different angles) are given
in Figure \ref{fig:perspective_g}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)) }\CommentTok{# 2 in 1}
\KeywordTok{persp}\NormalTok{(x1, x2, y, }\DataTypeTok{phi=}\DecValTok{30}\NormalTok{, }\DataTypeTok{theta=}\OperatorTok{-}\DecValTok{5}\NormalTok{, }\DataTypeTok{shade=}\DecValTok{2}\NormalTok{, }\DataTypeTok{border=}\OtherTok{NA}\NormalTok{)}
\KeywordTok{persp}\NormalTok{(x1, x2, y, }\DataTypeTok{phi=}\DecValTok{30}\NormalTok{, }\DataTypeTok{theta=}\DecValTok{75}\NormalTok{, }\DataTypeTok{shade=}\DecValTok{2}\NormalTok{, }\DataTypeTok{border=}\OtherTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:perspective_g}{%
\centering
\includegraphics{06-optimisation-iterative-figures/perspective_g-1.pdf}
\caption{Perspective plots of \(g(x_1,x_2)\)}\label{fig:perspective_g}
}
\end{figure}

\begin{description}
\item[Remark.]
As usual, depicting functions that are defined over
high-dimensional (3D and higher) domains
is\ldots{} difficult.
Usually 1D or 2D projections can give us some neat intuitions though.
\end{description}

\hypertarget{example-optimisation-problems-in-machine-learning}{%
\subsection{Example Optimisation Problems in Machine Learning}\label{example-optimisation-problems-in-machine-learning}}

In \textbf{multiple linear regression} we were minimising
the sum of squared residuals
\[
\min_{\boldsymbol\beta\in\mathbb{R}^p}
\sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_i \right)^2.
\]

In \textbf{binary logistic regression} we were minimising the cross-entropy:
\[
\min_{\boldsymbol\beta\in\mathbb{R}^p}
-\frac{1}{n} \sum_{i=1}^n
\left(\begin{array}{r}
     y_i \log \left(\frac{1}{1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}}}                                                        \right)\\
+ (1-y_i)\log \left(\frac{e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}}}{1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}}\right)
\end{array}\right).
\]

\hypertarget{iterative-methods}{%
\section{Iterative Methods}\label{iterative-methods}}

\hypertarget{introduction-11}{%
\subsection{Introduction}\label{introduction-11}}

Many optimisation algorithms are built around the following scheme:

\begin{quote}
\emph{Starting from a random point, perform a walk,
in each step deciding where to go based on the idea
of where the location of the minimum might be.}
\end{quote}

\begin{description}
\item[Example.]
Imagine we're to cycle from Deakin University's Burwood
Campus to the CBD not knowing the route and with GPS disabled --
we'll have to ask many people along the way, but we'll eventually
(because most people are good) get to some CBD
(say, in Perth).
\end{description}

More formally, we are interested in iterative
algorithms that operate in a greedy-like manner:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\mathbf{x}^{(0)}\) -- initial guess (e.g., generated at random)
\item
  for \(i=1,...,M\):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \(\mathbf{x}^{(i)} = \mathbf{x}^{(i-1)}+\text{[guessed direction]}\)
  \item
    if \(|f(\mathbf{x}^{(i)})-f(\mathbf{x}^{(i-1)})| < \varepsilon\) break
  \end{enumerate}
\item
  return \(\mathbf{x}^{(i)}\) as result
\end{enumerate}

\bigskip

Note that there are two stopping criteria, based on:

\begin{itemize}
\tightlist
\item
  \(M\) = maximum number of iterations,
\item
  \(\varepsilon\) = tolerance, e.g, \(10^{-8}\).
\end{itemize}

\hypertarget{example-in-r-4}{%
\subsection{Example in R}\label{example-in-r-4}}

R has a built-in function, \texttt{optim()}, that provides an implementation
of (amongst others) \textbf{the BFGS method}
(proposed by Broyden, Fletcher, Goldfarb and Shanno in 1970).

\begin{description}
\item[Remark.]
(*) BFGS uses the assumption that the objective function
is smooth -- the {[}guessed direction{]} is determined by computing
the (partial) derivatives (or their finite-difference approximations).
However, they might work well even if this is not the case.
We'll be able to derive similar algorithms (called quasi-Newton ones) ourselves
once we learn about Taylor series approximation
by reading a book/taking a course on calculus.
\end{description}

Here, we shall use the BFGS as a \emph{black-box} continuous optimisation method,
i.e., without going into how it has been defined (in terms of our assumed math
skills, it might be too early for this).
Despite that, will still be able to point out a few interesting
patterns.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{optim}\NormalTok{(par, fn, }\DataTypeTok{method=}\StringTok{"BFGS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

where:

\begin{itemize}
\tightlist
\item
  \texttt{par} -- an initial guess (a numeric vector of length \(p\))
\item
  \texttt{fn} -- an objective function to minimise (takes a vector of length \(p\)
  on input, returns a single number)
\end{itemize}

Let us minimise the \(g\) function defined above (the one with the 2D domain):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# g needs to be rewritten to accept a 2-ary vector}
\NormalTok{g_vectorised <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x12) }\KeywordTok{g}\NormalTok{(x12[}\DecValTok{1}\NormalTok{], x12[}\DecValTok{2}\NormalTok{])}
\CommentTok{# random starting point with coordinates in [-5, 5]}
\NormalTok{(x12_init <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{-5}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -2.1242  2.8831
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(res <-}\StringTok{ }\KeywordTok{optim}\NormalTok{(x12_init, g_vectorised, }\DataTypeTok{method=}\StringTok{"BFGS"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $par
## [1] -1.5423  2.1564
## 
## $value
## [1] 1.4131e-12
## 
## $counts
## function gradient 
##      101       21 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
\end{verbatim}

Note that:

\begin{itemize}
\tightlist
\item
  \texttt{par} gives the location of the local minimum found,
\item
  \texttt{value} gives the value of \(g\) at \texttt{par},
\item
  \texttt{convergence} of 0 is a successful one (we were able
  to satisfy the
  \(|f(\mathbf{x}^{(i)})-f(\mathbf{x}^{(i-1)})| < \varepsilon\) condition).
\end{itemize}

We can even depict the points that the algorithm is ``visiting'',
see Figure \ref{fig:gbfgsvisit}.

\begin{description}
\item[Remark.]
(*) Technically, the algorithm needs to evaluate a few more points
in order to make the decision on where to go next (BFGS approximates the
gradient and the Hessian matrix).
\end{description}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g_vectorised_plot <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x12) \{}
    \KeywordTok{points}\NormalTok{(x12[}\DecValTok{1}\NormalTok{], x12[}\DecValTok{2}\NormalTok{], }\DataTypeTok{col=}\DecValTok{2}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{3}\NormalTok{) }\CommentTok{# draw}
    \KeywordTok{g}\NormalTok{(x12[}\DecValTok{1}\NormalTok{], x12[}\DecValTok{2}\NormalTok{]) }\CommentTok{# return value}
\NormalTok{\}}
\KeywordTok{contour}\NormalTok{(x1, x2, y, }\DataTypeTok{nlevels=}\DecValTok{25}\NormalTok{)}
\NormalTok{res <-}\StringTok{ }\KeywordTok{optim}\NormalTok{(x12_init, g_vectorised_plot, }\DataTypeTok{method=}\StringTok{"BFGS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:gbfgsvisit}{%
\centering
\includegraphics{06-optimisation-iterative-figures/gbfgsvisit-1.pdf}
\caption{Each plotting symbol marks a point where the objective function was evaluated by the BFGS method}\label{fig:gbfgsvisit}
}
\end{figure}

\hypertarget{convergence-to-local-optima}{%
\subsection{Convergence to Local Optima}\label{convergence-to-local-optima}}

We were lucky, because the local minimum that the algorithm has found
coincides with the global minimum.

Let's see where does the BFGS algorithm converge if seek the minimum
of the above \(g\) starting
from many randomly chosen points
uniformly distributed over the square \([-5,5]\times[-5,5]\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res_value <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(}\DecValTok{1000}\NormalTok{, \{}
    \CommentTok{# this will be iterated 100 times}
\NormalTok{    x12_init <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{-5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{    res <-}\StringTok{ }\KeywordTok{optim}\NormalTok{(x12_init, g_vectorised, }\DataTypeTok{method=}\StringTok{"BFGS"}\NormalTok{)}
\NormalTok{    res}\OperatorTok{$}\NormalTok{value }\CommentTok{# return value from each iteration}
\NormalTok{\})}
\KeywordTok{table}\NormalTok{(}\KeywordTok{round}\NormalTok{(res_value,}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##     0 0.695 1.356 1.705 
##   273   352   156   219
\end{verbatim}

Unfortunately, we find the global minimum only in \(\sim 25\%\) cases,
compare Figure \ref{fig:bfgs_multi_hist}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(res_value, }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{, }\DataTypeTok{breaks=}\DecValTok{100}\NormalTok{, }\DataTypeTok{main=}\OtherTok{NA}\NormalTok{); }\KeywordTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:bfgs_multi_hist}{%
\centering
\includegraphics{06-optimisation-iterative-figures/bfgs_multi_hist-1.pdf}
\caption{A histogram of the objective function's value at the local minimum found when using a random initial guess}\label{fig:bfgs_multi_hist}
}
\end{figure}

Figure \ref{fig:bfgs_multi_where} depicts all the random starting points and where
do we converge from them.

\begin{figure}
\hypertarget{fig:bfgs_multi_where}{%
\centering
\includegraphics{06-optimisation-iterative-figures/bfgs_multi_where-1.pdf}
\caption{Each line segment connect a starting point to the point of BFGS's convergence; note that by starting in the neighbourhood of \((0,-4)\) we can actually end up in any of the 4 local minima}\label{fig:bfgs_multi_where}
}
\end{figure}

\hypertarget{random-restarts}{%
\subsection{Random Restarts}\label{random-restarts}}

A kind of ``remedy'' for the above limitation
could be provided by \emph{repeated local search}:
in order to robustify an optimisation
procedure it is often advised to consider
multiple random initial points
and pick the best solution amongst the identified local optima.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# N             - number of restarts}
\CommentTok{# par_generator - a function generating initial guesses}
\CommentTok{# ...           - further arguments to optim()}
\NormalTok{optim_with_restarts <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(par_generator, ..., }\DataTypeTok{N=}\DecValTok{10}\NormalTok{) \{}
\NormalTok{    res_best <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{value=}\OtherTok{Inf}\NormalTok{) }\CommentTok{# cannot be worse than this}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{N) \{}
\NormalTok{        res <-}\StringTok{ }\KeywordTok{optim}\NormalTok{(}\KeywordTok{par_generator}\NormalTok{(), ...)}
        \ControlFlowTok{if}\NormalTok{ (res}\OperatorTok{$}\NormalTok{value }\OperatorTok{<}\StringTok{ }\NormalTok{res_best}\OperatorTok{$}\NormalTok{value)}
\NormalTok{            res_best <-}\StringTok{ }\NormalTok{res }\CommentTok{# a better candidate found}
\NormalTok{    \}}
\NormalTok{    res_best}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{optim_with_restarts}\NormalTok{(}\ControlFlowTok{function}\NormalTok{() }\KeywordTok{runif}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{-5}\NormalTok{, }\DecValTok{5}\NormalTok{),}
\NormalTok{    g_vectorised, }\DataTypeTok{method=}\StringTok{"BFGS"}\NormalTok{, }\DataTypeTok{N=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $par
## [1] -1.5423  2.1564
## 
## $value
## [1] 3.9702e-13
## 
## $counts
## function gradient 
##       48       17 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
\end{verbatim}

\begin{exercise}

Food for thought:
Can we really really guarantee that the global minimum will be found within \(N\) tries?

\end{exercise}

\begin{solution}

Absolutely not.

\end{solution}

\hypertarget{gradient-descent}{%
\section{Gradient Descent}\label{gradient-descent}}

\hypertarget{function-gradient}{%
\subsection{Function Gradient (*)}\label{function-gradient}}

How to choose the {[}guessed direction{]} in our iterative optimisation algorithm?

If we are minimising a smooth function, the simplest possible choice
is to use the information included in the objective's \textbf{gradient},
which provides us with the information about the direction where the
function decreases the fastest.

\bigskip

\begin{description}
\item[Definition.]
(*) Gradient of \(f:\mathbb{R}^p\to\mathbb{R}\),
denoted \(\nabla f:\mathbb{R}^p\to\mathbb{R}^p\),
is the vector of all its partial derivatives,
(\(\nabla\) -- nabla symbol = differential operator)
\[
\nabla f(\mathbf{x}) = \left[
\begin{array}{c}
\displaystyle\frac{\partial f}{\partial x_1}(\mathbf{x})\\
\vdots\\
\displaystyle\frac{\partial f}{\partial x_p}(\mathbf{x})
\end{array}
\right]
\]
If we have a function \(f(x_1,...,x_p)\),
the partial derivative w.r.t. the \(i\)-th variable,
denoted
\(\frac{\partial f}{\partial x_i}\)
is like an ordinary derivative w.r.t. \(x_i\)
where \(x_1,...,x_{i-1},x_{i+1},...,x_p\) are assumed constant.
\item[Remark.]
Function differentiation is an important concept -- see how it's referred to
in, e.g., the \texttt{keras} package manual at \url{https://keras.rstudio.com/reference/fit.html}.
\end{description}

Recall our \(g\) function defined above:
\[
g(x_1,x_2)=\log\left((x_1^{2}+x_2-5)^{2}+(x_1+x_2^{2}-3)^{2}+x_1^2-1.60644\dots\right)
\]

It can be shown (*) that:
\[
\begin{array}{ll}
\frac{\partial g}{\partial x_1}(x_1,x_2)=&
\displaystyle\frac{
4x_1(x_1^{2}+x_2-5)+2(x_1+x_2^{2}-3)+2x_1
}{(x_1^{2}+x_2-5)^{2}+(x_1+x_2^{2}-3)^{2}+x_1^2-1.60644\dots}
\\
\frac{\partial g}{\partial x_2}(x_1,x_2)=&
\displaystyle\frac{
2(x_1^{2}+x_2-5)+4x_2(x_1+x_2^{2}-3)
}{(x_1^{2}+x_2-5)^{2}+(x_1+x_2^{2}-3)^{2}+x_1^2-1.60644\dots}
\end{array}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grad_g_vectorised <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
    \KeywordTok{c}\NormalTok{(}
        \DecValTok{4}\OperatorTok{*}\NormalTok{x[}\DecValTok{1}\NormalTok{]}\OperatorTok{*}\NormalTok{(x[}\DecValTok{1}\NormalTok{]}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{x[}\DecValTok{2}\NormalTok{]}\OperatorTok{-}\DecValTok{5}\NormalTok{)}\OperatorTok{+}\DecValTok{2}\OperatorTok{*}\NormalTok{(x[}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{x[}\DecValTok{2}\NormalTok{]}\OperatorTok{^}\DecValTok{2-3}\NormalTok{)}\OperatorTok{+}\DecValTok{2}\OperatorTok{*}\NormalTok{x[}\DecValTok{1}\NormalTok{],}
        \DecValTok{2}\OperatorTok{*}\NormalTok{(x[}\DecValTok{1}\NormalTok{]}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{x[}\DecValTok{2}\NormalTok{]}\OperatorTok{-}\DecValTok{5}\NormalTok{)}\OperatorTok{+}\DecValTok{4}\OperatorTok{*}\NormalTok{x[}\DecValTok{2}\NormalTok{]}\OperatorTok{*}\NormalTok{(x[}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{x[}\DecValTok{2}\NormalTok{]}\OperatorTok{^}\DecValTok{2-3}\NormalTok{)}
\NormalTok{    )}\OperatorTok{/}\NormalTok{(}
\NormalTok{        (x[}\DecValTok{1}\NormalTok{]}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{x[}\DecValTok{2}\NormalTok{]}\OperatorTok{-}\DecValTok{5}\NormalTok{)}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{(x[}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\NormalTok{x[}\DecValTok{2}\NormalTok{]}\OperatorTok{^}\DecValTok{2-3}\NormalTok{)}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{x[}\DecValTok{1}\NormalTok{]}\OperatorTok{^}\DecValTok{2}\FloatTok{-1.60644366086443841}
\NormalTok{    )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{three-facts-on-the-gradient}{%
\subsection{Three Facts on the Gradient}\label{three-facts-on-the-gradient}}

For now, we should emphasise three important facts:

\bigskip

\begin{description}
\item[Fact 1.]
If we are incapable of deriving the gradient analytically,
we can rely on its finite differences approximation.
Each partial derivative can be estimated by means of:
\[
\frac{\partial f}{\partial x_i}(x_1,\dots,x_p) \simeq
\frac{
f(x_1,...,x_i+\delta,...,x_p)-f(x_1,...,x_i,...,x_p)
}{
\delta
}
\]
for some small \(\delta>0\), say, \(\delta=10^{-6}\).
\item[Remark.]
(*) Actually, a function's partial derivative, by definition,
is the limit of the above as \(\delta\to 0\).
\end{description}

Example implementation:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# gradient of f at x=c(x[1],...,x[p])}
\NormalTok{grad_approx <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(f, x, }\DataTypeTok{delta=}\FloatTok{1e-6}\NormalTok{) \{}
\NormalTok{    p <-}\StringTok{ }\KeywordTok{length}\NormalTok{(x)}
\NormalTok{    gf <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(p) }\CommentTok{# vector of length p}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{p) \{}
\NormalTok{        xi <-}\StringTok{ }\NormalTok{x}
\NormalTok{        xi[i] <-}\StringTok{ }\NormalTok{xi[i]}\OperatorTok{+}\NormalTok{delta}
\NormalTok{        gf[i] <-}\StringTok{ }\KeywordTok{f}\NormalTok{(xi)}
\NormalTok{    \}}
\NormalTok{    (gf}\OperatorTok{-}\KeywordTok{f}\NormalTok{(x))}\OperatorTok{/}\NormalTok{delta}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{description}
\item[Remark.]
(*) Interestingly, some modern vector/matrix algebra frameworks
like TensorFlow (upon which \texttt{keras} is built) or PyTorch, feature
methods to ``derive'' the gradient algorithmically
(autodiff; automatic differentiation).
\end{description}

Sanity check:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{grad_approx}\NormalTok{(g_vectorised, }\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -3.1865 -1.3656
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{grad_g_vectorised}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -3.1865 -1.3656
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{grad_approx}\NormalTok{(g_vectorised, }\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{1.542255693}\NormalTok{, }\FloatTok{2.15640528979}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.0588e-05 1.9817e-05
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{grad_g_vectorised}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{1.542255693}\NormalTok{, }\FloatTok{2.15640528979}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.1292e-09 3.5771e-10
\end{verbatim}

By the way, there is also the \texttt{grad()} function in package \texttt{numDeriv}
that might be a little more accurate (uses a different approximation
formula).

\begin{description}
\item[Fact 2.]
The gradient of \(f\) at \(\mathbf{x}\), \(\nabla f(\mathbf{x})\),
is a vector that points in the direction of the steepest slope.
On the other hand, minus gradient, \(-\nabla f(\mathbf{x})\), is the direction where the function decreases the fastest.
\item[Remark.]
(*) This can be shown by considering a function's first-order Taylor series approximation.
\end{description}

Each gradient is a vector, therefore it can be depicted as an arrow.
Figure \ref{fig:gradplots} illustrates a few scaled gradients of the \(g\)
function at different points -- each arrow connects a point
\(\mathbf{x}\) to \(\mathbf{x}\pm 0.1\nabla f(\mathbf{x})\).

\begin{figure}
\hypertarget{fig:gradplots}{%
\centering
\includegraphics{06-optimisation-iterative-figures/gradplots-1.pdf}
\caption{Scaled radients (pink arrows) and minus gradients (blue arrows) of \(g(x_1,x_2)\) at different points}\label{fig:gradplots}
}
\end{figure}

Note that the blue arrows point more or less in the direction of the local minimum.
Therefore, in our iterative algorithm,
we may try taking the direction of the minus gradient!
How far should we go in that direction? Well, a bit.
We will refer to the desired step size as the \textbf{learning rate}, \(\eta\).

\bigskip

This will be called the \textbf{gradient descent} method (GD;
Cauchy, 1847).

\begin{description}
\item[Fact 3.]
If a function \(f\) has a local minimum at \(\mathbf{x}^*\),
then its gradient vanishes there, i.e.,
\(\nabla {f}(\mathbf{x}^*)=[0,\dots,0]\).
\end{description}

\bigskip

Note that the above condition is a necessary, not sufficient
one. For example, the gradient also vanishes at a maximum or
at a saddle point. In fact, we have what follows.

\begin{description}
\item[Theorem.]
(***) More generally, a twice-differentiable function
has a local minimum at \(\mathbf{x}^*\) if and only if
its gradient vanishes there and \(\nabla^2 {f}(\mathbf{x}^*)\)
(Hessian matrix = matrix of all second-order derivatives)
is positive-definite.
\end{description}

\hypertarget{gradient-descent-algorithm-gd}{%
\subsection{Gradient Descent Algorithm (GD)}\label{gradient-descent-algorithm-gd}}

Taking the above into account, we arrive at the gradient descent algorithm:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\mathbf{x}^{(0)}\) -- initial guess (e.g., generated at random)
\item
  for \(i=1,...,M\):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \(\mathbf{x}^{(i)} = \mathbf{x}^{(i-1)}-\eta \nabla f(\mathbf{x}^{(i-1)})\)
  \item
    if \(|f(\mathbf{x}^{(i)})-f(\mathbf{x}^{(i-1)})| < \varepsilon\) break
  \end{enumerate}
\item
  return \(\mathbf{x}^{(i)}\) as result
\end{enumerate}

where \(\eta>0\) is a step size frequently
referred to as the \emph{learning rate}, because that's much more cool.
We usually set \(\eta\) of small order of magnitude, say \(0.01\) or \(0.1\).

An implementation of the gradient descent algorithm is straightforward.
In essence, it's the \texttt{par\ \textless{}-\ par\ -\ eta*grad\_g\_vectorised(par)} expression
run in a loop, until convergence.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# par   - initial guess}
\CommentTok{# fn    - a function to be minimised}
\CommentTok{# gr    - a function to return the gradient of fn}
\CommentTok{# eta   - learning rate}
\CommentTok{# maxit - maximum number of iterations}
\CommentTok{# tol   - convergence tolerance}
\NormalTok{optim_gd <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(par, fn, gr, }\DataTypeTok{eta=}\FloatTok{0.01}\NormalTok{,}
                        \DataTypeTok{maxit=}\DecValTok{1000}\NormalTok{, }\DataTypeTok{tol=}\FloatTok{1e-8}\NormalTok{) \{}
\NormalTok{    f_last <-}\StringTok{ }\KeywordTok{fn}\NormalTok{(par)}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{maxit) \{}
\NormalTok{        par <-}\StringTok{ }\NormalTok{par }\OperatorTok{-}\StringTok{ }\NormalTok{eta}\OperatorTok{*}\KeywordTok{grad_g_vectorised}\NormalTok{(par) }\CommentTok{# update step}
\NormalTok{        f_cur <-}\StringTok{ }\KeywordTok{fn}\NormalTok{(par)}
        \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{abs}\NormalTok{(f_cur}\OperatorTok{-}\NormalTok{f_last) }\OperatorTok{<}\StringTok{ }\NormalTok{tol) }\ControlFlowTok{break}
\NormalTok{        f_last <-}\StringTok{ }\NormalTok{f_cur}
\NormalTok{    \}}
    \KeywordTok{list}\NormalTok{( }\CommentTok{# see ?optim, section `Value`}
        \DataTypeTok{par=}\NormalTok{par,}
        \DataTypeTok{value=}\KeywordTok{g_vectorised}\NormalTok{(par),}
        \DataTypeTok{counts=}\NormalTok{i,}
        \DataTypeTok{convergence=}\KeywordTok{as.integer}\NormalTok{(i}\OperatorTok{==}\NormalTok{maxit)}
\NormalTok{    )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Tests of the \(g\) function.
First, let's try \(\eta=0.01\).
Figure \ref{fig:etastepplot} zooms in the contour plot so that we can see
the actual path the algorithm has taken.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eta <-}\StringTok{ }\FloatTok{0.01}
\NormalTok{res <-}\StringTok{ }\KeywordTok{optim_gd}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{), g_vectorised, grad_g_vectorised, }\DataTypeTok{eta=}\NormalTok{eta)}
\KeywordTok{str}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 4
##  $ par        : num [1:2] -1.54 2.16
##  $ value      : num 1.33e-08
##  $ counts     : int 135
##  $ convergence: int 0
\end{verbatim}

\begin{figure}
\hypertarget{fig:etastepplot}{%
\centering
\includegraphics{06-optimisation-iterative-figures/etastepplot-1.pdf}
\caption{Path taken by the gradient descent algorithm with \(\eta=0.01\)}\label{fig:etastepplot}
}
\end{figure}

Now let's try \(\eta=0.05\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eta <-}\StringTok{ }\FloatTok{0.05}
\NormalTok{res <-}\StringTok{ }\KeywordTok{optim_gd}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{), g_vectorised, grad_g_vectorised, }\DataTypeTok{eta=}\NormalTok{eta)}
\KeywordTok{str}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 4
##  $ par        : num [1:2] -1.54 2.15
##  $ value      : num 0.000203
##  $ counts     : int 417
##  $ convergence: int 0
\end{verbatim}

With an increased step size, the algorithm needed
many more iterations (3 times as many),
see Figure \ref{fig:etadef2f} for the path taken.

\begin{figure}
\hypertarget{fig:etadef2f}{%
\centering
\includegraphics{06-optimisation-iterative-figures/etadef2f-1.pdf}
\caption{Path taken by the gradient descent algorithm with \(\eta=0.05\)}\label{fig:etadef2f}
}
\end{figure}

And now for something completely different: \(\eta=0.1\), see Figure \ref{fig:etadef3f}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eta <-}\StringTok{ }\FloatTok{0.1}
\NormalTok{res <-}\StringTok{ }\KeywordTok{optim_gd}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{), g_vectorised, grad_g_vectorised, }\DataTypeTok{eta=}\NormalTok{eta)}
\KeywordTok{str}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 4
##  $ par        : num [1:2] -1.52 2.33
##  $ value      : num 0.507
##  $ counts     : int 1000
##  $ convergence: int 1
\end{verbatim}

\begin{figure}
\hypertarget{fig:etadef3f}{%
\centering
\includegraphics{06-optimisation-iterative-figures/etadef3f-1.pdf}
\caption{Path taken by the gradient descent algorithm with \(\eta=0.1\)}\label{fig:etadef3f}
}
\end{figure}

The algorithm failed to converge.

\bigskip

If the learning rate \(\eta\) is too small, the convergence might be too slow
or we might get stuck at a plateau.
On the other hand, if \(\eta\) is too large, we might be overshooting
and end up bouncing around the minimum.

This is why many optimisation libraries (including \texttt{keras}/TensorFlow)
implement some
of the following ideas:

\begin{itemize}
\item
  \emph{learning rate decay} -- start with large \(\eta\),
  decreasing it in every iteration, say, by some percent;
\item
  \emph{line search} -- determine optimal \(\eta\) in every step
  by solving a 1-dimensional optimisation problem w.r.t.
  \(\eta\in[0,\eta_{\max}]\);
\item
  \emph{momentum} -- the update step is based on a combination of the gradient direction
  and the previous change of the parameters, \(\Delta\mathbf{x}\);
  can be used to accelerate search in the relevant direction
  and minimise oscillations.
\end{itemize}

\begin{exercise}

Try implementing at least the first of the above
heuristics yourself. You can set \texttt{eta\ \textless{}-\ eta*0.95} in every iteration
of the gradient descent procedure.

\end{exercise}

\hypertarget{example-mnist}{%
\subsection{Example: MNIST (*)}\label{example-mnist}}

In the previous chapter we've
studied the MNIST dataset.
Let us go back to the task of fitting a multiclass logistic regression model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"keras"}\NormalTok{)}
\NormalTok{mnist <-}\StringTok{ }\KeywordTok{dataset_mnist}\NormalTok{()}

\CommentTok{# get train/test images in greyscale}
\NormalTok{X_train <-}\StringTok{ }\NormalTok{mnist}\OperatorTok{$}\NormalTok{train}\OperatorTok{$}\NormalTok{x}\OperatorTok{/}\DecValTok{255} \CommentTok{# to [0,1]}
\NormalTok{X_test  <-}\StringTok{ }\NormalTok{mnist}\OperatorTok{$}\NormalTok{test}\OperatorTok{$}\NormalTok{x}\OperatorTok{/}\DecValTok{255}  \CommentTok{# to [0,1]}

\CommentTok{# get the corresponding labels in \{0,1,...,9\}:}
\NormalTok{Y_train <-}\StringTok{ }\NormalTok{mnist}\OperatorTok{$}\NormalTok{train}\OperatorTok{$}\NormalTok{y}
\NormalTok{Y_test  <-}\StringTok{ }\NormalTok{mnist}\OperatorTok{$}\NormalTok{test}\OperatorTok{$}\NormalTok{y}
\end{Highlighting}
\end{Shaded}

The labels need to be one-hot encoded:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one_hot_encode <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(Y) \{}
    \KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{is.numeric}\NormalTok{(Y))}
\NormalTok{    c1 <-}\StringTok{ }\KeywordTok{min}\NormalTok{(Y) }\CommentTok{# first class label}
\NormalTok{    cK <-}\StringTok{ }\KeywordTok{max}\NormalTok{(Y) }\CommentTok{# last class label}
\NormalTok{    K <-}\StringTok{ }\NormalTok{cK}\OperatorTok{-}\NormalTok{c1}\OperatorTok{+}\DecValTok{1} \CommentTok{# number of classes}
\NormalTok{    Y2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{nrow=}\KeywordTok{length}\NormalTok{(Y), }\DataTypeTok{ncol=}\NormalTok{K)}
\NormalTok{    Y2[}\KeywordTok{cbind}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(Y), Y}\OperatorTok{-}\NormalTok{c1}\OperatorTok{+}\DecValTok{1}\NormalTok{)] <-}\StringTok{ }\DecValTok{1}
\NormalTok{    Y2}
\NormalTok{\}}

\NormalTok{Y_train2 <-}\StringTok{ }\KeywordTok{one_hot_encode}\NormalTok{(Y_train)}
\NormalTok{Y_test2 <-}\StringTok{ }\KeywordTok{one_hot_encode}\NormalTok{(Y_test)}
\end{Highlighting}
\end{Shaded}

Our task is to find the parameters \(\mathbf{B}\)
that minimise cross entropy \(E(\mathbf{B})\) over the training set:

\[
\min_{\mathbf{B}\in\mathbb{R}^{785\times 10}}
-\frac{1}{n^\text{train}} \sum_{i=1}^{n^\text{train}}
\log \Pr(Y=y_i^\text{train}|\mathbf{x}_{i,\cdot}^\text{train},\mathbf{B}).
\]

In the previous chapter, we've relied on the methods implemented
in the \texttt{keras} package. Let's do that all by ourselves now.

In order to come up with a working version of the gradient
descent procedure for classifying of MNIST digits,
we will need to derive and implement \texttt{grad\_cross\_entropy()}.
We do that below using matrix notation.

\begin{description}
\item[Remark.]
In the first reading, you can jump to the \emph{Safe landing zone}
below with no much loss in what we're trying to convey here
(you will then treat \texttt{grad\_cross\_entropy()} as a black-box function).
Nevertheless, keep in mind that this is the kind of maths you
will need to master anyway sooner than later -- this is inevitable.
Perhaps you should go back to, e.g., the appendix on Matrix Computations
with R or the chapter on Linear Regression? Learning maths is not a linear,
step-by-step process. Everyone is different and will have a different
path to success. The material needs to be frequently revisited,
it will ``click''
someday, don't you worry; good stuff isn't built in a day or seven.
\end{description}

Recall that the output of the logistic regression model
(1-layer neural network with softmax) can be written
in the matrix form as:
\[
\hat{\mathbf{Y}}=\mathrm{softmax}\left(
\dot{\mathbf{X}}\,\mathbf{B}
\right),
\]
where
\(\dot{\mathbf{X}}\in\mathbb{R}^{n\times 785}\) is a matrix
representing \(n\) images of size \(28\times 28\), augmented with a column of \(1\)s,
and
\(\mathbf{B}\in\mathbb{R}^{785\times 10}\) is the coefficients matrix
and \(\mathrm{softmax}\) is applied on each matrix row separately.

Of course, by the definition of matrix multiplication,
\(\hat{\mathbf{Y}}\) will be a matrix of size
\(n\times 10\), where \(\hat{y}_{i,k}\) represents the predicted probability
that the \(i\)-th image depicts the \(k\)-th digit.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert to matrices of size n*784}
\CommentTok{# and add a column of 1s}
\NormalTok{X_train1 <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\FloatTok{1.0}\NormalTok{, }\KeywordTok{matrix}\NormalTok{(X_train, }\DataTypeTok{ncol=}\DecValTok{28}\OperatorTok{*}\DecValTok{28}\NormalTok{))}
\NormalTok{X_test1  <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\FloatTok{1.0}\NormalTok{, }\KeywordTok{matrix}\NormalTok{(X_test, }\DataTypeTok{ncol=}\DecValTok{28}\OperatorTok{*}\DecValTok{28}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The \texttt{nn\_predict()} function implements the above formula
for \(\hat{\mathbf{Y}}\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{softmax <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(T) \{}
\NormalTok{    T <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(T)}
\NormalTok{    T}\OperatorTok{/}\KeywordTok{rowSums}\NormalTok{(T)}
\NormalTok{\}}

\NormalTok{nn_predict <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(B, X) \{}
    \KeywordTok{softmax}\NormalTok{(X }\OperatorTok{%*%}\StringTok{ }\NormalTok{B)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let's define the functions to compute the cross-entropy
(which we shall minimise)
and accuracy (which we shall report to a user):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cross_entropy <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(Y_true, Y_pred) \{}
    \OperatorTok{-}\KeywordTok{sum}\NormalTok{(Y_true}\OperatorTok{*}\KeywordTok{log}\NormalTok{(Y_pred))}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(Y_true)}
\NormalTok{\}}

\NormalTok{accuracy <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(Y_true, Y_pred) \{}
    \CommentTok{# both arguments are one-hot encoded}
\NormalTok{    Y_true_decoded <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(Y_true, }\DecValTok{1}\NormalTok{, which.max)}
\NormalTok{    Y_pred_decoded <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(Y_pred, }\DecValTok{1}\NormalTok{, which.max)}
    \CommentTok{# proportion of equal corresponding pairs:}
    \KeywordTok{mean}\NormalTok{(Y_true_decoded }\OperatorTok{==}\StringTok{ }\NormalTok{Y_pred_decoded)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

It may be shown (**) that the gradient of cross-entropy
(with respect to the parameter matrix \(\mathbf{B}\))
can be expressed in the matrix form as:

\[
\frac{1}{n} \dot{\mathbf{X}}^T\, (\hat{\mathbf{Y}}-\mathbf{Y})
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grad_cross_entropy <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(X, Y_true, Y_pred) \{}
    \KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{(Y_pred}\OperatorTok{-}\NormalTok{Y_true)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(Y_true)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Of course, we could always substitute the gradient
with the finite difference approximation. Yet, this would be much slower).

The more mathematically inclined reader will sure notice that
by expanding the formulas given
in the previous chapter, we can write cross-entropy
in the non-matrix form
(\(n\) -- number of samples, \(K\) -- number of classes,
\(p+1\) -- number of model parameters;
in our case \(K=10\) and \(p=784\)) as:

\[
\begin{array}{rcl}
E(\mathbf{B}) &=& -\displaystyle\frac{1}{n} \displaystyle\sum_{i=1}^n \displaystyle\sum_{k=1}^K y_{i,k}
\log\left(
\frac{
\exp\left(
\displaystyle\sum_{j=0}^p \dot{x}_{i,j} \beta_{j,k}
\right)
}{
\displaystyle\sum_{c=1}^K \exp\left(
\displaystyle\sum_{j=0}^p \dot{x}_{i,j} \beta_{j,c}
\right)
}
\right)\\
&=&
\displaystyle\frac{1}{n} \displaystyle\sum_{i=1}^n
\left(
\log \left(\displaystyle\sum_{k=1}^K \exp\left(
\displaystyle\sum_{j=0}^p \dot{x}_{i,j} \beta_{j,k}
\right)\right)
- \displaystyle\sum_{k=1}^K y_{i,k} \displaystyle\sum_{j=0}^p \dot{x}_{i,j} \beta_{j,k}
\right).
\end{array}
\]

Partial derivatives of cross-entropy w.r.t. \(\beta_{a,b}\) in non-matrix form
can be derived (**) so as to get:

\[
\begin{array}{rcl}
\displaystyle\frac{\partial E}{\partial \beta_{a,b}}(\mathbf{B}) &=&
\displaystyle\frac{1}{n} \displaystyle\sum_{i=1}^n \dot{x}_{i,a}
\left(
\frac{
\exp\left(
\displaystyle\sum_{j=0}^p \dot{x}_{i,j} \beta_{j,b}
\right)
}{
\displaystyle\sum_{k=1}^K \exp\left(
\displaystyle\sum_{j=0}^p \dot{x}_{i,j} \beta_{j,k}
\right)
}
- y_{i,b}
\right)\\
&=&
\displaystyle\frac{1}{n} \displaystyle\sum_{i=1}^n \dot{x}_{i,a}
\left(
\hat{y}_{i,b} - y_{i,b}
\right).
\end{array}
\]

\begin{description}
\item[Safe landing zone.]
In case you're lost with the above, continue from here.
However, in the near future, harden up and revisit the skipped material
to get the most out of our discussion.
\end{description}

We now have all the building blocks to
implement the gradient descent method.
The algorithm below follows exactly the same scheme as the one in the
\(g\) function example. This time, however, we play with a parameter matrix
\(\mathbf{B}\) (not a parameter vector \([x_1, x_2]\)) and we compute
the gradient of cross-entropy (by means of \texttt{grad\_cross\_entropy()}),
not the gradient of \(g\).

Note that a call to \texttt{system.time(expr)} measures the time (in seconds)
spent evaluating an expression \texttt{expr}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# random matrix of size 785x10 - initial guess}
\NormalTok{B <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{ncol}\NormalTok{(X_train1)}\OperatorTok{*}\KeywordTok{ncol}\NormalTok{(Y_train2)),}
    \DataTypeTok{nrow=}\KeywordTok{ncol}\NormalTok{(X_train1))}
\NormalTok{eta <-}\StringTok{ }\FloatTok{0.1}   \CommentTok{# learning rate}
\NormalTok{maxit <-}\StringTok{ }\DecValTok{100} \CommentTok{# number of GD iterations}
\KeywordTok{system.time}\NormalTok{(\{ }\CommentTok{# measure time spent}
    \CommentTok{# for simplicity, we stop only when we reach maxit}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{maxit) \{}
\NormalTok{        B <-}\StringTok{ }\NormalTok{B }\OperatorTok{-}\StringTok{ }\NormalTok{eta}\OperatorTok{*}\KeywordTok{grad_cross_entropy}\NormalTok{(}
\NormalTok{            X_train1, Y_train2, }\KeywordTok{nn_predict}\NormalTok{(B, X_train1))}
\NormalTok{    \}}
\NormalTok{\}) }\CommentTok{# `user` - processing time in seconds:}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
## 140.570  44.231  75.933
\end{verbatim}

Unfortunately, the method's convergence is really slow
(we are optimising over \(7850\) parameters\ldots{})
and the results after 100 iterations are disappointing:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{accuracy}\NormalTok{(Y_train2, }\KeywordTok{nn_predict}\NormalTok{(B, X_train1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.46462
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{accuracy}\NormalTok{(Y_test2,  }\KeywordTok{nn_predict}\NormalTok{(B, X_test1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4735
\end{verbatim}

Recall that in the previous chapter
we obtained much better classification accuracy by using the \texttt{keras} package.
What are we doing wrong then? Maybe \texttt{keras} implements some Super-Fancy
Hyper Optimisation Framework (TM) (R) that we could get access to for
only \$19.99 per month?

\hypertarget{stochastic-gradient-descent-sgd}{%
\subsection{Stochastic Gradient Descent (SGD) (*)}\label{stochastic-gradient-descent-sgd}}

In turns out that there's a very simple cure for the slow convergence of our
method.

It might be shocking for some, but sometimes the true global minimum
of cross-entropy for the whole training set
is not exactly what we \emph{really} want.
In our predictive modelling task, we are minimising train error,
but what we actually desire is to minimise the test error
(which we cannot refer to while training = no cheating!).

It is therefore rational to assume that both the train and the test set
consist of random digits independently sampled from the set of ``all the possible
digits out there in the world''.

Looking at the original objective (cross-entropy):

\[
E(\mathbf{B}) =
-\frac{1}{n^\text{train}} \sum_{i=1}^{n^\text{train}}
\log \Pr(Y=y_i^\text{train}|\mathbf{x}_{i,\cdot}^\text{train},\mathbf{B}).
\]

How about we try fitting to different random samples of the train set
in each iteration of the gradient descent method
instead of fitting to the whole train set?

\[
E(\mathbf{B}) \simeq
-\frac{1}{b} \sum_{i=1}^b
\log \Pr(Y=y_{\text{random\_index}_i}^\text{train}|\mathbf{x}_{\text{random\_index}_i,\cdot}^\text{train},\mathbf{B}),
\]

where \(b\) is some fixed batch size.
Such an approach is often called \textbf{stochastic gradient descent}.

\begin{description}
\item[Remark.]
This scheme is sometimes referred to as \textbf{mini-batch} gradient descent
in the literature; some researchers reserve the term ``stochastic''
only for batches of size 1.
\end{description}

Stochastic gradient descent can be implemented very easily:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{ncol}\NormalTok{(X_train1)}\OperatorTok{*}\KeywordTok{ncol}\NormalTok{(Y_train2)),}
    \DataTypeTok{nrow=}\KeywordTok{ncol}\NormalTok{(X_train1))}
\NormalTok{eta <-}\StringTok{ }\FloatTok{0.1}
\NormalTok{maxit <-}\StringTok{ }\DecValTok{100}
\NormalTok{batch_size <-}\StringTok{ }\DecValTok{32}
\KeywordTok{system.time}\NormalTok{(\{}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{maxit) \{}
\NormalTok{        wh <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(X_train1), }\DataTypeTok{size=}\NormalTok{batch_size)}
\NormalTok{        B <-}\StringTok{ }\NormalTok{B }\OperatorTok{-}\StringTok{ }\NormalTok{eta}\OperatorTok{*}\KeywordTok{grad_cross_entropy}\NormalTok{(}
\NormalTok{            X_train1[wh,], Y_train2[wh,],}
            \KeywordTok{nn_predict}\NormalTok{(B, X_train1[wh,])}
\NormalTok{        )}
\NormalTok{    \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##   0.182   0.008   0.190
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{accuracy}\NormalTok{(Y_train2, }\KeywordTok{nn_predict}\NormalTok{(B, X_train1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.46198
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{accuracy}\NormalTok{(Y_test2,  }\KeywordTok{nn_predict}\NormalTok{(B, X_test1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4693
\end{verbatim}

The errors are much worse but at least we got the (useless)
solution very quickly. That's the ``fail fast'' rule in practice.

However, why don't we increase the number of iterations and see what
happens? We've allowed the classic gradient descent to scrabble around
the MNIST dataset for almost 2 minutes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{ncol}\NormalTok{(X_train1)}\OperatorTok{*}\KeywordTok{ncol}\NormalTok{(Y_train2)),}
    \DataTypeTok{nrow=}\KeywordTok{ncol}\NormalTok{(X_train1))}
\NormalTok{eta <-}\StringTok{ }\FloatTok{0.1}
\NormalTok{maxit <-}\StringTok{ }\DecValTok{10000}
\NormalTok{batch_size <-}\StringTok{ }\DecValTok{32}
\KeywordTok{system.time}\NormalTok{(\{}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{maxit) \{}
\NormalTok{        wh <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(X_train1), }\DataTypeTok{size=}\NormalTok{batch_size)}
\NormalTok{        B <-}\StringTok{ }\NormalTok{B }\OperatorTok{-}\StringTok{ }\NormalTok{eta}\OperatorTok{*}\KeywordTok{grad_cross_entropy}\NormalTok{(}
\NormalTok{            X_train1[wh,], Y_train2[wh,],}
            \KeywordTok{nn_predict}\NormalTok{(B, X_train1[wh,])}
\NormalTok{        )}
\NormalTok{    \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##  16.724   0.162  16.885
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{accuracy}\NormalTok{(Y_train2, }\KeywordTok{nn_predict}\NormalTok{(B, X_train1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.89222
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{accuracy}\NormalTok{(Y_test2,  }\KeywordTok{nn_predict}\NormalTok{(B, X_test1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8935
\end{verbatim}

Bingo! Let's take a closer look at how the train/test error
behaves in each iteration for different batch sizes.
Figures \ref{fig:mnist_sgd} and \ref{fig:mnist_sgd2b} depict
the cases of \texttt{batch\_size} of 32 and 128, respectively.

The time needed to go through 10000 iterations with batch size of 32 is:

\begin{verbatim}
##    user  system elapsed 
## 120.859  20.356  72.073
\end{verbatim}

\begin{figure}
\hypertarget{fig:mnist_sgd}{%
\centering
\includegraphics{06-optimisation-iterative-figures/mnist_sgd-1.pdf}
\caption{Cross-entropy and accuracy on the train and test set in each iteration of SGD; batch size of 32}\label{fig:mnist_sgd}
}
\end{figure}

What's more, batch size of 128 takes:

\begin{verbatim}
##    user  system elapsed 
##  467.50  198.25  189.89
\end{verbatim}

\begin{figure}
\hypertarget{fig:mnist_sgd2b}{%
\centering
\includegraphics{06-optimisation-iterative-figures/mnist_sgd2b-1.pdf}
\caption{Cross-entropy and accuracy on the train and test set in each iteration of SGD; batch size of 128}\label{fig:mnist_sgd2b}
}
\end{figure}

\begin{exercise}

Draw conclusions.

\end{exercise}

\hypertarget{a-note-on-convex-optimisation}{%
\section{A Note on Convex Optimisation (*)}\label{a-note-on-convex-optimisation}}

Are there any cases where we are sure that
a local minimum is the global minimum?
It turns out that the answer to this is positive;
for example, when we minimise objective functions
that fulfil a special property defined below.

First let's note that given two points
\(\mathbf{x}_{1},\mathbf{x}_{2}\in \mathbb{R}^p\),
by taking any \(\theta\in[0,1]\), the point defined as:

\[
\mathbf{t} = \theta \mathbf{x}_{1}+(1-\theta)\mathbf{x}_{2}
\]

lies on a (straight) line segment between \(\mathbf{x}_1\) and \(\mathbf{x}_2\).

\begin{description}
\item[Definition.]
We say that a function \(f:\mathbb{R}^p\to\mathbb{R}\) is \emph{convex}, whenever:
\[
(\forall \mathbf{x}_{1},\mathbf{x}_{2}\in \mathbb{R}^p)
(\forall \theta\in [0,1])\quad
f(\theta \mathbf{x}_{1}+(1-\theta)\mathbf{x}_{2})\leq \theta f(\mathbf{x}_{1})+(1-\theta)f(\mathbf{x}_{2})
\]
\end{description}

In other words, the function's value at any convex combination of two points
is not greater than that combination of the function values at these two points.
See Figure \ref{fig:convex_function} for a graphical illustration of the above.

\begin{figure}
\hypertarget{fig:convex_function}{%
\centering
\includegraphics[width=0.75\textwidth,height=\textheight]{figures/convex_function.pdf}
\caption{An illustration of the definition of a convex function}\label{fig:convex_function}
}
\end{figure}

The following result addresses the question we posed at the beginning
of this section.

\begin{description}
\item[Theorem.]
For any \emph{convex} function \(f\), if \(f\) has a local minimum at \(\mathbf{x}^+\)
then \(\mathbf{x}^+\) is also its global minimum.
\end{description}

Convex functions are ubiquitous in machine learning, but of course not every
objective function we are going to deal with will fulfil this property.
Here are some basic examples of convex functions and how they come into being,
see, e.g., (Boyd \& Vandenberghe \protect\hyperlink{ref-boyd_vandenberghe}{2004}) for more:

\begin{itemize}
\tightlist
\item
  the functions mapping \(x\) to \(x, x^2, |x|, e^x\) are all convex,
\item
  \(f(x)=|x|^p\) is convex for all \(p\ge 1\),
\item
  if \(f\) is convex, then \(-f\) is concave,
\item
  if \(f_1\) and \(f_2\) are convex, then \(w_1 f_1 + w_2 f_2\) are convex for any
  \(w_1,w_2\ge 0\),
\item
  if \(f_1\) and \(f_2\) are convex, then \(\max\{f_1, f_2\}\) is convex,
\item
  if \(f\) and \(g\) are convex and \(g\) is non-decreasing, then \(g(f(x))\) is convex.
\end{itemize}

The above feature the building blocks of our error measures in supervised
learning problems! In particular, sum of squared residuals in linear regression
is a convex function of the underlying parameters. Also,
cross-entropy in logistic regression is a convex function
of the underlying parameters.

\begin{description}
\item[Theorem.]
(***) If a function is twice differentiable,
then its convexity can be judged based on the positive-definiteness
of its Hessian matrix.
\end{description}

Note that optimising convex functions is \emph{relatively} easy,
especially if they are differentiable.
This is because they are quite well-behaving.
However, it doesn't mean that we an analytic solution to the problem
of their minimisation.
Methods such as gradient descent or BFGS should work well
(unless there are vast regions where a function is constant or
the function's is defined over a large number of parameters).

\begin{description}
\item[Remark.]
(**) There is a special class of constrained optimisation
problems called linear and, more generally, quadratic programming that involves
convex functions.
Moreover, the Karush--Kuhn--Tucker (KKT) conditions address the more
general problem of minimisation with constraints (i.e., not over the whole
\(\mathbb{R}^p\) set); see (Nocedal \& Wright \protect\hyperlink{ref-nocedal_wright}{2006}, Fletcher \protect\hyperlink{ref-fletcher}{2008}) for more details.
\item[Remark.]
Not only functions, but also sets can be said to be convex.
We say that \(C\subseteq \mathbb{R}^p\) is a \emph{convex set},
whenever the line segment joining any two points in \(C\)
is fully included in \(C\). More formally,
for every \(\mathbf{x}_1\in C\) and \(\mathbf{x}_2\in C\),
it holds \(\theta \mathbf{x}_{1}+(1-\theta)\mathbf{x}_{2} \in C\) for
all \(\theta\in [0,1]\); see Figure \ref{fig:convex_set} for an illustration.
\end{description}

\begin{figure}
\hypertarget{fig:convex_set}{%
\centering
\includegraphics[width=0.75\textwidth,height=\textheight]{figures/convex_set.pdf}
\caption{A convex and a non-convex set}\label{fig:convex_set}
}
\end{figure}

\hypertarget{outro-5}{%
\section{Outro}\label{outro-5}}

\hypertarget{remarks-5}{%
\subsection{Remarks}\label{remarks-5}}

Solving continuous problems with many variables (e.g., deep neural networks)
is time consuming -- the more variables to optimise over (e.g.,
model parameters, think the number of interconnections between all the neurons), the slower
the optimisation process.

Moreover, it might be the case that the sole objective function takes long
to compute (think of image classification with large training samples).

\begin{description}
\item[Remark.]
(*) Although theoretically possible, good luck
fitting a logistic regression model to MNIST with \texttt{optim()}'s BFGS -- there are 7850 variables!
\end{description}

Training \emph{deep} neural networks with SGD is even slower (more parameters),
but there is a trick to propagate weight updates layer by layer,
called \emph{backpropagation} (actually used in every neural network library),
see, e.g., (Sarle \& others \protect\hyperlink{ref-aifaq}{2002}) and (Goodfellow et al. \protect\hyperlink{ref-deeplearn}{2016}).
Moreover, \texttt{keras} and similar libraries implement automatic differentiation
procedures that make its user's life much easier (swiping some of the tedious
math under the comfy carpet).

\bigskip

\texttt{keras} implements various optimisers that
we can refer to in the \texttt{compile()} function,
see
\url{https://keras.rstudio.com/reference/compile.html}
and
\url{https://keras.io/optimizers/}:

\begin{itemize}
\item
  \texttt{SGD} -- stochastic gradient descent supporting momentum and learning rate decay,
\item
  \texttt{RMSprop} -- divides the gradient by a running average of its recent magnitude,
\item
  \texttt{Adam} -- adaptive momentum,
\end{itemize}

and so on. These are all non-complicated variations of the pure stochastic GD.
Some of them are just tricks that work well in some examples and destroy
the convergence on many other ones.
You can get into their details in a dedicated book/course aimed at covering
neural networks (see, e.g., (Sarle \& others \protect\hyperlink{ref-aifaq}{2002}), (Goodfellow et al. \protect\hyperlink{ref-deeplearn}{2016})),
but we have already developed some
good intuitions here.

\bigskip

Keep in mind that with methods such as GD or SGD, there is no guarantee we reach a minimum,
but an approximate solution is better than no solution at all.
Also sometimes (especially in ML applications)
we don't really need the actual minimum (e.g., when optimising
the error with respect to the train set).
Those ``mathematically pure'' will find that a bit\ldots{} unaesthetic,
but here we are. Maybe the solution makes your boss or client happy,
maybe it generates revenue. Maybe it helps solve some other problem.
Some claim that \emph{a} solution is better than no solution at all, remember?
But\ldots{} is it really always the case though?

\hypertarget{further-reading-5}{%
\subsection{Further Reading}\label{further-reading-5}}

Recommended further reading: (Nocedal \& Wright \protect\hyperlink{ref-nocedal_wright}{2006}), (Boyd \& Vandenberghe \protect\hyperlink{ref-boyd_vandenberghe}{2004}),
(Fletcher \protect\hyperlink{ref-fletcher}{2008}).

\hypertarget{clustering}{%
\chapter{Clustering}\label{clustering}}

\hypertarget{unsupervised-learning}{%
\section{Unsupervised Learning}\label{unsupervised-learning}}

\hypertarget{introduction-12}{%
\subsection{Introduction}\label{introduction-12}}

In \textbf{unsupervised learning} (learning without a teacher),
the input data points \(\mathbf{x}_{1,\cdot},\dots,\mathbf{x}_{n,\cdot}\)
are not assigned any reference labels (compare Figure \ref{fig:unsupervised}).

\begin{figure}
\hypertarget{fig:unsupervised}{%
\centering
\includegraphics{07-clustering-figures/unsupervised-1.pdf}
\caption{Unsupervised learning: ``But what it is exactly that I have to do here?''}\label{fig:unsupervised}
}
\end{figure}

Our aim now is to discover the \textbf{underlying structure in the data},
whatever that means.

\hypertarget{main-types-of-unsupervised-learning-problems}{%
\subsection{Main Types of Unsupervised Learning Problems}\label{main-types-of-unsupervised-learning-problems}}

It turns out, however, that certain classes of unsupervised learning
problems are not only intellectually stimulating,
but practically useful at the same time.

In particular, in \textbf{dimensionality reduction} we seek a meaningful
\emph{projection} of a high dimensional
space (think: many variables/columns).

\begin{figure}
\hypertarget{fig:princomp}{%
\centering
\includegraphics{07-clustering-figures/princomp-1.pdf}
\caption{Principal component analysis (a dimensionality reduction technique) applied on three features of red wines}\label{fig:princomp}
}
\end{figure}

For instance, Figure \ref{fig:princomp} reveals that the
``alcohol'', ``response'' and ``residual.sugar'' dimensions of the Wine Quality
dataset that we have studied earlier on can actually be nicely depicted (with
no much loss of information) on a two-dimensional plot.
It turns out that the wine experts' opinion on a wine's quality is highly
correlated with the amount of\ldots{} alcohol in a bottle.
On the other hand, sugar is orthogonal (unrelated) to these two.

Amongst example dimensionality reduction methods we find:

\begin{itemize}
\tightlist
\item
  Multidimensional scaling (MDS)
\item
  Principal component analysis (PCA)
\item
  Kernel PCA
\item
  t-SNE
\item
  Autoencoders (deep learning)
\end{itemize}

See, for example, (Hastie et al. \protect\hyperlink{ref-esl}{2017}) for more details.

\bigskip

Furthermore, in \textbf{anomaly detection}, our task is to identify rare, suspicious, ab-normal
or out-standing items.
For example, these can be cars on walkways in a park's security camera footage.

\begin{figure}
\hypertarget{fig:anomaly_detection}{%
\centering
\includegraphics{07-clustering-figures/anomaly_detection-1.pdf}
\caption{Outliers can be thought of anomalies of some sort}\label{fig:anomaly_detection}
}
\end{figure}

\bigskip

Finally, the aim of \textbf{clustering} is to automatically discover some \emph{naturally occurring}
subgroups in the data set, compare Figure \ref{fig:clustering_illustration}.
For example, these may be customers having different shopping patterns
(such as ``young parents'', ``students'', ``boomers'').

\begin{figure}
\hypertarget{fig:clustering_illustration}{%
\centering
\includegraphics{07-clustering-figures/clustering_illustration-1.pdf}
\caption{NEWS FLASH! SCIENTISTS SHOWED (by writing about it) THAT SOME VERY IMPORTANT THING (Iris dataset) COMES IN THREE DIFFERENT FLAVOURS (by applying the 3-means clustering algorithm)!}\label{fig:clustering_illustration}
}
\end{figure}

\hypertarget{definitions}{%
\subsection{Definitions}\label{definitions}}

Formally, given \(K\ge 2\), \textbf{clustering} aims is to find a \emph{special kind}
of a \textbf{\(K\)-partition} of the input data set \(\mathbf{X}\).

\begin{description}
\item[Definition.]
We say that \(\mathcal{C}=\{C_1,\dots,C_K\}\) is a \textbf{\(K\)-partition}
of \(\mathbf{X}\) of size \(n\),
whenever:

\begin{itemize}
\tightlist
\item
  \(C_k\neq\emptyset\) for all \(k\) (each set is nonempty),
\item
  \(C_k\cap C_l=\emptyset\) for all \(k\neq l\) (sets are pairwise disjoint),
\item
  \(\bigcup_{k=1}^K C_k=\mathbf{X}\) (no point is neglected).
\end{itemize}
\end{description}

This can also be thought of as assigning each point a unique label \(\{1,\dots,K\}\)
(think: colouring of the points, where each number has a colour).
We will consider the point \(\mathbf{x}_{i,\cdot}\) as labelled \(j\)
if and only if it belongs to cluster \(C_j\), i.e., \(\mathbf{x}_{i,\cdot}\in C_j\).

Example applications of clustering:

\begin{itemize}
\tightlist
\item
  \emph{taxonomisation}: e.g.,
  partition the consumers to more ``uniform''
  groups to better understand who they are and what do they need,
\item
  \emph{image processing}:
  e.g., object detection, like tumour tissues on medical images,
\item
  \emph{complex networks analysis}:
  e.g., detecting communities in friendship,
  retweets and other networks,
\item
  \emph{fine-tuning supervised learning algorithms}:
  e.g., recommender systems indicating content
  that was rated highly by users from the same group
  or learning multiple manifolds in a dimension reduction task.
\end{itemize}

The number of possible \(K\)-partitions of a set with \(n\) elements is given by
\emph{the Stirling number of the second kind}:

\[
\left\{{n \atop K}\right\}={\frac  {1}{K!}}\sum _{{j=0}}^{{K}}(-1)^{{K-j}}{\binom  {K}{j}}j^{n};
\]

e.g., already \(\left\{{n \atop 2}\right\}=2^{n-1}-1\)
and \(\left\{{n \atop 3}\right\}=O(3^n)\) -- that is a lot.
Certainly, we are not just interested in ``any'' partition -- some of them
will be more meaningful or valuable than others.
However, even one of the most famous ML textbooks provides us with only
a vague hint of what we might be looking for:

\begin{description}
\item[``Definition''.]
Clustering concerns ``segmenting a collection of objects into subsets
so that those within each cluster are more \textbf{closely related}
to one another than objects assigned to different clusters'' (Hastie et al. \protect\hyperlink{ref-esl}{2017}).
\end{description}

It is not uncommon
to equate the general definition of data clustering problems with\ldots{} the
particular outputs yield by specific clustering algorithms. It some sense,
that sounds fair. From this perspective, we might be interested in
identifying the two main types of clustering algorithms:

\begin{itemize}
\tightlist
\item
  \textbf{parametric} (model-based):

  \begin{itemize}
  \tightlist
  \item
    find clusters of specific shapes or following specific multidimensional
    probability distributions,
  \item
    e.g., \(K\)-means, expectation-maximisation for Gaussian mixtures (EM),
    average linkage agglomerative clustering;
  \end{itemize}
\item
  \textbf{nonparametric} (model-free):

  \begin{itemize}
  \tightlist
  \item
    identify high-density or well-separable regions,
    perhaps in the presence of noise points,
  \item
    e.g., single linkage agglomerative clustering, Genie, (H)DBSCAN, BIRCH.
  \end{itemize}
\end{itemize}

In this chapter we'll take a look at two classical approaches to clustering:

\begin{itemize}
\tightlist
\item
  \emph{K-means clustering} that looks for a specific number of clusters,
\item
  \emph{(agglomerative) hierarchical clustering} that outputs a whole hierarchy
  of nested data partitions.
\end{itemize}

\hypertarget{k-means-clustering}{%
\section{K-means Clustering}\label{k-means-clustering}}

\hypertarget{example-in-r-5}{%
\subsection{Example in R}\label{example-in-r-5}}

Let's begin our clustering adventure
by applying the \(K\)-means clustering method to find \(K=3\) groups
in the famous Fisher's \texttt{iris} data set (variables \texttt{Sepal.Width}
and \texttt{Petal.Length} variables only):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(iris[,}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{)])}
\CommentTok{# never forget to set nstart>>1!}
\NormalTok{km <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(X, }\DataTypeTok{centers=}\DecValTok{3}\NormalTok{, }\DataTypeTok{nstart=}\DecValTok{10}\NormalTok{)}
\NormalTok{km}\OperatorTok{$}\NormalTok{cluster }\CommentTok{# labels assigned to each of 150 points:}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [34] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [67] 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [100] 3 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 3 2 3 2 3 2 2 3 3 2 2 2 2
## [133] 2 2 2 2 2 2 3 2 2 2 2 2 2 2 3 2 2 2
\end{verbatim}

\begin{description}
\item[Remark.]
Later we'll see that \texttt{nstart} is responsible for random restarting the
(local) optimisation procedure, just as we did in the previous chapter.
\end{description}

Let's draw a scatter plot that depicts the detected clusters:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{col=}\NormalTok{km}\OperatorTok{$}\NormalTok{cluster)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:kmeans12}{%
\centering
\includegraphics{07-clustering-figures/kmeans12-1.pdf}
\caption{3-means clustering on a projection of the Iris dataset}\label{fig:kmeans12}
}
\end{figure}

The colours in Figure \ref{fig:kmeans12} indicate the detected clusters.
The left group is clearly well-separated from the other two.

What can we do with this information? Well, if we were experts on plants
(in the 1930s), that'd definitely be something ground-breaking.
Figure \ref{fig:kmeans123} is a version of the aforementioned scatter plot
now with the true iris species added.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{col=}\NormalTok{km}\OperatorTok{$}\NormalTok{cluster, }\DataTypeTok{pch=}\KeywordTok{as.numeric}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Species))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:kmeans123}{%
\centering
\includegraphics{07-clustering-figures/kmeans123-1.pdf}
\caption{3-means clustering (colours) vs true Iris species (shapes)}\label{fig:kmeans123}
}
\end{figure}

Here is a contingency table for detected clusters vs.~true iris species:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(C <-}\StringTok{ }\KeywordTok{table}\NormalTok{(km}\OperatorTok{$}\NormalTok{cluster, iris}\OperatorTok{$}\NormalTok{Species))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##     setosa versicolor virginica
##   1     50          0         0
##   2      0          2        41
##   3      0         48         9
\end{verbatim}

It turns out that the discovered partition matches the original iris
species very well. We have just made a ``discovery'' in the field of
botany (actually some research fields classify their objects of study
into families, genres etc. by means of such tools).

Were the actual Iris species what we had hoped to match?
Was that our aim? Well, surely we have had begun our journey with
``clear minds'' (yet with hungry eyes). Note that the true class labels
were not used during the clustering procedure -- we're dealing with
an unsupervised learning problem here. The result turned useful,
it's a win.

\begin{description}
\item[Remark.]
(*) There are several indices that assess the
similarity of two partitions, for example the Adjusted Rand Index (ARI)
the Normalised Mutual Information Score (NMI)
or set matching-based measures,
see, e.g., (Hubert \& Arabie \protect\hyperlink{ref-comparing_paritions}{1985}), (Rezaei \& Fränti \protect\hyperlink{ref-external_cluster_validity}{2016}).
\end{description}

\hypertarget{problem-statement}{%
\subsection{Problem Statement}\label{problem-statement}}

The aim of \emph{\(K\)-means clustering} is to find \(K\) ``good'' cluster centres
\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\).

Then, a point \(\mathbf{x}_{i,\cdot}\) will be assigned to the
cluster represented by the closest centre. Here, by \emph{closest}
we mean the \emph{squared} Euclidean distance.

More formally,
assuming all the points are in a \(p\)-dimensional space, \(\mathbb{R}^p\),
we define the distance between the \(i\)-th point and the \(k\)-th
centre as:

\[
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}) = \| \mathbf{x}_{i,\cdot} - \boldsymbol\mu_{k,\cdot} \|^2 =  \sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\]

Then the \(i\)-th point's cluster is determined by:

\[
\mathrm{C}(i) = \mathrm{arg}\min_{k=1,\dots,K}
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}),
\]

where, as usual, \(\mathrm{arg}\min\) (argument minimum) is the index
\(k\) that minimises the given expression.

In the previous example, the three identified cluster centres in \(\mathbb{R}^2\)
are given by (see Figure \ref{fig:kmeans_problem1} for illustration):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km}\OperatorTok{$}\NormalTok{centers}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Petal.Length Sepal.Width
## 1       1.4620      3.4280
## 2       5.6721      3.0326
## 3       4.3281      2.7509
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{col=}\NormalTok{km}\OperatorTok{$}\NormalTok{cluster, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{) }\CommentTok{# asp=1 gives the same scale on both axes}
\KeywordTok{points}\NormalTok{(km}\OperatorTok{$}\NormalTok{centers, }\DataTypeTok{cex=}\DecValTok{2}\NormalTok{, }\DataTypeTok{col=}\DecValTok{4}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:kmeans_problem1}{%
\centering
\includegraphics{07-clustering-figures/kmeans_problem1-1.pdf}
\caption{Cluster centres (blue dots) identified by the 3-means algorithm}\label{fig:kmeans_problem1}
}
\end{figure}

Figure \ref{fig:kmeans_problem2} depicts the partition
of the whole \(\mathbb{R}^2\) space
into clusters based on the closeness to the three cluster centres.

\begin{figure}
\hypertarget{fig:kmeans_problem2}{%
\centering
\includegraphics{07-clustering-figures/kmeans_problem2-1.pdf}
\caption{The division of the whole space into three sets based on the proximity to cluster centres (a so-called Voronoi diagram)}\label{fig:kmeans_problem2}
}
\end{figure}

To compute the distances between all the points and the cluster centres,
we may call \texttt{pdist::pdist()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"pdist"}\NormalTok{)}
\NormalTok{D <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{pdist}\NormalTok{(X, km}\OperatorTok{$}\NormalTok{centers))}\OperatorTok{^}\DecValTok{2}
\KeywordTok{head}\NormalTok{(D)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]   [,2]   [,3]
## [1,] 0.009028 18.469 9.1348
## [2,] 0.187028 18.252 8.6357
## [3,] 0.078228 19.143 9.3709
## [4,] 0.109028 17.411 8.1199
## [5,] 0.033428 18.573 9.2946
## [6,] 0.279428 16.530 8.2272
\end{verbatim}

where \texttt{D{[}i,k{]}} gives the squared Euclidean distance between
\(\mathbf{x}_{i,\cdot}\) and \(\boldsymbol\mu_{k,\cdot}\).

The cluster memberships the (\(\mathrm{arg}\min\)s)
can now be determined by:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(idx <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(D, }\DecValTok{1}\NormalTok{, which.min)) }\CommentTok{# for every row of D...}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [34] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [67] 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [100] 3 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 3 2 3 2 3 2 2 3 3 2 2 2 2
## [133] 2 2 2 2 2 2 3 2 2 2 2 2 2 2 3 2 2 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{all}\NormalTok{(km}\OperatorTok{$}\NormalTok{cluster }\OperatorTok{==}\StringTok{ }\NormalTok{idx) }\CommentTok{# sanity check}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\hypertarget{algorithms-for-the-k-means-problem}{%
\subsection{Algorithms for the K-means Problem}\label{algorithms-for-the-k-means-problem}}

All good, but how do we find ``good'' cluster centres?
Good, better, best\ldots{} yet again we are in a need for a goodness-of-fit metric.
In the \(K\)-means clustering, we determine
\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)
that minimise the total within-cluster distances (distances from each point
to each own cluster centre):

\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{C(i),\cdot}),
\]

Note that the \(\boldsymbol\mu\)s are also ``hidden'' inside the point-to-cluster
belongingness mapping, \(\mathrm{C}\).
Expanding the above yields:

\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{k=1,\dots,K}
\sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\right).
\]

Unfortunately, the \(\min\) operator in the objective function
makes this optimisation problem not tractable
with the methods discussed in the previous chapter.

The above problem is \emph{hard} to solve (* more precisely,
it is an NP-hard problem).
Therefore, in practice we use various heuristics to solve it.
The \texttt{kmeans()} function itself implements 3 of them:
the Hartigan-Wong, Lloyd (a.k.a. Lloyd-Forgy) and MacQueen algorithms.

\begin{description}
\item[Remark.]
(*) Technically, there is no such thing as ``\emph{the} K-means algorithm'' --
all the aforementioned methods are particular heuristic
approaches to solving the K-means
clustering problem formalised as the above optimisation task.
By setting \texttt{nstart\ =\ 10} above, we ask the (Hartigan-Wong, which is
the default one in \texttt{kmeans()}) algorithm
to find 10 solution candidates obtained by considering different random
initial clusterings and choose the best one (with respect to the sum
of within-cluster distances) amongst them. This does not guarantee
finding the optimal solution, especially for very unbalanced datasets,
but increases the likelihood of such.
\item[Remark.]
The \emph{squared} Euclidean distance was of course chosen to make computations
easier. It turns out that for any given subset of input points
\(\mathbf{x}_{i_1,\cdot},\dots,\mathbf{x}_{i_m,\cdot}\),
the point \(\boldsymbol\mu_{k,\cdot}\) that minimises the total distances
to all of them, i.e.,

\[
    \min_{\boldsymbol\mu_{k,\cdot}\in \mathbb{R}^p}
    \sum_{\ell=1}^m \left(
    \sum_{j=1}^p \left(x_{i_\ell,j}-\mu_{k,j}\right)^2
    \right),
\]

is exactly these points' \emph{centroid} -- which is given by
the componentwise arithmetic means of their coordinates.

For example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colMeans}\NormalTok{(X[km}\OperatorTok{$}\NormalTok{cluster }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{,]) }\CommentTok{# centroid of the points in the 1st cluster}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Petal.Length  Sepal.Width 
##        1.462        3.428
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km}\OperatorTok{$}\NormalTok{centers[}\DecValTok{1}\NormalTok{,] }\CommentTok{# the centre of the 1st cluster}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Petal.Length  Sepal.Width 
##        1.462        3.428
\end{verbatim}
\end{description}

\bigskip

Among the various heuristics to solve the K-means problem,
Lloyd's algorithm (1957) is perhaps the simplest.
This is probably the reason why it is sometimes referred
to as ``the'' K-means algorithm:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start with random cluster centres \(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\).
\item
  For each point \(\mathbf{x}_{i,\cdot}\), determine its closest centre \(C(i)\in\{1,\dots,K\}\):

  \[
   \mathrm{C}(i) = \mathrm{arg}\min_{k=1,\dots,K}
   d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}).
   \]
\item
  For each cluster \(k\in\{1,\dots,K\}\), compute the new cluster centre \(\boldsymbol\mu_{k,\cdot}\) as the centroid of
  all the point indices \(i\) such that \(C(i)=k\).
\item
  If the cluster centres changed since the last iteration, go to step 2,
  otherwise stop and return the result.
\end{enumerate}

\bigskip

(*) Here's an example implementation.
As the initial cluster centres, let's pick some ``noisy'' versions
of \(K\) randomly chosen points in \(\mathbf{X}\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{)}
\NormalTok{K <-}\StringTok{ }\DecValTok{3}

\CommentTok{# Random initial cluster centres:}
\NormalTok{M <-}\StringTok{ }\KeywordTok{jitter}\NormalTok{(X[}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(X), K),])}
\NormalTok{M}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Petal.Length Sepal.Width
## [1,]       5.1004      3.0814
## [2,]       4.7091      3.1861
## [3,]       3.3196      2.4094
\end{verbatim}

In what follows, we will be maintaining a matrix
such that \texttt{D{[}i,k{]}} is the distance between the \(i\)-th point
and the \(k\)-th centre and a vector such that \texttt{idx{[}i{]}}
denotes the index of the cluster centre closest to the \texttt{i}-th point.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{D <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{pdist}\NormalTok{(X, M))}\OperatorTok{^}\DecValTok{2}
\NormalTok{idx <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(D, }\DecValTok{1}\NormalTok{, which.min)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{repeat}\NormalTok{ \{}
    \CommentTok{# Determine the new cluster centres:}
\NormalTok{    M <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{K, }\ControlFlowTok{function}\NormalTok{(k) \{}
        \CommentTok{# the centroid of all points in the k-th cluster:}
        \KeywordTok{colMeans}\NormalTok{(X[idx}\OperatorTok{==}\NormalTok{k,])}
\NormalTok{    \}))}

    \CommentTok{# Store the previous cluster belongingness info:}
\NormalTok{    old_idx <-}\StringTok{ }\NormalTok{idx}

    \CommentTok{# Recompute D and idx:}
\NormalTok{    D <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{pdist}\NormalTok{(X, M))}\OperatorTok{^}\DecValTok{2}
\NormalTok{    idx <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(D, }\DecValTok{1}\NormalTok{, which.min)}

    \CommentTok{# Check if converged already:}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{all}\NormalTok{(idx }\OperatorTok{==}\StringTok{ }\NormalTok{old_idx)) }\ControlFlowTok{break}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let's compare the obtained cluster centres with the ones returned
by \texttt{kmeans()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{M }\CommentTok{# our result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Petal.Length Sepal.Width
## [1,]       5.6721      3.0326
## [2,]       4.3281      2.7509
## [3,]       1.4620      3.4280
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km}\OperatorTok{$}\NormalTok{center }\CommentTok{# result of kmeans()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Petal.Length Sepal.Width
## 1       1.4620      3.4280
## 2       5.6721      3.0326
## 3       4.3281      2.7509
\end{verbatim}

These two represent exactly the same 3-partitions
(note that the actual labels (the order of centres) are not important).

The value of the objective function (total within-cluster distances)
at the identified candidate solution
is equal to:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(D[}\KeywordTok{cbind}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(X),idx)]) }\CommentTok{# indexing with a 2-column matrix!}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 40.737
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km}\OperatorTok{$}\NormalTok{tot.withinss }\CommentTok{# as reported by kmeans()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 40.737
\end{verbatim}

We would need it if we were to implement the \texttt{nstart} functionality,
which is left as an:

\begin{exercise}

(*) Wrap the implementation of the Lloyd algorithm into a standalone R function,
with a similar look-and-feel as the original \texttt{kmeans()}.

\end{exercise}

\begin{figure}
\hypertarget{fig:kmeanimpl_plot}{%
\centering
\includegraphics{07-clustering-figures/kmeanimpl_plot-1.pdf}
\caption{The arrows denote the cluster centres in each iteration of the Lloyd algorithm}\label{fig:kmeanimpl_plot}
}
\end{figure}

On a side note, our algorithm needed \texttt{4}
iterations to identify the (locally optimal) cluster centres.
Figure \ref{fig:kmeanimpl_plot} depicts its quest for the clustering grail.

\hypertarget{agglomerative-hierarchical-clustering}{%
\section{Agglomerative Hierarchical Clustering}\label{agglomerative-hierarchical-clustering}}

\hypertarget{introduction-13}{%
\subsection{Introduction}\label{introduction-13}}

In K-means, we need to specify the number of clusters, \(K\), in advance.
What if we don't have any idea how to choose this parameter (which is often
the case)?

Also, the problem with K-means is that there is no guarantee that a
\(K\)-partition is any ``similar'' to the \(K'\)-one for \(K\neq K'\),
see Figure \ref{fig:kmeans_different_K}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km1 <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(X, }\DecValTok{3}\NormalTok{, }\DataTypeTok{nstart=}\DecValTok{10}\NormalTok{)}
\NormalTok{km2 <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(X, }\DecValTok{4}\NormalTok{, }\DataTypeTok{nstart=}\DecValTok{10}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{col=}\NormalTok{km1}\OperatorTok{$}\NormalTok{cluster, }\DataTypeTok{pch=}\NormalTok{km2}\OperatorTok{$}\NormalTok{cluster, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:kmeans_different_K}{%
\centering
\includegraphics{07-clustering-figures/kmeans_different_K-1.pdf}
\caption{3-means (colours) vs.~4-means (symbols) on example data; the ``circle'' cluster cannot decide if it likes the green or the black one more}\label{fig:kmeans_different_K}
}
\end{figure}

Hierarchical methods, on the other hand, output a whole hierarchy
of mutually \emph{nested} partitions, which increase the interpretability
of the results.
A \(K\)-partition for any \(K\) can be extracted later at any time.

In this book we will be interested in \emph{agglomerative} hierarchical algorithms:

\begin{itemize}
\item
  at the lowest level of the hierarchy, each point belongs to its own
  cluster (there are \(n\) singletons);
\item
  at the highest level of the hierarchy,
  there is one cluster that embraces all the points;
\item
  moving from the \(i\)-th to the \((i+1)\)-th level,
  we select (somehow; see below) a pair of clusters to be merged.
\end{itemize}

\hypertarget{example-in-r-6}{%
\subsection{Example in R}\label{example-in-r-6}}

The most basic implementation of a few
agglomerative hierarchical clustering algorithms
is provided by the \texttt{hclust()} function, which works on a pairwise
distance matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Euclidean distances between all pairs of points:}
\NormalTok{D <-}\StringTok{ }\KeywordTok{dist}\NormalTok{(X)}
\CommentTok{# Apply Complete Linkage (the default, details below):}
\NormalTok{h <-}\StringTok{ }\KeywordTok{hclust}\NormalTok{(D) }\CommentTok{# method="complete"}
\KeywordTok{print}\NormalTok{(h)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## hclust(d = D)
## 
## Cluster method   : complete 
## Distance         : euclidean 
## Number of objects: 150
\end{verbatim}

\begin{description}
\item[Remark.]
There are \(n(n-1)/2\) unique pairwise distances between \(n\) points.
Don't try calling \texttt{dist()} on large data matrices.
Already \(n=100{,}000\) points consumes 40 GB of available memory
(assuming that each distance is stored as an 8-byte double-precision floating
point number); packages \texttt{fastcluster} and \texttt{genie}, among other,
aim to solve this problem.
\end{description}

The obtained hierarchy (\emph{tree}) can be \emph{cut} at an arbitrary level
by applying the \texttt{cutree()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cutree}\NormalTok{(h, }\DataTypeTok{k=}\DecValTok{3}\NormalTok{) }\CommentTok{# extract the 3-partition}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [34] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
##  [67] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [100] 2 3 2 3 3 3 3 2 3 3 3 2 3 3 2 2 2 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3
## [133] 3 2 3 3 3 3 2 3 3 2 2 3 3 2 2 2 3 2
\end{verbatim}

The cuts of the hierarchy at different levels
are depicted in Figure \ref{fig:complete_linkage_hier5_intro}.
The obtained 3-partition also matches the true Iris species quite well.
However, now it makes total sense to ``zoom'' our partitioning in or out
and investigate how are the subgroups decomposed or aggregated when we change
\(K\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{col=}\KeywordTok{cutree}\NormalTok{(h, }\DataTypeTok{k=}\DecValTok{5}\NormalTok{), }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{"top"}\NormalTok{, }\DataTypeTok{legend=}\StringTok{"k=5"}\NormalTok{, }\DataTypeTok{bg=}\StringTok{"white"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{col=}\KeywordTok{cutree}\NormalTok{(h, }\DataTypeTok{k=}\DecValTok{4}\NormalTok{), }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{"top"}\NormalTok{, }\DataTypeTok{legend=}\StringTok{"k=4"}\NormalTok{, }\DataTypeTok{bg=}\StringTok{"white"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{col=}\KeywordTok{cutree}\NormalTok{(h, }\DataTypeTok{k=}\DecValTok{3}\NormalTok{), }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{"top"}\NormalTok{, }\DataTypeTok{legend=}\StringTok{"k=3"}\NormalTok{, }\DataTypeTok{bg=}\StringTok{"white"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{col=}\KeywordTok{cutree}\NormalTok{(h, }\DataTypeTok{k=}\DecValTok{2}\NormalTok{), }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(}\StringTok{"top"}\NormalTok{, }\DataTypeTok{legend=}\StringTok{"k=2"}\NormalTok{, }\DataTypeTok{bg=}\StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:complete_linkage_hier5_intro}{%
\centering
\includegraphics{07-clustering-figures/complete_linkage_hier5_intro-1.pdf}
\caption{Complete linkage -- 4 different cuts}\label{fig:complete_linkage_hier5_intro}
}
\end{figure}

\hypertarget{linkage-functions}{%
\subsection{Linkage Functions}\label{linkage-functions}}

Let's formalise the clustering process.
Initially, \(\mathcal{C}^{(0)}=\{\{\mathbf{x}_{1,\cdot}\},\dots,\{\mathbf{x}_{n,\cdot}\}\}\),
i.e., each point is a member of its own cluster.

While an agglomerative hierarchical clustering algorithm is being computed,
there are \(n-k\) clusters at the \(k\)-th step of the procedure,
\(\mathcal{C}^{(k)}=\{C_1^{(k)},\dots,C_{n-k}^{(k)}\}\).

When proceeding from step \(k\) to \(k+1\),
we determine the two groups \(C_u^{(k)}\) and \(C_v^{(k)}\), \(u<v\),
to be \emph{merged} together so that the clustering at the higher level
is of the form:
\[
\mathcal{C}^{(k+1)} = \left\{
C_1^{(k)},\dots,C_{u-1}^{(k)},
C_u^{(k)}{\cup C_v^{(k)}},
C_{u+1}^{(k)},\dots,C_{v-1}^{(k)},
C_{v+1}^{(k)},\dots,C_{n-k}^{(k)}
\right\}.
\]

Thus, \((\mathcal{C}^{(0)}, \mathcal{C}^{(1)}, \dots, \mathcal{C}^{(n-1)})\)
form a sequence of \emph{nested} partitions of
the input dataset
with the last level being just one big cluster,
\(\mathcal{C}^{(n-1)}=\left\{ \{\mathbf{x}_{1,\cdot},\mathbf{x}_{2,\cdot},\dots,\mathbf{x}_{n,\cdot}\} \right\}\).

\bigskip

There is one component missing -- how to determine the pair
of clusters \(C_u^{(k)}\) and \(C_v^{(k)}\) to be merged with each other
at the \(k\)-th iteration?
Of course this will be expressed as some optimisation problem (although this time,
a simple one)! The decision will be based on:
\[
\mathrm{arg}\min_{u < v} d^*(C_u^{(k)}, C_v^{(k)}),
\]
where \(d^*(C_u^{(k)}, C_v^{(k)})\) is the \emph{distance} between two clusters
\(C_u^{(k)}\) and \(C_v^{(k)}\).

Note that we usually only consider the distances between \emph{individual points},
not sets of points. Hence, \(d^*\) must be a suitable extension
of a pointwise distance \(d\) (usually the Euclidean metric)
to whole sets.

We will assume that \(d^*(\{\mathbf{x}_{i,\cdot}\},\{\mathbf{x}_{j,\cdot}\})= d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot})\), i.e., the distance between
singleton clusters is the same as the distance between the points themselves.
As far as more populous point groups are concerned, there are many popular
choices of \(d^*\) (which in the context of hierarchical clustering we call
\emph{linkage functions}):

\begin{itemize}
\item
  single linkage:

  \[
    d_\text{S}^*(C_u^{(k)}, C_v^{(k)}) =
    \min_{\mathbf{x}_{i,\cdot}\in C_u^{(k)}, \mathbf{x}_{j,\cdot}\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}),
    \]
\item
  complete linkage:

  \[
    d_\text{C}^*(C_u^{(k)}, C_v^{(k)}) =
    \max_{\mathbf{x}_{i,\cdot}\in C_u^{(k)}, \mathbf{x}_{j,\cdot}\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}),
    \]
\item
  average linkage:

  \[
    d_\text{A}^*(C_u^{(k)}, C_v^{(k)}) =
    \frac{1}{|C_u^{(k)}| |C_v^{(k)}|} \sum_{\mathbf{x}_{i,\cdot}\in C_u^{(k)}}\sum_{\mathbf{x}_{j,\cdot}\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}).
    \]
\end{itemize}

An illustration of the way different linkages are computed
is given in Figure \ref{fig:linkages}.

\begin{figure}
\hypertarget{fig:linkages}{%
\centering
\includegraphics{07-clustering-figures/linkages-1.pdf}
\caption{In single linkage, we find the closest pair of points; in complete linkage, we seek the pair furthest away from each other; in average linkage, we determine the arithmetic mean of all pairwise distances}\label{fig:linkages}
}
\end{figure}

\bigskip

Assuming \(d_\text{S}^*\), \(d_\text{C}^*\) or \(d_\text{A}^*\)
in the aforementioned procedure leads to single, complete or average
linkage-based agglomerative hierarchical clustering algorithms,
respectively
(referred to as single linkage etc. for brevity).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hs <-}\StringTok{ }\KeywordTok{hclust}\NormalTok{(D, }\DataTypeTok{method=}\StringTok{"single"}\NormalTok{)}
\NormalTok{hc <-}\StringTok{ }\KeywordTok{hclust}\NormalTok{(D, }\DataTypeTok{method=}\StringTok{"complete"}\NormalTok{)}
\NormalTok{ha <-}\StringTok{ }\KeywordTok{hclust}\NormalTok{(D, }\DataTypeTok{method=}\StringTok{"average"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Figure \ref{fig:linkages_hier52} compares the 5-, 4- and 3-partitions
obtained by applying the 3 above linkages. Note that it's in very nature of
the single linkage algorithm that it's highly sensitive to outliers.

\begin{figure}
\hypertarget{fig:linkages_hier52}{%
\centering
\includegraphics{07-clustering-figures/linkages_hier52-1.pdf}
\caption{3 cuts of 3 different hierarchies}\label{fig:linkages_hier52}
}
\end{figure}

\hypertarget{cluster-dendrograms}{%
\subsection{Cluster Dendrograms}\label{cluster-dendrograms}}

A \emph{dendrogram} (which we can plot by calling \texttt{plot(h)}, where \texttt{h} is
the result returned by \texttt{hclust()}) depicts the distances
(as defined by the linkage function) between the clusters merged at every stage
of the agglomerative procedure.
This can provide us with some insight into the underlying data structure
as well as with hits about at which level the tree could be cut.

Figure \ref{fig:dendrograms} depicts the three dendrograms
that correspond to the clusterings obtained by applying different linkages.
Each tree has 150 leaves (at the bottom) that represent the 150 points
in our example dataset. Each ``edge'' (joint) represents a group of points
being merged. For instance, the very top joint in the middle subfigure
is located at height of \(\simeq 6\), which is exactly the
maximal pairwise distance (complete linkage) between the points
in the last two last clusters.

\begin{figure}
\hypertarget{fig:dendrograms}{%
\centering
\includegraphics{07-clustering-figures/dendrograms-1.pdf}
\caption{Cluster dendrograms for the single, complete and average linkages}\label{fig:dendrograms}
}
\end{figure}

\hypertarget{exercises-in-r-3}{%
\section{Exercises in R}\label{exercises-in-r-3}}

\hypertarget{clustering-of-the-world-factbook}{%
\subsection{Clustering of the World Factbook}\label{clustering-of-the-world-factbook}}

Let's perform a cluster analysis of countries
based on the information contained in the World Factbook dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factbook <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/world_factbook_2020.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{exercise}

Remove all the columns that consist of more than 40 missing values.
Then remove all the rows with at least 1 missing value.

\end{exercise}

\begin{solution}

To remove appropriate columns, we must first count the number
of \texttt{NA}s in them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{count_na_in_columns <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(factbook, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)))}
\NormalTok{factbook <-}\StringTok{ }\NormalTok{factbook[count_na_in_columns }\OperatorTok{<=}\StringTok{ }\DecValTok{40}\NormalTok{] }\CommentTok{# column removal}
\end{Highlighting}
\end{Shaded}

Getting rid of the rows plagued by missing values is as simple as calling
the \texttt{na.omit()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factbook <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(factbook) }\CommentTok{# row removal}
\KeywordTok{dim}\NormalTok{(factbook) }\CommentTok{# how many rows and cols remained}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 203  23
\end{verbatim}

Missing value removal is necessary for metric-based
clustering methods, especially K-means. Otherwise, some of the computed distances
would be not available.

\end{solution}

\begin{exercise}

Standardise all the numeric columns.

\end{exercise}

\begin{solution}

Distance-based
methods are very sensitive to the order of magnitude of
the variables, and our dataset is a mess with regards to this
(population, GDP, birth rate, oil production etc.) -- standardisation
of variables is definitely a good idea:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(factbook)) }\CommentTok{# skip `country`}
\NormalTok{    factbook[[i]] <-}\StringTok{ }\NormalTok{(factbook[[i]]}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(factbook[[i]]))}\OperatorTok{/}
\StringTok{                        }\KeywordTok{sd}\NormalTok{(factbook[[i]])}
\end{Highlighting}
\end{Shaded}

Recall that Z-scores (values of the standardised variables)
have a very intuitive interpretation: \(0\) is the value equal to the column
mean, \(1\) is one standard deviation above the mean, \(-2\) is two standard deviations
below the mean etc.

\end{solution}

\begin{exercise}

Apply the 2-means algorithm, i.e., K-means with \(K=2\).
Analyse the results.

\end{exercise}

\begin{solution}

Calling \texttt{kmeans()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(factbook[}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DecValTok{2}\NormalTok{, }\DataTypeTok{nstart=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's split the country list w.r.t. the obtained cluster labels.
It turns out that the obtained partition is heavily imbalanced, so we'll
print only the contents of the first group:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km_countries <-}\StringTok{ }\KeywordTok{split}\NormalTok{(factbook[[}\DecValTok{1}\NormalTok{]], km}\OperatorTok{$}\NormalTok{cluster)}
\NormalTok{km_countries[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "China"         "India"         "United States"
\end{verbatim}

With regards to which criteria has the K-means algorithm distinguished
the countries? Let's inspect the cluster centres to check the average
Z-scores of all the countries in each cluster:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t}\NormalTok{(km}\OperatorTok{$}\NormalTok{centers) }\CommentTok{# transposed for readability}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                        1          2
## area                            3.661581 -0.0549237
## population                      6.987279 -0.1048092
## median_age                      0.477991 -0.0071699
## population_growth_rate         -0.252774  0.0037916
## birth_rate                     -0.501030  0.0075155
## death_rate                      0.153915 -0.0023087
## net_migration_rate              0.236449 -0.0035467
## infant_mortality_rate          -0.139577  0.0020937
## life_expectancy_at_birth        0.251541 -0.0037731
## total_fertility_rate           -0.472716  0.0070907
## gdp_purchasing_power_parity     7.213681 -0.1082052
## gdp_real_growth_rate            0.369499 -0.0055425
## gdp_per_capita_ppp              0.298103 -0.0044715
## labor_force                     6.914319 -0.1037148
## taxes_and_other_revenues       -0.922735  0.0138410
## budget_surplus_or_deficit      -0.012627  0.0001894
## inflation_rate_consumer_prices -0.096626  0.0014494
## exports                         5.341178 -0.0801177
## imports                         5.956538 -0.0893481
## telephones_fixed_lines          5.989858 -0.0898479
## internet_users                  6.997126 -0.1049569
## airports                        4.551832 -0.0682775
\end{verbatim}

Countries in Cluster 2 are\ldots{} average (Z-scores \(\simeq 0\)).
On the other hand, the three countries in Cluster 1
dominate the others w.r.t. area, population, GDP PPP, labour force etc.

\end{solution}

\begin{exercise}

Apply the complete linkage agglomerative hierarchical clustering algorithm.

\end{exercise}

\begin{solution}

Recall that the complete linkage-based method is implemented in the
\texttt{hclust()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d <-}\StringTok{ }\KeywordTok{dist}\NormalTok{(factbook[}\OperatorTok{-}\DecValTok{1}\NormalTok{]) }\CommentTok{# skip `country`}
\NormalTok{h <-}\StringTok{ }\KeywordTok{hclust}\NormalTok{(d, }\DataTypeTok{method=}\StringTok{"complete"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A ``nice'' number of clusters to divide our dataset into
can be read from the dendrogram, see Figure \ref{fig:clustering_factbook7}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(h, }\DataTypeTok{labels=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{); }\KeywordTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:clustering_factbook7}{%
\centering
\includegraphics{07-clustering-figures/clustering_factbook7-1.pdf}
\caption{Cluster dendrogram for the World Factbook dataset -- Complete linkage}\label{fig:clustering_factbook7}
}
\end{figure}

It seems that a 9-partition might reveal something interesting,
because it will distinguish two larger country groups.
However, there will be many singletons if we do so either way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{cutree}\NormalTok{(h, }\DecValTok{9}\NormalTok{)}
\NormalTok{h_countries <-}\StringTok{ }\KeywordTok{split}\NormalTok{(factbook[[}\DecValTok{1}\NormalTok{]], y)}
\KeywordTok{sapply}\NormalTok{(h_countries, length) }\CommentTok{# number of elements in each cluster}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   1   2   3   4   5   6   7   8   9 
## 138  56   1   1   3   1   1   1   1
\end{verbatim}

Most likely this is not an interesting partitioning of this dataset,
therefore we'll not be exploring it any further.

\end{solution}

\begin{exercise}

Apply the Genie clustering algorithm.

\end{exercise}

\begin{solution}

The Genie algorithm (Gagolewski et al. \protect\hyperlink{ref-genie}{2016}) is a hierarchical clustering algorithm
implemented in R package \texttt{genie}. It's interface is compatible with \texttt{hclust()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"genie"}\NormalTok{)}
\NormalTok{d <-}\StringTok{ }\KeywordTok{dist}\NormalTok{(factbook[}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\NormalTok{g <-}\StringTok{ }\KeywordTok{hclust2}\NormalTok{(d)}
\end{Highlighting}
\end{Shaded}

The cluster dendrogram in Figure \ref{fig:clustering_factbook10}
reveals 3 evident clusters.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(g, }\DataTypeTok{labels=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{); }\KeywordTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:clustering_factbook10}{%
\centering
\includegraphics{07-clustering-figures/clustering_factbook10-1.pdf}
\caption{Cluster dendrogram for the World Factbook dataset -- Genie algorithm}\label{fig:clustering_factbook10}
}
\end{figure}

Let's determine the 3-partition of the data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{cutree}\NormalTok{(g, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here are few countries in each cluster:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{cutree}\NormalTok{(g, }\DecValTok{3}\NormalTok{)}
\KeywordTok{sapply}\NormalTok{(}\KeywordTok{split}\NormalTok{(factbook}\OperatorTok{$}\NormalTok{countr, y), sample, }\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      1                    2                                  
## [1,] "Dominican Republic" "Congo, Republic of the"           
## [2,] "Venezuela"          "Sao Tome and Principe"            
## [3,] "Sri Lanka"          "Tanzania"                         
## [4,] "Malta"              "Botswana"                         
## [5,] "China"              "Congo, Democratic Republic of the"
## [6,] "Tajikistan"         "Malawi"                           
##      3             
## [1,] "Lithuania"   
## [2,] "Portugal"    
## [3,] "Korea, South"
## [4,] "Bulgaria"    
## [5,] "Germany"     
## [6,] "Moldova"
\end{verbatim}

We can draw the countries in each cluster
on a map by using the \texttt{rworldmap} package (see its documentation for more details),
see Figure \ref{fig:clustering_factbook12}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"rworldmap"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"RColorBrewer"}\NormalTok{)}
\NormalTok{mapdata <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{country=}\NormalTok{factbook}\OperatorTok{$}\NormalTok{country, }\DataTypeTok{cluster=}\NormalTok{y,}
    \DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{# 3 country names must be adjusted to get a match}
\NormalTok{mapdata}\OperatorTok{$}\NormalTok{country[mapdata}\OperatorTok{$}\NormalTok{country }\OperatorTok{==}\StringTok{ "Czechia"}\NormalTok{] <-}\StringTok{ "Czech Republic"}
\NormalTok{mapdata}\OperatorTok{$}\NormalTok{country[mapdata}\OperatorTok{$}\NormalTok{country }\OperatorTok{==}\StringTok{ "Eswatini"}\NormalTok{] <-}\StringTok{ "Swaziland"}
\NormalTok{mapdata}\OperatorTok{$}\NormalTok{country[mapdata}\OperatorTok{$}\NormalTok{country }\OperatorTok{==}\StringTok{ "Cabo Verde"}\NormalTok{] <-}\StringTok{ "Cape Verde"}
\NormalTok{mapdata <-}\StringTok{ }\KeywordTok{joinCountryData2Map}\NormalTok{(mapdata, }\DataTypeTok{joinCode=}\StringTok{"NAME"}\NormalTok{,}
  \DataTypeTok{nameJoinColumn=}\StringTok{"country"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 203 codes from your data successfully matched countries in the map
## 0 codes from your data failed to match with a country code in the map
## 40 codes from the map weren't represented in your data
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\KeywordTok{mapCountryData}\NormalTok{(mapdata, }\DataTypeTok{nameColumnToPlot=}\StringTok{"cluster"}\NormalTok{,}
    \DataTypeTok{catMethod=}\StringTok{"categorical"}\NormalTok{, }\DataTypeTok{missingCountryCol=}\StringTok{"gray"}\NormalTok{,}
    \DataTypeTok{colourPalette=}\KeywordTok{brewer.pal}\NormalTok{(}\DecValTok{3}\NormalTok{, }\StringTok{"Set1"}\NormalTok{),}
    \DataTypeTok{mapTitle=}\StringTok{""}\NormalTok{, }\DataTypeTok{addLegend=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:clustering_factbook12}{%
\centering
\includegraphics{07-clustering-figures/clustering_factbook12-1.pdf}
\caption{3 clusters discovered by the Genie algorithm}\label{fig:clustering_factbook12}
}
\end{figure}

Here are the average Z-scores in each cluster:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(}\KeywordTok{split}\NormalTok{(factbook[}\OperatorTok{-}\DecValTok{1}\NormalTok{], y), colMeans), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                     1      2      3
## area                            0.124 -0.068 -0.243
## population                      0.077 -0.058 -0.130
## median_age                      0.118 -1.219  1.261
## population_growth_rate         -0.227  1.052 -0.757
## birth_rate                     -0.316  1.370 -0.930
## death_rate                     -0.439  0.071  1.075
## net_migration_rate             -0.123  0.053  0.260
## infant_mortality_rate          -0.366  1.399 -0.835
## life_expectancy_at_birth        0.354 -1.356  0.812
## total_fertility_rate           -0.363  1.332 -0.758
## gdp_purchasing_power_parity     0.084 -0.213  0.052
## gdp_real_growth_rate           -0.062  0.126  0.002
## gdp_per_capita_ppp              0.021 -0.744  0.905
## labor_force                     0.087 -0.096 -0.107
## taxes_and_other_revenues       -0.095 -0.584  1.006
## budget_surplus_or_deficit      -0.113 -0.188  0.543
## inflation_rate_consumer_prices  0.044 -0.013 -0.099
## exports                        -0.013 -0.318  0.447
## imports                         0.007 -0.308  0.379
## telephones_fixed_lines          0.048 -0.244  0.186
## internet_users                  0.093 -0.178 -0.016
## airports                        0.104 -0.131 -0.107
\end{verbatim}

That is really interesting! The interpretation of the above is left
to the reader.

\end{solution}

\hypertarget{unbalance-dataset-k-means-needs-multiple-starts}{%
\subsection{Unbalance Dataset -- K-Means Needs Multiple Starts}\label{unbalance-dataset-k-means-needs-multiple-starts}}

Let us consider a benchmark (artificial) dataset
proposed in (Rezaei \& Fränti \protect\hyperlink{ref-external_cluster_validity}{2016}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unbalance <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/sipu_unbalance.csv"}\NormalTok{,}
    \DataTypeTok{header=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{sep=}\StringTok{" "}\NormalTok{, }\DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{,}
    \DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{))}
\NormalTok{unbalance <-}\StringTok{ }\NormalTok{unbalance}\OperatorTok{/}\DecValTok{10000-30} \CommentTok{# a more user-friendly scale}
\end{Highlighting}
\end{Shaded}

According to its authors, this dataset is comprised of 8 clusters:
there are 3 groups on the lefthand side (2000 points each)
and 5 on the right side (100 each).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(unbalance, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:sipu_unbalance2}{%
\centering
\includegraphics{07-clustering-figures/sipu_unbalance2-1.pdf}
\caption{\texttt{sipu\_unbalance} dataset}\label{fig:sipu_unbalance2}
}
\end{figure}

\begin{exercise}

Apply the K-means algorithm with \(K=8\).

\end{exercise}

\begin{solution}

Of course, here by ``the'' K-means we mean the default method
available in the \texttt{kmeans()} function.
The clustering results are depicted in Figure \ref{fig:sipu_unbalance3a}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(unbalance, }\DecValTok{8}\NormalTok{, }\DataTypeTok{nstart=}\DecValTok{10}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(unbalance, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{, }\DataTypeTok{col=}\NormalTok{km}\OperatorTok{$}\NormalTok{cluster)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:sipu_unbalance3a}{%
\centering
\includegraphics{07-clustering-figures/sipu_unbalance3a-1.pdf}
\caption{Results of K-means on the \texttt{sipu\_unbalance} dataset}\label{fig:sipu_unbalance3a}
}
\end{figure}

This is far from what we expected.
The total within-cluster distances are equal to:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km}\OperatorTok{$}\NormalTok{tot.withinss}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 21713
\end{verbatim}

Increasing the number of restarts even further improves the solution,
but the local minimum is still far from the global one,
compare Figure \ref{fig:sipu_unbalance3b}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km <-}\StringTok{ }\KeywordTok{suppressWarnings}\NormalTok{(}\KeywordTok{kmeans}\NormalTok{(unbalance, }\DecValTok{8}\NormalTok{, }\DataTypeTok{nstart=}\DecValTok{1000}\NormalTok{, }\DataTypeTok{iter.max=}\DecValTok{1000}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(unbalance, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{, }\DataTypeTok{col=}\NormalTok{km}\OperatorTok{$}\NormalTok{cluster)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:sipu_unbalance3b}{%
\centering
\includegraphics{07-clustering-figures/sipu_unbalance3b-1.pdf}
\caption{Results of K-means on the \texttt{sipu\_unbalance} dataset -- many more restarts}\label{fig:sipu_unbalance3b}
}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km}\OperatorTok{$}\NormalTok{tot.withinss}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4378
\end{verbatim}

\end{solution}

\begin{exercise}

Apply the K-means algorithm starting from a ``good'' initial guess
on the true cluster centres.

\end{exercise}

\begin{solution}

Clustering is -- in its essence -- an unsupervised learning method,
so what we're going to do now could be called, let's be blunt about it, cheating.
Luckily, we have an oracle at our disposal -- it has provided us
with the following educated guesses (by looking at the scatter plot)
about the localisation of the cluster centres:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cntr <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{, }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{, }\KeywordTok{c}\NormalTok{(}
   \DecValTok{-15}\NormalTok{,   }\DecValTok{5}\NormalTok{,}
   \DecValTok{-12}\NormalTok{,   }\DecValTok{10}\NormalTok{,}
   \DecValTok{-10}\NormalTok{,   }\DecValTok{5}\NormalTok{,}
    \DecValTok{15}\NormalTok{,   }\DecValTok{0}\NormalTok{,}
    \DecValTok{15}\NormalTok{,   }\DecValTok{10}\NormalTok{,}
    \DecValTok{20}\NormalTok{,   }\DecValTok{5}\NormalTok{,}
    \DecValTok{25}\NormalTok{,   }\DecValTok{0}\NormalTok{,}
    \DecValTok{25}\NormalTok{,   }\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Running \texttt{kmeans()} yields the clustering depicted in Figure \ref{fig:sipu_unbalance6}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(unbalance, cntr)}
\KeywordTok{plot}\NormalTok{(unbalance, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{, }\DataTypeTok{col=}\NormalTok{km}\OperatorTok{$}\NormalTok{cluster)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:sipu_unbalance6}{%
\centering
\includegraphics{07-clustering-figures/sipu_unbalance6-1.pdf}
\caption{Results of K-means on the \texttt{sipu\_unbalance} dataset -- an educated guess on the cluster centres' locations}\label{fig:sipu_unbalance6}
}
\end{figure}

The total within-cluster distances are now equal to:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{km}\OperatorTok{$}\NormalTok{tot.withinss}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2144.9
\end{verbatim}

This is finally the globally optimal solution to the K-means problem
we were asked to solve. Recall that the algorithms implemented in the \texttt{kmeans()}
function are just fast heuristics that are supposed to find
local optima of the K-means objective function, which is given
by the within-cluster sum of squared Euclidean distances.

\end{solution}

\hypertarget{clustering-of-typical-2d-benchmark-datasets}{%
\subsection{Clustering of Typical 2D Benchmark Datasets}\label{clustering-of-typical-2d-benchmark-datasets}}

Let us consider a few clustering benchmark datasets
available at \url{https://github.com/gagolews/clustering_benchmarks_v1}
and \url{http://cs.joensuu.fi/sipu/datasets/}.
Here is a list of file names together with the
corresponding numbers of clusters (as given by datasets' authors):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{files <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"datasets/wut_isolation.csv"}\NormalTok{,}
           \StringTok{"datasets/wut_mk2.csv"}\NormalTok{,}
           \StringTok{"datasets/wut_z3.csv"}\NormalTok{,}
           \StringTok{"datasets/sipu_aggregation.csv"}\NormalTok{,}
           \StringTok{"datasets/sipu_pathbased.csv"}\NormalTok{,}
           \StringTok{"datasets/sipu_unbalance.csv"}\NormalTok{)}
\NormalTok{Ks <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

All the datasets are two-dimensional, hence we'll be able to visualise
the obtained results and assess the sensibility of the obtained clusterings.

\begin{exercise}

Apply the K-means, the single, average and complete linkage
and the Genie algorithm on the aforementioned datasets and discuss the results.

\end{exercise}

\begin{solution}

Apart from a call to the Genie algorithm with the default parameters,
we will also look at the results it generates when we set
\texttt{giniThreshold} of \texttt{0.5}.

The following function is our workhorse that will perform all the computations
and will draw all the figures for a single dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clusterise <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(file, K) \{}
\NormalTok{    X <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(file,}
        \DataTypeTok{header=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{sep=}\StringTok{" "}\NormalTok{, }\DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{,}
        \DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{    d <-}\StringTok{ }\KeywordTok{dist}\NormalTok{(X)}
    \KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{))}
    \KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}

\NormalTok{    y <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(X, K, }\DataTypeTok{nstart=}\DecValTok{10}\NormalTok{)}\OperatorTok{$}\NormalTok{cluster}
    \KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{, }\DataTypeTok{col=}\NormalTok{y, }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{axes=}\OtherTok{FALSE}\NormalTok{)}
    \KeywordTok{mtext}\NormalTok{(}\StringTok{"K-means"}\NormalTok{, }\DataTypeTok{line=}\FloatTok{0.5}\NormalTok{)}

\NormalTok{    y <-}\StringTok{ }\KeywordTok{cutree}\NormalTok{(}\KeywordTok{hclust}\NormalTok{(d, }\StringTok{"complete"}\NormalTok{), K)}
    \KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{, }\DataTypeTok{col=}\NormalTok{y, }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{axes=}\OtherTok{FALSE}\NormalTok{)}
    \KeywordTok{mtext}\NormalTok{(}\StringTok{"Complete Linkage"}\NormalTok{, }\DataTypeTok{line=}\FloatTok{0.5}\NormalTok{)}

\NormalTok{    y <-}\StringTok{ }\KeywordTok{cutree}\NormalTok{(}\KeywordTok{hclust}\NormalTok{(d, }\StringTok{"average"}\NormalTok{), K)}
    \KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{, }\DataTypeTok{col=}\NormalTok{y, }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{axes=}\OtherTok{FALSE}\NormalTok{)}
    \KeywordTok{mtext}\NormalTok{(}\StringTok{"Average Linkage"}\NormalTok{, }\DataTypeTok{line=}\FloatTok{0.5}\NormalTok{)}

\NormalTok{    y <-}\StringTok{ }\KeywordTok{cutree}\NormalTok{(}\KeywordTok{hclust}\NormalTok{(d, }\StringTok{"single"}\NormalTok{), K)}
    \KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{, }\DataTypeTok{col=}\NormalTok{y, }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{axes=}\OtherTok{FALSE}\NormalTok{)}
    \KeywordTok{mtext}\NormalTok{(}\StringTok{"Single Linkage"}\NormalTok{, }\DataTypeTok{line=}\FloatTok{0.5}\NormalTok{)}

\NormalTok{    y <-}\StringTok{ }\KeywordTok{cutree}\NormalTok{(genie}\OperatorTok{::}\KeywordTok{hclust2}\NormalTok{(d), K) }\CommentTok{# thresholdGini=0.3}
    \KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{, }\DataTypeTok{col=}\NormalTok{y, }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{axes=}\OtherTok{FALSE}\NormalTok{)}
    \KeywordTok{mtext}\NormalTok{(}\StringTok{"Genie (default)"}\NormalTok{, }\DataTypeTok{line=}\FloatTok{0.5}\NormalTok{)}

\NormalTok{    y <-}\StringTok{ }\KeywordTok{cutree}\NormalTok{(genie}\OperatorTok{::}\KeywordTok{hclust2}\NormalTok{(d, }\DataTypeTok{thresholdGini=}\FloatTok{0.5}\NormalTok{), K)}
    \KeywordTok{plot}\NormalTok{(X, }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{, }\DataTypeTok{col=}\NormalTok{y, }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{axes=}\OtherTok{FALSE}\NormalTok{)}
    \KeywordTok{mtext}\NormalTok{(}\StringTok{"Genie (g=0.5)"}\NormalTok{, }\DataTypeTok{line=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Applying the above as \texttt{clusterise(files{[}i{]},\ Ks{[}i{]})} yields
Figures \ref{fig:clustering_benchmarks_plot1}-\ref{fig:clustering_benchmarks_plot6}.

\begin{figure}
\hypertarget{fig:clustering_benchmarks_plot1}{%
\centering
\includegraphics{07-clustering-figures/clustering_benchmarks_plot1-1.pdf}
\caption{Clustering of the \texttt{wut\_isolation} dataset}\label{fig:clustering_benchmarks_plot1}
}
\end{figure}

\begin{figure}
\hypertarget{fig:clustering_benchmarks_plot2}{%
\centering
\includegraphics{07-clustering-figures/clustering_benchmarks_plot2-1.pdf}
\caption{Clustering of the \texttt{wut\_mk2} dataset}\label{fig:clustering_benchmarks_plot2}
}
\end{figure}

\begin{figure}
\hypertarget{fig:clustering_benchmarks_plot3}{%
\centering
\includegraphics{07-clustering-figures/clustering_benchmarks_plot3-1.pdf}
\caption{Clustering of the \texttt{wut\_z3} dataset}\label{fig:clustering_benchmarks_plot3}
}
\end{figure}

\begin{figure}
\hypertarget{fig:clustering_benchmarks_plot4}{%
\centering
\includegraphics{07-clustering-figures/clustering_benchmarks_plot4-1.pdf}
\caption{Clustering of the \texttt{sipu\_aggregation} dataset}\label{fig:clustering_benchmarks_plot4}
}
\end{figure}

\begin{figure}
\hypertarget{fig:clustering_benchmarks_plot5}{%
\centering
\includegraphics{07-clustering-figures/clustering_benchmarks_plot5-1.pdf}
\caption{Clustering of the \texttt{sipu\_pathbased} dataset}\label{fig:clustering_benchmarks_plot5}
}
\end{figure}

\begin{figure}
\hypertarget{fig:clustering_benchmarks_plot6}{%
\centering
\includegraphics{07-clustering-figures/clustering_benchmarks_plot6-1.pdf}
\caption{Clustering of the \texttt{sipu\_unbalance} dataset}\label{fig:clustering_benchmarks_plot6}
}
\end{figure}

Note that, by definition, K-means is only able to detect clusters
of convex shapes. The Genie algorithm, on the other hand, might
fail to detect clusters of very small sizes amongst the more populous ones.
Single linkage is very sensitive to outliers in data -- it often outputs
clusters of cardinality 1.

\end{solution}

\hypertarget{outro-6}{%
\section{Outro}\label{outro-6}}

\hypertarget{remarks-6}{%
\subsection{Remarks}\label{remarks-6}}

Unsupervised learning is often performed during the data pre-processing
and exploration stage.
Assessing the quality of clustering is particularly challenging as,
unlike in a supervised setting,
we have no access to ``ground truth'' information.

In practice, we often apply different clustering algorithms
and just see where they lead us. There's no teacher that would
tell us what we should do, so whatever we do is awesome, right?
Well, not precisely. Most frequently, you, my dear reader, will work
for some party that's genuinely
interested in your explaining why did you spent the last month coming up
with nothing useful at all. Thus, the main body of work related to proving
the use-ful\emph{l}/less-ness will be on you.

Clustering methods can aid us in supervised tasks
-- instead of fitting a single ``large model'', it might be
useful to fit separate models to each cluster.

\bigskip

To sum up, the aim of K-means is to find
\(K\) clusters based on the notion of the points' closeness
to the cluster centres. Remember that \(K\) must be set in advance.
By definition (* via its relation to Voronoi diagrams),
all clusters will be of convex shapes.

However, we may try applying \(K'\)-means for \(K' \gg K\)
to obtain a ``fine grained'' compressed representation of data and then
combine the (sub)clusters into more meaningful groups
using other methods (such as the hierarchical ones).

Iterative K-means algorithms are very fast (e.g., a mini-batch
version of the algorithm can be implement to speed up the optimisation
process) even for large data sets,
but they may fail to find a desirable solution, especially if clusters
are unbalanced.

\bigskip

Hierarchical methods, on the other hand, output
a whole family of mutually nested partitions, which may provide us
with insight into the underlying structure of data data.
Unfortunately, there is no easy way to assign new points
to existing clusters; yet, you can always build a classifier
(e.g., a decision tree or a neural network) that learns
the discovered labels.

A linkage scheme must be chosen with care, for instance, single linkage
can be sensitive to outliers. However, it is generally the fastest.
The methods implemented in \texttt{hclust()} are generally slow; they have
time complexity between \(O(n^2)\) and \(O(n^3)\).

\begin{description}
\item[Remark.]
Note that the \texttt{fastcluster} package provides a more efficient
and memory-saving
implementation of some methods available via a call to \texttt{hclust()}.
See also the \texttt{genie} package for a super-robust version of the single linkage
algorithm based on the datasets's Euclidean minimum spanning tree,
which can be computed quite quickly.
\end{description}

Finally, note that all the discussed clustering methods are based on the
notion of pairwise distances. These of course tend
to behave weirdly in high-dimensional
spaces (``the curse of dimensionality''). Moreover, some hardcore
feature engineering might be needed to obtain meaningful results.

\hypertarget{further-reading-6}{%
\subsection{Further Reading}\label{further-reading-6}}

Recommended further reading: (James et al. \protect\hyperlink{ref-islr}{2017}: Section 10.3)

Other: (Hastie et al. \protect\hyperlink{ref-esl}{2017}: Section 14.3)

Additionally, check out other noteworthy clustering approaches:

\begin{itemize}
\tightlist
\item
  Genie (see R package \texttt{genie}) (Gagolewski et al. \protect\hyperlink{ref-genie}{2016}, Cena \& Gagolewski \protect\hyperlink{ref-genieowa}{2020})
\item
  ITM (Müller et al. \protect\hyperlink{ref-itm}{2012})
\item
  DBSCAN, HDBSCAN* (Ling \protect\hyperlink{ref-pre_dbscan}{1973}, Ester et al. \protect\hyperlink{ref-dbscan}{1996}, Campello et al. \protect\hyperlink{ref-hdbscan}{2015})
\item
  K-medoids, K-medians
\item
  Fuzzy C-means (a.k.a. weighted K-means) (Bezdek et al. \protect\hyperlink{ref-cmeans}{1984})
\item
  Spectral clustering; e.g., (Ng et al. \protect\hyperlink{ref-spectral_nips}{2001})
\item
  BIRCH (Zhang et al. \protect\hyperlink{ref-birch}{1996})
\end{itemize}

\hypertarget{optimisation-with-genetic-algorithms}{%
\chapter{Optimisation with Genetic Algorithms}\label{optimisation-with-genetic-algorithms}}

\hypertarget{introduction-14}{%
\section{Introduction}\label{introduction-14}}

\hypertarget{recap}{%
\subsection{Recap}\label{recap}}

Recall that an \textbf{optimisation task} deals with finding an element \(\mathbf{x}\)
in a \textbf{search space} \(\mathbb{D}\),
that minimises or maximises an \textbf{objective function} \(f:\mathbb{D}\to\mathbb{R}\):
\[
\min_{\mathbf{x}\in \mathbb{D}} f(\mathbf{x}) \quad\text{or}\quad\max_{\mathbf{x}\in \mathbb{D}} f(\mathbf{x}),
\]

In one of the previous chapters, we were dealing with
\textbf{unconstrained continuous optimisation},
i.e., we assumed the search space is \(\mathbb{D}=\mathbb{R}^p\) for some \(p\).

~\\

Example problems of this kind: minimising mean squared error
in linear regression
or minimising cross-entropy in logistic regression.

The class of general-purpose iterative algorithms we've previously studied
fit into the following scheme:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\mathbf{x}^{(0)}\) -- initial guess (e.g., generated at random)
\item
  for \(i=1,...,M\):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \(\mathbf{x}^{(i)} = \mathbf{x}^{(i-1)}+\text{[guessed direction, e.g.,}-\eta\nabla f(\mathbf{x})\text{]}\)
  \item
    if \(|f(\mathbf{x}^{(i)})-f(\mathbf{x}^{(i-1)})| < \varepsilon\) break
  \end{enumerate}
\item
  return \(\mathbf{x}^{(i)}\) as result
\end{enumerate}

where:

\begin{itemize}
\tightlist
\item
  \(M\) = maximum number of iterations
\item
  \(\varepsilon\) = tolerance, e.g, \(10^{-8}\)
\item
  \(\eta>0\) = learning rate
\end{itemize}

The algorithms such as gradient descent and BFGS (see \texttt{optim()})
give satisfactory results in the case of \textbf{smooth and well-behaving objective functions}.

However, if an objective has, e.g., many plateaus (regions where it is almost constant),
those methods might easily get stuck in local minima.

The K-means clustering's objective function is a not particularly pleasant
one -- it involves a nested search for the closest cluster, with the use of the \(\min\) operator.

\hypertarget{k-means-revisited}{%
\subsection{K-means Revisited}\label{k-means-revisited}}

In \textbf{K-means clustering} we are minimising the squared Euclidean distance
to each point's cluster centre:
\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{k=1,\dots,K}
\sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\right).
\]

This is an (NP-)hard problem!
There is no efficient exact algorithm.

We need approximations. In the last chapter, we have
discussed the iterative Lloyd's algorithm (1957),
which is amongst a few procedures implemented in the \texttt{kmeans()} function.

To recall, Lloyd's algorithm (1957) is sometimes referred to as ``the'' K-means algorithm:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start with random cluster centres \(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\).
\item
  For each point \(\mathbf{x}_{i,\cdot}\), determine its closest centre \(C(i)\in\{1,\dots,K\}\).
\item
  For each cluster \(k\in\{1,\dots,K\}\), compute the new cluster centre \(\boldsymbol\mu_{k,\cdot}\) as the componentwise arithmetic mean
  of the coordinates of all the point indices \(i\) such that \(C(i)=k\).
\item
  If the cluster centres changed since last iteration, go to step 2, otherwise stop and return the result.
\end{enumerate}

~\\

As the procedure might get stuck in a local minimum,
a few restarts are recommended (as usual).

Hence, we are used to calling:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{kmeans}\NormalTok{(X, }\DataTypeTok{centers=}\NormalTok{k, }\DataTypeTok{nstart=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{optim-vs.-kmeans}{%
\subsection{optim() vs.~kmeans()}\label{optim-vs.-kmeans}}

Let us compare how a general-purpose optimiser such as the BFGS algorithm
implemented in \texttt{optim()} compares with a customised, problem-specific solver.

We will need some benchmark data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen_cluster <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(n, p, m, s) \{}
\NormalTok{    vectors <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{p), }\DataTypeTok{nrow=}\NormalTok{n, }\DataTypeTok{ncol=}\NormalTok{p)}
\NormalTok{    unit_vectors <-}\StringTok{ }\NormalTok{vectors}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{rowSums}\NormalTok{(vectors}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\NormalTok{    unit_vectors}\OperatorTok{*}\KeywordTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, s)}\OperatorTok{+}\KeywordTok{rep}\NormalTok{(m, }\DataTypeTok{each=}\NormalTok{n)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The above function generates \(n\) points in \(\mathbb{R}^p\)
from a distribution centred at \(\mathbf{m}\in\mathbb{R}^p\),
spread randomly in every possible direction with scale factor \(s\).

Two example clusters in \(\mathbb{R}^2\):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot the "black" cluster}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{gen_cluster}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{2}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\DecValTok{1}\NormalTok{), }\DataTypeTok{col=}\StringTok{"#00000022"}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{16}\NormalTok{,}
    \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\DataTypeTok{asp=}\DecValTok{1}\NormalTok{, }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{)}
\CommentTok{# plot the "red" cluster}
\KeywordTok{points}\NormalTok{(}\KeywordTok{gen_cluster}\NormalTok{(}\DecValTok{250}\NormalTok{, }\DecValTok{2}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\DecValTok{1}\NormalTok{), }\FloatTok{0.5}\NormalTok{), }\DataTypeTok{col=}\StringTok{"#ff000022"}\NormalTok{, }\DataTypeTok{pch=}\DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:gendata_example}{%
\centering
\includegraphics{08-optimisation-genetic-figures/gendata_example-1.pdf}
\caption{plot of chunk gendata\_example}\label{fig:gendata_example}
}
\end{figure}

Let's generate the benchmark dataset \(\mathbf{X}\)
that consists of three clusters in a high-dimensional space.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{p  <-}\StringTok{ }\DecValTok{32}
\NormalTok{Ns <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\NormalTok{Ms <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{s  <-}\StringTok{ }\FloatTok{1.5}\OperatorTok{*}\NormalTok{p}
\NormalTok{K  <-}\StringTok{ }\KeywordTok{length}\NormalTok{(Ns)}

\NormalTok{X <-}\StringTok{ }\KeywordTok{lapply}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{K, }\ControlFlowTok{function}\NormalTok{(k)}
    \KeywordTok{gen_cluster}\NormalTok{(Ns[k], p, }\KeywordTok{rep}\NormalTok{(Ms[k], p), s))}
\NormalTok{X <-}\StringTok{ }\KeywordTok{do.call}\NormalTok{(rbind, X) }\CommentTok{# rbind(X[[1]], X[[2]], X[[3]])}
\end{Highlighting}
\end{Shaded}

The objective function for the K-means clustering problem:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"FNN"}\NormalTok{)}
\NormalTok{get_fitness <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(mu, X) \{}
    \CommentTok{# For each point in X,}
    \CommentTok{# get the index of the closest point in mu:}
\NormalTok{    memb <-}\StringTok{ }\NormalTok{FNN}\OperatorTok{::}\KeywordTok{get.knnx}\NormalTok{(mu, X, }\DecValTok{1}\NormalTok{)}\OperatorTok{$}\NormalTok{nn.index}

    \CommentTok{# compute the sum of squared distances}
    \CommentTok{# between each point and its closes cluster centre:}
    \KeywordTok{sum}\NormalTok{((X}\OperatorTok{-}\NormalTok{mu[memb,])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Setting up the solvers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min_HartiganWong <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(mu0, X)}
    \KeywordTok{get_fitness}\NormalTok{(}
        \CommentTok{# algorithm="Hartigan-Wong"}
        \KeywordTok{kmeans}\NormalTok{(X, mu0, }\DataTypeTok{iter.max=}\DecValTok{100}\NormalTok{)}\OperatorTok{$}\NormalTok{centers,}
\NormalTok{    X)}
\NormalTok{min_Lloyd <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(mu0, X)}
    \KeywordTok{get_fitness}\NormalTok{(}
        \KeywordTok{kmeans}\NormalTok{(X, mu0, }\DataTypeTok{iter.max=}\DecValTok{100}\NormalTok{, }\DataTypeTok{algorithm=}\StringTok{"Lloyd"}\NormalTok{)}\OperatorTok{$}\NormalTok{centers,}
\NormalTok{    X)}
\NormalTok{min_optim <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(mu0, X)}
    \KeywordTok{optim}\NormalTok{(mu0,}
        \ControlFlowTok{function}\NormalTok{(mu, X) \{}
            \KeywordTok{get_fitness}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(mu, }\DataTypeTok{nrow=}\KeywordTok{nrow}\NormalTok{(mu0)), X)}
\NormalTok{        \}, }\DataTypeTok{X=}\NormalTok{X, }\DataTypeTok{method=}\StringTok{"BFGS"}\NormalTok{, }\DataTypeTok{control=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{reltol=}\FloatTok{1e-16}\NormalTok{)}
\NormalTok{    )}\OperatorTok{$}\NormalTok{val}
\end{Highlighting}
\end{Shaded}

Running the simulation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nstart <-}\StringTok{ }\DecValTok{100}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{res <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{(nstart, \{}
\NormalTok{  mu0 <-}\StringTok{ }\NormalTok{X[}\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(X), K),]}
    \KeywordTok{c}\NormalTok{(}
        \DataTypeTok{HartiganWong=}\KeywordTok{min_HartiganWong}\NormalTok{(mu0, X),}
        \DataTypeTok{Lloyd=}\KeywordTok{min_Lloyd}\NormalTok{(mu0, X),}
        \DataTypeTok{optim=}\KeywordTok{min_optim}\NormalTok{(mu0, X)}
\NormalTok{    )}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Notice a considerable variability of the
objective function at the local minima found:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{t}\NormalTok{(res)), }\DataTypeTok{horizontal=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:gendata5}{%
\centering
\includegraphics{08-optimisation-genetic-figures/gendata5-1.pdf}
\caption{plot of chunk gendata5}\label{fig:gendata5}
}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{apply}\NormalTok{(res, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x)}
    \KeywordTok{c}\NormalTok{(}\KeywordTok{summary}\NormalTok{(x), }\DataTypeTok{sd=}\KeywordTok{sd}\NormalTok{(x))}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         HartiganWong    Lloyd  optim
## Min.          421889 425119.5 422989
## 1st Qu.       424663 433669.3 432446
## Median        427129 438502.2 440033
## Mean          426557 438075.0 440635
## 3rd Qu.       428243 441381.3 446614
## Max.          431869 450469.7 466303
## sd              2301   5709.3  10888
\end{verbatim}

Of course, we are interested in the smallest value of the objective,
because we're trying to pinpoint the global minimum.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{apply}\NormalTok{(res, }\DecValTok{1}\NormalTok{, min))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## HartiganWong        Lloyd        optim 
##       421889       425119       422989
\end{verbatim}

The Hartigan-Wong algorithm (the default one in \texttt{kmeans()})
is the most reliable one of the three:

\begin{itemize}
\tightlist
\item
  it gives the best solution (low bias)
\item
  the solutions have the lowest degree of variability (low variance)
\item
  it is the fastest:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"microbenchmark"}\NormalTok{)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{mu0 <-}\StringTok{ }\NormalTok{X[}\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(X), K),]}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{microbenchmark}\NormalTok{(}
    \DataTypeTok{HartiganWong=}\KeywordTok{min_HartiganWong}\NormalTok{(mu0, X),}
    \DataTypeTok{Lloyd=}\KeywordTok{min_Lloyd}\NormalTok{(mu0, X),}
    \DataTypeTok{optim=}\KeywordTok{min_optim}\NormalTok{(mu0, X),}
    \DataTypeTok{times=}\DecValTok{10}
\NormalTok{), }\DataTypeTok{unit=}\StringTok{"relative"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           expr       min        lq     mean    median       uq
## 1 HartiganWong    1.1815    1.1783    1.328    1.2691    1.374
## 2        Lloyd    1.0000    1.0000    1.000    1.0000    1.000
## 3        optim 1806.1290 1719.5071 1600.846 1651.9828 1463.777
##         max neval
## 1    1.5241    10
## 2    1.0000    10
## 3 1499.4590    10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{min}\NormalTok{(res))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 421889
\end{verbatim}

Is it the global minimum?

\begin{quote}
We don't know, we just didn't happen to find anything better (yet).
\end{quote}

Did we put enough effort to find it?

\begin{quote}
Well, maybe. We can try more random restarts:
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res_tried_very_hard <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(X, K, }\DataTypeTok{nstart=}\DecValTok{100000}\NormalTok{, }\DataTypeTok{iter.max=}\DecValTok{10000}\NormalTok{)}\OperatorTok{$}\NormalTok{centers}
\KeywordTok{print}\NormalTok{(}\KeywordTok{get_fitness}\NormalTok{(res_tried_very_hard, X))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 421889
\end{verbatim}

Is it good enough?

\begin{quote}
It depends what we'd like to do with this. Does it make your boss happy?
Does it generate revenue? Does it help solve any other problem?
Is it useful anyhow?
Are you really looking for the global minimum?
\end{quote}

\hypertarget{genetic-algorithms}{%
\section{Genetic Algorithms}\label{genetic-algorithms}}

\hypertarget{introduction-15}{%
\subsection{Introduction}\label{introduction-15}}

What if our optimisation problem cannot be solved reliably with
gradient-based methods like those in \texttt{optim()} and we don't have
any custom solver for the task at hand?

There are a couple of useful \emph{metaheuristics} in the literature
that can serve this purpose.

Most of them rely on clever randomised search.

They are slow to run and don't guarantee anything, but yet they still might be useful -- some claim that \textbf{a} solution is better than no solution at all.

There is a wide class of \textbf{nature-inspired} algorithms (that traditionally
belong to the subfield of AI called \emph{computational intelligence} or \emph{soft computing});
see, e.g, (Simon \protect\hyperlink{ref-evolution}{2013}):

\begin{itemize}
\item
  evolutionary algorithms -- inspired by the principle of natural selection

  \begin{quote}
  maintain a population of candidate solutions, let the ``fittest'' combine with each
  other to generate new ``offspring'' solutions.
  \end{quote}
\item
  swarm algorithms

  \begin{quote}
  maintain a herd of candidate solutions, allow them to ``explore'' the environment,
  ``communicate'' with each other in order to seek the best spot to ``go to''.
  \end{quote}

  For example:

  \begin{itemize}
  \tightlist
  \item
    ant colony
  \item
    bees
  \item
    cuckoo search
  \item
    particle sward
  \item
    krill herd
  \end{itemize}
\item
  other metaheuristics:

  \begin{itemize}
  \tightlist
  \item
    harmony search
  \item
    memetic algorithm
  \item
    firefly algorithm
  \end{itemize}
\end{itemize}

All of these sound fancy, but the general ideas behind them are pretty simple.

\hypertarget{overview-of-the-method}{%
\subsection{Overview of the Method}\label{overview-of-the-method}}

Genetic algorithms (GAs) are amongst the most popular evolutionary approaches.

They are based on Charles Darwin's work on evolution by natural selection;
first proposed by John Holland in the 1960s.

See (Goldberg, 1989) for a comprehensive overview
and (Simon, 2013) for extensions.

Here is the general idea of a GA (there might be many)
to minimise a given objective/fitness function \(f\)
over a given domain \(D\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Generate a random initial population of individuals -- \(n_\text{pop}\) points in \(D\),
  e.g., \(n_\text{pop}=32\)
\item
  Repeat until some convergence criterion is not met:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    evaluate the fitness of each individual
  \item
    select the pairs of the individuals for reproduction, the fitter
    should be selected more eagerly
  \item
    apply crossover operations to create offspring
  \item
    slightly mutate randomly selected individuals
  \item
    replace the old population with the new one
  \end{enumerate}
\end{enumerate}

\hypertarget{example-implementation---ga-for-k-means}{%
\subsection{Example Implementation - GA for K-means}\label{example-implementation---ga-for-k-means}}

Initial setup:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{# simulation parameters:}
\NormalTok{npop <-}\StringTok{ }\DecValTok{32}
\NormalTok{niter <-}\StringTok{ }\DecValTok{100}

\CommentTok{# randomly generate an initial population of size `npop`:}
\NormalTok{pop <-}\StringTok{ }\KeywordTok{lapply}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{npop, }\ControlFlowTok{function}\NormalTok{(i) X[}\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(X), K),])}

\CommentTok{# evaluate fitness of each individual:}
\NormalTok{cur_fitness <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(pop, get_fitness, X)}
\NormalTok{cur_best_fitness <-}\StringTok{ }\KeywordTok{min}\NormalTok{(cur_fitness)}
\NormalTok{best_fitness <-}\StringTok{ }\NormalTok{cur_best_fitness}
\end{Highlighting}
\end{Shaded}

Each individual in the population is just the set of \(K\)
candidate cluster centres represented as a matrix in \(\mathbb{R}^{K\times p}\).

Let's assume that the fitness of each individual should be a function of
the rank of the objective function's value
(smallest objective == highest rank == best fit).

For the crossover, we will sample pairs of individuals with
probabilities proportional to their fitness.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selection <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(cur_fitness) \{}
\NormalTok{    npop <-}\StringTok{ }\KeywordTok{length}\NormalTok{(cur_fitness)}
\NormalTok{    probs <-}\StringTok{ }\KeywordTok{rank}\NormalTok{(}\OperatorTok{-}\NormalTok{cur_fitness)}
\NormalTok{    probs <-}\StringTok{ }\NormalTok{probs}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(probs)}
\NormalTok{    left  <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(npop, npop, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{prob=}\NormalTok{probs)}
\NormalTok{    right <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(npop, npop, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{prob=}\NormalTok{probs)}
    \KeywordTok{cbind}\NormalTok{(left, right)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

An example crossover combines each cluster centre
in such a way that we take a few coordinates of the ``left'' parent
and the remaining ones from the ``right'' parent:

\begin{figure}
\hypertarget{fig:crossover}{%
\centering
\includegraphics[width=0.75\textwidth,height=\textheight]{figures/crossover.pdf}
\caption{}\label{fig:crossover}
}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crossover <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(pop, pairs, K, p) \{}
\NormalTok{    old_pop <-}\StringTok{ }\NormalTok{pop}
\NormalTok{    pop <-}\StringTok{ }\NormalTok{pop[pairs[,}\DecValTok{2}\NormalTok{]]}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(pop)) \{}
\NormalTok{        wh <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(p}\DecValTok{-1}\NormalTok{, K, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{)}
        \ControlFlowTok{for}\NormalTok{ (l }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{K)}
\NormalTok{            pop[[j]][l,}\DecValTok{1}\OperatorTok{:}\NormalTok{wh[l]] <-}
\StringTok{                }\NormalTok{old_pop[[pairs[j,}\DecValTok{1}\NormalTok{]]][l,}\DecValTok{1}\OperatorTok{:}\NormalTok{wh[l]]}
\NormalTok{    \}}
\NormalTok{    pop}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Mutation (occurring with a very small probability)
substitutes some cluster centre with a random vector from the input dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mutate <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(pop, X, K) \{}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(pop)) \{}
       \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{) }\OperatorTok{<}\StringTok{ }\FloatTok{0.025}\NormalTok{) \{}
\NormalTok{          szw <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{K, }\DecValTok{1}\NormalTok{)}
\NormalTok{          pop[[j]][szw,] <-}\StringTok{ }\NormalTok{X[}\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(X), }\KeywordTok{length}\NormalTok{(szw)),]}
\NormalTok{       \}}
\NormalTok{    \}}
\NormalTok{    pop}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We also need a function that checks if
the new cluster centres aren't too far away from the input points.

If it happens that we have empty clusters, our solution is degenerate and
we must correct it.

All ``bad'' cluster centres will be substituted with randomly chosen points from \(\mathbf{X}\).

Moreover, we will recompute the cluster centres as the componentwise arithmetic mean
of the closest points, just like in Lloyd's algorithm, to speed up convergence.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{recompute_mus <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(pop, X, K) \{}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(pop)) \{}
    \CommentTok{# get nearest cluster centres for each point:}
\NormalTok{    memb <-}\StringTok{ }\KeywordTok{get.knnx}\NormalTok{(pop[[j]], X, }\DecValTok{1}\NormalTok{)}\OperatorTok{$}\NormalTok{nn.index}
\NormalTok{    sz <-}\StringTok{ }\KeywordTok{tabulate}\NormalTok{(memb, K) }\CommentTok{# number of points in each cluster}
    \CommentTok{# if there are empty clusters, fix them:}
\NormalTok{    szw <-}\StringTok{ }\KeywordTok{which}\NormalTok{(sz}\OperatorTok{==}\DecValTok{0}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{length}\NormalTok{(szw)}\OperatorTok{>}\DecValTok{0}\NormalTok{) \{ }\CommentTok{# random points in X will be new cluster centres}
\NormalTok{        pop[[j]][szw,] <-}\StringTok{ }\NormalTok{X[}\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(X), }\KeywordTok{length}\NormalTok{(szw)),]}
\NormalTok{        memb <-}\StringTok{ }\NormalTok{FNN}\OperatorTok{::}\KeywordTok{get.knnx}\NormalTok{(pop[[j]], X, }\DecValTok{1}\NormalTok{)}\OperatorTok{$}\NormalTok{nn.index}
\NormalTok{        sz <-}\StringTok{ }\KeywordTok{tabulate}\NormalTok{(memb, K)}
\NormalTok{    \}}
    \CommentTok{# recompute cluster centres - componentwise average:}
\NormalTok{    pop[[j]][,] <-}\StringTok{ }\DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ (l }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(X))}
\NormalTok{        pop[[j]][memb[l],] <-}\StringTok{ }\NormalTok{pop[[j]][memb[l],]}\OperatorTok{+}\NormalTok{X[l,]}
\NormalTok{    pop[[j]] <-}\StringTok{ }\NormalTok{pop[[j]]}\OperatorTok{/}\NormalTok{sz}
\NormalTok{  \}}
\NormalTok{  pop}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We are ready to build our genetic algorithm to solve the K-means clustering problem:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{niter) \{}
\NormalTok{    pairs <-}\StringTok{ }\KeywordTok{selection}\NormalTok{(cur_fitness)}
\NormalTok{    pop <-}\StringTok{ }\KeywordTok{crossover}\NormalTok{(pop, pairs, K, p)}
\NormalTok{    pop <-}\StringTok{ }\KeywordTok{mutate}\NormalTok{(pop, X, K)}
\NormalTok{    pop <-}\StringTok{ }\KeywordTok{recompute_mus}\NormalTok{(pop, X, K)}
    \CommentTok{# re-evaluate fitness:}
\NormalTok{    cur_fitness <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(pop, get_fitness, X)}
\NormalTok{    cur_best_fitness <-}\StringTok{ }\KeywordTok{min}\NormalTok{(cur_fitness)}
    \CommentTok{# give feedback on what's going on:}
    \ControlFlowTok{if}\NormalTok{ (cur_best_fitness }\OperatorTok{<}\StringTok{ }\NormalTok{best_fitness) \{}
\NormalTok{        best_fitness <-}\StringTok{ }\NormalTok{cur_best_fitness}
\NormalTok{        best_mu <-}\StringTok{ }\NormalTok{pop[[}\KeywordTok{which.min}\NormalTok{(cur_fitness)]]}
        \KeywordTok{cat}\NormalTok{(}\KeywordTok{sprintf}\NormalTok{(}\StringTok{"%5d: f_best=%10.5f}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, i, best_fitness))}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     1: f_best=435638.52165
##     2: f_best=428808.89706
##     4: f_best=428438.45125
##     6: f_best=422277.99136
##     8: f_best=421889.46265
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{get_fitness}\NormalTok{(best_mu, X))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 421889
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{get_fitness}\NormalTok{(res_tried_very_hard, X))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 421889
\end{verbatim}

It works! :)

\hypertarget{outro-7}{%
\section{Outro}\label{outro-7}}

\hypertarget{remarks-7}{%
\subsection{Remarks}\label{remarks-7}}

For any \(p\ge 1\), the search space type determines the problem class:

\begin{itemize}
\item
  \(\mathbb{D}\subseteq\mathbb{R}^p\) -- \textbf{continuous optimisation}

  In particular:

  \begin{itemize}
  \item
    \(\mathbb{D}=\mathbb{R}^p\) -- continuous unconstrained
  \item
    \(\mathbb{D}=[a_1,b_1]\times\dots\times[a_n,b_n]\) -- continuous with box constraints
  \item
    constrained with \(k\) linear inequality constraints

    \[
      \left\{
      \begin{array}{lll}
      a_{1,1} x_1 + \dots + a_{1,p} x_p &\le& b_1 \\
      &\vdots&\\
      a_{k,1} x_1 + \dots + a_{k,p} x_p &\le& b_k \\
      \end{array}
      \right.
      \]
  \end{itemize}
\end{itemize}

However, there are other possibilities as well:

\begin{itemize}
\item
  \(\mathbb{D}\subseteq\mathbb{Z}^p\) (\(\mathbb{Z}\) -- the set of integers) -- \textbf{discrete optimisation}

  In particular:

  \begin{itemize}
  \tightlist
  \item
    \(\mathbb{D}=\{0,1\}^p\) -- 0--1 optimisation (hard!)
  \end{itemize}
\item
  \(\mathbb{D}\) is finite (but perhaps large, its objects can be enumerated) -- \textbf{combination optimisation}

  For example:

  \begin{itemize}
  \tightlist
  \item
    \(\mathbb{D}=\) all possible routes between two points on a map.
  \end{itemize}
\end{itemize}

\begin{quote}
These optimisation tasks tend to be much harder than the continuous ones.
\end{quote}

Genetic algorithms might come in handy in such cases.

Specialised methods, customised to solve a specific problem (like Lloyd's algorithm)
will often outperform generic ones (like SGD, genetic algorithms)
in terms of speed and reliability.

All in all, we prefer a suboptimal solution obtained by means of heuristics
to no solution at all.

Problems that you could try solving with GAs include variable selection
in multiple regression -- finding the subset of features optimising the AIC
(this is a hard problem to and forward selection was just a simple greed heuristic).

Other interesting algorithms:

\begin{itemize}
\tightlist
\item
  Hill Climbing (a simple variation of GD with no gradient use)
\item
  Simulated annealing
\item
  CMA-ES
\item
  Tabu search
\item
  Particle swarm optimisation (e.g, \texttt{hydroPSO} package)
\item
  Artificial bee/ant colony optimisation
\item
  Cuckoo Search
\item
  Differential Evolution (e.g., \texttt{DEoptim} package)
\end{itemize}

\hypertarget{further-reading-7}{%
\subsection{Further Reading}\label{further-reading-7}}

Recommended further reading:

\begin{itemize}
\tightlist
\item
  (Goldberg \protect\hyperlink{ref-genetic}{1989})
\end{itemize}

Other:

\begin{itemize}
\tightlist
\item
  (Simon \protect\hyperlink{ref-evolution}{2013})
\end{itemize}

See also package \texttt{GA}.

\hypertarget{recommender-systems}{%
\chapter{Recommender Systems}\label{recommender-systems}}

\hypertarget{introduction-16}{%
\section{Introduction}\label{introduction-16}}

\emph{Recommender (recommendation) systems}
aim to predict the rating a \emph{user} would give to an \emph{item}.

For example:

\begin{itemize}
\tightlist
\item
  playlist generators (Spotify, YouTube, Netflix, \ldots{}),
\item
  content recommendations (Facebook, Instagram, Twitter, Apple News, \ldots{}),
\item
  product recommendations (Amazon, Alibaba, \ldots{}).
\end{itemize}

Implementing recommender systems, according to (Ricci et al. \protect\hyperlink{ref-ricci_etal}{2011}),
might:

\begin{itemize}
\tightlist
\item
  increase the number of items sold,
\item
  increase users' satisfaction,
\item
  increase users' fidelity,
\item
  allow a company to sell more diverse items,
\item
  allow to better understand what users want.
\end{itemize}

\begin{exercise}

Think of the last time you found some recommendation useful.

\end{exercise}

They can also increase the users' frustration.

\begin{exercise}

Think of the last time you found a recommendation useless and
irritating. What might be the reasons why you have been provided
with such a suggestion?

\end{exercise}

\hypertarget{the-netflix-prize}{%
\subsection{The Netflix Prize}\label{the-netflix-prize}}

In 2006 Netflix (back then a DVD rental company) released one of the most famous
benchmark sets for recommender systems, which helped boost the research
on algorithms in this field.

See \url{https://www.kaggle.com/netflix-inc/netflix-prize-data};
data archived at
\url{https://web.archive.org/web/20090925184737/http://archive.ics.uci.edu/ml/datasets/Netflix+Prize}
and \url{https://archive.org/details/nf_prize_dataset.tar}

The dataset consists of:

\begin{itemize}
\tightlist
\item
  480,189 users
\item
  17,770 movies
\item
  100,480,507 ratings in the training sample:

  \begin{itemize}
  \tightlist
  \item
    \texttt{MovieID}
  \item
    \texttt{CustomerID}
  \item
    \texttt{Rating} from 1 to 5
  \item
    \texttt{Title}
  \item
    \texttt{YearOfRelease} from 1890 to 2005
  \item
    \texttt{Date} of rating in the range 1998-11-01 to 2005-12-31
  \end{itemize}
\end{itemize}

The \emph{quiz set} consists of 1,408,342 ratings
and it was used by the competitors to assess the quality of their
algorithms and compute the leaderboard scores.

Root mean squared error (RMSE) of predicted vs.~true rankings was chosen as a
performance metric.

The \emph{test set} of 1,408,789 ratings (not make publicly available)
was used to determine the winner.

On 21 September 2009, the grand prize of US\$1,000,000 was given
to the BellKor's Pragmatic Chaos team which improved over
the Netflix's \emph{Cinematch} algorithm by 10.06\%,
achieving the winning RMSE of 0.8567 on the test subset.

\hypertarget{main-approaches}{%
\subsection{Main Approaches}\label{main-approaches}}

Current recommender systems are quite complex and use a fusion
of various approaches, also those based on external knowledge bases.

However, we may distinguish at least two core approaches,
see (Ricci et al. \protect\hyperlink{ref-ricci_etal}{2011}) for more:

\begin{itemize}
\item
  \emph{Collaborative Filtering} is based on the assumption that if
  two people interact with the same product,
  they're likely to have other interests in common as well.

  \begin{quote}
  John and Mary both like bananas and apples and dislike spinach. John likes
  sushi. Mary hasn't tried sushi yet. It seems they might have similar tastes,
  so we recommend that Mary should give sushi a try.
  \end{quote}
\item
  \emph{Content-based Filtering} builds users' profiles that represent information
  about what kind of products they like.

  \begin{quote}
  We have discovered that John likes fruit but dislikes vegetables.
  An orange is a fruit (an item similar to those he liked in the past)
  with which John is yet to interact. Thus, it is suggested that John should
  give it a try.
  \end{quote}
\end{itemize}

Jim Bennett, at that time the vice president of recommendation systems at Netflix
on the idea behind the original Cinematch algorithm (see \url{https://www.technologyreview.com/s/406637/the-1-million-netflix-challenge/}
and \url{https://web.archive.org/web/20070821194257/http://www.netflixprize.com/faq}):

\begin{quote}
First, you collect 100 million user ratings for about 18,000 movies. Take any two movies and find the people who have rated both of them. Then look to see if the people who rate one of the movies highly rate the other one highly, if they liked one and not the other, or if they didn't like either movie. Based on their ratings, Cinematch sees whether there's a correlation between those people. Now, do this for all possible pairs of 65,000 movies.
\end{quote}

\begin{exercise}

Is the above an example of the collaborative or context-based filtering?

\end{exercise}

\hypertarget{formalism-2}{%
\subsection{Formalism}\label{formalism-2}}

Let \(\mathcal{U}=\{ U_1,\dots,U_n \}\) denote the set of \(n\) users.

Let \(\mathcal{I}=\{ I_1,\dots,I_p \}\) denote the set of \(p\) items.

Let \(\mathbf{R}\in\mathbb{R}^{n\times p}\) be a user-item matrix such that:
\[
r_{u,i}=\left\{
\begin{array}{ll}
r & \text{if the $u$-th user ranked the $i$-th item as $r>0$}\\
0 & \text{if the $u$-th user hasn't interacted with the $i$-th item yet}\\
\end{array}
\right.
\]

\begin{description}
\tightlist
\item[Remark.]
Note that \texttt{0} is used to denote a missing value (\texttt{NA}) here.
\end{description}

In particular, we can assume:

\begin{itemize}
\tightlist
\item
  \(r_{u,i}\in\{0,1,\dots,5\}\) (ratings on the scale 1--5 or no interaction)
\item
  \(r_{u,i}\in\{0,1\}\) (``Like'' or no interaction)
\end{itemize}

The aim of an recommender system is to predict the rating \(\hat{r}_{u,i}\)
that the \(u\)-th user would give to the \(i\)-th item provided that currently
\(r_{u,i}=0\).

\hypertarget{collaborative-filtering}{%
\section{Collaborative Filtering}\label{collaborative-filtering}}

\hypertarget{example}{%
\subsection{Example}\label{example}}

\begin{longtable}[]{@{}llllll@{}}
\toprule
. & Apple & Banana & Sushi & Spinach & Orange\tabularnewline
\midrule
\endhead
Anne & 1 & 5 & 5 & & 1\tabularnewline
Beth & 1 & 1 & 5 & 5 & 1\tabularnewline
John & 5 & 5 & & 1 &\tabularnewline
Kate & 1 & 1 & 5 & 5 & 1\tabularnewline
Mark & 5 & 5 & 1 & 1 & 5\tabularnewline
Sara & ? & 5 & & ? & 5\tabularnewline
\bottomrule
\end{longtable}

In \textbf{user-based collaborative filtering}, we seek users with similar
preference profiles/rating patters.

\begin{quote}
``User A has similar behavioural patterns as user B, so A should suggested
with what B likes.''
\end{quote}

In \textbf{item-based collaborative filtering}, we seek items with similar (dis)likeability
structure.

\begin{quote}
``Users who (dis)liked X also (dis)liked Y''.
\end{quote}

\begin{exercise}

Will Sara enjoy her spinach? Will Sara enjoy her apple?

\end{exercise}

An example \(\mathbf{R}\) matrix in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{R <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}
    \KeywordTok{c}\NormalTok{(}
     \DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
     \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{,}
     \DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
     \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{,}
     \DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{,}
     \DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}
\NormalTok{    ), }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{nrow=}\DecValTok{6}\NormalTok{, }\DataTypeTok{ncol=}\DecValTok{5}\NormalTok{,}
    \DataTypeTok{dimnames=}\KeywordTok{list}\NormalTok{(}
        \KeywordTok{c}\NormalTok{(}\StringTok{"Anne"}\NormalTok{, }\StringTok{"Beth"}\NormalTok{, }\StringTok{"John"}\NormalTok{, }\StringTok{"Kate"}\NormalTok{, }\StringTok{"Mark"}\NormalTok{, }\StringTok{"Sara"}\NormalTok{),}
        \KeywordTok{c}\NormalTok{(}\StringTok{"Apple"}\NormalTok{, }\StringTok{"Banana"}\NormalTok{, }\StringTok{"Sushi"}\NormalTok{, }\StringTok{"Spinach"}\NormalTok{, }\StringTok{"Orange"}\NormalTok{)}
\NormalTok{    )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{R}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Apple Banana Sushi Spinach Orange
## Anne     1      5     5       0      1
## Beth     1      1     5       5      1
## John     5      5     0       1      0
## Kate     1      1     5       5      1
## Mark     5      5     1       1      5
## Sara     0      5     0       0      5
\end{verbatim}

\hypertarget{similarity-measures}{%
\subsection{Similarity Measures}\label{similarity-measures}}

Assuming \(\mathbf{a},\mathbf{b}\) are two sequences of length \(k\)
(in our setting, \(k\) is equal to either \(n\) or \(p\)),
let \(S\) be the following similarity measure between two rating vectors:

\[
S(\mathbf{a},\mathbf{b}) = \frac{ \sum_{i=1}^k a_i b_i
}{
\sqrt{ \sum_{i=1}^k a_i^2 }
\sqrt{ \sum_{i=1}^k b_i^2 }
}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cosim <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(a, b) }\KeywordTok{sum}\NormalTok{(a}\OperatorTok{*}\NormalTok{b)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{(a}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\KeywordTok{sum}\NormalTok{(b}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We call it the \textbf{cosine similarity}.
We have \(S(\mathbf{a},\mathbf{b})\in[-1,1]\),
where we get \(1\) for two identical elements.
Similarity of 0 is obtained for two unrelated (``orthogonal'') vectors.
For nonnegative sequences, negative similarities are not generated.

\begin{quote}
(*) Another frequently considered similarity measure
is a version of the Pearson correlation coefficient that
ignores all \(0\)-valued observations,
see also the \texttt{use} argument to the \texttt{cor()} function.
\end{quote}

\hypertarget{user-based-collaborative-filtering}{%
\subsection{User-Based Collaborative Filtering}\label{user-based-collaborative-filtering}}

\textbf{User-based} approaches involve comparing each user against every other user
(pairwise comparisons of the rows in \(\mathbf{R}\)). This yields a similarity degree
between the \(i\)-th and the \(j\)-th user:

\[
s^U_{i,j} = S(\mathbf{r}_{i,\cdot},\mathbf{r}_{j,\cdot}).
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SU <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{nrow=}\KeywordTok{nrow}\NormalTok{(R), }\DataTypeTok{ncol=}\KeywordTok{nrow}\NormalTok{(R),}
    \DataTypeTok{dimnames=}\KeywordTok{dimnames}\NormalTok{(R)[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)]) }\CommentTok{# and empty n*n matrix}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(R)) \{}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(R)) \{}
\NormalTok{        SU[i,j] <-}\StringTok{ }\KeywordTok{cosim}\NormalTok{(R[i,], R[j,])}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(SU, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Anne Beth John Kate Mark Sara
## Anne 1.00 0.61 0.58 0.61 0.63 0.59
## Beth 0.61 1.00 0.29 1.00 0.39 0.19
## John 0.58 0.29 1.00 0.29 0.81 0.50
## Kate 0.61 1.00 0.29 1.00 0.39 0.19
## Mark 0.63 0.39 0.81 0.39 1.00 0.81
## Sara 0.59 0.19 0.50 0.19 0.81 1.00
\end{verbatim}

In order to obtain the previously unobserved
rating \(\hat{r}_{u,i}\) using the user-based approach, we typically
look for the \(K\) most similar users and aggregate their corresponding
scores (for some \(K\ge 1\)).

More formally, let \(\{U_{v_1},\dots,U_{v_K}\}\in\mathcal{U}\setminus\{U_u\}\) be the set
of users maximising \(s^U_{u, v_1}, \dots, s^U_{u, v_K}\)
and having \(r_{v_1, i},\dots,r_{v_K, i}>0\).
Then:
\[
\hat{r}_{u,i} = \frac{1}{K} \sum_{\ell=1}^K r_{v_\ell, i}.
\]

\begin{description}
\tightlist
\item[Remark.]
The arithmetic mean can be replaced with, e.g.,
the more or a weighted arithmetic mean where weights are proportional to \(s^U_{u, v_\ell}\)
\end{description}

This is very similar to the \(K\)-nearest neighbour heuristic!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{K <-}\StringTok{ }\DecValTok{2}
\NormalTok{(sim <-}\StringTok{ }\KeywordTok{order}\NormalTok{(SU[}\StringTok{"Sara"}\NormalTok{,], }\DataTypeTok{decreasing=}\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6 5 1 3 2 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sim gives the indices of people in decreasing order}
\CommentTok{# of similarity to Sara:}
\KeywordTok{dimnames}\NormalTok{(R)[[}\DecValTok{1}\NormalTok{]][sim] }\CommentTok{# the corresponding names}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Sara" "Mark" "Anne" "John" "Beth" "Kate"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Remove those who haven't tried Spinach yet (including Sara):}
\NormalTok{sim <-}\StringTok{ }\NormalTok{sim[ R[sim, }\StringTok{"Spinach"}\NormalTok{]}\OperatorTok{>}\DecValTok{0}\NormalTok{ ]}
\KeywordTok{dimnames}\NormalTok{(R)[[}\DecValTok{1}\NormalTok{]][sim]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Mark" "John" "Beth" "Kate"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# aggregate the Spinach ratings of the K most similar people:}
\KeywordTok{mean}\NormalTok{(R[sim[}\DecValTok{1}\OperatorTok{:}\NormalTok{K], }\StringTok{"Spinach"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\hypertarget{item-based-collaborative-filtering}{%
\subsection{Item-Based Collaborative Filtering}\label{item-based-collaborative-filtering}}

\textbf{Item-based} schemes rely on pairwise comparisons between the items
(columns in \(\mathbf{R}\)). Hence, a similarity degree between the \(i\)-th and the \(j\)-th
item is given by:

\[
s^I_{i,j} = S(\mathbf{r}_{\cdot,i},\mathbf{r}_{\cdot,j}).
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SI <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{nrow=}\KeywordTok{ncol}\NormalTok{(R), }\DataTypeTok{ncol=}\KeywordTok{ncol}\NormalTok{(R),}
    \DataTypeTok{dimnames=}\KeywordTok{dimnames}\NormalTok{(R)[}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)]) }\CommentTok{# an empty p*p matrix}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(R)) \{}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(R)) \{}
\NormalTok{        SI[i,j] <-}\StringTok{ }\KeywordTok{cosim}\NormalTok{(R[,i], R[,j])}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(SI, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Apple Banana Sushi Spinach Orange
## Apple    1.00   0.78  0.32    0.38   0.53
## Banana   0.78   1.00  0.45    0.27   0.78
## Sushi    0.32   0.45  1.00    0.81   0.32
## Spinach  0.38   0.27  0.81    1.00   0.29
## Orange   0.53   0.78  0.32    0.29   1.00
\end{verbatim}

In order to obtain the previously unobserved
rating \(\hat{r}_{u,i}\) using the item-based approach, we typically
look for the \(K\) most similar items and aggregate their corresponding
scores (for some \(K\ge 1\))

More formally, let \(\{I_{j_1},\dots,I_{j_K}\}\in\mathcal{I}\setminus\{I_i\}\) be the set
of items maximising \(s^I_{i, j_1}, \dots, s^I_{i, j_K}\) and having \(r_{u, j_1},\dots,r_{u, j_K}>0\). Then:

\[
\hat{r}_{u,i} = \frac{1}{K} \sum_{\ell=1}^K r_{u, j_\ell}.
\]

\begin{description}
\tightlist
\item[Remark.]
Similarly to the previous case,
the arithmetic mean can be replaced with, e.g.,
the mode or
a weighted arithmetic mean where weights are proportional to \(s^I_{i, j_\ell}\).
\end{description}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{K <-}\StringTok{ }\DecValTok{2}
\NormalTok{(sim <-}\StringTok{ }\KeywordTok{order}\NormalTok{(SI[}\StringTok{"Apple"}\NormalTok{,], }\DataTypeTok{decreasing=}\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 5 4 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sim gives the indices of items in decreasing order}
\CommentTok{# of similarity to Apple:}
\KeywordTok{dimnames}\NormalTok{(R)[[}\DecValTok{2}\NormalTok{]][sim] }\CommentTok{# the corresponding item types}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Apple"   "Banana"  "Orange"  "Spinach" "Sushi"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Remove these which Sara haven't tried yet (e.g., Apples):}
\NormalTok{sim <-}\StringTok{ }\NormalTok{sim[ R[}\StringTok{"Sara"}\NormalTok{, sim]}\OperatorTok{>}\DecValTok{0}\NormalTok{ ]}
\KeywordTok{dimnames}\NormalTok{(R)[[}\DecValTok{2}\NormalTok{]][sim]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Banana" "Orange"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# aggregate Sara's ratings of the K most similar items:}
\KeywordTok{mean}\NormalTok{(R[}\StringTok{"Sara"}\NormalTok{, sim[}\DecValTok{1}\OperatorTok{:}\NormalTok{K]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

\hypertarget{exercise-the-movielens-dataset}{%
\section{Exercise: The MovieLens Dataset (*)}\label{exercise-the-movielens-dataset}}

\hypertarget{dataset}{%
\subsection{Dataset}\label{dataset}}

Let's make a few recommendations based on the MovieLens-9/2018-Small
dataset available at
\url{https://grouplens.org/datasets/movielens/latest/},
see also \url{https://movielens.org/} and (Harper \& Konstan \protect\hyperlink{ref-movielens}{2015}).

The dataset consists of
ca. 100,000 ratings to 9,000 movies by 600 users. It was last updated
on September 2018.

This is already a pretty large dataset! We might run into problems
with memory usage and high run-time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/ml-9-2018-small/movies.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{head}\NormalTok{(movies, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   movieId                    title
## 1       1         Toy Story (1995)
## 2       2           Jumanji (1995)
## 3       3  Grumpier Old Men (1995)
## 4       4 Waiting to Exhale (1995)
##                                        genres
## 1 Adventure|Animation|Children|Comedy|Fantasy
## 2                  Adventure|Children|Fantasy
## 3                              Comedy|Romance
## 4                        Comedy|Drama|Romance
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nrow}\NormalTok{(movies)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9742
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ratings <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"datasets/ml-9-2018-small/ratings.csv"}\NormalTok{,}
    \DataTypeTok{comment.char=}\StringTok{"#"}\NormalTok{, }\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{head}\NormalTok{(ratings, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   userId movieId rating timestamp
## 1      1       1      4 964982703
## 2      1       3      4 964981247
## 3      1       6      4 964982224
## 4      1      47      5 964983815
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nrow}\NormalTok{(ratings)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 100836
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(ratings}\OperatorTok{$}\NormalTok{rating)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   0.5     1   1.5     2   2.5     3   3.5     4   4.5     5 
##  1370  2811  1791  7551  5550 20047 13136 26818  8551 13211
\end{verbatim}

\hypertarget{data-cleansing}{%
\subsection{Data Cleansing}\label{data-cleansing}}

\texttt{movieId}s should be re-encoded, as not every film is mentioned/rated in the database.
We will re-map the \texttt{movieId}s to consecutive integers.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the list of all rated movieIds:}
\NormalTok{movieId2 <-}\StringTok{ }\KeywordTok{unique}\NormalTok{(ratings}\OperatorTok{$}\NormalTok{movieId)}
\CommentTok{# max user Id (these could've been cleaned up too):}
\NormalTok{(n <-}\StringTok{ }\KeywordTok{max}\NormalTok{(ratings}\OperatorTok{$}\NormalTok{userId))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 610
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# number of unique movies:}
\NormalTok{(p <-}\StringTok{ }\KeywordTok{length}\NormalTok{(movieId2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9724
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove unrated movies:}
\NormalTok{movies <-}\StringTok{ }\NormalTok{movies[movies}\OperatorTok{$}\NormalTok{movieId }\OperatorTok{%in%}\StringTok{ }\NormalTok{movieId2, ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# we'll map movieId2[i] to i for each i=1,...,p:}
\NormalTok{movies}\OperatorTok{$}\NormalTok{movieId  <-}\StringTok{ }\KeywordTok{match}\NormalTok{(movies}\OperatorTok{$}\NormalTok{movieId, movieId2)}
\NormalTok{ratings}\OperatorTok{$}\NormalTok{movieId <-}\StringTok{ }\KeywordTok{match}\NormalTok{(ratings}\OperatorTok{$}\NormalTok{movieId, movieId2)}
\CommentTok{# order the movies by the new movieId so that}
\CommentTok{# the movie with Id==i is in the i-th row:}
\NormalTok{movies <-}\StringTok{ }\NormalTok{movies[}\KeywordTok{order}\NormalTok{(movies}\OperatorTok{$}\NormalTok{movieId),]}
\KeywordTok{stopifnot}\NormalTok{(}\KeywordTok{all}\NormalTok{(movies}\OperatorTok{$}\NormalTok{movieId }\OperatorTok{==}\StringTok{ }\DecValTok{1}\OperatorTok{:}\NormalTok{p)) }\CommentTok{# sanity check}
\end{Highlighting}
\end{Shaded}

We will use a sparse matrix data type (from R package \texttt{Matrix})
to store the ratings data, \(\mathbf{R}\in\mathbb{R}^{n\times p}\).

\begin{description}
\tightlist
\item[Remark.]
\emph{Sparse} matrices contain many zeros. Instead of storing all the
\(np=5931640\) elements, only the lists of non-zero ones are saved,
\(100836\) values in total.
This way, we might save a lot of memory.
The drawback is that, amongst others, random access to the elements
in a sparse matrix takes more time.
\end{description}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"Matrix"}\NormalTok{)}
\NormalTok{R <-}\StringTok{ }\KeywordTok{Matrix}\NormalTok{(}\FloatTok{0.0}\NormalTok{, }\DataTypeTok{sparse=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{n, }\DataTypeTok{ncol=}\NormalTok{p)}
\CommentTok{# This is a vectorised operation;}
\CommentTok{# it is faster than a for loop over each row}
\CommentTok{# in the ratings matrix:}
\NormalTok{R[}\KeywordTok{cbind}\NormalTok{(ratings}\OperatorTok{$}\NormalTok{userId, ratings}\OperatorTok{$}\NormalTok{movieId)] <-}\StringTok{ }\NormalTok{ratings}\OperatorTok{$}\NormalTok{rating}
\end{Highlighting}
\end{Shaded}

Let's preview a few first rows and columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{R[}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{18}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 6 x 18 sparse Matrix of class "dgCMatrix"
##                                         
## [1,] 4 4 4 5 5 3 5 4 5 5 5 5 3 5 4 5 3 3
## [2,] . . . . . . . . . . . . . . . . . .
## [3,] . . . . . . . . . . . . . . . . . .
## [4,] . . . 2 . . . . . . . . . . 2 5 1 .
## [5,] 4 . . . 4 . . 4 . . . . . . . . 5 2
## [6,] . 5 4 4 1 . . 5 4 . 3 4 . 3 . . 2 5
\end{verbatim}

\hypertarget{item-item-similarities}{%
\subsection{Item-Item Similarities}\label{item-item-similarities}}

To recall, the cosine similarity between
\(\mathbf{r}_{\cdot,i},\mathbf{r}_{\cdot,j}\in\mathbb{R}^n\)
is given by:

\[
s_{i,j}^I = S_C(\mathbf{r}_{\cdot,i},\mathbf{r}_{\cdot,j}) = \frac{\sum_{k=1}^n r_{k,i} \, r_{k,j}}{
    \sqrt{\sum_{k=1}^n r_{k,i}^2}\sqrt{\sum_{k=1}^n r_{k,j}^2}
}
\]

In vector/matrix algebra notation, this is:

\[
s_{i,j}^I = S_C(\mathbf{r}_{\cdot,i},\mathbf{r}_{\cdot,j}) = \frac{\mathbf{r}_{\cdot,i}^T\, \mathbf{r}_{\cdot,j}}{
\sqrt{{\mathbf{r}_{\cdot,i}^T\, \mathbf{r}_{\cdot,i}}} \sqrt{{\mathbf{r}_{\cdot,j}^T\, \mathbf{r}_{\cdot,j}}}
}
\]

As \(\mathbf{R}\in\mathbb{R}^{n\times p}\),
we can compute all the \(p\times p\) cosine similarities
at once by applying:

\[
\mathbf{S}^I = \frac{\mathbf{R}^T \mathbf{R}}{
\mathbf{l} \mathbf{l}^T
}
\]

where \(\mathbf{l}\in\mathbf{R}^{p\times 1}\) is a column vector
such that with
\(l_i = \sqrt{{\mathbf{r}_{\cdot,i}^T\, \mathbf{r}_{\cdot,i}}}\)
and by \(\frac{\cdot}{\cdot}\) we mean elementwise division.

Cosine similarities for item-item comparisons:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{norms <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{colSums}\NormalTok{(R}\OperatorTok{^}\DecValTok{2}\NormalTok{)))}
\NormalTok{Rx <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{crossprod}\NormalTok{(R, R))}
\NormalTok{SI <-}\StringTok{ }\NormalTok{Rx}\OperatorTok{/}\KeywordTok{tcrossprod}\NormalTok{(norms)}
\NormalTok{SI[}\KeywordTok{is.nan}\NormalTok{(SI)] <-}\StringTok{ }\DecValTok{0} \CommentTok{# there were some divisions by zero}
\end{Highlighting}
\end{Shaded}

\begin{description}
\item[Remark.]
\texttt{crossprod(A,B)} gives \(\mathbf{A}^T \mathbf{B}\)
and \texttt{tcrossprod(A,B)} gives \(\mathbf{A} \mathbf{B}^T\).
\end{description}

\hypertarget{example-recommendations}{%
\subsection{Example Recommendations}\label{example-recommendations}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{recommend <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(i, K, SI, movies) \{}
    \CommentTok{# get K most similar movies to the i-th one}
\NormalTok{    ms <-}\StringTok{ }\KeywordTok{order}\NormalTok{(SI[i,], }\DataTypeTok{decreasing=}\OtherTok{TRUE}\NormalTok{)}
    \KeywordTok{data.frame}\NormalTok{(}
        \DataTypeTok{Title=}\NormalTok{movies}\OperatorTok{$}\NormalTok{title[ms[}\DecValTok{1}\OperatorTok{:}\NormalTok{K]],}
        \DataTypeTok{SIC=}\NormalTok{SI[i,ms[}\DecValTok{1}\OperatorTok{:}\NormalTok{K]],}
        \DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}
\NormalTok{    )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies}\OperatorTok{$}\NormalTok{title[}\DecValTok{1215}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Monty Python's The Meaning of Life (1983)"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{recommend}\NormalTok{(}\DecValTok{1215}\NormalTok{, }\DecValTok{10}\NormalTok{, SI, movies)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                               Title     SIC
## 1         Monty Python's The Meaning of Life (1983) 1.00000
## 2               Monty Python's Life of Brian (1979) 0.61097
## 3            Monty Python and the Holy Grail (1975) 0.51415
## 4  House of Flying Daggers (Shi mian mai fu) (2004) 0.49322
## 5      Hitchhiker's Guide to the Galaxy, The (2005) 0.45482
## 6                      Bowling for Columbine (2002) 0.45051
## 7                          Shaun of the Dead (2004) 0.44566
## 8                 O Brother, Where Art Thou? (2000) 0.44541
## 9                                Ghost World (2001) 0.44416
## 10                         Full Metal Jacket (1987) 0.44285
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies}\OperatorTok{$}\NormalTok{title[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Toy Story (1995)"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{recommend}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, SI, movies)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                                Title     SIC
## 1                                   Toy Story (1995) 1.00000
## 2                                 Toy Story 2 (1999) 0.57260
## 3                               Jurassic Park (1993) 0.56564
## 4               Independence Day (a.k.a. ID4) (1996) 0.56426
## 5          Star Wars: Episode IV - A New Hope (1977) 0.55739
## 6                                Forrest Gump (1994) 0.54710
## 7                              Lion King, The (1994) 0.54115
## 8  Star Wars: Episode VI - Return of the Jedi (1983) 0.54109
## 9                         Mission: Impossible (1996) 0.53891
## 10                              Groundhog Day (1993) 0.53417
\end{verbatim}

\ldots{}and so on.

\hypertarget{clustering-1}{%
\subsection{Clustering}\label{clustering-1}}

All our ratings are \(r_{i,j}\ge 0\), therefore the cosine similarity is
\(s_{i,j}^I\ge 0\). Moreover, it holds \(s_{i,j}^I\le 1\).
Thus, a cosine similarity matrix can be turned into
a dissimilarity matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DI <-}\StringTok{ }\FloatTok{1.0}\OperatorTok{-}\NormalTok{SI}
\NormalTok{DI[DI}\OperatorTok{<}\DecValTok{0}\NormalTok{] <-}\StringTok{ }\FloatTok{0.0} \CommentTok{# account for numeric inaccuracies}
\NormalTok{DI <-}\StringTok{ }\KeywordTok{as.dist}\NormalTok{(DI)}
\end{Highlighting}
\end{Shaded}

This enables us to perform, e.g., the cluster analysis of items:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"genie"}\NormalTok{)}
\NormalTok{h <-}\StringTok{ }\KeywordTok{hclust2}\NormalTok{(DI)}
\KeywordTok{plot}\NormalTok{(h, }\DataTypeTok{labels=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{ann=}\OtherTok{FALSE}\NormalTok{); }\KeywordTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:movielens8}{%
\centering
\includegraphics{09-recommenders-figures/movielens8-1.pdf}
\caption{Cluster dendrogram for the movies}\label{fig:movielens8}
}
\end{figure}

A 14-partition might look nice, let's give it a try:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c <-}\StringTok{ }\KeywordTok{cutree}\NormalTok{(h, }\DataTypeTok{k=}\DecValTok{14}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Example movies in the 3rd cluster:

Bottle Rocket (1996), Clerks (1994), Star Wars: Episode IV - A New Hope (1977), Swingers (1996), Monty Python's Life of Brian (1979), E.T. the Extra-Terrestrial (1982), Monty Python and the Holy Grail (1975), Star Wars: Episode V - The Empire Strikes Back (1980), Princess Bride, The (1987), Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981), Star Wars: Episode VI - Return of the Jedi (1983), Blues Brothers, The (1980), Duck Soup (1933), Groundhog Day (1993), Back to the Future (1985), Young Frankenstein (1974), Indiana Jones and the Last Crusade (1989), Grosse Pointe Blank (1997), Austin Powers: International Man of Mystery (1997), Men in Black (a.k.a. MIB) (1997)

The definitely have something in common!

Example movies in the 1st cluster:

Toy Story (1995), Heat (1995), Seven (a.k.a. Se7en) (1995), Usual Suspects, The (1995), From Dusk Till Dawn (1996), Braveheart (1995), Rob Roy (1995), Desperado (1995), Billy Madison (1995), Dumb \& Dumber (Dumb and Dumber) (1994), Ed Wood (1994), Pulp Fiction (1994), Stargate (1994), Tommy Boy (1995), Clear and Present Danger (1994), Forrest Gump (1994), Jungle Book, The (1994), Mask, The (1994), Fugitive, The (1993), Jurassic Park (1993)

\ldots{} and so forth.

\hypertarget{outro-8}{%
\section{Outro}\label{outro-8}}

\hypertarget{remarks-8}{%
\subsection{Remarks}\label{remarks-8}}

Good recommender systems are perfect tools to increase the revenue
of any user-centric enterprise.

Not a single algorithm, but an ensemble (a proper combination) of different approaches
is often used in practice, see the Further Reading section below for the detailed
information of the Netflix Prize winners.

Recommender systems are an interesting fusion of the techniques we
have already studied -- linear models, K-nearest neighbours etc.

Building recommender systems is challenging, because
data is large yet often sparse.
For instance, the ratio of available ratings
vs.~all possible user-item valuations for the Netflix Prize
(obviously, it is just a sample of
the complete dataset that Netflix has) is equal to:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{100480507}\OperatorTok{/}\NormalTok{(}\DecValTok{480189}\OperatorTok{*}\DecValTok{17770}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.011776
\end{verbatim}

A \emph{sparse matrix} (see R package \texttt{Matrix}) data structure is
often used for storing of and computing over such data effectively.

Note that some users are \emph{biased} in the sense that they are more critical or
enthusiastic than
average users.

\begin{exercise}

Is 3 stars a ``bad'', ``fair enough'' or ``good'' rating for you?
Would you go to a bar/restaurant ranked 3.0 by you favourite Maps app community?

\end{exercise}

It is particularly challenging to predict the preferences of users
that cast few ratings, e.g., those who just signed up (\emph{the cold start problem}).

\begin{quote}
``Hill et al.~{[}1995{]} have shown that users provide inconsistent ratings when asked to rate the same movie at different times. They suggest that an algorithm cannot be more accurate than the variance in a user's ratings for the same item.'' (Herlocker et al. \protect\hyperlink{ref-herlocker_etal}{2004}: p.~6)
\end{quote}

It is good to take into account the temporal (time-based) characteristics of data
as well as external knowledge
(e.g., how long ago a rating was cast,
what is a film's genre).

The presented approaches are vulnerable to attacks -- bots may be used
to promote or inhibit selected items.

\hypertarget{further-reading-8}{%
\subsection{Further Reading}\label{further-reading-8}}

Recommended further reading:
(Herlocker et al. \protect\hyperlink{ref-herlocker_etal}{2004}),
(Ricci et al. \protect\hyperlink{ref-ricci_etal}{2011}),
(Lü \& others \protect\hyperlink{ref-lu_etal}{2012}),
(Harper \& Konstan \protect\hyperlink{ref-movielens}{2015}).
See also the Netflix prize winners: (Koren \protect\hyperlink{ref-bellkor_netflix}{2009}),
(Töscher et al. \protect\hyperlink{ref-bigchaos_netflix}{2009}),
(Piotte \& Chabbert \protect\hyperlink{ref-pragmatictheory_netflix}{2009}).
Also don't forget to take a look at
the R package \texttt{recommenderlab} (amongst others).

\hypertarget{appendix-appendix}{%
\appendix \addcontentsline{toc}{chapter}{\appendixname}}


\hypertarget{notation-convention}{%
\chapter{Notation Convention}\label{notation-convention}}

\hypertarget{abbreviations}{%
\subsubsection*{Abbreviations}\label{abbreviations}}


a.k.a. == also known as

w.r.t. == with respect to

s.t. == such that

iff == if and only if

e.g.~== for example (Latin: \emph{exempli gratia})

i.e.~== that is (Latin: \emph{id est})

etc. == and so forth (Latin: \emph{et cetera})

AI == artificial intelligence

API == application programming interface

GA == genetic algorithm

GD == gradient descent

GLM == generalised linear model

ML == machine learning

NN == neural network

SGD == stochastic gradient descent

IDE = integrated development environment

\hypertarget{notation-convention-logic-and-set-theory}{%
\subsubsection*{Notation Convention -- Logic and Set Theory}\label{notation-convention-logic-and-set-theory}}


\(\forall\) -- for all

\(\exists\) -- exists

By writing \(x \in \{a, b, c\}\) we mean that
``\(x\) is in a set that consists of \(a\), \(b\) and \(c\)''
or ``\(x\) is either \(a\), \(b\) or \(c\)''

\(A\subseteq B\) -- set \(A\) is a subset of set \(B\)
(every element in \(A\) belongs to \(B\),
\(x\in A\) implies that \(x\in B\))

\(A\cup B\) -- union (sum) of two sets,
\(x\in A\cup B\) iff \(x\in A\) or \(x\in B\)

\(A\cap B\) -- intersection (sum) of two sets,
\(x\in A\cap B\) iff \(x\in A\) and \(x\in B\)

\(A\setminus B\) -- difference of two sets,
\(x\in A\setminus B\) iff \(x\in A\) and \(x\not\in B\)

\(A\times B\) -- Cartesian product of two sets,
\(A\times B = \{ (a,b): a\in A, b\in B \}\)

\(A^p = A\times A \times \dots\times A\) (\(p\) times) for any \(p\)

\hypertarget{notation-convention-symbols}{%
\subsubsection*{Notation Convention -- Symbols}\label{notation-convention-symbols}}


\(\mathbf{X,Y,A,I,C}\) -- bold (I use it for denoting vectors and matrices)

\(\mathbb{X,Y,A,I,C}\) -- blackboard bold (I sometimes use it for sets)

\(\mathcal{X,Y,A,I,C}\) -- calligraphic (I use it for set families = sets of sets)

\(X, x, \mathbf{X}, \mathbf{x}\) -- inputs (usually)

\(Y, y, \mathbf{Y}, \mathbf{y}\) -- outputs

\(\hat{Y}, \hat{y}, \hat{\mathbf{Y}}, \hat{\mathbf{y}}\) -- predicted outputs (usually)

\begin{itemize}
\item
  \(X\) -- independent/explanatory/predictor variable
\item
  \(Y\) -- dependent/response/predicted variable
\end{itemize}

\(\mathbb{R}\) -- the set of real numbers, \(\mathbb{R}=(-\infty, \infty)\)

\(\mathbb{N}\) -- the set of natural numbers, \(\mathbb{N}=\{1,2,3,\dots\}\)

\(\mathbb{N}_0\) -- the set of natural numbers including zero, \(\mathbb{N}_0=\mathbb{N}\cup\{0\}\)

\(\mathbb{Z}\) -- the set of integer numbers, \(\mathbb{Z}=\{\dots,-2,-1,0,1,2,\dots\}\)

\([0,1]\) -- the unit interval

\((a, b)\) -- an open interval; \(x\in(a,b)\) iff \(a < x < b\) for some \(a< b\)

\([a, b]\) -- a closed interval; \(x\in[a,b]\) iff \(a \le x \le b\) for some \(a\le b\)

\hypertarget{notation-convention-vectors-and-matrices}{%
\subsubsection*{Notation Convention -- Vectors and Matrices}\label{notation-convention-vectors-and-matrices}}


\(\boldsymbol{x}=(x_1,\dots,x_n)\) -- a sequence of \(n\) elements (\(n\)-ary sequence/vector)

if it consists of real numbers, we write \(\boldsymbol{x}\in\mathbb{R}^n\)

\(\mathbf{x}=[x_1\ x_2\ \dots\ x_p]\) -- a row vector, \(\mathbf{x}\in\mathbb{R}^{1\times p}\)
(a matrix with 1 row)

\(\mathbf{x}=[x_1\ x_2\ \dots\ x_n]^T\) -- a column vector, \(\mathbf{x}\in\mathbb{R}^{n\times 1}\)
(a matrix with 1 column)

\(\mathbf{X}\in\mathbb{R}^{n\times p}\) -- matrix with \(n\) rows and \(p\) columns

\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} \\
\end{array}
\right]
\]

\(x_{i,j}\) -- element in the \(i\)-th row, \(j\)-th column

\(\mathbf{x}_{i,\cdot}\) -- the \(i\)-th row of \(\mathbf{X}\)

\(\mathbf{x}_{\cdot,j}\) -- the \(j\)-th column of \(\mathbf{X}\)

\[
\mathbf{X}=\left[
\begin{array}{c}
\mathbf{x}_{1,\cdot} \\
\mathbf{x}_{2,\cdot} \\
\vdots\\
\mathbf{x}_{n,\cdot} \\
\end{array}
\right]
=
\left[
\begin{array}{cccc}
\mathbf{x}_{\cdot,1} &
\mathbf{x}_{\cdot,2} &
\cdots &
\mathbf{x}_{\cdot,p} \\
\end{array}
\right].
\]

\[
\mathbf{x}_{i,\cdot} = \left[
\begin{array}{cccc}
x_{i,1} &
x_{i,2} &
\cdots &
x_{i,p} \\
\end{array}
\right].
\]

\[
\mathbf{x}_{\cdot,j} = \left[
\begin{array}{cccc}
x_{1,j} &
x_{2,j} &
\cdots &
x_{n,j} \\
\end{array}
\right]^T=\left[
\begin{array}{c}
{x}_{1,j} \\
{x}_{2,j} \\
\vdots\\
{x}_{n,j} \\
\end{array}
\right],
\]

\({}^T\) denotes the matrix transpose;
\(\mathbf{B}=\mathbf{A}^T\) is a matrix such that \(b_{i,j}=a_{j,i}\).

\(\|\boldsymbol{x}\| = \|\boldsymbol{x}\|_2 = \sqrt{ \sum_{i=1}^n x_i^2 }\) -- the Euclidean norm

\hypertarget{notation-convention-functions}{%
\subsubsection*{Notation Convention -- Functions}\label{notation-convention-functions}}


\(f:X\to Y\) means that \(f\) is a function mapping inputs from set \(X\) (the domain of \(f\))
to elements of \(Y\) (the codomain)

\(x\mapsto x^2\) denotes a (inline) function mapping \(x\) to \(x^2\),
equivalent to \texttt{function(x)\ x\^{}2} in R

\(\exp x = e^x\) -- exponential function with base \(e\simeq 2.718\)

\(\log x\) -- natural logarithm (base \(e\))

it holds \(e^x = y\) iff \(\log y = x\)

\(\log ab = \log a + \log b\)

\(\log a^c = c\log a\)

\(\log a/b = \log a - \log b\)

\(\log 1 = 0\)

\(\log e = 1\)

hence \(\log e^x = x\)

\hypertarget{notation-convention-sums-and-products}{%
\subsubsection*{Notation Convention -- Sums and Products}\label{notation-convention-sums-and-products}}


\(\sum_{i=1}^n x_i=x_1+x_2+\dots+x_n\)

\(\sum_{i=1,\dots,n} x_i\) -- the same

\(\sum_{i\in\{1,\dots,n\}} x_i\) -- the same

note display (stand-alone) \(\displaystyle\sum_{i=1}^n x_i\) vs text (in-line) \(\textstyle\sum_{i=1}^n x_i\) style

\(\prod_{i=1}^n x_i = x_1 x_2 \dots x_n\)

\hypertarget{setting-up-the-r-environment}{%
\chapter{Setting Up the R Environment}\label{setting-up-the-r-environment}}

\hypertarget{installing-r}{%
\section{Installing R}\label{installing-r}}

R and Python are \emph{the} languages of modern data science.
The former is slightly more oriented towards data modelling,
analysis and visualisation as well as statistical computing.
It has a gentle learning curve, which makes is very suitable even
for beginners -- just like us!

R is available for Windows as well as MacOS, Linux and other Unix-like
operating systems.
It can be downloaded from the R project website,
see \url{https://www.r-project.org/} (or installed through system-specific package
repositories).

\begin{description}
\item[Remark.]
From now on we assume that you have installed the R environment.
\end{description}

\hypertarget{installing-an-ide}{%
\section{Installing an IDE}\label{installing-an-ide}}

As we wish to make our first steps with the R language
as stress- and hassle-free as possible, let's stick to a very user-friendly
development environment called RStudio, which can be downloaded from
\url{https://rstudio.com/products/rstudio/} (choose RStudio Desktop Open Source Edition).

\begin{description}
\item[Remark.]
There are of course many other options for working with R, both interactive
and non-interactive, including Jupyter Notebooks (see \url{https://irkernel.github.io/}),
dynamically generated reports (see \url{https://yihui.org/knitr/options/})
and plain shell scripts executed from a terminal.
However, for now let's leave that to more
advanced users.
\end{description}

\hypertarget{installing-recommended-packages}{%
\section{Installing Recommended Packages}\label{installing-recommended-packages}}

Once we get the above up and running, from within RStudio,
we need to install a few packages which we're going to use during the course
of this course. Execute the following commands in the R console
(bottom-left Rstudio pane):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pkgs <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Cairo"}\NormalTok{, }\StringTok{"DEoptim"}\NormalTok{, }\StringTok{"fastcluster"}\NormalTok{, }\StringTok{"FNN"}\NormalTok{, }\StringTok{"genie"}\NormalTok{,}
    \StringTok{"gsl"}\NormalTok{, }\StringTok{"hydroPSO"}\NormalTok{, }\StringTok{"ISLR"}\NormalTok{, }\StringTok{"keras"}\NormalTok{, }\StringTok{"Matrix"}\NormalTok{,}
    \StringTok{"microbenchmark"}\NormalTok{, }\StringTok{"pdist"}\NormalTok{, }\StringTok{"RColorBrewer"}\NormalTok{,}
    \StringTok{"recommenderlab"}\NormalTok{, }\StringTok{"rpart"}\NormalTok{, }\StringTok{"rpart.plot"}\NormalTok{, }\StringTok{"rworldmap"}\NormalTok{,}
    \StringTok{"scatterplot3d"}\NormalTok{, }\StringTok{"stringi"}\NormalTok{, }\StringTok{"tensorflow"}\NormalTok{, }\StringTok{"tidyr"}\NormalTok{,}
    \StringTok{"titanic"}\NormalTok{, }\StringTok{"vioplot"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(pkgs)}
\end{Highlighting}
\end{Shaded}

What is more, in order to be able to play with neural networks,
we will need some Python environment, for example
the Anaconda Distribution Python 3.x, see
\url{https://www.anaconda.com/distribution/}.

\begin{description}
\item[Remark.]
Do \textbf{not} download Python 2.7.
\end{description}

Installation instructions can be found at
\url{https://docs.anaconda.com/anaconda/install/}.
This is required for the R packages tensorflow and keras,
see \url{https://tensorflow.rstudio.com/installation/}.
Once this is installed, execute the following R commands in the console:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"tensorflow"}\NormalTok{)}
\KeywordTok{install_tensorflow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{first-r-script-in-rstudio}{%
\section{First R Script in RStudio}\label{first-r-script-in-rstudio}}

Let's open RStudio and perform the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create a New Project where we will store all the scripts
  related to this book. Click \emph{File} → \emph{New Project}
  and then choose to start in a brand new working directory,
  in any location you like.
  Choose \emph{New Project} as the project type.

  From now on, we are assuming that the project name is \emph{LMLCR}
  and the project has been opened. All source files we create
  will be relative to the project directory.
\item
  Create a new R source file, \emph{File} → \emph{New File} → \emph{R Script}.
  Save the file as, for example, \emph{sandbox\_01.R}.

  The source editor (top left pane) behaves just like any other text editor.
  Standard keyboard shortcuts are available, such as CTRL+C
  and CTRL+V (Cmd+C and Cmd+V on MacOS) for copy and paste, respectively.

  A list of keyboard shortcuts is available at
  \url{https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts}
\item
  Input the following R code into the editor:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# My first R script}
\CommentTok{# This is a comment}

\CommentTok{# Another comment}

\CommentTok{# Everything from '#' to the end of the line}
\CommentTok{#     is ignored by the R interpreter}
\KeywordTok{print}\NormalTok{(}\StringTok{"Hello world"}\NormalTok{) }\CommentTok{# prints a given character string}
\KeywordTok{print}\NormalTok{(}\DecValTok{2}\OperatorTok{+}\DecValTok{2}\NormalTok{) }\CommentTok{# evaluates the expression and prints the result}
\NormalTok{x <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DataTypeTok{length.out=}\DecValTok{100}\NormalTok{) }\CommentTok{# a new numeric vector}
\NormalTok{y <-}\StringTok{ }\NormalTok{x}\OperatorTok{^}\DecValTok{2} \CommentTok{# squares every element in x}
\KeywordTok{plot}\NormalTok{(x, y, }\DataTypeTok{las=}\DecValTok{1}\NormalTok{, }\DataTypeTok{type=}\StringTok{"l"}\NormalTok{) }\CommentTok{# plots y as a function of x}
\end{Highlighting}
\end{Shaded}
\item
  Execute the 5 above commands, line by line,
  by positioning the keyboard cursor accordingly and
  pressing Ctrl+Enter (Cmd+Return on MacOS).

  Each time, the command will be copied to the console
  (bottom-left pane)
  and evaluated.

  The last line generates a nice plot which will appear in the
  bottom-right pane.
\end{enumerate}

While you learn, we recommend that you get used to writing your
code in an R script and executing it just as we did above.

On a side note, you can execute (source) the whole script
by pressing Ctrl+Shift+S (Cmd+Shift+S on MacOS).

\hypertarget{vector-algebra-in-r}{%
\chapter{Vector Algebra in R}\label{vector-algebra-in-r}}

This chapter is a step-by-step guide to vector computations in R.
It also explains the basic mathematical notation around vectors.

You're encouraged to not only simply \emph{read} the chapter,
but also to execute yourself the R code provided.
Play with it, do some experiments, get curious about
how R works. Read the documentation on the functions you are calling,
e.g., \texttt{?seq}, \texttt{?sample} and so on.

Technical and mathematical literature isn't belletristic.
It requires \emph{active} (\emph{pro-active} even) thinking.
Sometimes going through a single page can take an hour. Or a day.
If you don't understand something, keep thinking, go back, ask yourself
questions, take a look at other sources. This is not a \emph{linear} process.
This is what makes it fun and creative.
To become a good programmer you need a lot of practice, there
are no shortcuts. But the whole endeavour is worth the hassle!

\hypertarget{motivation-1}{%
\section{Motivation}\label{motivation-1}}

Vector and matrix algebra provides us with a convenient language for
expressing computations on sequential and tabular data.

Vector and matrix algebra operations are supported by every
major programming language -- either natively
(e.g., R, Matlab, GNU Octave, Mathematica)
or via an additional library/package
(e.g, Python with numpy, tensorflow or pytorch;
C++ with Eigen/Armadillo; C, C++ or Fortran with LAPACK).

By using matrix notation, we generate more concise and readable code.

For instance, given two vectors \(\boldsymbol{x}=(x_1,\dots,x_n)\)
and \(\boldsymbol{y}=(y_1,\dots,y_n)\) like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\FloatTok{3.5}\NormalTok{, }\FloatTok{2.3}\NormalTok{,}\OperatorTok{-}\FloatTok{6.5}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{2.9}\NormalTok{, }\FloatTok{8.2}\NormalTok{,}\OperatorTok{-}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Instead of writing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s <-}\StringTok{ }\DecValTok{0}
\NormalTok{n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(x)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n)}
\NormalTok{    s <-}\StringTok{ }\NormalTok{s }\OperatorTok{+}\StringTok{ }\NormalTok{(x[i]}\OperatorTok{-}\NormalTok{y[i])}\OperatorTok{^}\DecValTok{2}
\KeywordTok{sqrt}\NormalTok{(s)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9.1159
\end{verbatim}

to mean:

\[
\sqrt{
(x_1-y_1)^2 + (x_2-y_2)^2 + \dots + (x_n-y_n)^2
}
=
\sqrt{\sum_{i=1}^n (x_i-y_i)^2},
\]

which denotes the (Euclidean) distance between the two vectors
(the square root of the sum of squared differences between
the corresponding elements in \(\boldsymbol{x}\) and \(\boldsymbol{y}\)),
we shall soon become used to writing:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{((x}\OperatorTok{-}\NormalTok{y)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9.1159
\end{verbatim}

or:

\[
\sqrt{(\boldsymbol{x}-\boldsymbol{y})^{T}(\boldsymbol{x}-\boldsymbol{y})}
\]

or even:

\[
\|\boldsymbol{x}-\boldsymbol{y}\|_2
\]

In order to be able to read this notation,
we only have to get to know the most common ``building blocks''.
There are just a few of them, but it'll take some time until we become comfortable
with their use.

What's more, we should note that vectorised code might be
much faster than the \texttt{for} loop-based one (a.k.a. ``iterative'' style):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"microbenchmark"}\NormalTok{)}
\NormalTok{n <-}\StringTok{ }\DecValTok{10000}
\NormalTok{x <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(n) }\CommentTok{# n random numbers in [0,1]}
\NormalTok{y <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(n)}
\KeywordTok{print}\NormalTok{(}\KeywordTok{microbenchmark}\NormalTok{(}
    \DataTypeTok{t1=}\NormalTok{\{}
        \CommentTok{# "iterative" style}
\NormalTok{        s <-}\StringTok{ }\DecValTok{0}
\NormalTok{        n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(x)}
        \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n)}
\NormalTok{            s <-}\StringTok{ }\NormalTok{s }\OperatorTok{+}\StringTok{ }\NormalTok{(x[i]}\OperatorTok{-}\NormalTok{y[i])}\OperatorTok{^}\DecValTok{2}
        \KeywordTok{sqrt}\NormalTok{(s)}
\NormalTok{    \},}
    \DataTypeTok{t2=}\NormalTok{\{}
        \CommentTok{# "vectorised" style}
        \KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{((x}\OperatorTok{-}\NormalTok{y)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\NormalTok{    \}}
\NormalTok{), }\DataTypeTok{signif=}\DecValTok{3}\NormalTok{, }\DataTypeTok{unit=}\StringTok{'relative'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Unit: relative
##  expr min  lq mean median   uq  max neval
##    t1 128 112 90.7   92.1 85.1 92.9   100
##    t2   1   1  1.0    1.0  1.0  1.0   100
\end{verbatim}

\hypertarget{numeric-vectors}{%
\section{Numeric Vectors}\label{numeric-vectors}}

\hypertarget{creating-numeric-vectors}{%
\subsection{Creating Numeric Vectors}\label{creating-numeric-vectors}}

First let's introduce a few ways with which we can create numeric vectors.

\hypertarget{c}{%
\subsubsection{\texorpdfstring{\texttt{c()}}{c()}}\label{c}}

The \texttt{c()} function \emph{c}ombines a given list of values to form a sequence:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4 5 6 7 8
\end{verbatim}

Note that when we use the assignment operator, \texttt{\textless{}-} or \texttt{=} (both are
equivalent), printing of the output is suppressed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{) }\CommentTok{# doesn't print anything}
\KeywordTok{print}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3
\end{verbatim}

However, we can enforce it by parenthesising the whole expression:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3
\end{verbatim}

In order to determine that \texttt{x} is indeed a numeric vector,
we call:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mode}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{description}
\item[Remark.]
These two functions might return different results.
For instance, in the next chapter we note that
a numeric matrix will yield \texttt{mode()} of \texttt{numeric} and
\texttt{class()} of \texttt{matrix}.
\end{description}

What is more, we can get the number of elements in \texttt{x} by calling:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\hypertarget{seq}{%
\subsubsection{\texorpdfstring{\texttt{seq()}}{seq()}}\label{seq}}

To create an arithmetic progression,
i.e., a sequence of equally-spaced numbers,
we can call the \texttt{seq()} function

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 3 5 7 9
\end{verbatim}

If we access the function's documentation (by executing \texttt{?seq} in the console),
we'll note that the function takes a couple of parameters:
\texttt{from}, \texttt{to}, \texttt{by}, \texttt{length.out} etc.

The above call is equivalent to:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{seq}\NormalTok{(}\DataTypeTok{from=}\DecValTok{1}\NormalTok{, }\DataTypeTok{to=}\DecValTok{9}\NormalTok{, }\DataTypeTok{by=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 3 5 7 9
\end{verbatim}

The \texttt{by} argument can be replaced with \texttt{length.out}, which gives the desired
size:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length.out=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.00 0.25 0.50 0.75 1.00
\end{verbatim}

Note that R supports partial matching of argument names:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{len=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.00 0.25 0.50 0.75 1.00
\end{verbatim}

Quite often we need progressions with step equal to 1 or -1.
Such vectors can be generated by applying the \texttt{:} operator.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\OperatorTok{:}\DecValTok{10}     \CommentTok{# from:to (inclusive)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{-1}\OperatorTok{:-}\DecValTok{10}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  -1  -2  -3  -4  -5  -6  -7  -8  -9 -10
\end{verbatim}

\hypertarget{rep}{%
\subsubsection{\texorpdfstring{\texttt{rep()}}{rep()}}\label{rep}}

Moreover, \texttt{rep()} replicates a given vector.
Check out the function's documentation (see \texttt{?rep}) for
the meaning of the arguments provided below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 1 1 1 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 2 3 1 2 3 1 2 3 1 2 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 1 2 2 2 2 3 3 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DataTypeTok{each=}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 1 1 1 2 2 2 2 3 3 3 3
\end{verbatim}

\hypertarget{pseudo-random-vectors}{%
\subsubsection{Pseudo-Random Vectors}\label{pseudo-random-vectors}}

We can also generate vectors of pseudo-random values.
For instance, the following generates 5 deviates from the uniform distribution
(every number has the same probability) on the unit (i.e., \([0,1]\)) interval:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.56490 0.55881 0.44148 0.20764 0.66964
\end{verbatim}

We call such numbers pseudo-random, because they are generated arithmetically.
In fact, by setting the random number generator's state (also called the \emph{seed}),
we can obtain \emph{reproducible} results.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\KeywordTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{# a,b,c,d,e}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.28758 0.78831 0.40898 0.88302 0.94047
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{# f,g,h,i,j}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.045556 0.528105 0.892419 0.551435 0.456615
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\KeywordTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{# a,b,c,d,e again!}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.28758 0.78831 0.40898 0.88302 0.94047
\end{verbatim}

Note the difference between the uniform distribution on \([0,1]\)
and the normal distribution with expected value of \(0\) and standard deviation
of \(1\) (also called the standard normal distribution),
see Figure \ref{fig:runif_vs_rnorm}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\CommentTok{# align plots in one row and two columns}
\KeywordTok{hist}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2500}\NormalTok{)); }\KeywordTok{box}\NormalTok{()}
\KeywordTok{hist}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{, }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2500}\NormalTok{)); }\KeywordTok{box}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:runif_vs_rnorm}{%
\centering
\includegraphics{92-R-vectors-figures/runif_vs_rnorm-1.pdf}
\caption{Uniformly vs.~normally distributed random variables}\label{fig:runif_vs_rnorm}
}
\end{figure}

Another useful function samples a number of values from a given vector,
either with or without replacement:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{) }\CommentTok{# with replacement}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  3  3 10  2  6  5  4  6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DataTypeTok{replace=}\OtherTok{FALSE}\NormalTok{) }\CommentTok{# without replacement}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  9  5  3  8  1  4  6 10
\end{verbatim}

Note that if \texttt{n} is a single number,
\texttt{sample(n,\ ...)} is equivalent to \texttt{sample(1:n,\ ...)}.
This is a dangerous behaviour than may lead to bugs in our code.
Read more at \texttt{?sample}.

\hypertarget{vector-scalar-operations}{%
\subsection{Vector-Scalar Operations}\label{vector-scalar-operations}}

Mathematically, we sometimes refer to a vector that is reduced to a single component
as a \emph{scalar}. We are used to denoting such objects with lowercase letters
such as \(a, b, i, s, x\in\mathbb{R}\).

\begin{description}
\item[Remark.]
Note that some programming languages distinguish between
atomic numerical entities and length-one vectors, e.g., \texttt{7} vs.~\texttt{{[}7{]}} in Python.
This is not the case in R, where \texttt{length(7)} returns 1.
\end{description}

Vector-scalar arithmetic operations
such as \(s\boldsymbol{x}\) (multiplication of a vector \(\boldsymbol{x}=(x_1,\dots, x_n)\)
by a scalar \(s\)) result in a vector \(\boldsymbol{y}\) such that \(y_i=s x_i\), \(i=1,\dots,n\).

The same rule holds for, e.g., \(s+\boldsymbol{x}\), \(\boldsymbol{x}-s\), \(\boldsymbol{x}/s\).

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.5} \OperatorTok{*}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  0.5  5.0 50.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{10} \OperatorTok{+}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 11 12 13 14 15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DataTypeTok{by=}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\DecValTok{10}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0 0.2 0.4 0.6 0.8 1.0
\end{verbatim}

By \(-\boldsymbol{x}\) we will mean \((-1)\boldsymbol{x}\):

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{-}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length.out=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  0.00 -0.25 -0.50 -0.75 -1.00
\end{verbatim}

Note that in R the same rule applies for exponentiation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{5}\NormalTok{)}\OperatorTok{^}\DecValTok{2} \CommentTok{# synonym: (1:5)**2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  0  1  4  9 16 25
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\OperatorTok{^}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1  2  4  8 16 32
\end{verbatim}

However, in mathematics, we are \textbf{not} used to writing
\(2^{\boldsymbol{x}}\) or \(\boldsymbol{x}^2\).

\hypertarget{vector-vector-operations}{%
\subsection{Vector-Vector Operations}\label{vector-vector-operations}}

Let \(\boldsymbol{x}=(x_1,\dots,x_n)\) and \(\boldsymbol{y}=(y_1,\dots,y_n)\) be two vectors of identical lengths.

Arithmetic operations \(\boldsymbol{x}+\boldsymbol{y}\) and \(\boldsymbol{x}-\boldsymbol{y}\) are performed \emph{elementwise},
i.e., they result in a vector \(\boldsymbol{z}\) such that
\(z_i=x_i+y_i\) and \(z_i=x_i-y_i\), respectively, \(i=1,\dots,n\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,  }\DecValTok{2}\NormalTok{,   }\DecValTok{3}\NormalTok{,    }\DecValTok{4}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{x}\OperatorTok{+}\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]    2   12  103 1004
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OperatorTok{-}\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]    0   -8  -97 -996
\end{verbatim}

Although in mathematics we are \textbf{not} used to using any special notation
for elementwise multiplication, division and exponentiation, this is available in R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OperatorTok{*}\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]    1   20  300 4000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OperatorTok{/}\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.000 0.200 0.030 0.004
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y}\OperatorTok{^}\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1e+00 1e+02 1e+06 1e+12
\end{verbatim}

\begin{description}
\item[Remark.]
\texttt{1e+12} is a number written in the \emph{scientific notation}.
It means ``1 times 10 to the power of 12'', i.e., \(1\times 10^{12}\). Physicists
love this notation, because they are used to dealing with very small (think sizes of quarks) and very large (think distances between galaxies) entities.
\end{description}

Moreover, in R the \textbf{recycling rule} is applied if we perform elementwise
operations on vectors of \emph{different} lengths -- the shorter
vector is recycled as many times as needed to match the length of the longer
vector, just as if we were performing:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DataTypeTok{length.out=}\DecValTok{12}\NormalTok{) }\CommentTok{# recycle 1,2,3 to get 12 values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 2 3 1 2 3 1 2 3 1 2 3
\end{verbatim}

Therefore:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\OperatorTok{:}\DecValTok{6} \OperatorTok{*}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4 5 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\OperatorTok{:}\DecValTok{6} \OperatorTok{*}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1 20  3 40  5 60
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\OperatorTok{:}\DecValTok{6} \OperatorTok{*}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]   1  20 300   4  50 600
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\OperatorTok{:}\DecValTok{6} \OperatorTok{*}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in 1:6 * c(1, 10, 100, 1000): longer object length is not a
## multiple of shorter object length
\end{verbatim}

\begin{verbatim}
## [1]    1   20  300 4000    5   60
\end{verbatim}

Note that a warning is not an error -- we still get a result
that makes sense.

\hypertarget{aggregation-functions}{%
\subsection{Aggregation Functions}\label{aggregation-functions}}

R implements a couple of \emph{aggregation} functions:

\begin{itemize}
\tightlist
\item
  \texttt{sum(x)} = \(\sum_{i=1}^n x_i=x_1+x_2+\dots+x_n\)
\item
  \texttt{prod(x)} = \(\prod_{i=1}^n x_i=x_1 x_2 \dots x_n\)
\item
  \texttt{mean(x)} = \(\frac{1}{n}\sum_{i=1}^n x_i\) -- arithmetic mean
\item
  \texttt{var(x)} = \texttt{sum((x-mean(x))\^{}2)/(length(x)-1)} =
  \(\frac{1}{n-1} \sum_{i=1}^n \left(x_i - \frac{1}{n}\sum_{j=1}^n x_j \right)^2\)
  -- variance
\item
  \texttt{sd(x)} = \texttt{sqrt(var(x))} -- standard deviation
\end{itemize}

see also: \texttt{min()}, \texttt{max()}, \texttt{median()}, \texttt{quantile()}.

\begin{description}
\item[Remark.]
Remember that you can always access the R manual by typing
\texttt{?functionname}, e.g., \texttt{?quantile}.
\item[Remark.]
Note that \(\sum_{i=1}^n x_i\) can also be written
as \(\displaystyle\sum_{i=1}^n x_i\) or even \(\displaystyle\sum_{i=1,\dots,n} x_i\).
These all mean the sum of \(x_i\) for \(i\) from \(1\) to \(n\),
that is, the sum of \(x_1\), \(x_2\), \ldots{}, \(x_n\),
i.e.,
\(x_1+x_2+\dots+x_n\).
\end{description}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\KeywordTok{mean}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.49728
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{median}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.48995
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{min}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.00046535
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{max}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9994
\end{verbatim}

\hypertarget{special-functions}{%
\subsection{Special Functions}\label{special-functions}}

Furthermore, R supports numerous mathematical functions, e.g.,
\texttt{sqrt()}, \texttt{abs()}, \texttt{round()}, \texttt{log()}, \texttt{exp()}, \texttt{cos()}, \texttt{sin()}.
All of them are vectorised -- when applied on a vector of length \(n\),
they yield a vector of length \(n\) in result.

For example, here is how we can compute the square roots of all the
integers between 1 and 9:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.0000 1.4142 1.7321 2.0000 2.2361 2.4495 2.6458 2.8284 3.0000
\end{verbatim}

Vectorisation is super-convenient when it comes to, for instance,
plotting (see Figure \ref{fig:sincos}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\OperatorTok{*}\NormalTok{pi, }\DecValTok{6}\OperatorTok{*}\NormalTok{pi, }\DataTypeTok{length.out=}\DecValTok{51}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(x, }\KeywordTok{sin}\NormalTok{(x), }\DataTypeTok{type=}\StringTok{"l"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{cos}\NormalTok{(x), }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{) }\CommentTok{# add a curve to the current plot}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:sincos}{%
\centering
\includegraphics{92-R-vectors-figures/sincos-1.pdf}
\caption{An example plot of the sine and cosine functions}\label{fig:sincos}
}
\end{figure}

\begin{exercise}

Try increasing the \texttt{length.out} argument to make the curves smoother.

\end{exercise}

\hypertarget{norms-and-distances}{%
\subsection{Norms and Distances}\label{norms-and-distances}}

Norms are used to measure the \emph{size} of an object.
Mathematically, we will also be interested in the following norms:

\begin{itemize}
\tightlist
\item
  Euclidean norm:
  \[
  \|\boldsymbol{x}\| = \|\boldsymbol{x}\|_2 = \sqrt{ \sum_{i=1}^n x_i^2 }
  \]
  this is nothing else than the \emph{length} of the vector \(\boldsymbol{x}\)
\item
  Manhattan (taxicab) norm:
  \[
  \|\boldsymbol{x}\|_1 = \sum_{i=1}^n |x_i|
  \]
\item
  Chebyshev (maximum) norm:
  \[
  \|\boldsymbol{x}\|_\infty = \max_{i=1,\dots,n} |x_i|
  = \max\{ |x_1|, |x_2|, \dots, |x_n| \}
  \]
\end{itemize}

The above norms can be easily implemented by means of the building blocks
introduced above. This is super easy:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{(z}\OperatorTok{^}\DecValTok{2}\NormalTok{)) }\CommentTok{# or norm(z, "2"); Euclidean}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.2361
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{abs}\NormalTok{(z))    }\CommentTok{# Manhattan}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{max}\NormalTok{(}\KeywordTok{abs}\NormalTok{(z))    }\CommentTok{# Chebyshev}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

Also note that all the norms easily generate the corresponding
\emph{distances} (metrics) between two given points. In particular:

\[
\| \boldsymbol{x}-\boldsymbol{y} \| = \sqrt{
\sum_{i=1}^n \left(x_i-y_i\right)^2
}
\]

gives the \emph{Euclidean distance} (metric) between the two vectors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{u <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{v <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{((u}\OperatorTok{-}\NormalTok{v)}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

This is the ``normal'' distance that you learned at school.

\hypertarget{dot-product}{%
\subsection{Dot Product (*)}\label{dot-product}}

What is more, given two vectors of identical lengths,
\(\boldsymbol{x}\) and \(\boldsymbol{y}\),
we define their \emph{dot product} (a.k.a. \emph{scalar} or \emph{inner product}) as:

\[
\boldsymbol{x}\cdot\boldsymbol{y} = \sum_{i=1}^n x_i y_i.
\]

Let's stress that this is not the same as the
elementwise vector multiplication in R -- the result is a single number.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{u <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{v <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\KeywordTok{sum}\NormalTok{(u}\OperatorTok{*}\NormalTok{v)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{description}
\item[Remark.]
(*) Note that the squared Euclidean norm of a vector is equal to the dot
product of the vector and itself,
\(\|\boldsymbol{x}\|^2 = \boldsymbol{x}\cdot\boldsymbol{x}\).
\end{description}

(*) Interestingly, a dot product has a nice geometrical interpretation:
\[
\boldsymbol{x}\cdot\boldsymbol{y} = \|\boldsymbol{x}\| \|\boldsymbol{y}\|
\cos\alpha
\]
where \(\alpha\) is the angle between the two vectors.
In other words, it is the product of the lengths of the two vectors
and the cosine of the angle between them.
Note that we can get the cosine part by computing the dot product
of the \emph{normalised}
vectors, i.e., such that their lengths are equal to 1.

For example, the two vectors \texttt{u} and \texttt{v} defined above
can be depicted as in Figure \ref{fig:vectorplot}.

\begin{figure}
\hypertarget{fig:vectorplot}{%
\centering
\includegraphics{92-R-vectors-figures/vectorplot-1.pdf}
\caption{Example vectors in 2D}\label{fig:vectorplot}
}
\end{figure}

We can compute the angle between them by calling:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(len_u <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{(u}\OperatorTok{^}\DecValTok{2}\NormalTok{))) }\CommentTok{# length == Euclidean norm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(len_v <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{(v}\OperatorTok{^}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.4142
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(cos_angle_uv <-}\StringTok{ }\NormalTok{(}\KeywordTok{sum}\NormalTok{(u}\OperatorTok{*}\NormalTok{v)}\OperatorTok{/}\NormalTok{(len_u}\OperatorTok{*}\NormalTok{len_v))) }\CommentTok{# cosine of the angle}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.70711
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{acos}\NormalTok{(cos_angle_uv)}\OperatorTok{*}\DecValTok{180}\OperatorTok{/}\NormalTok{pi }\CommentTok{# angle in degs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 45
\end{verbatim}

\hypertarget{missing-and-other-special-values}{%
\subsection{Missing and Other Special Values}\label{missing-and-other-special-values}}

R has a notion of a missing (not-available) value.
It is very useful in data analysis, as we sometimes don't have an information
on an object's feature. For instance, we might not know a patient's age
if he was admitted to the hospital unconscious.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Operations on missing values generally result in missing values --
that makes a lot sense.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{+}\StringTok{ }\DecValTok{11}\OperatorTok{:}\DecValTok{15}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12 14 NA 18 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

If we wish to compute a vector's aggregate after all,
we can get rid of the missing values by calling \texttt{na.omit()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)) }\CommentTok{# mean of non-missing values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

We can also make use of the \texttt{na.rm} parameter of the \texttt{mean()} function
(however, not every aggregation function has it -- always refer to documentation).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(x, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{description}
\item[Remark.]
Note that in R, a dot has no special meaning.
\texttt{na.omit} is as good of a function's name or variable identifier
as \texttt{na\_omit}, \texttt{naOmit}, \texttt{NAOMIT}, \texttt{naomit} and \texttt{NaOmit}.
Note that, however, R is a case-sensitive language -- these are all different
symbols.
Read more in the \emph{Details} section of \texttt{?make.names}.
\end{description}

Moreover, some arithmetic operations can result in infinities (\(\pm \infty\)):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{log}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -Inf
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{10}\OperatorTok{^}\DecValTok{1000} \CommentTok{# too large}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] Inf
\end{verbatim}

Also, sometimes we'll get a \emph{not-a-number}, \texttt{NaN}. This is not a missing value,
but a ``invalid'' result.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sqrt}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in sqrt(-1): NaNs produced
\end{verbatim}

\begin{verbatim}
## [1] NaN
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{log}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in log(-1): NaNs produced
\end{verbatim}

\begin{verbatim}
## [1] NaN
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{Inf}\OperatorTok{-}\OtherTok{Inf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NaN
\end{verbatim}

\hypertarget{logical-vectors}{%
\section{Logical Vectors}\label{logical-vectors}}

\hypertarget{creating-logical-vectors}{%
\subsection{Creating Logical Vectors}\label{creating-logical-vectors}}

In R there are 3 (!) logical values:
\texttt{TRUE}, \texttt{FALSE} and geez, I don't know, \texttt{NA} maybe?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE  TRUE    NA FALSE FALSE  TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(x <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{NA}\NormalTok{), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE    NA  TRUE FALSE    NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mode}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "logical"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "logical"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

\begin{description}
\item[Remark.]
By default, \texttt{T} is a synonym for \texttt{TRUE} and \texttt{F} for \texttt{FALSE}. This
may be changed though so it's better not to rely on these.
\end{description}

\hypertarget{logical-operations}{%
\subsection{Logical Operations}\label{logical-operations}}

Logical operators such as \texttt{\&} (and) and \texttt{\textbar{}} (or)
are performed in the same manner as arithmetic ones, i.e.:

\begin{itemize}
\tightlist
\item
  they are elementwise operations and
\item
  recycling rule is applied if necessary.
\end{itemize}

For example,

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{TRUE} \OperatorTok{&}\StringTok{ }\OtherTok{TRUE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{TRUE} \OperatorTok{&}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\OtherTok{FALSE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{) }\OperatorTok{|}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE  TRUE  TRUE
\end{verbatim}

The \texttt{!} operator stands for logical elementwise negation:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{!}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE  TRUE
\end{verbatim}

Generally, operations on \texttt{NA}s yield \texttt{NA} unless other solution
makes sense.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{u <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{NA}\NormalTok{)}
\NormalTok{v <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\OtherTok{NA}\NormalTok{)}
\NormalTok{u }\OperatorTok{&}\StringTok{ }\NormalTok{v }\CommentTok{# elementwise AND (conjunction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE    NA FALSE FALSE FALSE    NA FALSE    NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{u }\OperatorTok{|}\StringTok{ }\NormalTok{v }\CommentTok{# elementwise OR  (disjunction)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE  TRUE  TRUE  TRUE FALSE    NA  TRUE    NA    NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{!}\NormalTok{u    }\CommentTok{# elementwise NOT (negation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE  TRUE    NA
\end{verbatim}

\hypertarget{comparison-operations}{%
\subsection{Comparison Operations}\label{comparison-operations}}

We can compare the corresponding elements of two numeric vectors
and get a logical vector in result.
Operators such as \texttt{\textless{}} (less than), \texttt{\textless{}=} (less than or equal),
\texttt{==} (equal), \texttt{!=} (not equal), \texttt{\textgreater{}} (greater than) and \texttt{\textgreater{}=} (greater than or equal)
are again elementwise and use the recycling rule if necessary.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{3} \OperatorTok{<}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{5} \CommentTok{# c(3, 3, 3, 3, 3) < c(1, 2, 3, 4, 5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE FALSE FALSE  TRUE  TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\OperatorTok{:}\DecValTok{2} \OperatorTok{==}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{4} \CommentTok{# c(1,2,1,2) == c(1,2,3,4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE  TRUE FALSE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{-1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{(z }\OperatorTok{>=}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{&}\StringTok{ }\NormalTok{(z }\OperatorTok{<=}\StringTok{ }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE FALSE  TRUE  TRUE
\end{verbatim}

\hypertarget{aggregation-functions-1}{%
\subsection{Aggregation Functions}\label{aggregation-functions-1}}

Also note the following operations on \emph{logical} vectors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{10}
\KeywordTok{all}\NormalTok{(z }\OperatorTok{>=}\StringTok{ }\DecValTok{5}\NormalTok{) }\CommentTok{# are all values TRUE?}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{any}\NormalTok{(z }\OperatorTok{>=}\StringTok{ }\DecValTok{5}\NormalTok{) }\CommentTok{# is there any value TRUE?}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Moreover:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(z }\OperatorTok{>=}\StringTok{ }\DecValTok{5}\NormalTok{) }\CommentTok{# how many TRUE values are there?}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(z }\OperatorTok{>=}\StringTok{ }\DecValTok{5}\NormalTok{) }\CommentTok{# what is the proportion of TRUE values?}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6
\end{verbatim}

The behaviour of \texttt{sum()} and \texttt{mean()} is dictated by the fact
that, when interpreted in numeric terms, \texttt{TRUE} is interpreted as
numeric \texttt{1} and \texttt{FALSE} as \texttt{0}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0 1
\end{verbatim}

Therefore in the example above we have:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{>=}\StringTok{ }\DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(z }\OperatorTok{>=}\StringTok{ }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0 0 0 0 1 1 1 1 1 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(z }\OperatorTok{>=}\StringTok{ }\DecValTok{5}\NormalTok{)) }\CommentTok{# the same as sum(z >= 5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

Yes, there are 6 values equal to TRUE (or 6 ones after conversion), the sum
of zeros and ones gives the number of ones.

\hypertarget{character-vectors}{%
\section{Character Vectors}\label{character-vectors}}

\hypertarget{creating-character-vectors}{%
\subsection{Creating Character Vectors}\label{creating-character-vectors}}

Individual character strings can be created using double quotes or apostrophes.
These are the elements of character vectors

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(x <-}\StringTok{ "a string"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a string"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mode}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"aaa"}\NormalTok{, }\StringTok{'bb'}\NormalTok{, }\StringTok{"c"}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "aaa" "bb"  "c"   "aaa" "bb"  "c"
\end{verbatim}

\hypertarget{concatenating-character-vectors}{%
\subsection{Concatenating Character Vectors}\label{concatenating-character-vectors}}

To join (concatenate) the corresponding elements of two or more character vectors,
we call the \texttt{paste()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\StringTok{"1"}\NormalTok{, }\StringTok{"2"}\NormalTok{, }\StringTok{"3"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a 1" "b 2" "c 3"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\StringTok{"1"}\NormalTok{, }\StringTok{"2"}\NormalTok{, }\StringTok{"3"}\NormalTok{), }\DataTypeTok{sep=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a1" "b2" "c3"
\end{verbatim}

Also note:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{), }\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{) }\CommentTok{# the same as as.character(1:3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a 1" "b 2" "c 3"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{), }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{) }\CommentTok{# recycling}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a 1" "b 2" "c 3" "a 4" "b 5" "c 6"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{), }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\StringTok{"!"}\NormalTok{, }\StringTok{"?"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a 1 !" "b 2 ?" "c 3 !" "a 4 ?" "b 5 !" "c 6 ?"
\end{verbatim}

\hypertarget{collapsing-character-vectors}{%
\subsection{Collapsing Character Vectors}\label{collapsing-character-vectors}}

We can also collapse a sequence of strings to a single string:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{, }\StringTok{"d"}\NormalTok{), }\DataTypeTok{collapse=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "abcd"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{, }\StringTok{"d"}\NormalTok{), }\DataTypeTok{collapse=}\StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "a,b,c,d"
\end{verbatim}

\hypertarget{vector-subsetting}{%
\section{Vector Subsetting}\label{vector-subsetting}}

\hypertarget{subsetting-with-positive-indices}{%
\subsection{Subsetting with Positive Indices}\label{subsetting-with-positive-indices}}

In order to extract subsets (parts) of vectors, we use the square brackets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(x <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  10  20  30  40  50  60  70  80  90 100
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\DecValTok{1}\NormalTok{]         }\CommentTok{# the first element}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\KeywordTok{length}\NormalTok{(x)] }\CommentTok{# the last element}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 100
\end{verbatim}

More than one element at a time can also be extracted:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{] }\CommentTok{# the first three}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10 20 30
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{length}\NormalTok{(x))] }\CommentTok{# the first and the last}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  10 100
\end{verbatim}

For example, the \texttt{order()} function returns the indices of the
smallest, 2nd smallest, 3rd smallest, \ldots{}, the largest element in a given vector.
We will use this function when implementing our first classifier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{40}\NormalTok{)}
\NormalTok{(o <-}\StringTok{ }\KeywordTok{order}\NormalTok{(y))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3 4 2 5 1
\end{verbatim}

Hence, we see that the smallest element in \texttt{y} is at index 3
and the largest at index 1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y[o[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y[o[}\KeywordTok{length}\NormalTok{(y)]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 50
\end{verbatim}

Therefore, to get a sorted version of \texttt{y}, we call:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y[o] }\CommentTok{# see also sort(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10 20 30 40 50
\end{verbatim}

We can also obtain the 3 largest elements by calling:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y[}\KeywordTok{order}\NormalTok{(y, }\DataTypeTok{decreasing=}\OtherTok{TRUE}\NormalTok{)[}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 50 40 30
\end{verbatim}

\hypertarget{subsetting-with-negative-indices}{%
\subsection{Subsetting with Negative Indices}\label{subsetting-with-negative-indices}}

Subsetting with a vector of negative indices, \emph{excludes} the elements
at given positions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\OperatorTok{-}\DecValTok{1}\NormalTok{] }\CommentTok{# all but the first}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  20  30  40  50  60  70  80  90 100
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  40  50  60  70  80  90 100
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  40  60  70  90 100
\end{verbatim}

\hypertarget{subsetting-with-logical-vectors}{%
\subsection{Subsetting with Logical Vectors}\label{subsetting-with-logical-vectors}}

We may also subset a vector \(\boldsymbol{x}\)
of length \(n\) with a logical vector \(\boldsymbol{l}\) also of length \(n\).
The \(i\)-th element, \(x_i\), will be extracted if and only if the corresponding
\(l_i\) is true.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  10  50  70  80 100
\end{verbatim}

This gets along nicely with comparison operators that yield logical vectors
on output.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OperatorTok{>}\DecValTok{50}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[x}\OperatorTok{>}\DecValTok{50}\NormalTok{] }\CommentTok{# select elements in x that are greater than 50}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  60  70  80  90 100
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[x}\OperatorTok{<}\DecValTok{30} \OperatorTok{|}\StringTok{ }\NormalTok{x}\OperatorTok{>}\DecValTok{70}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  10  20  80  90 100
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[x}\OperatorTok{<}\KeywordTok{max}\NormalTok{(x)] }\CommentTok{# getting rid of the greatest element}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10 20 30 40 50 60 70 80 90
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[x }\OperatorTok{>}\StringTok{ }\KeywordTok{min}\NormalTok{(x) }\OperatorTok{&}\StringTok{ }\NormalTok{x }\OperatorTok{<}\StringTok{ }\KeywordTok{max}\NormalTok{(x)] }\CommentTok{# return all but the smallest and greatest one}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20 30 40 50 60 70 80 90
\end{verbatim}

Of course, e.g., \texttt{x{[}x\textless{}max(x){]}} returns a new, independent object.
In order to remove the greatest element in \texttt{x} permanently, we can write
\texttt{x\ \textless{}-\ x{[}x\textless{}max(x){]}}.

\hypertarget{replacing-elements}{%
\subsection{Replacing Elements}\label{replacing-elements}}

Note that the three above vector indexing schemes (positive,
negative, logical indices) allow for replacing specific elements with new
values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\OperatorTok{-}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{10000}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]    10 10000 10000 10000 10000 10000 10000 10000 10000 10000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{)] <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]    10 10000 10000 10000 10000 10000 10000     1     2     3
\end{verbatim}

\hypertarget{other-functions}{%
\subsection{Other Functions}\label{other-functions}}

\texttt{head()} and \texttt{tail()} return, respectively, a few (6 by default) first and last elements
of a vector.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(x) }\CommentTok{# head(x, 6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]    10 10000 10000 10000 10000 10000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tail}\NormalTok{(x, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3
\end{verbatim}

Sometimes the \texttt{which()} function can come in handy.
For a given logical vector, it returns all the indices
where \texttt{TRUE} elements are stored.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{which}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 3 4 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(y) }\CommentTok{# recall}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 50 30 10 20 40
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{which}\NormalTok{(y}\OperatorTok{>}\DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 5
\end{verbatim}

Note that \texttt{y{[}y\textgreater{}70{]}} gives the same result
as \texttt{y{[}which(y\textgreater{}70){]}} but is faster (because it involves less operations).

\texttt{which.min()} and \texttt{which.max()} return the index of the smallest
and the largest element, respectively:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{which.min}\NormalTok{(y) }\CommentTok{# where is the minimum?}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{which.max}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y[}\KeywordTok{which.min}\NormalTok{(y)] }\CommentTok{# min(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

\texttt{is.na()} indicates which elements are missing values (\texttt{NA}s):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{4}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\KeywordTok{is.na}\NormalTok{(z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE FALSE  TRUE FALSE  TRUE FALSE
\end{verbatim}

Therefore, to remove them from \texttt{z} permanently,
we can write (compare \texttt{na.omit()}, see also \texttt{is.finite()}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(z <-}\StringTok{ }\NormalTok{z[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(z)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 4 6
\end{verbatim}

\hypertarget{named-vectors}{%
\section{Named Vectors}\label{named-vectors}}

\hypertarget{creating-named-vectors}{%
\subsection{Creating Named Vectors}\label{creating-named-vectors}}

Vectors in R can be \emph{named} -- each element can be assigned a string label.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{99}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\KeywordTok{names}\NormalTok{(x) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{, }\StringTok{"d"}\NormalTok{, }\StringTok{"e"}\NormalTok{)}
\NormalTok{x }\CommentTok{# a named vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  a  b  c  d  e 
## 20 40 99 30 10
\end{verbatim}

Other ways to create named vectors include:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\DataTypeTok{a=}\DecValTok{1}\NormalTok{, }\DataTypeTok{b=}\DecValTok{2}\NormalTok{, }\DataTypeTok{c=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## a b c 
## 1 2 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{structure}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DataTypeTok{names=}\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## a b c 
## 1 2 3
\end{verbatim}

For instance, the \texttt{summary()} function returns a named vector:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(x) }\CommentTok{# NAMED vector, we don't want this here yet}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    10.0    20.0    30.0    39.8    40.0    99.0
\end{verbatim}

This gives the minimum, 1st quartile (25\%-quantile),
Median (50\%-quantile), aritmetic mean,
3rd quartile (75\%-quantile) and maximum.

Note that \texttt{x} is still a numeric vector, we can perform various operations
on it as usual:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 199
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[x}\OperatorTok{>}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  a  b  c  d  e 
## 20 40 99 30 10
\end{verbatim}

Names can be dropped by calling:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{unname}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20 40 99 30 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(x) }\CommentTok{# we need to know the type of x though}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20 40 99 30 10
\end{verbatim}

\hypertarget{subsetting-named-vectors-with-character-string-indices}{%
\subsection{Subsetting Named Vectors with Character String Indices}\label{subsetting-named-vectors-with-character-string-indices}}

It turns out that extracting elements from a named vector can \emph{also} be
performed by means of a vector of character string indices:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"d"}\NormalTok{, }\StringTok{"b"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  a  d  b 
## 20 30 40
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(x)[}\KeywordTok{c}\NormalTok{(}\StringTok{"Median"}\NormalTok{, }\StringTok{"Mean"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Median   Mean 
##   30.0   39.8
\end{verbatim}

\hypertarget{factors}{%
\section{Factors}\label{factors}}

Factors are \emph{special} kinds of vectors that are frequently used to
store qualitative data, e.g., species, groups, types.
Factors are convenient in situations where we have many observations,
but the number
of distinct (unique) values is relatively small.

\hypertarget{creating-factors}{%
\subsection{Creating Factors}\label{creating-factors}}

For example, the following character vector:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(col <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{), }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "green" "green" "green" "red"   "green" "red"   "red"   "red"  
##  [9] "green" "blue"
\end{verbatim}

can be converted to a factor by calling:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(fcol <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(col))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] green green green red   green red   red   red   green blue 
## Levels: blue green red
\end{verbatim}

Note how different is the way factors are printed out on the console.

\hypertarget{levels}{%
\subsection{Levels}\label{levels}}

We can easily obtain the list unique labels:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{levels}\NormalTok{(fcol)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "blue"  "green" "red"
\end{verbatim}

Those can be re-encoded by calling:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{levels}\NormalTok{(fcol) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"b"}\NormalTok{, }\StringTok{"g"}\NormalTok{, }\StringTok{"r"}\NormalTok{)}
\NormalTok{fcol}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] g g g r g r r r g b
## Levels: b g r
\end{verbatim}

To create a contingency table (in the form of a named numeric vector,
giving how many values are at each factor level),
we call:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(fcol)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## fcol
## b g r 
## 1 5 4
\end{verbatim}

\hypertarget{internal-representation}{%
\subsection{Internal Representation (*)}\label{internal-representation}}

Factors have a look-and-feel of character vectors,
however, internally they are represented as integer sequences.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(fcol)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mode}\NormalTok{(fcol)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(fcol)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 2 2 2 3 2 3 3 3 2 1
\end{verbatim}

These are always integers from \texttt{1} to \texttt{M} inclusive,
where \texttt{M} is the number of levels.
Their meaning is given by the \texttt{levels()} function:
in the example above, the meaning of the codes \texttt{1}, \texttt{2}, \texttt{3} is,
respectively, \texttt{b,\ g,\ r}.

If we wished to generate a factor with a specific order of labels,
we could call:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{factor}\NormalTok{(col, }\DataTypeTok{levels=}\KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] green green green red   green red   red   red   green blue 
## Levels: red green blue
\end{verbatim}

We can also assign different labels upon creation of a factor:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{factor}\NormalTok{(col, }\DataTypeTok{levels=}\KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{), }\DataTypeTok{labels=}\KeywordTok{c}\NormalTok{(}\StringTok{"r"}\NormalTok{, }\StringTok{"g"}\NormalTok{, }\StringTok{"b"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] g g g r g r r r g b
## Levels: r g b
\end{verbatim}

Knowing how factors are represented is important when we deal
with factors that are built around data that \emph{look like} numeric.
This is because their conversion to numeric
gives the internal codes, not the actual values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(f <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 3 0 1 4 0 0 1 4
## Levels: 0 1 3 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(f) }\CommentTok{# not necessarily what we want here}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 3 1 2 4 1 1 2 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(f)) }\CommentTok{# much better}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 3 0 1 4 0 0 1 4
\end{verbatim}

Moreover, that idea is labour-saving in contexts such as plotting
of data that are grouped into different classes.
For instance, here is a scatter plot
for the Sepal.Length and Petal.Width variables in the \texttt{iris} dataset (which
is an object of type \texttt{data.frame}, see below).
Flowers are of different Species, and we wish to indicate which point belongs
to which class:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{which_preview <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{51}\NormalTok{, }\DecValTok{69}\NormalTok{, }\DecValTok{101}\NormalTok{) }\CommentTok{# indexes we show below}
\NormalTok{iris}\OperatorTok{$}\NormalTok{Sepal.Length[which_preview]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.1 5.4 7.0 6.2 6.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris}\OperatorTok{$}\NormalTok{Petal.Width[which_preview]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2 0.2 1.4 1.5 2.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris}\OperatorTok{$}\NormalTok{Species[which_preview]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] setosa     setosa     versicolor versicolor virginica 
## Levels: setosa versicolor virginica
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Species)[which_preview]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 1 2 2 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Sepal.Length, }\CommentTok{# x (it's a vector)}
\NormalTok{     iris}\OperatorTok{$}\NormalTok{Petal.Width,  }\CommentTok{# y (it's a vector)}
     \DataTypeTok{col=}\KeywordTok{as.numeric}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Species), }\CommentTok{# colours}
     \DataTypeTok{pch=}\KeywordTok{as.numeric}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Species)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:irisplot_factors}{%
\centering
\includegraphics{92-R-vectors-figures/irisplot_factors-1.pdf}
\caption{\texttt{as.numeric()} on factors can be used to create different plotting styles}\label{fig:irisplot_factors}
}
\end{figure}

The above (see Figure \ref{fig:irisplot_factors})
was possible because the Species column is a factor object
with:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{levels}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Species)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "setosa"     "versicolor" "virginica"
\end{verbatim}

and the meaning of \texttt{pch} of 1, 2, 3, \ldots{} is ``circle'', ``triangle'', ``plus'', \ldots{},
respectively. What's more, there's a default palette that maps
consecutive integers to different colours:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{palette}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "black"   "red"     "green3"  "blue"    "cyan"    "magenta"
## [7] "yellow"  "gray"
\end{verbatim}

Hence, black circles mark irises from the 1st class, i.e., ``setosa''.

\hypertarget{lists}{%
\section{Lists}\label{lists}}

Numeric, logical and character vectors are \emph{atomic} objects -- each
component is of the same type. Let's take a look at what happens when
we create an atomic vector out of objects of different types:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\StringTok{"nine"}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\DecValTok{7}\NormalTok{, }\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "nine"  "FALSE" "7"     "TRUE"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\OtherTok{FALSE}\NormalTok{, }\DecValTok{7}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0 7 1 7
\end{verbatim}

In each case, we get an object of the most ``general'' type which is able to
represent our data.

On the other hand, R \emph{lists} are \emph{generalised} vectors.
They can consist of arbitrary R objects, possibly of mixed types --
also other lists.

\hypertarget{creating-lists}{%
\subsection{Creating Lists}\label{creating-lists}}

Most commonly, we create a generalised vector by calling the \texttt{list()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(l <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, letters, }\KeywordTok{runif}\NormalTok{(}\DecValTok{3}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 1 2 3 4 5
## 
## [[2]]
##  [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q"
## [18] "r" "s" "t" "u" "v" "w" "x" "y" "z"
## 
## [[3]]
## [1] 0.95683 0.45333 0.67757
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mode}\NormalTok{(l)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(l)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(l)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

There's a more compact way to print a list on the console:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(l)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 3
##  $ : int [1:5] 1 2 3 4 5
##  $ : chr [1:26] "a" "b" "c" "d" ...
##  $ : num [1:3] 0.957 0.453 0.678
\end{verbatim}

We can also convert an atomic vector to a list by calling:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.list}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 1
## 
## [[2]]
## [1] 2
## 
## [[3]]
## [1] 3
\end{verbatim}

\hypertarget{named-lists}{%
\subsection{Named Lists}\label{named-lists}}

List, like other vectors, may be assigned a \texttt{names} attribute.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(l) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{)}
\NormalTok{l}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $a
## [1] 1 2 3 4 5
## 
## $b
##  [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q"
## [18] "r" "s" "t" "u" "v" "w" "x" "y" "z"
## 
## $c
## [1] 0.95683 0.45333 0.67757
\end{verbatim}

\hypertarget{subsetting-and-extracting-from-lists}{%
\subsection{Subsetting and Extracting From Lists}\label{subsetting-and-extracting-from-lists}}

Applying a square brackets operator creates a sub-list, which is
of type list as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l[}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $b
##  [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q"
## [18] "r" "s" "t" "u" "v" "w" "x" "y" "z"
## 
## $c
## [1] 0.95683 0.45333 0.67757
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l[}\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"c"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $a
## [1] 1 2 3 4 5
## 
## $c
## [1] 0.95683 0.45333 0.67757
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $a
## [1] 1 2 3 4 5
\end{verbatim}

Note in the 3rd case we deal with a list of length one, not a numeric vector.

To \emph{extract} (dig into) a particular (single) element, we use double square brackets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l[[}\StringTok{"c"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.95683 0.45333 0.67757
\end{verbatim}

The latter can equivalently be written as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l}\OperatorTok{$}\NormalTok{c}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.95683 0.45333 0.67757
\end{verbatim}

\hypertarget{common-operations}{%
\subsection{Common Operations}\label{common-operations}}

Lists, because of their generality (they can store any kind of object),
have few dedicated operations.
In particular, it neither makes sense to add, multiply, \ldots{} two lists together
nor to aggregate them.

However, if we wish to run some operation on each element, we can call
list-apply:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(k <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{), }\DataTypeTok{y=}\KeywordTok{runif}\NormalTok{(}\DecValTok{6}\NormalTok{), }\DataTypeTok{z=}\KeywordTok{runif}\NormalTok{(}\DecValTok{3}\NormalTok{))) }\CommentTok{# a named list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $x
## [1] 0.57263 0.10292 0.89982 0.24609 0.04206
## 
## $y
## [1] 0.32792 0.95450 0.88954 0.69280 0.64051 0.99427
## 
## $z
## [1] 0.65571 0.70853 0.54407
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lapply}\NormalTok{(k, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $x
## [1] 0.37271
## 
## $y
## [1] 0.74992
## 
## $z
## [1] 0.6361
\end{verbatim}

The above computes the mean of each of the three numeric vectors
stored inside list \texttt{k}.
Moreover:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lapply}\NormalTok{(k, range)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $x
## [1] 0.04206 0.89982
## 
## $y
## [1] 0.32792 0.99427
## 
## $z
## [1] 0.54407 0.70853
\end{verbatim}

The built-in function \texttt{range(x)} returns \texttt{c(min(x),\ max(x))}.

\texttt{unlist()} tries (it might not always be possible)
to unwind a list to a simpler, atomic form:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{unlist}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(k, mean))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       x       y       z 
## 0.37271 0.74992 0.63610
\end{verbatim}

Moreover, \texttt{split(x,\ f)} classifies elements in a vector \texttt{x}
into subgroups defined by a factor (or an object coercible to)
of the same length.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(  }\DecValTok{1}\NormalTok{,   }\DecValTok{2}\NormalTok{,   }\DecValTok{3}\NormalTok{,   }\DecValTok{4}\NormalTok{,   }\DecValTok{5}\NormalTok{,   }\DecValTok{6}\NormalTok{,   }\DecValTok{7}\NormalTok{,   }\DecValTok{8}\NormalTok{,   }\DecValTok{9}\NormalTok{,  }\DecValTok{10}\NormalTok{)}
\NormalTok{f <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"a"}\NormalTok{, }\StringTok{"a"}\NormalTok{, }\StringTok{"c"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"a"}\NormalTok{, }\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{)}
\KeywordTok{split}\NormalTok{(x, f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $a
## [1] 1 3 4 8 9
## 
## $b
## [1]  2  6  7 10
## 
## $c
## [1] 5
\end{verbatim}

This is very useful when combined with \texttt{lapply()} and \texttt{unlist()}.
For instance, here are the mean sepal lengths
for each of the three flower species in the famous \texttt{iris} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{unlist}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(}\KeywordTok{split}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Sepal.Length, iris}\OperatorTok{$}\NormalTok{Species), mean))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     setosa versicolor  virginica 
##      5.006      5.936      6.588
\end{verbatim}

By the way, if we take a look at the documentation of \texttt{?lapply},
we will note that that this function is defined as \texttt{lapply(X,\ FUN,\ ...)}.
Here \texttt{...} denotes the optional arguments that will be passed to \texttt{FUN}.

In other words, \texttt{lapply(X,\ FUN,\ ...)} returns a list \texttt{Y} of length \texttt{length(X)}
such that \texttt{Y{[}{[}i{]}{]}\ \textless{}-\ FUN(X{[}{[}i{]}{]},\ ...)} for each \texttt{i}.
For example, \texttt{mean()} has an additional argument \texttt{na.rm} that
aims to remove missing values from the input vector.
Compare the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\KeywordTok{unlist}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(t, mean))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.5  NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{unlist}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(t, mean, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.5 3.0
\end{verbatim}

Of course, we can always pass a custom (self-made) function object as well:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min_mean_max <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
    \CommentTok{# the last expression evaluated in the function's body}
    \CommentTok{# gives its return value:}
    \KeywordTok{c}\NormalTok{(}\KeywordTok{min}\NormalTok{(x), }\KeywordTok{mean}\NormalTok{(x), }\KeywordTok{max}\NormalTok{(x))}
\NormalTok{\}}
\KeywordTok{lapply}\NormalTok{(k, min_mean_max)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $x
## [1] 0.04206 0.37271 0.89982
## 
## $y
## [1] 0.32792 0.74992 0.99427
## 
## $z
## [1] 0.54407 0.63610 0.70853
\end{verbatim}

or, more concisely (we can skip the curly braces here -- they are
normally used to group many expressions into one; also, if we don't plan
to re-use the function again, there's no need to give it a name):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lapply}\NormalTok{(k, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{c}\NormalTok{(}\KeywordTok{min}\NormalTok{(x), }\KeywordTok{mean}\NormalTok{(x), }\KeywordTok{max}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $x
## [1] 0.04206 0.37271 0.89982
## 
## $y
## [1] 0.32792 0.74992 0.99427
## 
## $z
## [1] 0.54407 0.63610 0.70853
\end{verbatim}

\hypertarget{further-reading-9}{%
\section{Further Reading}\label{further-reading-9}}

Recommended further reading: (Venables et al. \protect\hyperlink{ref-Rintro}{2020})

Other: (Deisenroth et al. \protect\hyperlink{ref-mml}{2020}), (Peng \protect\hyperlink{ref-rprogdatascience}{2019}), (Wickham \& Grolemund \protect\hyperlink{ref-r4ds}{2017})

\hypertarget{matrix-algebra-in-r}{%
\chapter{Matrix Algebra in R}\label{matrix-algebra-in-r}}

Vectors are one-dimensional objects -- they represent ``flat'' sequences of values.
Matrices, on the other hand, are two-dimensional -- they represent tabular data,
where values aligned into rows and columns. Matrices (and their extensions --
data frames, which we'll cover in the next chapter) are predominant
in data science, where objects are typically represented by means
of feature vectors.

Below are some examples of structured datasets in matrix forms.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(iris[,}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Sepal.Length Sepal.Width Petal.Length Petal.Width
## [1,]          5.1         3.5          1.4         0.2
## [2,]          4.9         3.0          1.4         0.2
## [3,]          4.7         3.2          1.3         0.2
## [4,]          4.6         3.1          1.5         0.2
## [5,]          5.0         3.6          1.4         0.2
## [6,]          5.4         3.9          1.7         0.4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WorldPhones}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      N.Amer Europe Asia S.Amer Oceania Africa Mid.Amer
## 1951  45939  21574 2876   1815    1646     89      555
## 1956  60423  29990 4708   2568    2366   1411      733
## 1957  64721  32510 5230   2695    2526   1546      773
## 1958  68484  35218 6662   2845    2691   1663      836
## 1959  71799  37598 6856   3000    2868   1769      911
## 1960  76036  40341 8220   3145    3054   1905     1008
## 1961  79831  43173 9053   3338    3224   2005     1076
\end{verbatim}

The aim of this chapter is to cover the most essential matrix operations,
both from the computational perspective and the mathematical one.

\hypertarget{creating-matrices}{%
\section{Creating Matrices}\label{creating-matrices}}

\hypertarget{matrix}{%
\subsection{\texorpdfstring{\texttt{matrix()}}{matrix()}}\label{matrix}}

A matrix can be created -- amongst others -- with a call to the \texttt{matrix()}
function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "matrix"
\end{verbatim}

Given a numeric vector of length 6, we've asked R to convert to
a numeric matrix with 2 rows (the \texttt{nrow} argument).
The number of columns has been deduced automatically
(otherwise, we would additionally have to pass \texttt{ncol=3} to the function).

Using mathematical notation,
above we have defined \(\mathbf{A}\in\mathbb{R}^{2\times 3}\):

\[
\mathbf{A}=
\left[
\begin{array}{ccc}
a_{1,1} & a_{1,2} & a_{1,3} \\
a_{2,1} & a_{2,2} & a_{2,3} \\
\end{array}
\right]
=
\left[
\begin{array}{ccc}
1 & 2 & 3 \\
4 & 5 & 6 \\
\end{array}
\right]
\]

We can fetch the size of the matrix by calling:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(A) }\CommentTok{# number of rows, number of columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 3
\end{verbatim}

We can also ``promote'' a ``flat'' vector to a column vector, i.e.,
a matrix with one column by calling:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.matrix}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## [1,]    1
## [2,]    2
## [3,]    3
\end{verbatim}

\hypertarget{stacking-vectors}{%
\subsection{Stacking Vectors}\label{stacking-vectors}}

Other ways to create a matrix involve stacking a couple of vectors
of equal lengths along each other:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rbind}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{4}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{7}\OperatorTok{:}\DecValTok{9}\NormalTok{) }\CommentTok{# row bind}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## [3,]    7    8    9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cbind}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{4}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{7}\OperatorTok{:}\DecValTok{9}\NormalTok{) }\CommentTok{# column bind}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    4    7
## [2,]    2    5    8
## [3,]    3    6    9
\end{verbatim}

These functions also allow for adding new rows/columns to existing matrices:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rbind}\NormalTok{(A, }\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{-2}\NormalTok{, }\DecValTok{-3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## [3,]   -1   -2   -3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cbind}\NormalTok{(A, }\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{-2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3   -1
## [2,]    4    5    6   -2
\end{verbatim}

\hypertarget{beyond-numeric-matrices}{%
\subsection{Beyond Numeric Matrices}\label{beyond-numeric-matrices}}

Note that logical matrices are possible as well.
For instance, knowing that comparison such as \texttt{\textless{}} and \texttt{==}
are performed elementwise also in the case of matrices, we can obtain:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OperatorTok{>=}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]  [,2] [,3]
## [1,] FALSE FALSE TRUE
## [2,]  TRUE  TRUE TRUE
\end{verbatim}

Moreover, although much more rarely used, we can define character
matrices:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{matrix}\NormalTok{(LETTERS[}\DecValTok{1}\OperatorTok{:}\DecValTok{12}\NormalTok{], }\DataTypeTok{ncol=}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,] "A"  "C"  "E"  "G"  "I"  "K" 
## [2,] "B"  "D"  "F"  "H"  "J"  "L"
\end{verbatim}

\hypertarget{naming-rows-and-columns}{%
\subsection{Naming Rows and Columns}\label{naming-rows-and-columns}}

Just like vectors could be equipped with \texttt{names} attribute:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\DataTypeTok{a=}\DecValTok{1}\NormalTok{, }\DataTypeTok{b=}\DecValTok{2}\NormalTok{, }\DataTypeTok{c=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## a b c 
## 1 2 3
\end{verbatim}

matrices can be assigned row and column labels
in the form of a list of two character vectors:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dimnames}\NormalTok{(A) <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
    \KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{),     }\CommentTok{# row labels}
    \KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{, }\StringTok{"z"}\NormalTok{) }\CommentTok{# column labels}
\NormalTok{)}
\NormalTok{A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x y z
## a 1 2 3
## b 4 5 6
\end{verbatim}

\hypertarget{other-methods}{%
\subsection{Other Methods}\label{other-methods}}

The \texttt{read.table()} (and its special case, \texttt{read.csv()}),
can be used to read a matrix from a text file.
We will cover it in the next chapter, because technically
it returns a data frame object (which we can convert to a matrix with a call
to \texttt{as.matrix()}).

\texttt{outer()} applies a given (vectorised) function
on each pair of elements from two vectors, forming a two-dimensional ``grid''.
More precisely \texttt{outer(x,\ y,\ f,\ ...)} returns a matrix \(\mathbf{Z}\) with
\texttt{length(x)}
rows and \texttt{length(y)} columns such that \(z_{i,j}=f(x_i, y_j, ...)\),
where \texttt{...} are optional further arguments to \texttt{f}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{outer}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{), }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, }\StringTok{"*"}\NormalTok{) }\CommentTok{# apply the multiplication operator}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    2    3    4    5
## [2,]   10   20   30   40   50
## [3,]  100  200  300  400  500
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{outer}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{), }\DecValTok{1}\OperatorTok{:}\DecValTok{8}\NormalTok{, paste, }\DataTypeTok{sep=}\StringTok{"-"}\NormalTok{) }\CommentTok{# concatenate strings}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8] 
## [1,] "A-1" "A-2" "A-3" "A-4" "A-5" "A-6" "A-7" "A-8"
## [2,] "B-1" "B-2" "B-3" "B-4" "B-5" "B-6" "B-7" "B-8"
\end{verbatim}

\texttt{simplify2array()} is an extension of the \texttt{unlist()} function.
Given a list of vectors, each of length one, it will return an ``unlisted'' vector.
However, if a list of equisized vectors of greater lengths is given,
these will be converted to a matrix.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{simplify2array}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{21}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1 11 21
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{simplify2array}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{11}\OperatorTok{:}\DecValTok{13}\NormalTok{, }\DecValTok{21}\OperatorTok{:}\DecValTok{23}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1   11   21
## [2,]    2   12   22
## [3,]    3   13   23
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{simplify2array}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{11}\OperatorTok{:}\DecValTok{12}\NormalTok{, }\DecValTok{21}\OperatorTok{:}\DecValTok{23}\NormalTok{)) }\CommentTok{# no can do}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 1
## 
## [[2]]
## [1] 11 12
## 
## [[3]]
## [1] 21 22 23
\end{verbatim}

\texttt{sapply(...)} is a nice application of the above, meaning \texttt{simplify2array(lapply(...))}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(}\KeywordTok{split}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Sepal.Length, iris}\OperatorTok{$}\NormalTok{Species), mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     setosa versicolor  virginica 
##      5.006      5.936      6.588
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(}\KeywordTok{split}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Sepal.Length, iris}\OperatorTok{$}\NormalTok{Species), summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         setosa versicolor virginica
## Min.     4.300      4.900     4.900
## 1st Qu.  4.800      5.600     6.225
## Median   5.000      5.900     6.500
## Mean     5.006      5.936     6.588
## 3rd Qu.  5.200      6.300     6.900
## Max.     5.800      7.000     7.900
\end{verbatim}

Of course, custom functions can also be applied:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min_mean_max <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
    \CommentTok{# returns a named vector with three elements}
    \CommentTok{# (note that the last expression in a function's body}
    \CommentTok{#  is its return value)}
    \KeywordTok{c}\NormalTok{(}\DataTypeTok{min=}\KeywordTok{min}\NormalTok{(x), }\DataTypeTok{mean=}\KeywordTok{mean}\NormalTok{(x), }\DataTypeTok{max=}\KeywordTok{max}\NormalTok{(x))}
\NormalTok{\}}
\KeywordTok{sapply}\NormalTok{(}\KeywordTok{split}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Sepal.Length, iris}\OperatorTok{$}\NormalTok{Species), min_mean_max)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      setosa versicolor virginica
## min   4.300      4.900     4.900
## mean  5.006      5.936     6.588
## max   5.800      7.000     7.900
\end{verbatim}

Lastly, \texttt{table(x,\ y)} creates a contingency matrix that
counts the number of unique pairs of corresponding elements
from two vectors of equal lengths.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"titanic"}\NormalTok{) }\CommentTok{# data on the passengers of the RMS Titanic}
\KeywordTok{table}\NormalTok{(titanic_train}\OperatorTok{$}\NormalTok{Survived)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   0   1 
## 549 342
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(titanic_train}\OperatorTok{$}\NormalTok{Sex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## female   male 
##    314    577
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(titanic_train}\OperatorTok{$}\NormalTok{Survived, titanic_train}\OperatorTok{$}\NormalTok{Sex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##     female male
##   0     81  468
##   1    233  109
\end{verbatim}

\hypertarget{internal-representation-1}{%
\subsection{Internal Representation (*)}\label{internal-representation-1}}

Note that by setting \texttt{byrow=TRUE} in a call to the \texttt{matrix()} function above,
we are reading the elements
of the input vector in the row-wise (row-major) fashion.
The default is the column-major order, which might be a little unintuitive
for some of us.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }\DataTypeTok{ncol=}\DecValTok{3}\NormalTok{, }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{B <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }\DataTypeTok{ncol=}\DecValTok{3}\NormalTok{) }\CommentTok{# byrow=FALSE}
\end{Highlighting}
\end{Shaded}

It turns out that is exactly the order in which the matrix is stored internally.
Under the hood, it is an ordinary numeric vector:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mode}\NormalTok{(B)    }\CommentTok{# == mode(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(B)  }\CommentTok{# == length(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 4 2 5 3 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.numeric}\NormalTok{(B)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4 5 6
\end{verbatim}

Also note that we can create a different \emph{view} on the same underlying
data vector:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(A) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{) }\CommentTok{# 3 rows, 2 columns}
\NormalTok{A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    1    5
## [2,]    4    3
## [3,]    2    6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(B) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{) }\CommentTok{# 3 rows, 2 columns}
\NormalTok{B}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    1    4
## [2,]    2    5
## [3,]    3    6
\end{verbatim}

\hypertarget{common-operations-1}{%
\section{Common Operations}\label{common-operations-1}}

\hypertarget{matrix-transpose}{%
\subsection{Matrix Transpose}\label{matrix-transpose}}

The matrix \emph{transpose} is denoted with \(\mathbf{A}^T\):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t}\NormalTok{(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    4    2
## [2,]    5    3    6
\end{verbatim}

Hence, \(\mathbf{B}=\mathbf{A}^T\) is a matrix such that \(b_{i,j}=a_{j,i}\).

In other words, in the transposed matrix, rows become columns and columns become rows.
For example:

\[
\mathbf{A}=
\left[
\begin{array}{ccc}
a_{1,1} & a_{1,2} & a_{1,3} \\
a_{2,1} & a_{2,2} & a_{2,3} \\
\end{array}
\right]
\qquad
\mathbf{A}^T=
\left[
\begin{array}{cc}
a_{1,1} & a_{2,1} \\
a_{1,2} & a_{2,2} \\
a_{1,3} & a_{2,3} \\
\end{array}
\right]
\]

\hypertarget{matrix-scalar-operations}{%
\subsection{Matrix-Scalar Operations}\label{matrix-scalar-operations}}

Operations such as \(s\mathbf{A}\) (multiplication of a matrix
by a scalar), \(-\mathbf{A}\), \(s+\mathbf{A}\) etc.
are applied on each element of the input matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{)}\OperatorTok{*}\NormalTok{A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]   -1   -2   -3
## [2,]   -4   -5   -6
\end{verbatim}

In R, the same rule holds when we compute other
operations (despite the fact that, mathematically,
e.g., \(\mathbf{A}^2\) or \(\mathbf{A}\ge 0\) might have a different meaning):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A}\OperatorTok{^}\DecValTok{2} \CommentTok{# this is not A-matrix-multiply-A, see below}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    4    9
## [2,]   16   25   36
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A}\OperatorTok{>=}\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]  [,2] [,3]
## [1,] FALSE FALSE TRUE
## [2,]  TRUE  TRUE TRUE
\end{verbatim}

\hypertarget{matrix-matrix-operations}{%
\subsection{Matrix-Matrix Operations}\label{matrix-matrix-operations}}

If \(\mathbf{A},\mathbf{B}\in\mathbb{R}^{n\times p}\)
are two matrices of identical sizes, then
\(\mathbf{A}+\mathbf{B}\) and
\(\mathbf{A}-\mathbf{B}\) are understood elementwise,
i.e., they result in \(\mathbf{C}\in\mathbb{R}^{n\times p}\)
such that \(c_{i,j}=a_{i,j}\pm b_{i,j}\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A}\OperatorTok{-}\NormalTok{A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    0    0    0
## [2,]    0    0    0
\end{verbatim}

In R (but not when we use mathematical notation),
all other arithmetic, logical and comparison operators are also
applied in an elementwise fashion.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A}\OperatorTok{*}\NormalTok{A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    4    9
## [2,]   16   25   36
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A}\OperatorTok{>}\DecValTok{2}\NormalTok{) }\OperatorTok{&}\StringTok{ }\NormalTok{(A}\OperatorTok{<=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]  [,2]  [,3]
## [1,] FALSE FALSE  TRUE
## [2,]  TRUE  TRUE FALSE
\end{verbatim}

\hypertarget{matrix-multiplication}{%
\subsection{Matrix Multiplication (*)}\label{matrix-multiplication}}

Mathematically, \(\mathbf{A}\mathbf{B}\)
denotes the \textbf{matrix multiplication}. It is a very different operation
to the elementwise multiplication.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    1    2
## [2,]    3    4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(I <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A }\OperatorTok{%*%}\StringTok{ }\NormalTok{I }\CommentTok{# matrix multiplication}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    1    2
## [2,]    3    4
\end{verbatim}

This is not the same as the elementwise \texttt{A*I}.

Matrix multiplication can only be performed on two matrices of
\emph{compatible sizes} -- the number of columns in the left matrix must match
the number of rows in the right operand.

Given \(\mathbf{A}\in\mathbb{R}^{n\times p}\)
and \(\mathbf{B}\in\mathbb{R}^{p\times m}\), their multiply is a matrix
\(\mathbf{C}=\mathbf{A}\mathbf{B}\in\mathbb{R}^{n\times m}\)
such that \(c_{i,j}\) is the dot product of the \(i\)-th row in \(\mathbf{A}\)
and the \(j\)-th column in \(\mathbf{B}\):
\[
c_{i,j} = \mathbf{a}_{i,\cdot} \cdot \mathbf{b}_{\cdot,j}
= \sum_{k=1}^p a_{i,k} b_{k, j}
\]
for \(i=1,\dots,n\) and \(j=1,\dots,m\).

\begin{exercise}

Multiply
a few simple matrices of sizes \(2\times 2\), \(2\times 3\), \(3\times 2\) etc.
using pen and paper and checking the results in R.

\end{exercise}

Also remember that, mathematically,
\emph{squaring} a matrix is done in terms of matrix multiplication,
i.e., \(\mathbf{A}^2 = \mathbf{A}\mathbf{A}\).
It can only be performed on \emph{square} matrices, i.e., ones with the same number
of rows and columns.
This is again different than R's elementwise \texttt{A\^{}2}.

Note that \(\mathbf{A}^T \mathbf{A}\)
gives the matrix that consists of the dot products of all the pairs
of columns in \(\mathbf{A}\).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{crossprod}\NormalTok{(A) }\CommentTok{# same as t(A) %*% A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]   10   14
## [2,]   14   20
\end{verbatim}

In one of the chapters on Regression, we note
that the Pearson linear correlation coefficient
can be beautifully expressed this way.

\hypertarget{aggregation-of-rows-and-columns}{%
\subsection{Aggregation of Rows and Columns}\label{aggregation-of-rows-and-columns}}

The \texttt{apply()} function may be used to transform or summarise
individual rows or columns in a matrix. More precisely:

\begin{itemize}
\tightlist
\item
  \texttt{apply(A,\ 1,\ f)} applies a given function \(f\) on each \emph{row} of \(\mathbf{A}\).
\item
  \texttt{apply(A,\ 2,\ f)} applies a given function \(f\) on each \emph{column} of \(\mathbf{A}\).
\end{itemize}

Usually, either \(f\) returns a single value (when we wish to aggregate
all the elements in a row/column) or returns the same number of values
(when we wish to transform a row/column). The latter case is covered in the next
subsection.

Let's compute the mean of each row and column in A:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{18}\NormalTok{, }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{nrow=}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    1    2    3    4    5    6
## [2,]    7    8    9   10   11   12
## [3,]   13   14   15   16   17   18
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(A, }\DecValTok{1}\NormalTok{, mean) }\CommentTok{# synonym: rowMeans(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  3.5  9.5 15.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(A, }\DecValTok{2}\NormalTok{, mean) }\CommentTok{# synonym: colMeans(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  7  8  9 10 11 12
\end{verbatim}

We can also fetch the minimal and maximal value by means of the \texttt{range()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(A, }\DecValTok{1}\NormalTok{, range)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    7   13
## [2,]    6   12   18
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(A, }\DecValTok{2}\NormalTok{, range)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    1    2    3    4    5    6
## [2,]   13   14   15   16   17   18
\end{verbatim}

Of course, a custom function can be provided as well.
Here we compute the minimum, average and maximum of each row:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(A, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(row) }\KeywordTok{c}\NormalTok{(}\KeywordTok{min}\NormalTok{(row), }\KeywordTok{mean}\NormalTok{(row), }\KeywordTok{max}\NormalTok{(row)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]  1.0  7.0 13.0
## [2,]  3.5  9.5 15.5
## [3,]  6.0 12.0 18.0
\end{verbatim}

\hypertarget{vectorised-special-functions}{%
\subsection{Vectorised Special Functions}\label{vectorised-special-functions}}

The special functions mentioned in the previous chapter, e.g.,
\texttt{sqrt()}, \texttt{abs()}, \texttt{round()}, \texttt{log()}, \texttt{exp()}, \texttt{cos()}, \texttt{sin()},
are also performed in an elementwise manner when applied on a matrix object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{A, }\DecValTok{2}\NormalTok{) }\CommentTok{# rounds every element in 1/A}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,] 1.00 0.50 0.33 0.25 0.20 0.17
## [2,] 0.14 0.12 0.11 0.10 0.09 0.08
## [3,] 0.08 0.07 0.07 0.06 0.06 0.06
\end{verbatim}

An example plot of the absolute values of sine and cosine functions
depicted using the \texttt{matplot()} function (see Figure \ref{fig:matplot_sincos}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\OperatorTok{*}\NormalTok{pi, }\DecValTok{6}\OperatorTok{*}\NormalTok{pi, }\DataTypeTok{by=}\NormalTok{pi}\OperatorTok{/}\DecValTok{100}\NormalTok{)}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{sin}\NormalTok{(x), }\KeywordTok{cos}\NormalTok{(x)) }\CommentTok{# a matrix with two columns}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{abs}\NormalTok{(Y) }\CommentTok{# take the absolute value of every element in Y}
\KeywordTok{matplot}\NormalTok{(x, Y, }\DataTypeTok{type=}\StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:matplot_sincos}{%
\centering
\includegraphics{93-R-matrices-figures/matplot_sincos-1.pdf}
\caption{Example plot with \texttt{matplot()}}\label{fig:matplot_sincos}
}
\end{figure}

\hypertarget{matrix-vector-operations}{%
\subsection{Matrix-Vector Operations}\label{matrix-vector-operations}}

Mathematically, there is no generally agreed upon
convention defining arithmetic operations between matrices and vectors.

\begin{quote}
(*) The only exception is the matrix -- vector multiplication in the case
where an argument is a column or a row vector, i.e., in fact, a matrix.
Hence, given \(\mathbf{A}\in\mathbb{R}^{n\times p}\) we may write
\(\mathbf{A}\mathbf{x}\)
only if \(\mathbf{x}\in\mathbb{R}^{p\times 1}\) is a column vector.
Similarly, \(\mathbf{y}\mathbf{A}\) makes only sense
whenever \(\mathbf{y}\in\mathbb{R}^{1\times n}\) is a row vector.
\end{quote}

\begin{description}
\item[Remark.]
Please take notice of the fact that we consistently
discriminate between different bold math fonts and letter cases:
\(\mathbf{X}\) is a matrix, \(\mathbf{x}\) is a row or column vector
(still a matrix, but a sequence-like one)
and \(\boldsymbol{x}\) is an ordinary vector (one-dimensional sequence).
\end{description}

However, in R, we might sometimes wish to vectorise
an arithmetic operation between a matrix and a vector in a row- or column-wise
fashion.
For example, if \(\mathbf{A}\in\mathbb{R}^{n\times p}\) is a matrix
and \(\mathbf{m}\in\mathbb{R}^{1\times p}\) is a row vector,
we might want to subtract \(m_i\) from each element in the \(i\)-th column.
Here, the \texttt{apply()} function comes in handy again.

Example: to create a \emph{centred} version of a given matrix,
we need to subtract from each element the arithmetic mean of its column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    2    5
## [2,]    2    4    8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(m <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(A, }\DecValTok{2}\NormalTok{, mean)) }\CommentTok{# same as colMeans(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.5 3.0 6.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t}\NormalTok{(}\KeywordTok{apply}\NormalTok{(A, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(r) r}\OperatorTok{-}\NormalTok{m)) }\CommentTok{# note the transpose here}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,] -0.5   -1 -1.5
## [2,]  0.5    1  1.5
\end{verbatim}

The above is equivalent to:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(A, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(c) c}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(c))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,] -0.5   -1 -1.5
## [2,]  0.5    1  1.5
\end{verbatim}

\hypertarget{matrix-subsetting}{%
\section{Matrix Subsetting}\label{matrix-subsetting}}

Example matrices:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{12}\NormalTok{, }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{nrow=}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3    4
## [2,]    5    6    7    8
## [3,]    9   10   11   12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B <-}\StringTok{ }\NormalTok{A}
\KeywordTok{dimnames}\NormalTok{(B) <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
    \KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{),     }\CommentTok{# row labels}
    \KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{, }\StringTok{"z"}\NormalTok{, }\StringTok{"w"}\NormalTok{) }\CommentTok{# column labels}
\NormalTok{)}
\NormalTok{B}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x  y  z  w
## a 1  2  3  4
## b 5  6  7  8
## c 9 10 11 12
\end{verbatim}

\hypertarget{selecting-individual-elements}{%
\subsection{Selecting Individual Elements}\label{selecting-individual-elements}}

Matrices are two-dimensional structures: items are aligned in rows and columns.
Hence, to extract an element from a matrix, we will need two indices.
Mathematically, given a matrix \(\mathbf{A}\), \(a_{i,j}\) stands for the element
in the \(i\)-th row and the \(j\)-th column. The same in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{] }\CommentTok{# 1st row, 2nd columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B[}\StringTok{"a"}\NormalTok{, }\StringTok{"y"}\NormalTok{] }\CommentTok{# using dimnames == B[1,2]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\hypertarget{selecting-rows-and-columns}{%
\subsection{Selecting Rows and Columns}\label{selecting-rows-and-columns}}

We will sometimes use the following notation to emphasise that
a matrix \(\mathbf{A}\) consists of \(n\) rows
or \(p\) columns:

\[
\mathbf{A}=\left[
\begin{array}{c}
\mathbf{a}_{1,\cdot} \\
\mathbf{a}_{2,\cdot} \\
\vdots\\
\mathbf{a}_{n,\cdot} \\
\end{array}
\right]
=
\left[
\begin{array}{cccc}
\mathbf{a}_{\cdot,1} &
\mathbf{a}_{\cdot,2} &
\cdots &
\mathbf{a}_{\cdot,p} \\
\end{array}
\right].
\]

Here, \(\mathbf{a}_{i,\cdot}\) is a \emph{row vector} of length \(p\),
i.e., a \((1\times p)\)-matrix:

\[
\mathbf{a}_{i,\cdot} = \left[
\begin{array}{cccc}
a_{i,1} &
a_{i,2} &
\cdots &
a_{i,p} \\
\end{array}
\right].
\]

Moreover, \(\mathbf{a}_{\cdot,j}\) is a \emph{column vector} of length \(n\),
i.e., an \((n\times 1)\)-matrix:

\[
\mathbf{a}_{\cdot,j} = \left[
\begin{array}{cccc}
a_{1,j} &
a_{2,j} &
\cdots &
a_{n,j} \\
\end{array}
\right]^T=\left[
\begin{array}{c}
{a}_{1,j} \\
{a}_{2,j} \\
\vdots\\
{a}_{n,j} \\
\end{array}
\right],
\]

We can extract individual rows and columns from a matrix by using
the following notation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A[}\DecValTok{1}\NormalTok{,] }\CommentTok{# 1st row}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 2 3 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A[,}\DecValTok{2}\NormalTok{] }\CommentTok{# 2nd column}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  2  6 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B[}\StringTok{"a"}\NormalTok{,] }\CommentTok{# of course, B[1,] is still legal}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## x y z w 
## 1 2 3 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B[,}\StringTok{"y"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  a  b  c 
##  2  6 10
\end{verbatim}

Note that by extracting a single row/column, we get an atomic (one-dimensional)
vector. However, we can preserve the dimensionality of the output object
by passing \texttt{drop=FALSE}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A[  }\DecValTok{1}\NormalTok{,    , drop=}\OtherTok{FALSE}\NormalTok{] }\CommentTok{# 1st row}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3    4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A[   ,   }\DecValTok{2}\NormalTok{, drop=}\OtherTok{FALSE}\NormalTok{] }\CommentTok{# 2nd column}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## [1,]    2
## [2,]    6
## [3,]   10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B[}\StringTok{"a"}\NormalTok{,    , drop=}\OtherTok{FALSE}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x y z w
## a 1 2 3 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B[   , }\StringTok{"y"}\NormalTok{, drop=}\OtherTok{FALSE}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    y
## a  2
## b  6
## c 10
\end{verbatim}

Now this is what we call proper row and column vectors!

\hypertarget{selecting-submatrices}{%
\subsection{Selecting Submatrices}\label{selecting-submatrices}}

To create a sub-block of a given matrix we pass two indexers,
possibly of length greater than one:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)] }\CommentTok{# rows 1,2 columns 1,2,4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    1    2    4
## [2,]    5    6    8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B[}\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x y w
## a 1 2 4
## b 5 6 8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{), }\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  3 11
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{), }\DecValTok{3}\NormalTok{, drop=}\OtherTok{FALSE}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## [1,]    3
## [2,]   11
\end{verbatim}

\hypertarget{selecting-based-on-logical-vectors-and-matrices}{%
\subsection{Selecting Based on Logical Vectors and Matrices}\label{selecting-based-on-logical-vectors-and-matrices}}

We can also subset a matrix with a logical matrix of the same size.
This always yields a (flat) vector in return.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A[A}\OperatorTok{>}\DecValTok{8}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  9 10 11 12
\end{verbatim}

Logical vectors can also be used as indexers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A[}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{),] }\CommentTok{# select 1st and 3rd row}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3    4
## [2,]    9   10   11   12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A[,}\KeywordTok{colMeans}\NormalTok{(A)}\OperatorTok{>}\DecValTok{6}\NormalTok{] }\CommentTok{# columns with means > 6}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]    3    4
## [2,]    7    8
## [3,]   11   12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B[B[,}\StringTok{"x"}\NormalTok{]}\OperatorTok{>}\DecValTok{1} \OperatorTok{&}\StringTok{ }\NormalTok{B[,}\StringTok{"x"}\NormalTok{]}\OperatorTok{<=}\DecValTok{9}\NormalTok{,] }\CommentTok{# All rows where x is in (1, 9]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x  y  z  w
## b 5  6  7  8
## c 9 10 11 12
\end{verbatim}

\hypertarget{selecting-based-on-two-column-matrices}{%
\subsection{Selecting Based on Two-Column Matrices}\label{selecting-based-on-two-column-matrices}}

Lastly, note that we can also index a matrix \texttt{A}
with a 2-column matrix \texttt{I}, i.e., \texttt{A{[}I{]}}.
This allows for an easy access to
\texttt{A{[}I{[}1,1{]},\ I{[}1,2{]}{]}}, \texttt{A{[}I{[}2,1{]},\ I{[}2,2{]}{]}}, \texttt{A{[}I{[}3,1{]},\ I{[}3,2{]}{]}}, \ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{I <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
           \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{)}
\NormalTok{A[I]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  2 11  6  1  8
\end{verbatim}

This is exactly
\texttt{A{[}1,\ 2{]},\ A{[}3,\ 3{]},\ A{[}2,\ 2{]},\ A{[}1,\ 1{]},\ A{[}2,\ 4{]}}.

\hypertarget{further-reading-10}{%
\section{Further Reading}\label{further-reading-10}}

Recommended further reading: (Venables et al. \protect\hyperlink{ref-Rintro}{2020})

Other: (Deisenroth et al. \protect\hyperlink{ref-mml}{2020}), (Peng \protect\hyperlink{ref-rprogdatascience}{2019}), (Wickham \& Grolemund \protect\hyperlink{ref-r4ds}{2017})

\hypertarget{data-frame-wrangling-in-r}{%
\chapter{Data Frame Wrangling in R}\label{data-frame-wrangling-in-r}}

R \texttt{data.frame}s are similar to matrices in the sense that
we use them to store tabular data.
However, in data frames each column can be of different type:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(rpart}\OperatorTok{::}\NormalTok{car90, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               Country Disp Disp2 Eng.Rev Front.Hd Frt.Leg.Room Frt.Shld
## Acura Integra   Japan  112   1.8    2935      3.5         41.5     53.0
## Acura Legend    Japan  163   2.7    2505      2.0         41.5     55.5
##               Gear.Ratio Gear2  HP HP.revs Height Length Luggage
## Acura Integra       3.26  3.21 130    6000   47.5    177      16
## Acura Legend        2.95  3.02 160    5900   50.0    191      14
##               Mileage Model2 Price Rear.Hd Rear.Seating RearShld
## Acura Integra      NA        11950     1.5         26.5     52.0
## Acura Legend       20        24760     2.0         28.5     55.5
##               Reliability Rim Sratio.m Sratio.p Steering Tank  Tires
## Acura Integra Much better R14       NA     0.86    power 13.2 195/60
## Acura Legend  Much better R15       NA     0.96    power 18.0 205/60
##               Trans1 Trans2 Turning   Type Weight Wheel.base Width
## Acura Integra  man.5 auto.4      37  Small   2700        102    67
## Acura Legend   man.5 auto.4      42 Medium   3265        109    69
\end{verbatim}

\hypertarget{creating-data-frames}{%
\section{Creating Data Frames}\label{creating-data-frames}}

Most frequently, we will be creating data frames
based on a series of numeric, logical, characters vectors of identical lengths.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
    \DataTypeTok{u=}\KeywordTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{),}
    \DataTypeTok{v=}\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{), }\DecValTok{5}\NormalTok{, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{w=}\NormalTok{LETTERS[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{]}
\NormalTok{)}
\KeywordTok{print}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          u     v w
## 1 0.181517  TRUE A
## 2 0.919723 FALSE B
## 3 0.311723 FALSE C
## 4 0.064152  TRUE D
## 5 0.396422 FALSE E
\end{verbatim}

Note that when we create objects of type data frame,
strings are automatically converted to factors.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(x}\OperatorTok{$}\NormalTok{w)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}

Throughout the history of computing with R, this has caused way too many bugs
(recall, for instance, what's the result of calling \texttt{as.numeric()} on a factor).
In order to change this behaviour, either pass \texttt{stringsAsFactors=FALSE}
argument to \texttt{data.frame()} or switch this feature off globally (recommended):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{options}\NormalTok{(}\DataTypeTok{stringsAsFactors=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Some objects, such as matrices, can easily be coerced to data frames:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{12}\NormalTok{, }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{nrow=}\DecValTok{3}\NormalTok{,}
    \DataTypeTok{dimnames=}\KeywordTok{list}\NormalTok{(}
        \OtherTok{NULL}\NormalTok{,     }\CommentTok{# row labels}
        \KeywordTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{, }\StringTok{"z"}\NormalTok{, }\StringTok{"w"}\NormalTok{) }\CommentTok{# column labels}
\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      x  y  z  w
## [1,] 1  2  3  4
## [2,] 5  6  7  8
## [3,] 9 10 11 12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.data.frame}\NormalTok{(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x  y  z  w
## 1 1  2  3  4
## 2 5  6  7  8
## 3 9 10 11 12
\end{verbatim}

Named lists are amongst other candidates for a meaningful conversion:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(l <-}\StringTok{ }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{split}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Sepal.Length, iris}\OperatorTok{$}\NormalTok{Species),}
    \ControlFlowTok{function}\NormalTok{(x) \{}
        \KeywordTok{c}\NormalTok{(}\DataTypeTok{min=}\KeywordTok{min}\NormalTok{(x), }\DataTypeTok{median=}\KeywordTok{median}\NormalTok{(x), }\DataTypeTok{mean=}\KeywordTok{mean}\NormalTok{(x), }\DataTypeTok{max=}\KeywordTok{max}\NormalTok{(x))}
\NormalTok{    \}))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $setosa
##    min median   mean    max 
##  4.300  5.000  5.006  5.800 
## 
## $versicolor
##    min median   mean    max 
##  4.900  5.900  5.936  7.000 
## 
## $virginica
##    min median   mean    max 
##  4.900  6.500  6.588  7.900
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.data.frame}\NormalTok{(l)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        setosa versicolor virginica
## min     4.300      4.900     4.900
## median  5.000      5.900     6.500
## mean    5.006      5.936     6.588
## max     5.800      7.000     7.900
\end{verbatim}

\hypertarget{importing-data-frames}{%
\section{Importing Data Frames}\label{importing-data-frames}}

Many interesting data frames come from external sources, such as csv files,
web APIs, SQL databases and so on.

In particular, \texttt{read.csv()} (see \texttt{?read.table} for a long list
of tunable parameters) imports data from plain text files
organised in a tabular manner (such as \emph{c}omma-\emph{s}eparated lists of \emph{v}alues):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f <-}\StringTok{ }\KeywordTok{tempfile}\NormalTok{() }\CommentTok{# temporary file name}
\KeywordTok{write.csv}\NormalTok{(x, f, }\DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{) }\CommentTok{# save data frame to file}
\KeywordTok{cat}\NormalTok{(}\KeywordTok{readLines}\NormalTok{(f), }\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{# print file contents}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## "u","v","w"
## 0.181517061544582,TRUE,"A"
## 0.919722604798153,FALSE,"B"
## 0.31172346835956,FALSE,"C"
## 0.0641516039613634,TRUE,"D"
## 0.396421572659165,FALSE,"E"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read.csv}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          u     v w
## 1 0.181517  TRUE A
## 2 0.919723 FALSE B
## 3 0.311723 FALSE C
## 4 0.064152  TRUE D
## 5 0.396422 FALSE E
\end{verbatim}

Note that CSV is by far the most portable format for exchanging matrix-like
objects between different programs (statistical or numeric computing
environments, spreadsheets etc.).

\hypertarget{data-frame-subsetting}{%
\section{Data Frame Subsetting}\label{data-frame-subsetting}}

\hypertarget{each-data-frame-is-a-list}{%
\subsection{Each Data Frame is a List}\label{each-data-frame-is-a-list}}

First of all, we should note that
each data frame is in fact represented as an ordinary named list:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.frame"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

Each column is stored as a separate list item.
Having said that, we shouldn't be surprised that we already know
how to perform quite a few operations on data frames:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(x) }\CommentTok{# number of columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(x)  }\CommentTok{# column labels}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "u" "v" "w"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OperatorTok{$}\NormalTok{u }\CommentTok{# accessing column `u` (synonym: x[["u"]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.181517 0.919723 0.311723 0.064152 0.396422
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[[}\DecValTok{2}\NormalTok{]] }\CommentTok{# 2nd column}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE FALSE  TRUE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)] }\CommentTok{# a sub-data.frame}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          u w
## 1 0.181517 A
## 2 0.919723 B
## 3 0.311723 C
## 4 0.064152 D
## 5 0.396422 E
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(x, class) }\CommentTok{# apply class() on each column}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         u         v         w 
## "numeric" "logical"  "factor"
\end{verbatim}

\hypertarget{each-data-frame-is-matrix-like}{%
\subsection{Each Data Frame is Matrix-like}\label{each-data-frame-is-matrix-like}}

Data frames can be considered as ``generalised'' matrices.
Therefore, operations such as subsetting will work in the same manner.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(x) }\CommentTok{# number of rows and columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{,] }\CommentTok{# first two rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         u     v w
## 1 0.18152  TRUE A
## 2 0.91972 FALSE B
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)] }\CommentTok{# 1st and 3rd column, synonym: x[c(1,3)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          u w
## 1 0.181517 A
## 2 0.919723 B
## 3 0.311723 C
## 4 0.064152 D
## 5 0.396422 E
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[,}\DecValTok{1}\NormalTok{] }\CommentTok{# synonym: x[[1]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.181517 0.919723 0.311723 0.064152 0.396422
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[,}\DecValTok{1}\NormalTok{,drop=}\OtherTok{FALSE}\NormalTok{] }\CommentTok{# synonym: x[1]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          u
## 1 0.181517
## 2 0.919723
## 3 0.311723
## 4 0.064152
## 5 0.396422
\end{verbatim}

Take a special note of selecting rows based on logical vectors.
For instance, let's extract all the rows from \texttt{x} where the values
in the column named \texttt{u} are between 0.3 and 0.6:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[x}\OperatorTok{$}\NormalTok{u}\OperatorTok{>=}\FloatTok{0.3} \OperatorTok{&}\StringTok{ }\NormalTok{x}\OperatorTok{$}\NormalTok{u}\OperatorTok{<=}\FloatTok{0.6}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         u     v w
## 3 0.31172 FALSE C
## 5 0.39642 FALSE E
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\OperatorTok{!}\NormalTok{(x[,}\StringTok{"u"}\NormalTok{]}\OperatorTok{<}\FloatTok{0.3} \OperatorTok{|}\StringTok{ }\NormalTok{x[,}\StringTok{"u"}\NormalTok{]}\OperatorTok{>}\FloatTok{0.6}\NormalTok{), ] }\CommentTok{# equivalent}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         u     v w
## 3 0.31172 FALSE C
## 5 0.39642 FALSE E
\end{verbatim}

Moreover, subsetting based on integer vectors can be used to
change the order of rows. Here is how we can sort the rows in \texttt{x}
with respect to the values in column \texttt{u}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(x_sorted <-}\StringTok{ }\NormalTok{x[}\KeywordTok{order}\NormalTok{(x}\OperatorTok{$}\NormalTok{u),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          u     v w
## 4 0.064152  TRUE D
## 1 0.181517  TRUE A
## 3 0.311723 FALSE C
## 5 0.396422 FALSE E
## 2 0.919723 FALSE B
\end{verbatim}

Let's stress that the programming style we emphasise on here is very
transparent. If we don't understand how a complex operation is being executed,
we can always decompose it into smaller chunks that can be studied separately.
For instance, as far as the last example is concerned, we can
take a look at the manual of \texttt{?order} and then inspect the result
of calling \texttt{order(x\$u)}.

On a side note, we can re-set the row names by referring to:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{row.names}\NormalTok{(x_sorted) <-}\StringTok{ }\OtherTok{NULL}
\NormalTok{x_sorted}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          u     v w
## 1 0.064152  TRUE D
## 2 0.181517  TRUE A
## 3 0.311723 FALSE C
## 4 0.396422 FALSE E
## 5 0.919723 FALSE B
\end{verbatim}

\hypertarget{common-operations-2}{%
\section{Common Operations}\label{common-operations-2}}

We already know how to filter rows based on logical conditions, e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{Petal.Width }\OperatorTok{>=}\StringTok{ }\FloatTok{1.2} \OperatorTok{&}\StringTok{ }\NormalTok{iris}\OperatorTok{$}\NormalTok{Petal.Width }\OperatorTok{<=}\StringTok{ }\FloatTok{1.3}\NormalTok{,}
    \KeywordTok{c}\NormalTok{(}\StringTok{"Petal.Width"}\NormalTok{, }\StringTok{"Species"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Petal.Width    Species
## 54          1.3 versicolor
## 56          1.3 versicolor
## 59          1.3 versicolor
## 65          1.3 versicolor
## 72          1.3 versicolor
## 74          1.2 versicolor
## 75          1.3 versicolor
## 83          1.2 versicolor
## 88          1.3 versicolor
## 89          1.3 versicolor
## 90          1.3 versicolor
## 91          1.2 versicolor
## 93          1.2 versicolor
## 95          1.3 versicolor
## 96          1.2 versicolor
## 97          1.3 versicolor
## 98          1.3 versicolor
## 100         1.3 versicolor
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{Sepal.Length }\OperatorTok{>}\StringTok{ }\FloatTok{6.5} \OperatorTok{&}\StringTok{ }\NormalTok{iris}\OperatorTok{$}\NormalTok{Species }\OperatorTok{==}\StringTok{ "versicolor"}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
## 51          7.0         3.2          4.7         1.4 versicolor
## 53          6.9         3.1          4.9         1.5 versicolor
## 59          6.6         2.9          4.6         1.3 versicolor
## 66          6.7         3.1          4.4         1.4 versicolor
## 76          6.6         3.0          4.4         1.4 versicolor
## 77          6.8         2.8          4.8         1.4 versicolor
## 78          6.7         3.0          5.0         1.7 versicolor
## 87          6.7         3.1          4.7         1.5 versicolor
\end{verbatim}

and aggregate information in individual columns:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(iris[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{], summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Sepal.Length Sepal.Width Petal.Length Petal.Width
## Min.          4.3000      2.0000        1.000      0.1000
## 1st Qu.       5.1000      2.8000        1.600      0.3000
## Median        5.8000      3.0000        4.350      1.3000
## Mean          5.8433      3.0573        3.758      1.1993
## 3rd Qu.       6.4000      3.3000        5.100      1.8000
## Max.          7.9000      4.4000        6.900      2.5000
\end{verbatim}

Quite frequently, we will be interested
in summarising data within subgroups generated by a list of factor-like variables.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(iris[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{], iris[}\DecValTok{5}\NormalTok{], mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Species Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1     setosa        5.006       3.428        1.462       0.246
## 2 versicolor        5.936       2.770        4.260       1.326
## 3  virginica        6.588       2.974        5.552       2.026
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ToothGrowth[}\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(ToothGrowth), }\DecValTok{5}\NormalTok{), ] }\CommentTok{# 5 random rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     len supp dose
## 12 16.5   VC  1.0
## 10  7.0   VC  0.5
## 17 13.6   VC  1.0
## 50 27.3   OJ  1.0
## 60 23.0   OJ  2.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(ToothGrowth[}\StringTok{"len"}\NormalTok{], ToothGrowth[}\KeywordTok{c}\NormalTok{(}\StringTok{"supp"}\NormalTok{, }\StringTok{"dose"}\NormalTok{)], median)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   supp dose   len
## 1   OJ  0.5 12.25
## 2   VC  0.5  7.15
## 3   OJ  1.0 23.45
## 4   VC  1.0 16.50
## 5   OJ  2.0 25.95
## 6   VC  2.0 25.95
\end{verbatim}

Taking into account that \texttt{split()} accepts a data frame input as well,
we can perform what follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(}
    \CommentTok{# split iris into 3 sub-data.frames:}
    \KeywordTok{split}\NormalTok{(iris, iris[}\DecValTok{5}\NormalTok{]),}
    \CommentTok{# on each sub-data.frame, apply the following function}
    \ControlFlowTok{function}\NormalTok{(df) \{}
        \CommentTok{# compute the mean of first four columns:}
        \KeywordTok{sapply}\NormalTok{(df[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{], mean)}
\NormalTok{    \})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              setosa versicolor virginica
## Sepal.Length  5.006      5.936     6.588
## Sepal.Width   3.428      2.770     2.974
## Petal.Length  1.462      4.260     5.552
## Petal.Width   0.246      1.326     2.026
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sapply}\NormalTok{(}\KeywordTok{split}\NormalTok{(iris, iris[}\DecValTok{5}\NormalTok{]), }\ControlFlowTok{function}\NormalTok{(df) \{}
    \KeywordTok{c}\NormalTok{(}\DataTypeTok{Sepal.Length=}\KeywordTok{summary}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Sepal.Length),}
      \DataTypeTok{Petal.Length=}\KeywordTok{summary}\NormalTok{(iris}\OperatorTok{$}\NormalTok{Petal.Length)}
\NormalTok{     )}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      setosa versicolor virginica
## Sepal.Length.Min.    4.3000     4.3000    4.3000
## Sepal.Length.1st Qu. 5.1000     5.1000    5.1000
## Sepal.Length.Median  5.8000     5.8000    5.8000
## Sepal.Length.Mean    5.8433     5.8433    5.8433
## Sepal.Length.3rd Qu. 6.4000     6.4000    6.4000
## Sepal.Length.Max.    7.9000     7.9000    7.9000
## Petal.Length.Min.    1.0000     1.0000    1.0000
## Petal.Length.1st Qu. 1.6000     1.6000    1.6000
## Petal.Length.Median  4.3500     4.3500    4.3500
## Petal.Length.Mean    3.7580     3.7580    3.7580
## Petal.Length.3rd Qu. 5.1000     5.1000    5.1000
## Petal.Length.Max.    6.9000     6.9000    6.9000
\end{verbatim}

The above syntax is not super-convenient,
but it only uses the building blocks that we have already mastered!
That should be very appealing to the minimalists.
Note that R packages such as data.table and dplyr offer more convenient
substitutes -- you can always learn them on your own
(which takes time, but it's worth the hassle).
They simplify the most common data wrangling tasks. Moreover,
they have been optimised for speed -- they can handle much larger data sets
efficiently.

\hypertarget{metaprogramming-and-formulas}{%
\section{Metaprogramming and Formulas (*)}\label{metaprogramming-and-formulas}}

R (together with a few other programming languages such as Lisp and Scheme,
that heavily inspired R's semantics) allows its programmers to apply
some \emph{metaprogramming} techniques, that is,
to write programs that manipulate unevaluated R expressions.

For instance, take a close look at the following plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\OperatorTok{*}\NormalTok{pi, }\DecValTok{2}\OperatorTok{*}\NormalTok{pi, }\DataTypeTok{length.out=}\DecValTok{101}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(z, }\KeywordTok{sin}\NormalTok{(z), }\DataTypeTok{type=}\StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:plotz_metaprogramming}{%
\centering
\includegraphics{94-R-data-frames-figures/plotz_metaprogramming-1.pdf}
\caption{Metaprogramming in action: Take a look at the Y axis label}\label{fig:plotz_metaprogramming}
}
\end{figure}

How did the \texttt{plot()} function know that we are plotting \texttt{sin} of \texttt{z}
(see Figure \ref{fig:plotz_metaprogramming})?
It turns out that, at any time, we not only have access to the value
of an object (such as the result of evaluating \texttt{sin(z)}, which is
a vector of 101 reals) but also to the expression that was passed as
a function's argument itself.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test_meta <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"x equals to "}\NormalTok{, x, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{# \textbackslash{}n == newline}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"x stemmed from "}\NormalTok{, }\KeywordTok{deparse}\NormalTok{(}\KeywordTok{substitute}\NormalTok{(x)), }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{\}}
\KeywordTok{test_meta}\NormalTok{(}\DecValTok{2}\OperatorTok{+}\DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## x equals to  9 
## x stemmed from  2 + 7
\end{verbatim}

This is very powerful and yet potentially very confusing to the users, because
we can write functions that don't compute the arguments provided
in a way we expect them to (i.e., following the R language specification).
Each function can constitute a new micro-verse, where with its own rules --
we should always refer to the documentation.

For instance, consider the \texttt{subset()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{subset}\NormalTok{(iris, Sepal.Length}\OperatorTok{>}\FloatTok{7.5}\NormalTok{, }\DataTypeTok{select=}\OperatorTok{-}\NormalTok{(Sepal.Width}\OperatorTok{:}\NormalTok{Petal.Width))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Sepal.Length   Species
## 106          7.6 virginica
## 118          7.7 virginica
## 119          7.7 virginica
## 123          7.7 virginica
## 132          7.9 virginica
## 136          7.7 virginica
\end{verbatim}

Neither \texttt{Sepal.Length\textgreater{}6} nor \texttt{-(Sepal.Width:Petal.Width)} make sense
as standalone R expressions! However, according to the \texttt{subset()} function's
own rules, the former expression is considered as a row selector
(here, \texttt{Sepal.Length} refers to a particular column \emph{within} the \texttt{iris} data frame).
The latter plays the role of a column filter (select everything but all the columns
between\ldots{}).

The data.table and dplyr packages (which are very popular)
rely on this language feature
all the time, so we shouldn't be surprised when we see them.

\bigskip

There is one more interesting language feature that is possible
thanks to metaprogramming.
\emph{Formulas} are special R objects that consist of two unevaluated
R expressions separated by a tilde (\texttt{\textasciitilde{}}).
For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{len }\OperatorTok{~}\StringTok{ }\NormalTok{supp}\OperatorTok{+}\NormalTok{dose}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## len ~ supp + dose
\end{verbatim}

A formula on its own has no meaning. However, many R functions
accept formulas as arguments and can interpret them in various different ways.

For example, the \texttt{lm()} function that fits a linear regression model,
uses formulas to specify the output and input variables:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(Sepal.Length}\OperatorTok{~}\NormalTok{Petal.Length}\OperatorTok{+}\NormalTok{Sepal.Width, }\DataTypeTok{data=}\NormalTok{iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Sepal.Length ~ Petal.Length + Sepal.Width, data = iris)
## 
## Coefficients:
##  (Intercept)  Petal.Length   Sepal.Width  
##        2.249         0.472         0.596
\end{verbatim}

On the other hand, \texttt{boxplot()} (see Figure \ref{fig:boxplot_metaprogramming})
allows for creating
separate box-and-whisker plots for each subgroup given by a combination
of factors.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boxplot}\NormalTok{(len}\OperatorTok{~}\NormalTok{supp}\OperatorTok{+}\NormalTok{dose, }\DataTypeTok{data=}\NormalTok{ToothGrowth,}
    \DataTypeTok{horizontal=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{col=}\StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\hypertarget{fig:boxplot_metaprogramming}{%
\centering
\includegraphics{94-R-data-frames-figures/boxplot_metaprogramming-1.pdf}
\caption{Example box plot created via the formula interface}\label{fig:boxplot_metaprogramming}
}
\end{figure}

The \texttt{aggregate()} function supports formulas too:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(Sepal.Length, Sepal.Width)}\OperatorTok{~}\NormalTok{Species, }\DataTypeTok{data=}\NormalTok{iris, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Species Sepal.Length Sepal.Width
## 1     setosa        5.006       3.428
## 2 versicolor        5.936       2.770
## 3  virginica        6.588       2.974
\end{verbatim}

We should therefore make sure that we know how every function interacts
with a formula -- information on that can be found in \texttt{?lm}, \texttt{?boxplot},
\texttt{?aggregate} and so forth.

\hypertarget{further-reading-11}{%
\section{Further Reading}\label{further-reading-11}}

Recommended further reading: (Venables et al. \protect\hyperlink{ref-Rintro}{2020})

Other: (Peng \protect\hyperlink{ref-rprogdatascience}{2019}), (Wickham \& Grolemund \protect\hyperlink{ref-r4ds}{2017})

R packages dplyr and data.table implement the most common
data frame wrangling procedures. You may find them very useful.
Moreover, they are very fast even for large data sets.
Additionally, the magrittr package provides a pipe operator, \texttt{\%\textgreater{}\%},
that simplifies the writing of complex, nested function calls.
Do note that not everyone is a big fan of these, however.

\hypertarget{references}{%
\chapter*{References}\label{references}}


\hypertarget{refs}{}
\leavevmode\hypertarget{ref-cmeans}{}%
Bezdek JC, Ehrlich R, Full W (1984) FCM: The fuzzy c-means clustering algorithm. \emph{Computer and Geosciences} 10, 191--203.

\leavevmode\hypertarget{ref-bishop}{}%
Bishop C (2006) \emph{Pattern recognition and machine learning}. Springer-Verlag \url{https://www.microsoft.com/en-us/research/people/cmbishop/}.

\leavevmode\hypertarget{ref-boyd_vandenberghe}{}%
Boyd S, Vandenberghe L (2004) \emph{Convex optimization}. Cambridge University Press \url{https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf}.

\leavevmode\hypertarget{ref-cart}{}%
Breiman L, Friedman J, Stone CJ, Olshen RA (1984) \emph{Classification and regression trees}. Chapman; Hall/CRC.

\leavevmode\hypertarget{ref-hdbscan}{}%
Campello RJGB, Moulavi D, Zimek A, Sander J (2015) Hierarchical density estimates for data clustering, visualization, and outlier detection. \emph{ACM Transactions on Knowledge Discovery from Data} 10, 5:1--5:51.

\leavevmode\hypertarget{ref-genieowa}{}%
Cena A, Gagolewski M (2020) Genie+OWA: Robustifying hierarchical clustering with OWA-based linkages. \emph{Information Sciences} in press, doi:10.1016/j.ins.2020.02.025.

\leavevmode\hypertarget{ref-wines}{}%
Cortez P, Cerdeira A, Almeida F, Matos T, Reis J (2009) Modeling wine preferences by data mining from physicochemical properties. \emph{Decision Support Systems} 47, 547--553.

\leavevmode\hypertarget{ref-mml}{}%
Deisenroth MP, Faisal AA, Ong CS (2020) \emph{Mathematics for machine learning}. Cambridge University Press \url{https://mml-book.com/}.

\leavevmode\hypertarget{ref-dbscan}{}%
Ester M, Kriegel H-P, Sander J, Xu X (1996) A density-based algorithm for discovering clusters in large spatial databases with noise \emph{Proc. KDD'96}, pp. 226--231.

\leavevmode\hypertarget{ref-fletcher}{}%
Fletcher R (2008) \emph{Practical methods of optimization}. Wiley.

\leavevmode\hypertarget{ref-genie}{}%
Gagolewski M, Bartoszuk M, Cena A (2016) Genie: A new, fast, and outlier-resistant hierarchical clustering algorithm. \emph{Information Sciences} 363, 8--23.

\leavevmode\hypertarget{ref-genetic}{}%
Goldberg DE (1989) \emph{Genetic algorithms in search, optimization and machine learning}. Addison-Wesley.

\leavevmode\hypertarget{ref-deeplearn}{}%
Goodfellow I, Bengio Y, Courville A (2016) \emph{Deep learning}. MIT Press \url{https://www.deeplearningbook.org/}.

\leavevmode\hypertarget{ref-movielens}{}%
Harper FM, Konstan JA (2015) The MovieLens datasets: History and context. \emph{ACM Transactions on Interactive Intelligent Systems} 5, 19:1--19:19.

\leavevmode\hypertarget{ref-esl}{}%
Hastie T, Tibshirani R, Friedman J (2017) \emph{The elements of statistical learning}. Springer-Verlag \url{https://web.stanford.edu/~hastie/ElemStatLearn/}.

\leavevmode\hypertarget{ref-herlocker_etal}{}%
Herlocker JL, Konstan JA, Terveen LG, Riedl JT (2004) Evaluating collaborative filtering recommender systems. \emph{ACM Transactions on Information Systems} 22, 5--53. \url{https://web.archive.org/web/20070306161407/http://web.engr.oregonstate.edu/~herlock/papers/eval_tois.pdf}.

\leavevmode\hypertarget{ref-comparing_paritions}{}%
Hubert L, Arabie P (1985) Comparing partitions. \emph{Journal of Classification} 2, 193--218.

\leavevmode\hypertarget{ref-islr}{}%
James G, Witten D, Hastie T, Tibshirani R (2017) \emph{An introduction to statistical learning with applications in R}. Springer-Verlag \url{http://faculty.marshall.usc.edu/gareth-james/ISL/}.

\leavevmode\hypertarget{ref-bellkor_netflix}{}%
Koren Y (2009) \emph{The BellKor solution to the Netflix grand prize}. \url{https://netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf}.

\leavevmode\hypertarget{ref-pre_dbscan}{}%
Ling RF (1973) A probability theory of cluster analysis. \emph{Journal of the American Statistical Association} 68, 159--164.

\leavevmode\hypertarget{ref-lu_etal}{}%
Lü L, others (2012) Recommender systems. \emph{Physics Reports} 519, 1--49. \url{https://arxiv.org/pdf/1202.1112.pdf}.

\leavevmode\hypertarget{ref-itm}{}%
Müller AC, Nowozin S, Lampert CH (2012) Information theoretic clustering using minimum spanning trees \emph{Proc. German conference on pattern recognition}, \url{https://github.com/amueller/information-theoretic-mst}.

\leavevmode\hypertarget{ref-spectral_nips}{}%
Ng AY, Jordan MI, Weiss Y (2001) On spectral clustering: Analysis and an algorithm \emph{Proc. Advances in neural information processing systems 14 (NIPS'01)}, \url{https://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf}.

\leavevmode\hypertarget{ref-nocedal_wright}{}%
Nocedal J, Wright SJ (2006) \emph{Numerical optimization}. Springer.

\leavevmode\hypertarget{ref-rprogdatascience}{}%
Peng RD (2019) \emph{R programming for data science}. \url{https://bookdown.org/rdpeng/rprogdatascience/}.

\leavevmode\hypertarget{ref-pragmatictheory_netflix}{}%
Piotte M, Chabbert M (2009) \emph{The Pragmatic Theory solution to the Netflix grand prize}. \url{https://netflixprize.com/assets/GrandPrize2009_BPC_PragmaticTheory.pdf}.

\leavevmode\hypertarget{ref-id3}{}%
Quinlan R (1986) Induction of decision trees. \emph{Machine Learning} 1, 81--106.

\leavevmode\hypertarget{ref-c45}{}%
Quinlan R (1993) \emph{C4.5: Programs for machine learning}. Morgan Kaufmann Publishers.

\leavevmode\hypertarget{ref-rpoject}{}%
R Development Core Team (2020) \emph{R: A language and environment for statistical computing}. R Foundation for Statistical Computing, Vienna, Austria \url{http://www.R-project.org}.

\leavevmode\hypertarget{ref-external_cluster_validity}{}%
Rezaei M, Fränti P (2016) Set-matching measures for external cluster validity. \emph{IEEE Transactions on Knowledge and Data Engineering} 28, 2173--2186.

\leavevmode\hypertarget{ref-ricci_etal}{}%
Ricci F, Rokach L, Shapira B, Kantor P (eds) (2011) \emph{Recommender systems handbook}. Springer \url{http://www.inf.unibz.it/~ricci/papers/intro-rec-sys-handbook.pdf}.

\leavevmode\hypertarget{ref-aifaq}{}%
Sarle WS, others (eds) (2002) The comp.ai.neural-nets FAQ. \url{http://www.faqs.org/faqs/ai-faq/neural-nets/part1/}.

\leavevmode\hypertarget{ref-evolution}{}%
Simon D (2013) \emph{Evolutionary optimization algorithms: Biologically-inspired and population-based approaches to computer intelligence}. Wiley.

\leavevmode\hypertarget{ref-rpart}{}%
Therneau TM, Atkinson EJ (2019) \emph{An introduction to recursive partitioning using the RPART routines}. \url{https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf}.

\leavevmode\hypertarget{ref-bigchaos_netflix}{}%
Töscher A, Jahrer M, Bell RM (2009) \emph{The BigChaos solution to the Netflix grand prize}. \url{https://netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf}.

\leavevmode\hypertarget{ref-Rintro}{}%
Venables WN, Smith DM, R Core Team (2020) \emph{An introduction to R}. \url{https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf}.

\leavevmode\hypertarget{ref-r4ds}{}%
Wickham H, Grolemund G (2017) \emph{R for data science}. O'Reilly \url{https://r4ds.had.co.nz/}.

\leavevmode\hypertarget{ref-birch}{}%
Zhang T, Ramakrishnan R, Livny M (1996) BIRCH: An efficient data clustering method for large databases \emph{Proc. ACM SIGMOD international conference on management of data -- SIGMOD'96}, pp. 103--114.

\end{document}
