<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.2 Multinomial Logistic Regression | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="5.2 Multinomial Logistic Regression | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.2 Multinomial Logistic Regression | Lightweight Machine Learning Classics with R" />
  
  <meta name="twitter:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  

<meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-7.html"/>
<link rel="next" href="artificial-neural-networks.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>{</a></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a><ul>
<li class="chapter" data-level="1.1.1" data-path="machine-learning.html"><a href="machine-learning.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.1.2" data-path="machine-learning.html"><a href="machine-learning.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.2.1" data-path="supervised-learning.html"><a href="supervised-learning.html#formalism"><i class="fa fa-check"></i><b>1.2.1</b> Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="supervised-learning.html"><a href="supervised-learning.html#desired-outputs"><i class="fa fa-check"></i><b>1.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="1.2.3" data-path="supervised-learning.html"><a href="supervised-learning.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple-regression.html"><a href="simple-regression.html"><i class="fa fa-check"></i><b>1.3</b> Simple Regression</a><ul>
<li class="chapter" data-level="1.3.1" data-path="simple-regression.html"><a href="simple-regression.html#introduction"><i class="fa fa-check"></i><b>1.3.1</b> Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-regression.html"><a href="simple-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>1.3.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.4.1" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> Introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html#solution-in-r"><i class="fa fa-check"></i><b>1.4.2</b> Solution in R</a></li>
<li class="chapter" data-level="1.4.3" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>1.4.3</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="outro.html"><a href="outro.html"><i class="fa fa-check"></i><b>1.5</b> Outro</a><ul>
<li class="chapter" data-level="1.5.1" data-path="outro.html"><a href="outro.html#remarks"><i class="fa fa-check"></i><b>1.5.1</b> Remarks</a></li>
<li class="chapter" data-level="1.5.2" data-path="outro.html"><a href="outro.html#further-reading"><i class="fa fa-check"></i><b>1.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>2.1</b> Introduction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-2.html"><a href="introduction-2.html#formalism-1"><i class="fa fa-check"></i><b>2.1.1</b> Formalism</a></li>
<li class="chapter" data-level="2.1.2" data-path="introduction-2.html"><a href="introduction-2.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>2.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#problem-formulation"><i class="fa fa-check"></i><b>2.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>2.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html"><i class="fa fa-check"></i><b>2.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="2.3.1" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#model-diagnostics"><i class="fa fa-check"></i><b>2.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.3.2" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#variable-selection"><i class="fa fa-check"></i><b>2.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="2.3.3" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#variable-transformation"><i class="fa fa-check"></i><b>2.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="2.3.4" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#predictive-vs.descriptive-power"><i class="fa fa-check"></i><b>2.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="outro-1.html"><a href="outro-1.html"><i class="fa fa-check"></i><b>2.4</b> Outro</a><ul>
<li class="chapter" data-level="2.4.1" data-path="outro-1.html"><a href="outro-1.html#remarks-1"><i class="fa fa-check"></i><b>2.4.1</b> Remarks</a></li>
<li class="chapter" data-level="2.4.2" data-path="outro-1.html"><a href="outro-1.html#other-methods-for-regression"><i class="fa fa-check"></i><b>2.4.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="2.4.3" data-path="outro-1.html"><a href="outro-1.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>2.4.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="2.4.4" data-path="outro-1.html"><a href="outro-1.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>2.4.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="2.4.5" data-path="outro-1.html"><a href="outro-1.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>2.4.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="2.4.6" data-path="outro-1.html"><a href="outro-1.html#further-reading-1"><i class="fa fa-check"></i><b>2.4.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-3.html"><a href="introduction-3.html"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-3.html"><a href="introduction-3.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction-3.html"><a href="introduction-3.html#factor-data-type"><i class="fa fa-check"></i><b>3.1.2</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction-3.html"><a href="introduction-3.html#data"><i class="fa fa-check"></i><b>3.1.3</b> Data</a></li>
<li class="chapter" data-level="3.1.4" data-path="introduction-3.html"><a href="introduction-3.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.4</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.5" data-path="introduction-3.html"><a href="introduction-3.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.5</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a><ul>
<li class="chapter" data-level="3.2.1" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#introduction-4"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#different-metrics"><i class="fa fa-check"></i><b>3.2.3</b> Different Metrics (*)</a></li>
<li class="chapter" data-level="3.2.4" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#standardisation-of-independent-variables"><i class="fa fa-check"></i><b>3.2.4</b> Standardisation of Independent Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html"><i class="fa fa-check"></i><b>3.3</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#main-routine"><i class="fa fa-check"></i><b>3.3.1</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.3.2" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#mode"><i class="fa fa-check"></i><b>3.3.2</b> Mode</a></li>
<li class="chapter" data-level="3.3.3" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#nn-search-routines"><i class="fa fa-check"></i><b>3.3.3</b> NN Search Routines (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="outro-2.html"><a href="outro-2.html"><i class="fa fa-check"></i><b>3.4</b> Outro</a><ul>
<li class="chapter" data-level="3.4.1" data-path="outro-2.html"><a href="outro-2.html#remarks-2"><i class="fa fa-check"></i><b>3.4.1</b> Remarks</a></li>
<li class="chapter" data-level="3.4.2" data-path="outro-2.html"><a href="outro-2.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.4.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.4.3" data-path="outro-2.html"><a href="outro-2.html#further-reading-2"><i class="fa fa-check"></i><b>3.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html"><i class="fa fa-check"></i><b>4</b> Classification with Trees and Linear Models</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction-5.html"><a href="introduction-5.html"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="introduction-5.html"><a href="introduction-5.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="introduction-5.html"><a href="introduction-5.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
<li class="chapter" data-level="4.1.3" data-path="introduction-5.html"><a href="introduction-5.html#discussed-methods-1"><i class="fa fa-check"></i><b>4.1.3</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html"><i class="fa fa-check"></i><b>4.2</b> Model Assessment and Selection</a><ul>
<li class="chapter" data-level="4.2.1" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html#performance-metrics"><i class="fa fa-check"></i><b>4.2.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="4.2.2" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>4.2.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="4.2.3" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>4.2.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>4.3</b> Decision Trees</a><ul>
<li class="chapter" data-level="4.3.1" data-path="decision-trees.html"><a href="decision-trees.html#introduction-6"><i class="fa fa-check"></i><b>4.3.1</b> Introduction</a></li>
<li class="chapter" data-level="4.3.2" data-path="decision-trees.html"><a href="decision-trees.html#example-in-r-1"><i class="fa fa-check"></i><b>4.3.2</b> Example in R</a></li>
<li class="chapter" data-level="4.3.3" data-path="decision-trees.html"><a href="decision-trees.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.3.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html"><i class="fa fa-check"></i><b>4.4</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="4.4.1" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#motivation"><i class="fa fa-check"></i><b>4.4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.4.2" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>4.4.2</b> Logistic Model</a></li>
<li class="chapter" data-level="4.4.3" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#example-in-r-2"><i class="fa fa-check"></i><b>4.4.3</b> Example in R</a></li>
<li class="chapter" data-level="4.4.4" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#loss-function"><i class="fa fa-check"></i><b>4.4.4</b> Loss Function</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="outro-3.html"><a href="outro-3.html"><i class="fa fa-check"></i><b>4.5</b> Outro</a><ul>
<li class="chapter" data-level="4.5.1" data-path="outro-3.html"><a href="outro-3.html#remarks-3"><i class="fa fa-check"></i><b>4.5.1</b> Remarks</a></li>
<li class="chapter" data-level="4.5.2" data-path="outro-3.html"><a href="outro-3.html#further-reading-3"><i class="fa fa-check"></i><b>4.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>5</b> Neural Networks</a><ul>
<li class="chapter" data-level="5.1" data-path="introduction-7.html"><a href="introduction-7.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="introduction-7.html"><a href="introduction-7.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>5.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="5.1.2" data-path="introduction-7.html"><a href="introduction-7.html#data-2"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>5.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="5.2.2" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#extending-logistic-regression"><i class="fa fa-check"></i><b>5.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#softmax-function"><i class="fa fa-check"></i><b>5.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="5.2.4" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>5.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="5.2.5" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>5.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="5.2.6" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>5.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html"><i class="fa fa-check"></i><b>5.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="5.3.1" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>5.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="5.3.2" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>5.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="5.3.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#example-in-r-3"><i class="fa fa-check"></i><b>5.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html"><i class="fa fa-check"></i><b>5.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="5.4.1" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#introduction-8"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>5.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="5.4.3" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>5.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="5.4.4" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>5.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="preprocessing-of-data.html"><a href="preprocessing-of-data.html"><i class="fa fa-check"></i><b>5.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="5.5.1" data-path="preprocessing-of-data.html"><a href="preprocessing-of-data.html#introduction-9"><i class="fa fa-check"></i><b>5.5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.5.2" data-path="preprocessing-of-data.html"><a href="preprocessing-of-data.html#image-deskewing"><i class="fa fa-check"></i><b>5.5.2</b> Image Deskewing</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="outro-4.html"><a href="outro-4.html"><i class="fa fa-check"></i><b>5.6</b> Outro</a><ul>
<li class="chapter" data-level="5.6.1" data-path="outro-4.html"><a href="outro-4.html#remarks-4"><i class="fa fa-check"></i><b>5.6.1</b> Remarks</a></li>
<li class="chapter" data-level="5.6.2" data-path="outro-4.html"><a href="outro-4.html#beyond-mnist"><i class="fa fa-check"></i><b>5.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="5.6.3" data-path="outro-4.html"><a href="outro-4.html#further-reading-4"><i class="fa fa-check"></i><b>5.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>6</b> Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-10.html"><a href="introduction-10.html"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="introduction-10.html"><a href="introduction-10.html#optimisation-problem"><i class="fa fa-check"></i><b>6.1.1</b> Optimisation Problem</a></li>
<li class="chapter" data-level="6.1.2" data-path="introduction-10.html"><a href="introduction-10.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>6.1.2</b> Example Optimisation Problems in Machine Learning</a></li>
<li class="chapter" data-level="6.1.3" data-path="introduction-10.html"><a href="introduction-10.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>6.1.3</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="6.1.4" data-path="introduction-10.html"><a href="introduction-10.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>6.1.4</b> Example Objective over a 2D Domain</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="iterative-methods.html"><a href="iterative-methods.html"><i class="fa fa-check"></i><b>6.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="iterative-methods.html"><a href="iterative-methods.html#introduction-11"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="iterative-methods.html"><a href="iterative-methods.html#example-in-r-4"><i class="fa fa-check"></i><b>6.2.2</b> Example in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="iterative-methods.html"><a href="iterative-methods.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>6.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="6.2.4" data-path="iterative-methods.html"><a href="iterative-methods.html#random-restarts"><i class="fa fa-check"></i><b>6.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gradient-descent.html"><a href="gradient-descent.html"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient-descent.html"><a href="gradient-descent.html#function-gradient"><i class="fa fa-check"></i><b>6.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="6.3.2" data-path="gradient-descent.html"><a href="gradient-descent.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>6.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="6.3.3" data-path="gradient-descent.html"><a href="gradient-descent.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>6.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="6.3.4" data-path="gradient-descent.html"><a href="gradient-descent.html#example-mnist"><i class="fa fa-check"></i><b>6.3.4</b> Example: MNIST</a></li>
<li class="chapter" data-level="6.3.5" data-path="gradient-descent.html"><a href="gradient-descent.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>6.3.5</b> Stochastic Gradient Descent (SGD)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="outro-5.html"><a href="outro-5.html"><i class="fa fa-check"></i><b>6.4</b> Outro</a><ul>
<li class="chapter" data-level="6.4.1" data-path="outro-5.html"><a href="outro-5.html#remarks-5"><i class="fa fa-check"></i><b>6.4.1</b> Remarks</a></li>
<li class="chapter" data-level="6.4.2" data-path="outro-5.html"><a href="outro-5.html#optimisers-in-keras"><i class="fa fa-check"></i><b>6.4.2</b> Optimisers in Keras</a></li>
<li class="chapter" data-level="6.4.3" data-path="outro-5.html"><a href="outro-5.html#note-on-search-spaces"><i class="fa fa-check"></i><b>6.4.3</b> Note on Search Spaces</a></li>
<li class="chapter" data-level="6.4.4" data-path="outro-5.html"><a href="outro-5.html#further-reading-5"><i class="fa fa-check"></i><b>6.4.4</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a><ul>
<li class="chapter" data-level="7.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="7.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#introduction-12"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>7.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="7.1.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering-1"><i class="fa fa-check"></i><b>7.1.3</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>7.2</b> K-means Clustering</a><ul>
<li class="chapter" data-level="7.2.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>7.2.1</b> Example in R</a></li>
<li class="chapter" data-level="7.2.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#problem-statement"><i class="fa fa-check"></i><b>7.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="7.2.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>7.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html"><i class="fa fa-check"></i><b>7.3</b> Hierarchical Methods</a><ul>
<li class="chapter" data-level="7.3.1" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#introduction-13"><i class="fa fa-check"></i><b>7.3.1</b> Introduction</a></li>
<li class="chapter" data-level="7.3.2" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#example-in-r-6"><i class="fa fa-check"></i><b>7.3.2</b> Example in R</a></li>
<li class="chapter" data-level="7.3.3" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>7.3.3</b> Agglomerative Hierarchical Clustering</a></li>
<li class="chapter" data-level="7.3.4" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#linkage-functions"><i class="fa fa-check"></i><b>7.3.4</b> Linkage Functions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="outro-6.html"><a href="outro-6.html"><i class="fa fa-check"></i><b>7.4</b> Outro</a><ul>
<li class="chapter" data-level="7.4.1" data-path="outro-6.html"><a href="outro-6.html#remarks-6"><i class="fa fa-check"></i><b>7.4.1</b> Remarks</a></li>
<li class="chapter" data-level="7.4.2" data-path="outro-6.html"><a href="outro-6.html#other-noteworthy-clustering-algorithms"><i class="fa fa-check"></i><b>7.4.2</b> Other Noteworthy Clustering Algorithms</a></li>
<li class="chapter" data-level="7.4.3" data-path="outro-6.html"><a href="outro-6.html#further-reading-6"><i class="fa fa-check"></i><b>7.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html"><i class="fa fa-check"></i><b>8</b> Optimisation with Genetic Algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="introduction-14.html"><a href="introduction-14.html"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="introduction-14.html"><a href="introduction-14.html#recap"><i class="fa fa-check"></i><b>8.1.1</b> Recap</a></li>
<li class="chapter" data-level="8.1.2" data-path="introduction-14.html"><a href="introduction-14.html#k-means-revisited"><i class="fa fa-check"></i><b>8.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="8.1.3" data-path="introduction-14.html"><a href="introduction-14.html#optim-vs.kmeans"><i class="fa fa-check"></i><b>8.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html"><i class="fa fa-check"></i><b>8.2</b> A Note on Convex Optimisation (*)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#introduction-15"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#convex-combinations"><i class="fa fa-check"></i><b>8.2.2</b> Convex Combinations (*)</a></li>
<li class="chapter" data-level="8.2.3" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#convex-functions"><i class="fa fa-check"></i><b>8.2.3</b> Convex Functions (*)</a></li>
<li class="chapter" data-level="8.2.4" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#examples"><i class="fa fa-check"></i><b>8.2.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html"><i class="fa fa-check"></i><b>8.3</b> Genetic Algorithms</a><ul>
<li class="chapter" data-level="8.3.1" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html#introduction-16"><i class="fa fa-check"></i><b>8.3.1</b> Introduction</a></li>
<li class="chapter" data-level="8.3.2" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html#overview-of-the-method"><i class="fa fa-check"></i><b>8.3.2</b> Overview of the Method</a></li>
<li class="chapter" data-level="8.3.3" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html#example-implementation---ga-for-k-means"><i class="fa fa-check"></i><b>8.3.3</b> Example Implementation - GA for K-means</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="outro-7.html"><a href="outro-7.html"><i class="fa fa-check"></i><b>8.4</b> Outro</a><ul>
<li class="chapter" data-level="8.4.1" data-path="outro-7.html"><a href="outro-7.html#remarks-7"><i class="fa fa-check"></i><b>8.4.1</b> Remarks</a></li>
<li class="chapter" data-level="8.4.2" data-path="outro-7.html"><a href="outro-7.html#further-reading-7"><i class="fa fa-check"></i><b>8.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>9</b> Recommender Systems</a><ul>
<li class="chapter" data-level="9.1" data-path="introduction-17.html"><a href="introduction-17.html"><i class="fa fa-check"></i><b>9.1</b> Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="introduction-17.html"><a href="introduction-17.html#what-is-a-recommender-system"><i class="fa fa-check"></i><b>9.1.1</b> What is a Recommender System?</a></li>
<li class="chapter" data-level="9.1.2" data-path="introduction-17.html"><a href="introduction-17.html#the-netflix-prize"><i class="fa fa-check"></i><b>9.1.2</b> The Netflix Prize</a></li>
<li class="chapter" data-level="9.1.3" data-path="introduction-17.html"><a href="introduction-17.html#main-approaches"><i class="fa fa-check"></i><b>9.1.3</b> Main Approaches</a></li>
<li class="chapter" data-level="9.1.4" data-path="introduction-17.html"><a href="introduction-17.html#formalism-2"><i class="fa fa-check"></i><b>9.1.4</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html"><i class="fa fa-check"></i><b>9.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="9.2.1" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#example"><i class="fa fa-check"></i><b>9.2.1</b> Example</a></li>
<li class="chapter" data-level="9.2.2" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#similarity-measures"><i class="fa fa-check"></i><b>9.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="9.2.3" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="9.2.4" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="movielens-dataset.html"><a href="movielens-dataset.html"><i class="fa fa-check"></i><b>9.3</b> MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="9.3.1" data-path="movielens-dataset.html"><a href="movielens-dataset.html#dataset"><i class="fa fa-check"></i><b>9.3.1</b> Dataset</a></li>
<li class="chapter" data-level="9.3.2" data-path="movielens-dataset.html"><a href="movielens-dataset.html#data-cleansing"><i class="fa fa-check"></i><b>9.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="9.3.3" data-path="movielens-dataset.html"><a href="movielens-dataset.html#item-item-similarities"><i class="fa fa-check"></i><b>9.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="9.3.4" data-path="movielens-dataset.html"><a href="movielens-dataset.html#example-recommendations"><i class="fa fa-check"></i><b>9.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="9.3.5" data-path="movielens-dataset.html"><a href="movielens-dataset.html#clustering-2"><i class="fa fa-check"></i><b>9.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="outro-8.html"><a href="outro-8.html"><i class="fa fa-check"></i><b>9.4</b> Outro</a><ul>
<li class="chapter" data-level="9.4.1" data-path="outro-8.html"><a href="outro-8.html#remarks-8"><i class="fa fa-check"></i><b>9.4.1</b> Remarks</a></li>
<li class="chapter" data-level="9.4.2" data-path="outro-8.html"><a href="outro-8.html#issues"><i class="fa fa-check"></i><b>9.4.2</b> Issues</a></li>
<li class="chapter" data-level="9.4.3" data-path="outro-8.html"><a href="outro-8.html#further-reading-8"><i class="fa fa-check"></i><b>9.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="section-15.html"><a href="section-15.html"><i class="fa fa-check"></i>}</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html"><i class="fa fa-check"></i><b>A</b> Vector Algebra in R</a><ul>
<li class="chapter" data-level="A.1" data-path="motivation-1.html"><a href="motivation-1.html"><i class="fa fa-check"></i><b>A.1</b> Motivation</a></li>
<li class="chapter" data-level="A.2" data-path="vector-scalar-operations.html"><a href="vector-scalar-operations.html"><i class="fa fa-check"></i><b>A.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="A.3" data-path="vector-vector-operations.html"><a href="vector-vector-operations.html"><i class="fa fa-check"></i><b>A.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="A.4" data-path="other-vector-operations.html"><a href="other-vector-operations.html"><i class="fa fa-check"></i><b>A.4</b> Other Vector Operations</a></li>
<li class="chapter" data-level="A.5" data-path="further-reading-9.html"><a href="further-reading-9.html"><i class="fa fa-check"></i><b>A.5</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html"><i class="fa fa-check"></i><b>B</b> Matrix Algebra in R</a><ul>
<li class="chapter" data-level="B.1" data-path="matrices.html"><a href="matrices.html"><i class="fa fa-check"></i><b>B.1</b> Matrices</a></li>
<li class="chapter" data-level="B.2" data-path="matrix-scalar-operations.html"><a href="matrix-scalar-operations.html"><i class="fa fa-check"></i><b>B.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="B.3" data-path="matrix-matrix-operations.html"><a href="matrix-matrix-operations.html"><i class="fa fa-check"></i><b>B.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="B.4" data-path="matrix-multiplication.html"><a href="matrix-multiplication.html"><i class="fa fa-check"></i><b>B.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="B.5" data-path="matrix-vector-operations.html"><a href="matrix-vector-operations.html"><i class="fa fa-check"></i><b>B.5</b> Matrix-Vector Operations</a></li>
<li class="chapter" data-level="B.6" data-path="further-reading-10.html"><a href="further-reading-10.html"><i class="fa fa-check"></i><b>B.6</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html"><i class="fa fa-check"></i><b>C</b> Data Frame Wrangling in R</a><ul>
<li class="chapter" data-level="C.1" data-path="to-do.html"><a href="to-do.html"><i class="fa fa-check"></i><b>C.1</b> TO DO</a><ul>
<li class="chapter" data-level="C.1.1" data-path="to-do.html"><a href="to-do.html#to-do-1"><i class="fa fa-check"></i><b>C.1.1</b> TO DO</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="further-reading-11.html"><a href="further-reading-11.html"><i class="fa fa-check"></i><b>C.2</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.1 2020-03-01 14:48 (e066e08)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multinomial-logistic-regression" class="section level2">
<h2><span class="header-section-number">5.2</span> Multinomial Logistic Regression</h2>
<div id="a-note-on-data-representation" class="section level3">
<h3><span class="header-section-number">5.2.1</span> A Note on Data Representation</h3>
<hr />
<p>So… you may now be wondering “how do we construct an image classifier,
this seems so complicated!”.</p>
<p>For a computer, (almost) everything is just numbers.</p>
<p>Instead of playing with <span class="math inline">\(n\)</span> matrices, each of size 28×28,
we may “flatten” the images so as to get
<span class="math inline">\(n\)</span> “long” vectors of length <span class="math inline">\(p=784\)</span>.</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb327-1" data-line-number="1">X_train2 &lt;-<span class="st"> </span><span class="kw">matrix</span>(X_train, <span class="dt">ncol=</span><span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</a>
<a class="sourceLine" id="cb327-2" data-line-number="2">X_test2  &lt;-<span class="st"> </span><span class="kw">matrix</span>(X_test, <span class="dt">ncol=</span><span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</a></code></pre></div>
<p>The classifiers studied here do not take the “spatial” positioning of
the pixels into account anyway.</p>
<blockquote>
<p>(*) See, however, convolutional neural networks (CNNs),
e.g., in <span class="citation">(Goodfellow, Bengio, and Courville <a href="references.html#ref-deeplearn">2016</a>)</span>.</p>
</blockquote>
<p>Hence, now we’re back to our “comfort zone”.</p>
</div>
<div id="extending-logistic-regression" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Extending Logistic Regression</h3>
<hr />
<p>Let us generalise the binary logistic regression model
to a 10-class one (or, more generally, <span class="math inline">\(K\)</span>-class one).</p>
<p>This time we will be modelling ten probabilities,
with
<span class="math inline">\(\Pr(Y=k|\mathbf{X},\mathbf{B})\)</span> denoting the <em>confidence</em> that a given image <span class="math inline">\(\mathbf{X}\)</span>
is in fact the <span class="math inline">\(k\)</span>-th digit:</p>
<p><span class="math display">\[
\begin{array}{lcl}
\Pr(Y=0|\mathbf{X},\mathbf{B})&amp;=&amp;\dots\\
\Pr(Y=1|\mathbf{X},\mathbf{B})&amp;=&amp;\dots\\
&amp;\vdots&amp;\\
\Pr(Y=9|\mathbf{X},\mathbf{B})&amp;=&amp;\dots\\
\end{array}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{B}\)</span> is the set of underlying model parameters
(to be determined soon).</p>
<hr />
<p>In binary logistic regression,
the class probabilities are obtained by “cleverly normalising”
the outputs of a linear model (so that we obtain a value in <span class="math inline">\([0,1]\)</span>).</p>
<p>In the multinomial case, we can use a separate linear model for each digit
so that <span class="math inline">\(\Pr(Y=k|\mathbf{X},\mathbf{B})\)</span>
is given as a function of
<span class="math display">\[\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p}.\]</span></p>
<p>Therefore, instead of a parameter vector of length <span class="math inline">\((p+1)\)</span>,
we will need a parameter matrix of size <span class="math inline">\((p+1)\times 10\)</span>
representing the model’s definition.</p>
<blockquote>
<p>Side note: upper case of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(B\)</span>.</p>
</blockquote>
<p>Then, these 10 numbers will have to be normalised
so as to they are positive and sum to <span class="math inline">\(1\)</span>.</p>
<hr />
<p>To maintain the spirit of the original model,
we can apply <span class="math inline">\(e^{-(\beta_{0,k} + \beta_{1,k} X_{1} + \dots + \beta_{p,k} X_{p})}\)</span>
to get a positive value,
because the co-domain of the exponential function <span class="math inline">\(t\mapsto e^t\)</span>
is <span class="math inline">\((0,\infty)\)</span>.</p>
<p>Then, dividing each output by the sum of all the outputs will guarantee that
the total sum equals 1.</p>
<p>This leads to:
<span class="math display">\[
\begin{array}{lcl}
\Pr(Y=0|\mathbf{X},\mathbf{B})&amp;=&amp;\displaystyle\frac{e^{-(\beta_{0,0} + \beta_{1,0} X_{1} +  \dots + \beta_{p,0} X_{p})}}{\sum_{k=0}^9 e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_p)}},\\
\Pr(Y=1|\mathbf{X},\mathbf{B})&amp;=&amp;\displaystyle\frac{e^{-(\beta_{0,1} + \beta_{1,1} X_{1} +  \dots + \beta_{p,1} X_{p})}}{\sum_{k=0}^9 e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p})}},\\
&amp;\vdots&amp;\\
\Pr(Y=9|\mathbf{X},\mathbf{B})&amp;=&amp;\displaystyle\frac{e^{-(\beta_{0,9} + \beta_{1,9} X_{1} +  \dots + \beta_{p,9} X_{p})}}{\sum_{k=0}^9 e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p})}}.\\
\end{array}
\]</span></p>
<p>Note that we get the binary logistic regression
if we fix <span class="math inline">\(\beta_{0,0}=\beta_{1,0}=\dots=\beta_{p,0}=0\)</span>
as <span class="math inline">\(e^0=1\)</span> and consider only the classes <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p>
</div>
<div id="softmax-function" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Softmax Function</h3>
<hr />
<p>The above transformation (that maps 10 arbitrary real numbers
to positive ones that sum to 1)
is called the <strong>softmax</strong> function (or <em>softargmax</em>).</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb328-1" data-line-number="1">softmax &lt;-<span class="st"> </span><span class="cf">function</span>(T) {</a>
<a class="sourceLine" id="cb328-2" data-line-number="2">    T2 &lt;-<span class="st"> </span><span class="kw">exp</span>(T) <span class="co"># ignore the minus sign above</span></a>
<a class="sourceLine" id="cb328-3" data-line-number="3">    T2<span class="op">/</span><span class="kw">sum</span>(T2)</a>
<a class="sourceLine" id="cb328-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb328-5" data-line-number="5"><span class="kw">round</span>(<span class="kw">rbind</span>(</a>
<a class="sourceLine" id="cb328-6" data-line-number="6">    <span class="kw">softmax</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,  <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)),</a>
<a class="sourceLine" id="cb328-7" data-line-number="7">    <span class="kw">softmax</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)),</a>
<a class="sourceLine" id="cb328-8" data-line-number="8">    <span class="kw">softmax</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">9</span>,  <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)),</a>
<a class="sourceLine" id="cb328-9" data-line-number="9">    <span class="kw">softmax</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">9</span>,  <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">8</span>))), <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    0 1.00    0    0    0 0.00    0    0  0.00
## [2,]    0    0 0.50    0    0    0 0.50    0    0  0.00
## [3,]    0    0 0.73    0    0    0 0.27    0    0  0.00
## [4,]    0    0 0.67    0    0    0 0.24    0    0  0.09</code></pre>
</div>
<div id="one-hot-encoding-and-decoding" class="section level3">
<h3><span class="header-section-number">5.2.4</span> One-Hot Encoding and Decoding</h3>
<hr />
<p>The ten class-belongingness-degrees can be decoded
to obtain a single label by simply choosing
the class that is assigned the highest probability.</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb330-1" data-line-number="1">y_pred &lt;-<span class="st"> </span><span class="kw">softmax</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">9</span>,  <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">8</span>))</a>
<a class="sourceLine" id="cb330-2" data-line-number="2"><span class="kw">round</span>(y_pred, <span class="dv">2</span>) <span class="co"># probabilities of class 0, 1, 2, ..., 9</span></a></code></pre></div>
<pre><code>##  [1] 0.00 0.00 0.67 0.00 0.00 0.00 0.24 0.00 0.00 0.09</code></pre>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb332-1" data-line-number="1"><span class="kw">which.max</span>(y_pred)<span class="op">-</span><span class="dv">1</span> <span class="co"># 1..10 -&gt; 0..9</span></a></code></pre></div>
<pre><code>## [1] 2</code></pre>
<blockquote>
<p><code>which.max(y)</code> returns an index <code>k</code> such that
<code>y[k]==max(y)</code> (recall that in R the first element in a vector is at index <code>1</code>).
Mathematically, we denote this operation as <span class="math inline">\(\mathrm{arg}\max_{k=1,\dots,K} y_k\)</span>.</p>
</blockquote>
<hr />
<p>To make processing the outputs of a logistic regression model more convenient,
we will apply the <strong>one-hot-encoding</strong> of the labels.</p>
<p>Here, each label will be represented as a 0-1 probability vector
– with probability 1 corresponding to the true class only.</p>
<p>For example:</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb334-1" data-line-number="1">y &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># true class (example)</span></a>
<a class="sourceLine" id="cb334-2" data-line-number="2">y2 &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb334-3" data-line-number="3">y2[y<span class="op">+</span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="dv">1</span> <span class="co"># +1 because we need 0..9 -&gt; 1..10</span></a>
<a class="sourceLine" id="cb334-4" data-line-number="4">y2  <span class="co"># one-hot-encoded y</span></a></code></pre></div>
<pre><code>##  [1] 0 0 1 0 0 0 0 0 0 0</code></pre>
<hr />
<p>To one-hot encode the reference outputs in R,
we start with a matrix of size <span class="math inline">\(n\times 10\)</span> populated with “0”s:</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb336-1" data-line-number="1">Y_train2 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span><span class="kw">length</span>(Y_train), <span class="dt">ncol=</span><span class="dv">10</span>)</a></code></pre></div>
<p>Next, for every <span class="math inline">\(i\)</span>, we insert a “1” in the <span class="math inline">\(i\)</span>-th row
and the (<code>Y_train[</code><span class="math inline">\(i\)</span><code>]+1</code>)-th column:</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb337-1" data-line-number="1"><span class="co"># Note the &quot;+1&quot; 0..9 -&gt; 1..10</span></a>
<a class="sourceLine" id="cb337-2" data-line-number="2">Y_train2[<span class="kw">cbind</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(Y_train), Y_train<span class="op">+</span><span class="dv">1</span>)] &lt;-<span class="st"> </span><span class="dv">1</span></a></code></pre></div>
<blockquote>
<p>In R, indexing a matrix <code>A</code> with a 2-column matrix <code>B</code>, i.e., <code>A[B]</code>,
allows for an easy access to
<code>A[B[1,1], B[1,2]]</code>, <code>A[B[2,1], B[2,2]]</code>, <code>A[B[3,1], B[3,2]]</code>, …</p>
</blockquote>
<hr />
<p>Sanity check:</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb338-1" data-line-number="1"><span class="kw">head</span>(Y_train)</a></code></pre></div>
<pre><code>## [1] 5 0 4 1 9 2</code></pre>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb340-1" data-line-number="1"><span class="kw">head</span>(Y_train2)</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    0    0    0    0    1    0    0    0     0
## [2,]    1    0    0    0    0    0    0    0    0     0
## [3,]    0    0    0    0    1    0    0    0    0     0
## [4,]    0    1    0    0    0    0    0    0    0     0
## [5,]    0    0    0    0    0    0    0    0    0     1
## [6,]    0    0    1    0    0    0    0    0    0     0</code></pre>
<hr />
<p>Let us generalise the above idea and write a function
that can one-hot-encode any vector of integer labels:</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb342-1" data-line-number="1">one_hot_encode &lt;-<span class="st"> </span><span class="cf">function</span>(Y) {</a>
<a class="sourceLine" id="cb342-2" data-line-number="2">    <span class="kw">stopifnot</span>(<span class="kw">is.numeric</span>(Y))</a>
<a class="sourceLine" id="cb342-3" data-line-number="3">    c1 &lt;-<span class="st"> </span><span class="kw">min</span>(Y) <span class="co"># first class label</span></a>
<a class="sourceLine" id="cb342-4" data-line-number="4">    cK &lt;-<span class="st"> </span><span class="kw">max</span>(Y) <span class="co"># last class label</span></a>
<a class="sourceLine" id="cb342-5" data-line-number="5">    K &lt;-<span class="st"> </span>cK<span class="op">-</span>c1<span class="op">+</span><span class="dv">1</span> <span class="co"># number of classes</span></a>
<a class="sourceLine" id="cb342-6" data-line-number="6"></a>
<a class="sourceLine" id="cb342-7" data-line-number="7">    Y2 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span><span class="kw">length</span>(Y), <span class="dt">ncol=</span>K)</a>
<a class="sourceLine" id="cb342-8" data-line-number="8">    Y2[<span class="kw">cbind</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(Y), Y<span class="op">-</span>c1<span class="op">+</span><span class="dv">1</span>)] &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb342-9" data-line-number="9">    Y2</a>
<a class="sourceLine" id="cb342-10" data-line-number="10">}</a></code></pre></div>
<p>Encode <code>Y_train</code> and <code>Y_test</code>:</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb343-1" data-line-number="1">Y_train2 &lt;-<span class="st"> </span><span class="kw">one_hot_encode</span>(Y_train)</a>
<a class="sourceLine" id="cb343-2" data-line-number="2">Y_test2 &lt;-<span class="st"> </span><span class="kw">one_hot_encode</span>(Y_test)</a></code></pre></div>
</div>
<div id="cross-entropy-revisited" class="section level3">
<h3><span class="header-section-number">5.2.5</span> Cross-entropy Revisited</h3>
<hr />
<p>In essence, we will be comparing
the probability vectors as generated by a classifier, <span class="math inline">\(\hat{Y}\)</span>:</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb344-1" data-line-number="1"><span class="kw">round</span>(y_pred, <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##  [1] 0.00 0.00 0.67 0.00 0.00 0.00 0.24 0.00 0.00 0.09</code></pre>
<p>with the one-hot-encoded true probabilities, <span class="math inline">\(Y\)</span>:</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb346-1" data-line-number="1">y2</a></code></pre></div>
<pre><code>##  [1] 0 0 1 0 0 0 0 0 0 0</code></pre>
<hr />
<p>It turns out that one of the definitions of cross-entropy introduced above
already handles the case of multiclass classification:
<span class="math display">\[
E(\mathbf{B}) =
-\frac{1}{n} \sum_{i=1}^n
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\mathbf{B}).
\]</span>
The smaller the probability corresponding to the ground-truth class
outputted by the classifier, the higher the penalty:</p>
<p><img src="05-classification-nnets-figures/unnamed-chunk-7-1.png" alt="plot of chunk unnamed-chunk-7" style="width:50.0%" /></p>
<hr />
<p>To sum up, we will be solving the optimisation problem:
<span class="math display">\[
\min_{\mathbf{B}\in\mathbb{R}^{(p+1)\times 10}}
-\frac{1}{n} \sum_{i=1}^n
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\mathbf{B}).
\]</span>
This has no analytical solution,
but can be solved using iterative methods
(see the chapter on optimisation).</p>
<hr />
<p>(*) Side note: A single term in the above formula,
<span class="math display">\[
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\mathbf{B})
\]</span>
given:</p>
<ul>
<li><code>y_pred</code> – a vector of 10 probabilities
generated by the model:
<span class="math display">\[
\left[\Pr(Y=0|\mathbf{x}_{i,\cdot},\mathbf{B})\  \Pr(Y=1|\mathbf{x}_{i,\cdot},\mathbf{B})\ \cdots\ \Pr(Y=9|\mathbf{x}_{i,\cdot},\mathbf{B})\right]
\]</span></li>
<li><code>y2</code> – a one-hot-encoded version of the true label, <span class="math inline">\(y_i\)</span>, of the form
<span class="math display">\[
\left[0\ 0\ \cdots\ 0\ 1\ 0\ \cdots\ 0\right]
\]</span></li>
</ul>
<p>can be computed as:</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb348-1" data-line-number="1"><span class="kw">sum</span>(y2<span class="op">*</span><span class="kw">log</span>(y_pred))</a></code></pre></div>
<pre><code>## [1] -0.4078174</code></pre>
</div>
<div id="problem-formulation-in-matrix-form" class="section level3">
<h3><span class="header-section-number">5.2.6</span> Problem Formulation in Matrix Form (**)</h3>
<hr />
<p>The definition of a multinomial logistic regression
model for a multiclass classification task involving classes <span class="math inline">\(\{1,2,\dots,K\}\)</span>
is slightly bloated.</p>
<p>Assuming that <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span> is the input matrix,
to compute the <span class="math inline">\(K\)</span> predicted probabilities for the <span class="math inline">\(i\)</span>-th input,
<span class="math display">\[
\left[
\hat{y}_{i,1}\ \hat{y}_{i,2}\ \cdots\ \hat{y}_{i,K}
\right],
\]</span>
given a parameter matrix <span class="math inline">\(\mathbf{B}^{(p+1)\times K}\)</span>, we apply:
<span class="math display">\[
\begin{array}{lcl}
\hat{y}_{i,1}=\Pr(Y=1|\mathbf{x}_{i,\cdot},\mathbf{B})&amp;=&amp;\displaystyle\frac{e^{\beta_{0,1} + \beta_{1,1} x_{i,1} +  \dots + \beta_{p,1} x_{i,p}}}{\sum_{k=1}^K e^{\beta_{0,k} + \beta_{1,k} x_{i,1} +  \dots + \beta_{p,k} x_{i,p}}},\\
&amp;\vdots&amp;\\
\hat{y}_{i,K}=\Pr(Y=K|\mathbf{x}_{i,\cdot},\mathbf{B})&amp;=&amp;\displaystyle\frac{e^{\beta_{0,K} + \beta_{1,K} x_{i,1} +  \dots + \beta_{p,K} x_{i,p}}}{\sum_{k=1}^K e^{\beta_{0,k} + \beta_{1,k} x_{i,1} +  \dots + \beta_{p,k} x_{i,p}}}.\\
\end{array}
\]</span></p>
<blockquote>
<p>We have dropped the minus sign in the exponentiation for the brevity of notation.
Note that we can always map <span class="math inline">\(b_{j,k}&#39;=-b_{j,k}\)</span>.</p>
</blockquote>
<p>It turns out we can make use of matrix notation
to tidy the above formulas.</p>
<hr />
<p>Denote the linear combinations prior to computing the softmax function with:
<span class="math display">\[
\begin{array}{lcl}
t_{i,1}&amp;=&amp;\beta_{0,1} + \beta_{1,1} x_{i,1} +  \dots + \beta_{p,1} x_{i,p}\\
&amp;\vdots&amp;\\
t_{i,K}&amp;=&amp;\beta_{0,K} + \beta_{1,K} x_{i,1} +  \dots + \beta_{p,K} x_{i,p}\\
\end{array}
\]</span></p>
<p>We have:</p>
<ul>
<li><span class="math inline">\(x_{i,j}\)</span> – <span class="math inline">\(i\)</span>-th observation, <span class="math inline">\(j\)</span>-th feature;</li>
<li><span class="math inline">\(\hat{y}_{i,k}\)</span> – <span class="math inline">\(i\)</span>-th observation, <span class="math inline">\(k\)</span>-th class probability;</li>
<li><span class="math inline">\(\beta_{j,k}\)</span> – coefficient for the <span class="math inline">\(j\)</span>-th feature when computing the <span class="math inline">\(k\)</span>-th class.</li>
</ul>
<p>Note that by augmenting <span class="math inline">\(\mathbf{\dot{X}}=[\boldsymbol{1}\ \mathbf{X}]\in\mathbb{R}^{n\times (p+1)}\)</span>,
where <span class="math inline">\(\dot{x}_{i,0}=1\)</span> and <span class="math inline">\(\dot{x}_{i,j}=x_{i,j}\)</span> for all <span class="math inline">\(j\ge 1\)</span> and all <span class="math inline">\(i\)</span>, we can write the above as:
<span class="math display">\[
\begin{array}{lclcl}
t_{i,1}&amp;=&amp;\sum_{j=0}^p \dot{x}_{i,j}\, \beta_{j,1} &amp;=&amp; \dot{\mathbf{x}}_{i,\cdot}\, \boldsymbol\beta_{\cdot,1}\\
&amp;\vdots&amp;\\
t_{i,K}&amp;=&amp;\sum_{j=0}^p \dot{x}_{i,j}\, \beta_{j,K} &amp;=&amp; \dot{\mathbf{x}}_{i,\cdot}\, \boldsymbol\beta_{\cdot,K}\\
\end{array}
\]</span></p>
<hr />
<p>We can get the <span class="math inline">\(K\)</span> linear combinations all at once
in the form of a row vector by writing:
<span class="math display">\[
\left[
t_{i,1}\ t_{i,2}\ \cdots\ t_{i,K}
\right]
=
{\mathbf{x}_{i,\cdot}}\, \mathbf{B}
\]</span></p>
<p>Moreover, we can do that for all the <span class="math inline">\(n\)</span> inputs by writing:
<span class="math display">\[
\mathbf{T}=\dot{\mathbf{X}}\,\mathbf{B}
\]</span>
Yes, this is a single matrix multiplication,
we have <span class="math inline">\(\mathbf{T}\in\mathbb{R}^{n\times K}\)</span>.</p>
<p>To obtain <span class="math inline">\(\hat{\mathbf{Y}}\)</span>, we have to apply the softmax function
on every row of <span class="math inline">\(\mathbf{T}\)</span>:
<span class="math display">\[
\hat{\mathbf{Y}}=\mathrm{softmax}\left(
\dot{\mathbf{X}}\,\mathbf{B}
\right).
\]</span></p>
<p>That’s it. Take some time to appreciate the elegance of this notation.</p>
<p>Methods for minimising crossentropy expressed in matrix form
will be discussed in the next chapter.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-7.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="artificial-neural-networks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
