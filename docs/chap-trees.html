<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Classification with Decision Trees | Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Classification with Decision Trees | Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Classification with Decision Trees | Machine Learning Classics with R" />
  
  <meta name="twitter:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  

<meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap-feature-engineering.html"/>
<link rel="next" href="chap-regression-simple.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

div.remark {
    border-left: 2px solid gray;
    padding-left: 1em;
}

div.definition {
    font-style: italic;
    border-left: 2px solid #550000;
    padding-left: 1em;
}

</style>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="chap-introduction.html"><a href="chap-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="chap-introduction.html"><a href="chap-introduction.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a><ul>
<li class="chapter" data-level="1.1.1" data-path="chap-introduction.html"><a href="chap-introduction.html#data-sources"><i class="fa fa-check"></i><b>1.1.1</b> Data Sources</a></li>
<li class="chapter" data-level="1.1.2" data-path="chap-introduction.html"><a href="chap-introduction.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chap-introduction.html"><a href="chap-introduction.html#input-data-x"><i class="fa fa-check"></i><b>1.2</b> Input Data, <strong>X</strong></a><ul>
<li class="chapter" data-level="1.2.1" data-path="chap-introduction.html"><a href="chap-introduction.html#abstract-formalism"><i class="fa fa-check"></i><b>1.2.1</b> Abstract Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="chap-introduction.html"><a href="chap-introduction.html#concrete-example"><i class="fa fa-check"></i><b>1.2.2</b> Concrete Example</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="chap-introduction.html"><a href="chap-introduction.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.3</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="1.3.1" data-path="chap-introduction.html"><a href="chap-introduction.html#dimensionality-reduction"><i class="fa fa-check"></i><b>1.3.1</b> Dimensionality Reduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="chap-introduction.html"><a href="chap-introduction.html#anomaly-detection"><i class="fa fa-check"></i><b>1.3.2</b> Anomaly Detection</a></li>
<li class="chapter" data-level="1.3.3" data-path="chap-introduction.html"><a href="chap-introduction.html#clustering"><i class="fa fa-check"></i><b>1.3.3</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="chap-introduction.html"><a href="chap-introduction.html#supervised-learning"><i class="fa fa-check"></i><b>1.4</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.4.1" data-path="chap-introduction.html"><a href="chap-introduction.html#desired-outputs-y"><i class="fa fa-check"></i><b>1.4.1</b> Desired Outputs, <strong>y</strong></a></li>
<li class="chapter" data-level="1.4.2" data-path="chap-introduction.html"><a href="chap-introduction.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.4.2</b> Types of Supervised Learning Problems</a></li>
<li class="chapter" data-level="1.4.3" data-path="chap-introduction.html"><a href="chap-introduction.html#one-dataset-many-problems"><i class="fa fa-check"></i><b>1.4.3</b> One Dataset – Many Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-hclust.html"><a href="chap-hclust.html"><i class="fa fa-check"></i><b>2</b> Agglomerative Hierarchical Clustering</a><ul>
<li class="chapter" data-level="2.1" data-path="chap-hclust.html"><a href="chap-hclust.html#dataset-partitions"><i class="fa fa-check"></i><b>2.1</b> Dataset Partitions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="chap-hclust.html"><a href="chap-hclust.html#label-vectors"><i class="fa fa-check"></i><b>2.1.1</b> Label Vectors</a></li>
<li class="chapter" data-level="2.1.2" data-path="chap-hclust.html"><a href="chap-hclust.html#k-partitions"><i class="fa fa-check"></i><b>2.1.2</b> K-Partitions</a></li>
<li class="chapter" data-level="2.1.3" data-path="chap-hclust.html"><a href="chap-hclust.html#interesting-partitions"><i class="fa fa-check"></i><b>2.1.3</b> “Interesting” Partitions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chap-hclust.html"><a href="chap-hclust.html#sec:euclidean"><i class="fa fa-check"></i><b>2.2</b> Euclidean Distance</a></li>
<li class="chapter" data-level="2.3" data-path="chap-hclust.html"><a href="chap-hclust.html#hierarchical-clustering-at-a-glance"><i class="fa fa-check"></i><b>2.3</b> Hierarchical Clustering at a Glance</a></li>
<li class="chapter" data-level="2.4" data-path="chap-hclust.html"><a href="chap-hclust.html#cluster-dendrograms"><i class="fa fa-check"></i><b>2.4</b> Cluster Dendrograms</a></li>
<li class="chapter" data-level="2.5" data-path="chap-hclust.html"><a href="chap-hclust.html#linkage-functions"><i class="fa fa-check"></i><b>2.5</b> Linkage Functions</a></li>
<li class="chapter" data-level="2.6" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>2.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-knn.html"><a href="chap-knn.html"><i class="fa fa-check"></i><b>3</b> Classification with Nearest Neighbours</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="chap-knn.html"><a href="chap-knn.html#k-nearest-neighbours-classifier"><i class="fa fa-check"></i><b>3.2</b> K-Nearest Neighbours Classifier</a></li>
<li class="chapter" data-level="3.3" data-path="chap-knn.html"><a href="chap-knn.html#example-in-r"><i class="fa fa-check"></i><b>3.3</b> Example in R</a></li>
<li class="chapter" data-level="3.4" data-path="chap-knn.html"><a href="chap-knn.html#classifier-assessment"><i class="fa fa-check"></i><b>3.4</b> Classifier Assessment</a></li>
<li class="chapter" data-level="3.5" data-path="chap-knn.html"><a href="chap-knn.html#classifier-selection"><i class="fa fa-check"></i><b>3.5</b> Classifier Selection</a></li>
<li class="chapter" data-level="3.6" data-path="chap-knn.html"><a href="chap-knn.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.6</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="3.6.1" data-path="chap-knn.html"><a href="chap-knn.html#main-routine"><i class="fa fa-check"></i><b>3.6.1</b> Main Routine</a></li>
<li class="chapter" data-level="3.6.2" data-path="chap-knn.html"><a href="chap-knn.html#sec:mode"><i class="fa fa-check"></i><b>3.6.2</b> Mode</a></li>
<li class="chapter" data-level="3.6.3" data-path="chap-knn.html"><a href="chap-knn.html#nn-search-methods"><i class="fa fa-check"></i><b>3.6.3</b> NN Search Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>3.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html"><i class="fa fa-check"></i><b>4</b> Feature Engineering</a><ul>
<li class="chapter" data-level="4.0.1" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#feature-engineering"><i class="fa fa-check"></i><b>4.0.1</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.0.2" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#different-metrics"><i class="fa fa-check"></i><b>4.0.2</b> Different Metrics (*)</a></li>
<li class="chapter" data-level="4.1" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>4.1</b> Exercises</a><ul>
<li class="chapter" data-level="4.1.1" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#wine-quality-best-k-nn-parameters-via-cross-validation"><i class="fa fa-check"></i><b>4.1.1</b> Wine Quality – Best K-NN Parameters via Cross-Validation (*)</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>4.2</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-trees.html"><a href="chap-trees.html"><i class="fa fa-check"></i><b>5</b> Classification with Decision Trees</a><ul>
<li class="chapter" data-level="5.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="chap-trees.html"><a href="chap-trees.html#classification-task"><i class="fa fa-check"></i><b>5.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="5.1.2" data-path="chap-trees.html"><a href="chap-trees.html#data"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="chap-trees.html"><a href="chap-trees.html#decision-trees"><i class="fa fa-check"></i><b>5.2</b> Decision Trees</a><ul>
<li class="chapter" data-level="5.2.1" data-path="chap-trees.html"><a href="chap-trees.html#introduction-1"><i class="fa fa-check"></i><b>5.2.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2.2" data-path="chap-knn.html"><a href="chap-knn.html#example-in-r"><i class="fa fa-check"></i><b>5.2.2</b> Example in R</a></li>
<li class="chapter" data-level="5.2.3" data-path="chap-trees.html"><a href="chap-trees.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>5.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>5.3</b> Exercises</a><ul>
<li class="chapter" data-level="5.3.1" data-path="chap-trees.html"><a href="chap-trees.html#edstats-where-girls-are-better-at-maths-than-boys"><i class="fa fa-check"></i><b>5.3.1</b> EdStats – Where Girls Are Better at Maths Than Boys?</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap-trees.html"><a href="chap-trees.html#edstats-and-world-factbook-joining-forces"><i class="fa fa-check"></i><b>5.3.2</b> EdStats and World Factbook – Joining Forces</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap-trees.html"><a href="chap-trees.html#wine-quality-random-forest-and-xgboost"><i class="fa fa-check"></i><b>5.3.3</b> Wine Quality – Random Forest and XGBoost (*)</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap-trees.html"><a href="chap-trees.html#outro"><i class="fa fa-check"></i><b>5.4</b> Outro</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html"><i class="fa fa-check"></i><b>6</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#simple-regression"><i class="fa fa-check"></i><b>6.1</b> Simple Regression</a><ul>
<li class="chapter" data-level="6.1.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>6.1.1</b> Introduction</a></li>
<li class="chapter" data-level="6.1.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>6.1.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#search-space-and-objective"><i class="fa fa-check"></i><b>6.1.3</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="chap-trees.html"><a href="chap-trees.html#introduction-1"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#solution-in-r"><i class="fa fa-check"></i><b>6.2.2</b> Solution in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#analytic-solution"><i class="fa fa-check"></i><b>6.2.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="6.2.4" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>6.2.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>6.3</b> Exercises</a><ul>
<li class="chapter" data-level="6.3.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>6.3.1</b> The Anscombe Quartet</a></li>
<li class="chapter" data-level="6.3.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#median-house-value-in-boston"><i class="fa fa-check"></i><b>6.3.2</b> Median House Value in Boston</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="chap-trees.html"><a href="chap-trees.html#outro"><i class="fa fa-check"></i><b>6.4</b> Outro</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html"><i class="fa fa-check"></i><b>7</b> Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a><ul>
<li class="chapter" data-level="7.1.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#formalism"><i class="fa fa-check"></i><b>7.1.1</b> Formalism</a></li>
<li class="chapter" data-level="7.1.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>7.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#multiple-linear-regression"><i class="fa fa-check"></i><b>7.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#problem-formulation"><i class="fa fa-check"></i><b>7.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="7.2.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>7.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#finding-the-best-model"><i class="fa fa-check"></i><b>7.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="7.3.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#model-diagnostics"><i class="fa fa-check"></i><b>7.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="7.3.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#variable-selection"><i class="fa fa-check"></i><b>7.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="7.3.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#variable-transformation"><i class="fa fa-check"></i><b>7.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="7.3.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>7.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>7.4</b> Exercises</a><ul>
<li class="chapter" data-level="7.4.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>7.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="7.4.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>7.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="7.4.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>7.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="7.4.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>7.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="7.4.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>7.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
<li class="chapter" data-level="7.4.6" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#median-house-value-in-boston-continued"><i class="fa fa-check"></i><b>7.4.6</b> Median House Value in Boston (Continued)</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="chap-trees.html"><a href="chap-trees.html#outro"><i class="fa fa-check"></i><b>7.5</b> Outro</a><ul>
<li class="chapter" data-level="7.5.1" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>7.5.1</b> Remarks</a></li>
<li class="chapter" data-level="7.5.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#other-methods-for-regression"><i class="fa fa-check"></i><b>7.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="7.5.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>7.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="7.5.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>7.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="7.5.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>7.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-logistic.html"><a href="chap-logistic.html"><i class="fa fa-check"></i><b>8</b> Classification with Linear Models</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="chap-trees.html"><a href="chap-trees.html#classification-task"><i class="fa fa-check"></i><b>8.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="8.1.2" data-path="chap-trees.html"><a href="chap-trees.html#data"><i class="fa fa-check"></i><b>8.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="chap-logistic.html"><a href="chap-logistic.html#binary-logistic-regression"><i class="fa fa-check"></i><b>8.2</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="8.2.1" data-path="chap-logistic.html"><a href="chap-logistic.html#motivation"><i class="fa fa-check"></i><b>8.2.1</b> Motivation</a></li>
<li class="chapter" data-level="8.2.2" data-path="chap-logistic.html"><a href="chap-logistic.html#logistic-model"><i class="fa fa-check"></i><b>8.2.2</b> Logistic Model</a></li>
<li class="chapter" data-level="8.2.3" data-path="chap-knn.html"><a href="chap-knn.html#example-in-r"><i class="fa fa-check"></i><b>8.2.3</b> Example in R</a></li>
<li class="chapter" data-level="8.2.4" data-path="chap-logistic.html"><a href="chap-logistic.html#loss-function-cross-entropy"><i class="fa fa-check"></i><b>8.2.4</b> Loss Function: Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>8.3</b> Exercises</a><ul>
<li class="chapter" data-level="8.3.1" data-path="chap-logistic.html"><a href="chap-logistic.html#edstats-fitting-of-binary-logistic-regression-models"><i class="fa fa-check"></i><b>8.3.1</b> EdStats – Fitting of Binary Logistic Regression Models</a></li>
<li class="chapter" data-level="8.3.2" data-path="chap-logistic.html"><a href="chap-logistic.html#edstats-variable-selection-in-binary-logistic-regression"><i class="fa fa-check"></i><b>8.3.2</b> EdStats – Variable Selection in Binary Logistic Regression (*)</a></li>
<li class="chapter" data-level="8.3.3" data-path="chap-logistic.html"><a href="chap-logistic.html#currency-exchange-rates-growthfall"><i class="fa fa-check"></i><b>8.3.3</b> Currency Exchange Rates Growth/Fall</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="chap-trees.html"><a href="chap-trees.html#outro"><i class="fa fa-check"></i><b>8.4</b> Outro</a><ul>
<li class="chapter" data-level="8.4.1" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>8.4.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html"><i class="fa fa-check"></i><b>9</b> Continuous Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="9.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#optimisation-problems"><i class="fa fa-check"></i><b>9.1.1</b> Optimisation Problems</a></li>
<li class="chapter" data-level="9.1.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>9.1.2</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="9.1.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>9.1.3</b> Example Objective over a 2D Domain</a></li>
<li class="chapter" data-level="9.1.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>9.1.4</b> Example Optimisation Problems in Machine Learning</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#iterative-methods"><i class="fa fa-check"></i><b>9.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="9.2.1" data-path="chap-trees.html"><a href="chap-trees.html#introduction-1"><i class="fa fa-check"></i><b>9.2.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2.2" data-path="chap-knn.html"><a href="chap-knn.html#example-in-r"><i class="fa fa-check"></i><b>9.2.2</b> Example in R</a></li>
<li class="chapter" data-level="9.2.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>9.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="9.2.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#random-restarts"><i class="fa fa-check"></i><b>9.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#gradient-descent"><i class="fa fa-check"></i><b>9.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="9.3.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#function-gradient"><i class="fa fa-check"></i><b>9.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="9.3.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>9.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="9.3.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>9.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="9.3.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-mnist"><i class="fa fa-check"></i><b>9.3.4</b> Example: MNIST (*)</a></li>
<li class="chapter" data-level="9.3.5" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>9.3.5</b> Stochastic Gradient Descent (SGD) (*)</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>9.4</b> A Note on Convex Optimisation (*)</a></li>
<li class="chapter" data-level="9.5" data-path="chap-trees.html"><a href="chap-trees.html#outro"><i class="fa fa-check"></i><b>9.5</b> Outro</a><ul>
<li class="chapter" data-level="9.5.1" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>9.5.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-kmeans.html"><a href="chap-kmeans.html"><i class="fa fa-check"></i><b>10</b> Clustering with K-Means</a><ul>
<li class="chapter" data-level="10.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#k-means-clustering"><i class="fa fa-check"></i><b>10.1</b> K-means Clustering</a><ul>
<li class="chapter" data-level="10.1.1" data-path="chap-knn.html"><a href="chap-knn.html#example-in-r"><i class="fa fa-check"></i><b>10.1.1</b> Example in R</a></li>
<li class="chapter" data-level="10.1.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#problem-statement"><i class="fa fa-check"></i><b>10.1.2</b> Problem Statement</a></li>
<li class="chapter" data-level="10.1.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>10.1.3</b> Algorithms for the K-means Problem</a></li>
<li class="chapter" data-level="10.1.4" data-path="chap-kmeans.html"><a href="chap-kmeans.html#k-means-revisited"><i class="fa fa-check"></i><b>10.1.4</b> K-means Revisited</a></li>
<li class="chapter" data-level="10.1.5" data-path="chap-kmeans.html"><a href="chap-kmeans.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>10.1.5</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>10.2</b> Exercises</a><ul>
<li class="chapter" data-level="10.2.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#clustering-of-the-world-factbook"><i class="fa fa-check"></i><b>10.2.1</b> Clustering of the World Factbook</a></li>
<li class="chapter" data-level="10.2.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#unbalance-dataset-k-means-needs-multiple-starts"><i class="fa fa-check"></i><b>10.2.2</b> Unbalance Dataset – K-Means Needs Multiple Starts</a></li>
<li class="chapter" data-level="10.2.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#clustering-of-typical-2d-benchmark-datasets"><i class="fa fa-check"></i><b>10.2.3</b> Clustering of Typical 2D Benchmark Datasets</a></li>
<li class="chapter" data-level="10.2.4" data-path="chap-kmeans.html"><a href="chap-kmeans.html#wine-quality-volatile.acidity-and-sulphates"><i class="fa fa-check"></i><b>10.2.4</b> Wine Quality – <code>volatile.acidity</code> and <code>sulphates</code></a></li>
<li class="chapter" data-level="10.2.5" data-path="chap-kmeans.html"><a href="chap-kmeans.html#wine-quality-chlorides-and-total.sulfur.dioxide"><i class="fa fa-check"></i><b>10.2.5</b> Wine Quality – <code>chlorides</code> and <code>total.sulfur.dioxide</code></a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chap-trees.html"><a href="chap-trees.html#outro"><i class="fa fa-check"></i><b>10.3</b> Outro</a><ul>
<li class="chapter" data-level="10.3.1" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>10.3.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html"><i class="fa fa-check"></i><b>11</b> Discrete Optimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#recap"><i class="fa fa-check"></i><b>11.1.1</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="chap-trees.html"><a href="chap-trees.html#outro"><i class="fa fa-check"></i><b>11.2</b> Outro</a><ul>
<li class="chapter" data-level="11.2.1" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>11.2.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-feature-selection.html"><a href="chap-feature-selection.html"><i class="fa fa-check"></i><b>12</b> Feature Selection</a><ul>
<li class="chapter" data-level="12.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>12.1</b> Introduction</a><ul>
<li class="chapter" data-level="12.1.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#recap"><i class="fa fa-check"></i><b>12.1.1</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="chap-trees.html"><a href="chap-trees.html#outro"><i class="fa fa-check"></i><b>12.2</b> Outro</a><ul>
<li class="chapter" data-level="12.2.1" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>12.2.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="chap-images.html"><a href="chap-images.html"><i class="fa fa-check"></i><b>13</b> Shallow and Deep Neural Networks</a><ul>
<li class="chapter" data-level="13.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a><ul>
<li class="chapter" data-level="13.1.1" data-path="chap-images.html"><a href="chap-images.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>13.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="13.1.2" data-path="chap-trees.html"><a href="chap-trees.html#data"><i class="fa fa-check"></i><b>13.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="chap-images.html"><a href="chap-images.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>13.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="13.2.1" data-path="chap-images.html"><a href="chap-images.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>13.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="13.2.2" data-path="chap-images.html"><a href="chap-images.html#extending-logistic-regression"><i class="fa fa-check"></i><b>13.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="13.2.3" data-path="chap-images.html"><a href="chap-images.html#softmax-function"><i class="fa fa-check"></i><b>13.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="13.2.4" data-path="chap-images.html"><a href="chap-images.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>13.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="13.2.5" data-path="chap-images.html"><a href="chap-images.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>13.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="13.2.6" data-path="chap-images.html"><a href="chap-images.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>13.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="chap-images.html"><a href="chap-images.html#artificial-neural-networks"><i class="fa fa-check"></i><b>13.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="13.3.1" data-path="chap-images.html"><a href="chap-images.html#artificial-neuron"><i class="fa fa-check"></i><b>13.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="13.3.2" data-path="chap-images.html"><a href="chap-images.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>13.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="13.3.3" data-path="chap-knn.html"><a href="chap-knn.html#example-in-r"><i class="fa fa-check"></i><b>13.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="chap-images.html"><a href="chap-images.html#deep-neural-networks"><i class="fa fa-check"></i><b>13.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="13.4.1" data-path="chap-trees.html"><a href="chap-trees.html#introduction-1"><i class="fa fa-check"></i><b>13.4.1</b> Introduction</a></li>
<li class="chapter" data-level="13.4.2" data-path="chap-images.html"><a href="chap-images.html#activation-functions"><i class="fa fa-check"></i><b>13.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="13.4.3" data-path="chap-images.html"><a href="chap-images.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>13.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="13.4.4" data-path="chap-images.html"><a href="chap-images.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>13.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="chap-images.html"><a href="chap-images.html#preprocessing-of-data"><i class="fa fa-check"></i><b>13.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="13.5.1" data-path="chap-images.html"><a href="chap-images.html#introduction-2"><i class="fa fa-check"></i><b>13.5.1</b> Introduction</a></li>
<li class="chapter" data-level="13.5.2" data-path="chap-images.html"><a href="chap-images.html#image-deskewing"><i class="fa fa-check"></i><b>13.5.2</b> Image Deskewing</a></li>
<li class="chapter" data-level="13.5.3" data-path="chap-images.html"><a href="chap-images.html#summary-of-all-the-models-considered"><i class="fa fa-check"></i><b>13.5.3</b> Summary of All the Models Considered</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="chap-trees.html"><a href="chap-trees.html#outro"><i class="fa fa-check"></i><b>13.6</b> Outro</a><ul>
<li class="chapter" data-level="13.6.1" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>13.6.1</b> Remarks</a></li>
<li class="chapter" data-level="13.6.2" data-path="chap-images.html"><a href="chap-images.html#beyond-mnist"><i class="fa fa-check"></i><b>13.6.2</b> Beyond MNIST</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="chap-recommenders.html"><a href="chap-recommenders.html"><i class="fa fa-check"></i><b>14</b> Recommender Systems</a><ul>
<li class="chapter" data-level="14.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>14.1</b> Introduction</a><ul>
<li class="chapter" data-level="14.1.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#the-netflix-prize"><i class="fa fa-check"></i><b>14.1.1</b> The Netflix Prize</a></li>
<li class="chapter" data-level="14.1.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#main-approaches"><i class="fa fa-check"></i><b>14.1.2</b> Main Approaches</a></li>
<li class="chapter" data-level="14.1.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#formalism"><i class="fa fa-check"></i><b>14.1.3</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#collaborative-filtering"><i class="fa fa-check"></i><b>14.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="14.2.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#example"><i class="fa fa-check"></i><b>14.2.1</b> Example</a></li>
<li class="chapter" data-level="14.2.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#similarity-measures"><i class="fa fa-check"></i><b>14.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="14.2.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>14.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="14.2.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>14.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#exercise-the-movielens-dataset"><i class="fa fa-check"></i><b>14.3</b> Exercise: The MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="14.3.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#dataset"><i class="fa fa-check"></i><b>14.3.1</b> Dataset</a></li>
<li class="chapter" data-level="14.3.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#data-cleansing"><i class="fa fa-check"></i><b>14.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="14.3.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#item-item-similarities"><i class="fa fa-check"></i><b>14.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="14.3.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#example-recommendations"><i class="fa fa-check"></i><b>14.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="14.3.5" data-path="chap-introduction.html"><a href="chap-introduction.html#clustering"><i class="fa fa-check"></i><b>14.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="chap-trees.html"><a href="chap-trees.html#outro"><i class="fa fa-check"></i><b>14.4</b> Outro</a><ul>
<li class="chapter" data-level="14.4.1" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>14.4.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="chap-text.html"><a href="chap-text.html"><i class="fa fa-check"></i><b>15</b> Natural Language Processing</a><ul>
<li class="chapter" data-level="15.1" data-path="chap-text.html"><a href="chap-text.html#to-do"><i class="fa fa-check"></i><b>15.1</b> TO DO</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-convention.html"><a href="appendix-convention.html"><i class="fa fa-check"></i><b>A</b> Notation Convention</a></li>
<li class="chapter" data-level="B" data-path="appendix-rintro.html"><a href="appendix-rintro.html"><i class="fa fa-check"></i><b>B</b> Setting Up the R Environment</a><ul>
<li class="chapter" data-level="B.1" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-r"><i class="fa fa-check"></i><b>B.1</b> Installing R</a></li>
<li class="chapter" data-level="B.2" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-an-ide"><i class="fa fa-check"></i><b>B.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="B.3" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-recommended-packages"><i class="fa fa-check"></i><b>B.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="B.4" data-path="appendix-rintro.html"><a href="appendix-rintro.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>B.4</b> First R Script in RStudio</a></li>
<li class="chapter" data-level="B.5" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>B.5</b> Exercises</a><ul>
<li class="chapter" data-level="B.5.1" data-path="appendix-rintro.html"><a href="appendix-rintro.html#first-steps-with-vectors"><i class="fa fa-check"></i><b>B.5.1</b> First Steps with Vectors</a></li>
<li class="chapter" data-level="B.5.2" data-path="appendix-rintro.html"><a href="appendix-rintro.html#basic-plotting"><i class="fa fa-check"></i><b>B.5.2</b> Basic Plotting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appendix-rvector.html"><a href="appendix-rvector.html"><i class="fa fa-check"></i><b>C</b> Vector Algebra in R</a><ul>
<li class="chapter" data-level="C.1" data-path="chap-logistic.html"><a href="chap-logistic.html#motivation"><i class="fa fa-check"></i><b>C.1</b> Motivation</a></li>
<li class="chapter" data-level="C.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#numeric-vectors"><i class="fa fa-check"></i><b>C.2</b> Numeric Vectors</a><ul>
<li class="chapter" data-level="C.2.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>C.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="C.2.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-vector-operations"><i class="fa fa-check"></i><b>C.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#aggregation-functions"><i class="fa fa-check"></i><b>C.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="C.2.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#special-functions"><i class="fa fa-check"></i><b>C.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="C.2.6" data-path="appendix-rvector.html"><a href="appendix-rvector.html#norms-and-distances"><i class="fa fa-check"></i><b>C.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="C.2.7" data-path="appendix-rvector.html"><a href="appendix-rvector.html#dot-product"><i class="fa fa-check"></i><b>C.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="C.2.8" data-path="appendix-rvector.html"><a href="appendix-rvector.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>C.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#logical-vectors"><i class="fa fa-check"></i><b>C.3</b> Logical Vectors</a><ul>
<li class="chapter" data-level="C.3.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-logical-vectors"><i class="fa fa-check"></i><b>C.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="C.3.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#logical-operations"><i class="fa fa-check"></i><b>C.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="C.3.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#comparison-operations"><i class="fa fa-check"></i><b>C.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="C.3.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#aggregation-functions-1"><i class="fa fa-check"></i><b>C.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#character-vectors"><i class="fa fa-check"></i><b>C.4</b> Character Vectors</a><ul>
<li class="chapter" data-level="C.4.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-character-vectors"><i class="fa fa-check"></i><b>C.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="C.4.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>C.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="C.4.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>C.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-subsetting"><i class="fa fa-check"></i><b>C.5</b> Vector Subsetting</a><ul>
<li class="chapter" data-level="C.5.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>C.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="C.5.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>C.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="C.5.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>C.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="C.5.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#replacing-elements"><i class="fa fa-check"></i><b>C.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="C.5.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#other-functions"><i class="fa fa-check"></i><b>C.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="appendix-rvector.html"><a href="appendix-rvector.html#named-vectors"><i class="fa fa-check"></i><b>C.6</b> Named Vectors</a><ul>
<li class="chapter" data-level="C.6.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-named-vectors"><i class="fa fa-check"></i><b>C.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="C.6.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>C.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="C.7" data-path="appendix-rvector.html"><a href="appendix-rvector.html#sec:factor"><i class="fa fa-check"></i><b>C.7</b> Factors</a><ul>
<li class="chapter" data-level="C.7.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-factors"><i class="fa fa-check"></i><b>C.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="C.7.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#levels"><i class="fa fa-check"></i><b>C.7.2</b> Levels</a></li>
<li class="chapter" data-level="C.7.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#internal-representation"><i class="fa fa-check"></i><b>C.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.8" data-path="appendix-rvector.html"><a href="appendix-rvector.html#lists"><i class="fa fa-check"></i><b>C.8</b> Lists</a><ul>
<li class="chapter" data-level="C.8.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-lists"><i class="fa fa-check"></i><b>C.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="C.8.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#named-lists"><i class="fa fa-check"></i><b>C.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="C.8.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>C.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="C.8.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#common-operations"><i class="fa fa-check"></i><b>C.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>C.9</b> Exercises</a><ul>
<li class="chapter" data-level="C.9.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#audeur-exchange-rates"><i class="fa fa-check"></i><b>C.9.1</b> AUD/EUR Exchange Rates</a></li>
</ul></li>
<li class="chapter" data-level="C.10" data-path="appendix-rvector.html"><a href="appendix-rvector.html#further-reading"><i class="fa fa-check"></i><b>C.10</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html"><i class="fa fa-check"></i><b>D</b> Matrix Algebra in R</a><ul>
<li class="chapter" data-level="D.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#creating-matrices"><i class="fa fa-check"></i><b>D.1</b> Creating Matrices</a><ul>
<li class="chapter" data-level="D.1.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix"><i class="fa fa-check"></i><b>D.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="D.1.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#stacking-vectors"><i class="fa fa-check"></i><b>D.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="D.1.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>D.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="D.1.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>D.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="D.1.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#other-methods"><i class="fa fa-check"></i><b>D.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="D.1.6" data-path="appendix-rvector.html"><a href="appendix-rvector.html#internal-representation"><i class="fa fa-check"></i><b>D.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#common-operations"><i class="fa fa-check"></i><b>D.2</b> Common Operations</a><ul>
<li class="chapter" data-level="D.2.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-transpose"><i class="fa fa-check"></i><b>D.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="D.2.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>D.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="D.2.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>D.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="D.2.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-multiplication"><i class="fa fa-check"></i><b>D.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="D.2.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>D.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="D.2.6" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#vectorised-special-functions"><i class="fa fa-check"></i><b>D.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="D.2.7" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-vector-operations"><i class="fa fa-check"></i><b>D.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-subsetting"><i class="fa fa-check"></i><b>D.3</b> Matrix Subsetting</a><ul>
<li class="chapter" data-level="D.3.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-individual-elements"><i class="fa fa-check"></i><b>D.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="D.3.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>D.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="D.3.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-submatrices"><i class="fa fa-check"></i><b>D.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="D.3.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>D.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="D.3.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>D.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>D.4</b> Exercises</a><ul>
<li class="chapter" data-level="D.4.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#currency-exchange-rates"><i class="fa fa-check"></i><b>D.4.1</b> Currency Exchange Rates</a></li>
<li class="chapter" data-level="D.4.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#currency-exchange-rates-relative-to-1999"><i class="fa fa-check"></i><b>D.4.2</b> Currency Exchange Rates Relative to 1999</a></li>
</ul></li>
<li class="chapter" data-level="D.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#further-reading"><i class="fa fa-check"></i><b>D.5</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="appendix-rdf.html"><a href="appendix-rdf.html"><i class="fa fa-check"></i><b>E</b> Data Frame Wrangling in R</a><ul>
<li class="chapter" data-level="E.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#creating-data-frames"><i class="fa fa-check"></i><b>E.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="E.2" data-path="appendix-rdf.html"><a href="appendix-rdf.html#importing-data-frames"><i class="fa fa-check"></i><b>E.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="E.3" data-path="appendix-rdf.html"><a href="appendix-rdf.html#data-frame-subsetting"><i class="fa fa-check"></i><b>E.3</b> Data Frame Subsetting</a><ul>
<li class="chapter" data-level="E.3.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>E.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="E.3.2" data-path="appendix-rdf.html"><a href="appendix-rdf.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>E.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#common-operations"><i class="fa fa-check"></i><b>E.4</b> Common Operations</a></li>
<li class="chapter" data-level="E.5" data-path="appendix-rdf.html"><a href="appendix-rdf.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>E.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="E.6" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>E.6</b> Exercises</a><ul>
<li class="chapter" data-level="E.6.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#urban-forest"><i class="fa fa-check"></i><b>E.6.1</b> Urban Forest</a></li>
</ul></li>
<li class="chapter" data-level="E.7" data-path="appendix-rdf.html"><a href="appendix-rdf.html#air-quality"><i class="fa fa-check"></i><b>E.7</b> Air Quality</a></li>
<li class="chapter" data-level="E.8" data-path="appendix-rvector.html"><a href="appendix-rvector.html#further-reading"><i class="fa fa-check"></i><b>E.8</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="appendix-datasets.html"><a href="appendix-datasets.html"><i class="fa fa-check"></i><b>F</b> Datasets</a><ul>
<li class="chapter" data-level="F.1" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:ssi"><i class="fa fa-check"></i><b>F.1</b> Sustainable Society Indices</a></li>
<li class="chapter" data-level="F.2" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:air-quality"><i class="fa fa-check"></i><b>F.2</b> Air Quality</a></li>
<li class="chapter" data-level="F.3" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:currency-exchange"><i class="fa fa-check"></i><b>F.3</b> Currency Exchange Rates</a></li>
<li class="chapter" data-level="F.4" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:urban-forest"><i class="fa fa-check"></i><b>F.4</b> Urban Forest</a></li>
<li class="chapter" data-level="F.5" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:wine-quality"><i class="fa fa-check"></i><b>F.5</b> Wine Quality</a></li>
<li class="chapter" data-level="F.6" data-path="appendix-datasets.html"><a href="appendix-datasets.html#the-world-factbook-countries-of-the-world"><i class="fa fa-check"></i><b>F.6</b> The World Factbook (Countries of the World)</a></li>
<li class="chapter" data-level="F.7" data-path="appendix-datasets.html"><a href="appendix-datasets.html#edstats-country-level-education-statistics"><i class="fa fa-check"></i><b>F.7</b> EdStats (Country-Level Education Statistics)</a></li>
<li class="chapter" data-level="F.8" data-path="appendix-datasets.html"><a href="appendix-datasets.html#food-and-nutrient-database-for-dietary-studies-fndds"><i class="fa fa-check"></i><b>F.8</b> Food and Nutrient Database for Dietary Studies (FNDDS)</a></li>
<li class="chapter" data-level="F.9" data-path="appendix-datasets.html"><a href="appendix-datasets.html#clustering-benchmarks"><i class="fa fa-check"></i><b>F.9</b> Clustering Benchmarks</a></li>
<li class="chapter" data-level="F.10" data-path="appendix-datasets.html"><a href="appendix-datasets.html#movie-lens-todo"><i class="fa fa-check"></i><b>F.10</b> Movie Lens (TODO)</a></li>
<li class="chapter" data-level="F.11" data-path="appendix-datasets.html"><a href="appendix-datasets.html#other-todo"><i class="fa fa-check"></i><b>F.11</b> Other (TODO)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.3 2020-08-12 16:16 (936944a)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap:trees" class="section level1">
<h1><span class="header-section-number">5</span> Classification with Decision Trees</h1>
<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->
<p><strong>TODO</strong> In this chapter, we will:</p>
<ul>
<li><p>…</p></li>
<li><p>…</p></li>
</ul>
<p>data generators – why 100% accuracy might not be possible,
why we need statistics or information theory (amongst others)
<span class="citation">(Devroye et al. <a href="references.html#ref-probtheorypatrecog">1996</a>)</span>, <span class="citation">(Blum et al. <a href="references.html#ref-foundds">2020</a>)</span> – research in machine learning
vs. research with machine learning (citations! - asymptotics etc.)
– 2 normal distributions, non-separable illustration</p>
<p>in <span class="citation">(Devroye et al. <a href="references.html#ref-probtheorypatrecog">1996</a>)</span> presented are several interesting results
of the asymptotic behaviour of the K-NN classifier when <span class="math inline">\(K\to\infty\)</span>
and <span class="math inline">\(K/n\to\infty\)</span> as <span class="math inline">\(n\to\infty\)</span> (e.g., both training sample size
<span class="math inline">\(n\)</span> and <span class="math inline">\(K\)</span> grow
but <span class="math inline">\(n\)</span> grows at least an order of magnitude faster than <span class="math inline">\(K\)</span>)</p>
<p>maths can be used to answer questions about the general behaviour of the method:
“it is always true that”, “with probability XX it holds”, “if XXX, then the expected is YYY” etc.</p>
<p>prediction vs. description</p>
<p>decision trees – towards models (explanation)</p>
<!-- TODO: citations

*Random Forests* and *XGBoost* (see also: *AdaBoost*)
SVMs
-->
<div id="introduction" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<div id="classification-task" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Classification Task</h3>
<p>Let <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span> be an input matrix
that consists of <span class="math inline">\(n\)</span> points in a <span class="math inline">\(p\)</span>-dimensional space (each of the <span class="math inline">\(n\)</span> objects
is described by means of <span class="math inline">\(p\)</span> numerical features)</p>
<p>Recall that in supervised learning, with each
<span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> we associate the desired output <span class="math inline">\(y_i\)</span>.</p>
<p>Hence, our dataset is <span class="math inline">\([\mathbf{X}\ \mathbf{y}]\)</span> –
where each object is represented as a row vector
<span class="math inline">\([\mathbf{x}_{i,\cdot}\ y_i]\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>:</p>
<p><span class="math display">\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p} &amp; y_1\\
x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p} &amp; y_2\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots    &amp; \vdots\\
x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p} &amp; y_n\\
\end{array}
\right].
\]</span></p>
<div style="margin-top: 1em">

</div>
<p>In this chapter we are still interested in <strong>classification</strong> tasks;
we assume that each <span class="math inline">\(y_i\)</span> is a descriptive label.</p>
<p>Let’s assume that we are faced with <strong>binary classification</strong> tasks.</p>
<p>Hence, there are only two possible labels that we traditionally denote with <span class="math inline">\(0\)</span>s and <span class="math inline">\(1\)</span>s.</p>
<p>For example:</p>
<table>
<thead>
<tr class="header">
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>no</td>
<td>yes</td>
</tr>
<tr class="even">
<td>false</td>
<td>true</td>
</tr>
<tr class="odd">
<td>failure</td>
<td>success</td>
</tr>
<tr class="even">
<td>healthy</td>
<td>ill</td>
</tr>
</tbody>
</table>
<p>Let’s recall the synthetic 2D dataset from the previous chapter
(true decision boundary is at <span class="math inline">\(X_1=0\)</span>), see Figure <a href="chap-trees.html#fig:synthetic">5.1</a>.</p>
<div class="figure"><span id="fig:synthetic"></span>
<img src="040-trees-figures/synthetic-1.svg" alt=" A synthetic 2D dataset with the true decision boundary at X_1=0" />
<p class="caption">Figure 5.1:  A synthetic 2D dataset with the true decision boundary at <span class="math inline">\(X_1=0\)</span></p>
</div>
</div>
<div id="data" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Data</h3>
<p>For illustration, we’ll be considering the Wine Quality dataset
(white wines only):</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb163-1" title="1">wine_quality &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/wine_quality_all.csv&quot;</span>,</a>
<a class="sourceLine" id="cb163-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a>
<a class="sourceLine" id="cb163-3" title="3">white_wines &lt;-<span class="st"> </span>wine_quality[wine_quality<span class="op">$</span>color <span class="op">==</span><span class="st"> &quot;white&quot;</span>,]</a>
<a class="sourceLine" id="cb163-4" title="4">(n &lt;-<span class="st"> </span><span class="kw">nrow</span>(white_wines)) <span class="co"># number of samples</span></a></code></pre></div>
<pre><code>## [1] 4898</code></pre>
<p>The input matrix <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span>
consists of the first 10 numeric variables:</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb165-1" title="1">X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(white_wines[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])</a>
<a class="sourceLine" id="cb165-2" title="2"><span class="kw">dim</span>(X)</a></code></pre></div>
<pre><code>## [1] 4898   10</code></pre>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb167-1" title="1"><span class="kw">head</span>(X, <span class="dv">2</span>) <span class="co"># first two rows</span></a></code></pre></div>
<pre><code>##      fixed.acidity volatile.acidity citric.acid residual.sugar
## 1600           7.0             0.27        0.36           20.7
## 1601           6.3             0.30        0.34            1.6
##      chlorides free.sulfur.dioxide total.sulfur.dioxide density  pH
## 1600     0.045                  45                  170   1.001 3.0
## 1601     0.049                  14                  132   0.994 3.3
##      sulphates
## 1600      0.45
## 1601      0.49</code></pre>
<p>The 11th variable measures the amount of alcohol (in %).</p>
<p>We will convert this dependent variable to a binary one:</p>
<ul>
<li>0 == (<code>alcohol  &lt; 12</code>) == lower-alcohol wines,</li>
<li>1 == (<code>alcohol &gt;= 12</code>) == higher-alcohol wines</li>
</ul>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb169-1" title="1"><span class="co"># recall that TRUE == 1</span></a>
<a class="sourceLine" id="cb169-2" title="2">Y &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(<span class="kw">as.numeric</span>(white_wines<span class="op">$</span>alcohol <span class="op">&gt;=</span><span class="st"> </span><span class="dv">12</span>)))</a>
<a class="sourceLine" id="cb169-3" title="3"><span class="kw">table</span>(Y)</a></code></pre></div>
<pre><code>## Y
##    0    1 
## 4085  813</code></pre>
<p>60/40% train-test split:</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb171-1" title="1"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># reproducibility matters</span></a>
<a class="sourceLine" id="cb171-2" title="2">random_indices &lt;-<span class="st"> </span><span class="kw">sample</span>(n)</a>
<a class="sourceLine" id="cb171-3" title="3"><span class="kw">head</span>(random_indices) <span class="co"># preview</span></a></code></pre></div>
<pre><code>## [1] 2463 2511 2227  526 4291 2986</code></pre>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb173-1" title="1"><span class="co"># first 60% of the indices (they are arranged randomly)</span></a>
<a class="sourceLine" id="cb173-2" title="2"><span class="co"># will constitute the train sample:</span></a>
<a class="sourceLine" id="cb173-3" title="3">train_indices &lt;-<span class="st"> </span>random_indices[<span class="dv">1</span><span class="op">:</span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.6</span>)]</a>
<a class="sourceLine" id="cb173-4" title="4">X_train &lt;-<span class="st"> </span>X[train_indices,]</a>
<a class="sourceLine" id="cb173-5" title="5">Y_train &lt;-<span class="st"> </span>Y[train_indices]</a>
<a class="sourceLine" id="cb173-6" title="6"><span class="co"># the remaining indices (40%) go to the test sample:</span></a>
<a class="sourceLine" id="cb173-7" title="7">X_test  &lt;-<span class="st"> </span>X[<span class="op">-</span>train_indices,]</a>
<a class="sourceLine" id="cb173-8" title="8">Y_test  &lt;-<span class="st"> </span>Y[<span class="op">-</span>train_indices]</a></code></pre></div>
<p>Let’s also compute <code>Z_train</code> and <code>Z_test</code>, being the standardised versions of <code>X_train</code>
and <code>X_test</code>, respectively.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb174-1" title="1">means &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">2</span>, mean) <span class="co"># column means</span></a>
<a class="sourceLine" id="cb174-2" title="2">sds   &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">2</span>, sd)   <span class="co"># column standard deviations</span></a>
<a class="sourceLine" id="cb174-3" title="3">Z_train &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(X_train, <span class="dv">1</span>, <span class="cf">function</span>(r) (r<span class="op">-</span>means)<span class="op">/</span>sds))</a>
<a class="sourceLine" id="cb174-4" title="4">Z_test  &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(X_test,  <span class="dv">1</span>, <span class="cf">function</span>(r) (r<span class="op">-</span>means)<span class="op">/</span>sds))</a></code></pre></div>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb175-1" title="1">get_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(Y_pred, Y_test)</a>
<a class="sourceLine" id="cb175-2" title="2">{</a>
<a class="sourceLine" id="cb175-3" title="3">    C &lt;-<span class="st"> </span><span class="kw">table</span>(Y_pred, Y_test) <span class="co"># confusion matrix</span></a>
<a class="sourceLine" id="cb175-4" title="4">    <span class="kw">stopifnot</span>(<span class="kw">dim</span>(C) <span class="op">==</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb175-5" title="5">    <span class="kw">c</span>(<span class="dt">Acc=</span>(C[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span><span class="kw">sum</span>(C),  <span class="co"># accuracy</span></a>
<a class="sourceLine" id="cb175-6" title="6">      <span class="dt">Prec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># precision</span></a>
<a class="sourceLine" id="cb175-7" title="7">      <span class="dt">Rec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">1</span>,<span class="dv">2</span>]),  <span class="co"># recall</span></a>
<a class="sourceLine" id="cb175-8" title="8">      <span class="dt">F=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># F-measure</span></a>
<a class="sourceLine" id="cb175-9" title="9">      <span class="co"># Confusion matrix items:</span></a>
<a class="sourceLine" id="cb175-10" title="10">      <span class="dt">TN=</span>C[<span class="dv">1</span>,<span class="dv">1</span>], <span class="dt">FN=</span>C[<span class="dv">1</span>,<span class="dv">2</span>],</a>
<a class="sourceLine" id="cb175-11" title="11">      <span class="dt">FP=</span>C[<span class="dv">2</span>,<span class="dv">1</span>], <span class="dt">TP=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb175-12" title="12">    ) <span class="co"># return a named vector</span></a>
<a class="sourceLine" id="cb175-13" title="13">}</a></code></pre></div>
</div>
</div>
<div id="decision-trees" class="section level2">
<h2><span class="header-section-number">5.2</span> Decision Trees</h2>
<div id="introduction-1" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Introduction</h3>
<p>Note that a K-NN classifier discussed in the previous chapter
is <strong>model-free</strong>.
The whole training set must be stored and referred to at all times.</p>
<p>Therefore, it doesn’t <em>explain</em> the data we have – we may use it solely
for the purpose of <em>prediction</em>.</p>
<p>Perhaps one of the most interpretable (and hence human-friendly) models
consist of decision rules of the form:</p>
<p><strong>IF <span class="math inline">\(x_{i,j_1}\le v_1\)</span> AND … AND <span class="math inline">\(x_{i,j_r}\le v_r\)</span> THEN <span class="math inline">\(\hat{y}_i=1\)</span>.</strong></p>
<p>These can be organised into a <strong>hierarchy</strong> for greater readability.</p>
<p>This idea inspired the notion of <strong>decision trees</strong> <span class="citation">(Breiman et al. <a href="references.html#ref-cart">1984</a>)</span>.</p>
<div class="figure">
<img src="040-trees-figures/plot_rpart-1.svg" alt="(#fig:plot_rpart) The simplest decision tree for the synthetic 2D dataset and the corresponding decision boundaries" />
<p class="caption">(#fig:plot_rpart) The simplest decision tree for the synthetic 2D dataset and the corresponding decision boundaries</p>
</div>
<p>Figure @ref(fig:plot_rpart2) depicts a very simple decision tree
for the aforementioned synthetic dataset.
There is only one decision boundary (based on <span class="math inline">\(X_1\)</span>) that splits
data into the “left” and “right” sides.
Each tree node reports 3 pieces of information:</p>
<ul>
<li>dominating class (0 or 1)</li>
<li>(relative) proportion of 1s represented in a node</li>
<li>(absolute) proportion of all observations in a node</li>
</ul>
<p>Figures @ref(fig:plot_rpart2) and @ref(fig:plot_rpart3) depict
trees with more decision rules.
Take a moment to contemplate how the corresponding decision boundaries
changed with the introduction of new decision rules.</p>
<div class="figure">
<img src="040-trees-figures/plot_rpart2-1.svg" alt="(#fig:plot_rpart2) A more complicated decision tree for the synthetic 2D dataset and the corresponding decision boundaries" />
<p class="caption">(#fig:plot_rpart2) A more complicated decision tree for the synthetic 2D dataset and the corresponding decision boundaries</p>
</div>
<div class="figure">
<img src="040-trees-figures/plot_rpart3-1.svg" alt="(#fig:plot_rpart3) An even more complicated decision tree for the synthetic 2D dataset and the corresponding decision boundaries" />
<p class="caption">(#fig:plot_rpart3) An even more complicated decision tree for the synthetic 2D dataset and the corresponding decision boundaries</p>
</div>
</div>
<div id="example-in-r" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Example in R</h3>
<p>We will use the <code>rpart()</code> function from the <code>rpart</code> package
to build a classification tree.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb176-1" title="1"><span class="kw">library</span>(<span class="st">&quot;rpart&quot;</span>)</a>
<a class="sourceLine" id="cb176-2" title="2"><span class="kw">library</span>(<span class="st">&quot;rpart.plot&quot;</span>)</a>
<a class="sourceLine" id="cb176-3" title="3"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a></code></pre></div>
<p><code>rpart()</code> uses a formula (<code>~</code>) interface, hence it will be easier
to feed it with data in a data.frame form.</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb177-1" title="1">XY_train &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.data.frame</span>(X_train), <span class="dt">Y=</span>Y_train)</a>
<a class="sourceLine" id="cb177-2" title="2">XY_test &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">as.data.frame</span>(X_test), <span class="dt">Y=</span>Y_test)</a></code></pre></div>
<p>Fit and plot a decision tree, see Figure @ref(fig:plot_rpart1).</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" title="1">t1 &lt;-<span class="st"> </span><span class="kw">rpart</span>(Y<span class="op">~</span>., <span class="dt">data=</span>XY_train, <span class="dt">method=</span><span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb178-2" title="2"><span class="kw">rpart.plot</span>(t1, <span class="dt">tweak=</span><span class="fl">1.1</span>, <span class="dt">fallen.leaves=</span><span class="ot">FALSE</span>, <span class="dt">digits=</span><span class="dv">3</span>)</a></code></pre></div>
<div class="figure">
<img src="040-trees-figures/plot_rpart1-1.svg" alt="(#fig:plot_rpart1) A decision tree for the white_wines dataset" />
<p class="caption">(#fig:plot_rpart1) A decision tree for the <code>white_wines</code> dataset</p>
</div>
<p>We can build less or more complex trees by playing
with the <code>cp</code> parameter, see Figures @ref(fig:plot_rpart222)
and <a href="chap-trees.html#fig:tree333">5.2</a>.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb179-1" title="1"><span class="co"># cp = complexity parameter, smaller → more complex tree</span></a>
<a class="sourceLine" id="cb179-2" title="2">t2 &lt;-<span class="st"> </span><span class="kw">rpart</span>(Y<span class="op">~</span>., <span class="dt">data=</span>XY_train, <span class="dt">method=</span><span class="st">&quot;class&quot;</span>, <span class="dt">cp=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb179-3" title="3"><span class="kw">rpart.plot</span>(t2, <span class="dt">tweak=</span><span class="fl">1.1</span>, <span class="dt">fallen.leaves=</span><span class="ot">FALSE</span>, <span class="dt">digits=</span><span class="dv">3</span>)</a></code></pre></div>
<div class="figure">
<img src="040-trees-figures/plot_rpart222-1.svg" alt="(#fig:plot_rpart222) A (simpler) decision tree for the white_wines dataset" />
<p class="caption">(#fig:plot_rpart222) A (simpler) decision tree for the <code>white_wines</code> dataset</p>
</div>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb180-1" title="1"><span class="co"># cp = complexity parameter, smaller → more complex tree</span></a>
<a class="sourceLine" id="cb180-2" title="2">t3 &lt;-<span class="st"> </span><span class="kw">rpart</span>(Y<span class="op">~</span>., <span class="dt">data=</span>XY_train, <span class="dt">method=</span><span class="st">&quot;class&quot;</span>, <span class="dt">cp=</span><span class="fl">0.00001</span>)</a>
<a class="sourceLine" id="cb180-3" title="3"><span class="kw">rpart.plot</span>(t3, <span class="dt">tweak=</span><span class="fl">1.1</span>, <span class="dt">fallen.leaves=</span><span class="ot">FALSE</span>, <span class="dt">digits=</span><span class="dv">3</span>)</a></code></pre></div>
<div class="figure"><span id="fig:tree333"></span>
<img src="040-trees-figures/tree333-1.svg" alt=" A (more complex) decision tree for the white_wines dataset" />
<p class="caption">Figure 5.2:  A (more complex) decision tree for the <code>white_wines</code> dataset</p>
</div>
<p>Trees with few decision rules actually are very nicely interpretable.
On the other hand, plotting of the complex ones is just hopeless;
we should treat them as “black boxes” instead.</p>
<!--
The fitted model is rather... simple.

Only the `alcohol` variable is taken into account.


Well note how these two distributions are shifted:


```r
#    horizontal=TRUE)
```





-->
<p>Let’s make some predictions:</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb181-1" title="1">Y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(t1, XY_test, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb181-2" title="2"><span class="kw">get_metrics</span>(Y_pred, Y_test)</a></code></pre></div>
<pre><code>##        Acc       Prec        Rec          F         TN         FN 
##    0.92857    0.80623    0.73502    0.76898 1587.00000   84.00000 
##         FP         TP 
##   56.00000  233.00000</code></pre>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" title="1">Y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(t2, XY_test, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb183-2" title="2"><span class="kw">get_metrics</span>(Y_pred, Y_test)</a></code></pre></div>
<pre><code>##        Acc       Prec        Rec          F         TN         FN 
##    0.90255    0.83871    0.49211    0.62028 1613.00000  161.00000 
##         FP         TP 
##   30.00000  156.00000</code></pre>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb185-1" title="1">Y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(t3, XY_test, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb185-2" title="2"><span class="kw">get_metrics</span>(Y_pred, Y_test)</a></code></pre></div>
<pre><code>##        Acc       Prec        Rec          F         TN         FN 
##    0.91837    0.73433    0.77603    0.75460 1554.00000   71.00000 
##         FP         TP 
##   89.00000  246.00000</code></pre>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Interestingly, <code>rpart()</code> also provides us with information
about the importance degrees of each independent variable.</p>
</dd>
</dl>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" title="1">t1<span class="op">$</span>variable.importance<span class="op">/</span><span class="kw">sum</span>(t1<span class="op">$</span>variable.importance)</a></code></pre></div>
<pre><code>##              density       residual.sugar        fixed.acidity 
##            0.6562490            0.1984221            0.0305167 
##            chlorides                   pH     volatile.acidity 
##            0.0215008            0.0209678            0.0192880 
##            sulphates total.sulfur.dioxide          citric.acid 
##            0.0184293            0.0140482            0.0119201 
##  free.sulfur.dioxide 
##            0.0086579</code></pre>
</div>
<div id="a-note-on-decision-tree-learning" class="section level3">
<h3><span class="header-section-number">5.2.3</span> A Note on Decision Tree Learning</h3>
<p>Learning an optimal decision tree is a computationally hard problem
– we need some heuristics.</p>
<p>Examples:</p>
<ul>
<li>ID3 (Iterative Dichotomiser 3) <span class="citation">(Quinlan <a href="references.html#ref-id3">1986</a>)</span></li>
<li>C4.5 algorithm <span class="citation">(Quinlan <a href="references.html#ref-c45">1993</a>)</span></li>
<li>CART by Leo Breiman et al., <span class="citation">(Breiman et al. <a href="references.html#ref-cart">1984</a>)</span></li>
</ul>
<p>(**) Decision trees are most often constructed by a <em>greedy</em>, <em>top-down</em>
<em>recursive partitioning</em>, see., e.g., <span class="citation">(Therneau &amp; Atkinson <a href="references.html#ref-rpart">2019</a>)</span>.</p>
</div>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">5.3</span> Exercises</h2>
<div id="edstats-where-girls-are-better-at-maths-than-boys" class="section level3">
<h3><span class="header-section-number">5.3.1</span> EdStats – Where Girls Are Better at Maths Than Boys?</h3>
<p>In this task we will consider the “wide” version of the EdStats
dataset:</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb189-1" title="1">edstats &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/edstats_2019_wide.csv&quot;</span>,</a>
<a class="sourceLine" id="cb189-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a>
<a class="sourceLine" id="cb189-3" title="3">edstats[<span class="dv">1</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>]</a></code></pre></div>
<pre><code>##   CountryName HD.HCI.AMRT HD.HCI.AMRT.FE HD.HCI.AMRT.MA HD.HCI.EYRS
## 1 Afghanistan      0.7797         0.8018         0.7597        8.58
##   HD.HCI.EYRS.FE
## 1           6.73</code></pre>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb191-1" title="1">meta &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/edstats_meta.csv&quot;</span>,</a>
<a class="sourceLine" id="cb191-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a></code></pre></div>
<p>This dataset is small, moreover, we’ll be more interested in the description
(understanding) of data, not prediction of the response variable
to unobserved samples. Note that we have the <em>population</em> of the World
countries at hand here (new countries do not arise on a daily basis).
Therefore, a train-test split won’t be performed.</p>
<!--

```r
#pairs(edstats[c("LO.PISA.MAT", "LO.PISA.SCI", "LO.PISA.REA")])
```
-->
<div class="exercise"><strong>Exercise.</strong>
<p>Add a 0/1 factor-type variable <code>girls_rule_maths</code> that is equal to 1
if and only if a country’s average score of 15-year-old female students on the PISA
mathematics scale is greater than the corresponding indicator for the male ones.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Recall that a conversion of a logical value to a number
yields 1 for <code>TRUE</code> and <code>0</code> for <code>FALSE</code>. Hence:</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb192-1" title="1">edstats<span class="op">$</span>girls_rule_maths &lt;-</a>
<a class="sourceLine" id="cb192-2" title="2"><span class="st">    </span><span class="kw">factor</span>(<span class="kw">as.numeric</span>(</a>
<a class="sourceLine" id="cb192-3" title="3">        edstats<span class="op">$</span>LO.PISA.MAT.FE<span class="op">&gt;</span>edstats<span class="op">$</span>LO.PISA.MAT.MA</a>
<a class="sourceLine" id="cb192-4" title="4">    ))</a>
<a class="sourceLine" id="cb192-5" title="5"><span class="kw">head</span>(edstats<span class="op">$</span>girls_rule_maths, <span class="dv">10</span>)</a></code></pre></div>
<pre><code>##  [1] &lt;NA&gt; 1    1    &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 0    &lt;NA&gt;
## Levels: 0 1</code></pre>
<p>Unfortunately, there are many missing values in the dataset.
More precisely:</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb194-1" title="1"><span class="kw">sum</span>(<span class="kw">is.na</span>(edstats<span class="op">$</span>girls_rule_maths)) <span class="co"># count</span></a></code></pre></div>
<pre><code>## [1] 187</code></pre>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb196-1" title="1"><span class="kw">mean</span>(<span class="kw">is.na</span>(edstats<span class="op">$</span>girls_rule_maths)) <span class="co"># proportion</span></a></code></pre></div>
<pre><code>## [1] 0.69776</code></pre>
<p>Countries such as Egypt, India, Iran or Venezuela
are not amongst the 79 members of the Programme for International
Student Assessment. Thus, we’ll have to deal with the data we have.</p>
<p>The percentage of counties where “girls rule” is equal to:</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb198-1" title="1"><span class="kw">mean</span>(edstats<span class="op">$</span>girls_rule_maths<span class="op">==</span><span class="dv">1</span>, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] 0.33333</code></pre>
<p>Here is the list of those counties:</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb200-1" title="1"><span class="kw">as.character</span>(<span class="kw">na.omit</span>(</a>
<a class="sourceLine" id="cb200-2" title="2">    edstats[edstats<span class="op">$</span>girls_rule_maths<span class="op">==</span><span class="dv">1</span>, <span class="st">&quot;CountryName&quot;</span>]</a>
<a class="sourceLine" id="cb200-3" title="3">))</a></code></pre></div>
<pre><code>##  [1] &quot;Albania&quot;              &quot;Algeria&quot;             
##  [3] &quot;Brunei Darussalam&quot;    &quot;Bulgaria&quot;            
##  [5] &quot;Cyprus&quot;               &quot;Dominican Republic&quot;  
##  [7] &quot;Finland&quot;              &quot;Georgia&quot;             
##  [9] &quot;Hong Kong SAR, China&quot; &quot;Iceland&quot;             
## [11] &quot;Indonesia&quot;            &quot;Israel&quot;              
## [13] &quot;Jordan&quot;               &quot;Lithuania&quot;           
## [15] &quot;Malaysia&quot;             &quot;Malta&quot;               
## [17] &quot;Moldova&quot;              &quot;North Macedonia&quot;     
## [19] &quot;Norway&quot;               &quot;Philippines&quot;         
## [21] &quot;Qatar&quot;                &quot;Saudi Arabia&quot;        
## [23] &quot;Sweden&quot;               &quot;Thailand&quot;            
## [25] &quot;Trinidad and Tobago&quot;  &quot;United Arab Emirates&quot;
## [27] &quot;Vietnam&quot;</code></pre>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Learn a decision tree that distinguishes between the countries where
girls are better at maths than boys and assess the quality of this classifier.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Let’s first create a subset of <code>edstats</code> that doesn’t include
the country names as well as the boys’ and girls’ math scores.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb202-1" title="1">edstats_subset &lt;-<span class="st"> </span>edstats[<span class="op">!</span>(<span class="kw">names</span>(edstats) <span class="op">%in%</span></a>
<a class="sourceLine" id="cb202-2" title="2"><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;CountryName&quot;</span>, <span class="st">&quot;LO.PISA.MAT.FE&quot;</span>, <span class="st">&quot;LO.PISA.MAT.MA&quot;</span>))]</a></code></pre></div>
<p>Fitting and plotting (see Figure <a href="chap-trees.html#fig:girlsboys7tree">5.3</a>)
of the tree can be performed as follows:</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb203-1" title="1"><span class="kw">library</span>(<span class="st">&quot;rpart&quot;</span>)</a>
<a class="sourceLine" id="cb203-2" title="2"><span class="kw">library</span>(<span class="st">&quot;rpart.plot&quot;</span>)</a>
<a class="sourceLine" id="cb203-3" title="3">tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(girls_rule_maths<span class="op">~</span>., <span class="dt">data=</span>edstats_subset,</a>
<a class="sourceLine" id="cb203-4" title="4">    <span class="dt">method=</span><span class="st">&quot;class&quot;</span>, <span class="dt">model=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb203-5" title="5"><span class="kw">rpart.plot</span>(tree)</a></code></pre></div>
<div class="figure"><span id="fig:girlsboys7tree"></span>
<img src="040-trees-figures/girlsboys7tree-1.svg" alt=" A decision tree explaining the girls_rule_maths variable" />
<p class="caption">Figure 5.3:  A decision tree explaining the <code>girls_rule_maths</code> variable</p>
</div>
<p>The variables included in the model are:</p>
<p>Note that the decision rules are well-interpretable, we can make a whole
story around it. Whether or not it is actually true – is a different… story.</p>
<p>To compute the basic classifier performance scores,
let’s recall the <code>get_metrics()</code> function:</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb204-1" title="1">get_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(Y_pred, Y_test)</a>
<a class="sourceLine" id="cb204-2" title="2">{</a>
<a class="sourceLine" id="cb204-3" title="3">    C &lt;-<span class="st"> </span><span class="kw">table</span>(Y_pred, Y_test) <span class="co"># confusion matrix</span></a>
<a class="sourceLine" id="cb204-4" title="4">    <span class="kw">stopifnot</span>(<span class="kw">dim</span>(C) <span class="op">==</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb204-5" title="5">    <span class="kw">c</span>(<span class="dt">Acc=</span>(C[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span><span class="kw">sum</span>(C), <span class="co"># accuracy</span></a>
<a class="sourceLine" id="cb204-6" title="6">      <span class="dt">Prec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># precision</span></a>
<a class="sourceLine" id="cb204-7" title="7">      <span class="dt">Rec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">1</span>,<span class="dv">2</span>]), <span class="co"># recall</span></a>
<a class="sourceLine" id="cb204-8" title="8">      <span class="dt">F=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># F-measure</span></a>
<a class="sourceLine" id="cb204-9" title="9">      <span class="co"># Confusion matrix items:</span></a>
<a class="sourceLine" id="cb204-10" title="10">      <span class="dt">TN=</span>C[<span class="dv">1</span>,<span class="dv">1</span>], <span class="dt">FN=</span>C[<span class="dv">1</span>,<span class="dv">2</span>],</a>
<a class="sourceLine" id="cb204-11" title="11">      <span class="dt">FP=</span>C[<span class="dv">2</span>,<span class="dv">1</span>], <span class="dt">TP=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb204-12" title="12">    ) <span class="co"># return a named vector</span></a>
<a class="sourceLine" id="cb204-13" title="13">}</a></code></pre></div>
<p>Now we can judge the tree’s character:</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb205-1" title="1">Y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(tree, edstats_subset, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb205-2" title="2"><span class="kw">get_metrics</span>(Y_pred, edstats_subset<span class="op">$</span>girls_rule_maths)</a></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.81481  1.00000  0.44444  0.61538 54.00000 15.00000  0.00000 12.00000</code></pre>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Learn a decision tree that this time doesn’t rely on any of the PISA indicators.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Let’s remove the unwanted variables:</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb207-1" title="1">edstats_subset &lt;-<span class="st"> </span>edstats[<span class="op">!</span>(<span class="kw">names</span>(edstats) <span class="op">%in%</span></a>
<a class="sourceLine" id="cb207-2" title="2"><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;LO.PISA.MAT&quot;</span>, <span class="st">&quot;LO.PISA.MAT.FE&quot;</span>, <span class="st">&quot;LO.PISA.MAT.MA&quot;</span>,</a>
<a class="sourceLine" id="cb207-3" title="3">      <span class="st">&quot;LO.PISA.REA&quot;</span>, <span class="st">&quot;LO.PISA.REA.FE&quot;</span>, <span class="st">&quot;LO.PISA.REA.MA&quot;</span>,</a>
<a class="sourceLine" id="cb207-4" title="4">      <span class="st">&quot;LO.PISA.SCI&quot;</span>, <span class="st">&quot;LO.PISA.SCI.FE&quot;</span>, <span class="st">&quot;LO.PISA.SCI.MA&quot;</span>,</a>
<a class="sourceLine" id="cb207-5" title="5">      <span class="st">&quot;CountryName&quot;</span>))]</a></code></pre></div>
<p>On a side note, this could be done more easily by calling, e.g.,
<code>stri_startswith_fixed(names(edstats), "LO.PISA")</code> from the <code>stringi</code> package.</p>
<p>Fitting and plotting (see Figure <a href="chap-trees.html#fig:girlsboys11tree">5.4</a>) of the tree:</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb208-1" title="1">tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(girls_rule_maths<span class="op">~</span>., <span class="dt">data=</span>edstats_subset,</a>
<a class="sourceLine" id="cb208-2" title="2">    <span class="dt">method=</span><span class="st">&quot;class&quot;</span>, <span class="dt">model=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb208-3" title="3"><span class="kw">rpart.plot</span>(tree)</a></code></pre></div>
<div class="figure"><span id="fig:girlsboys11tree"></span>
<img src="040-trees-figures/girlsboys11tree-1.svg" alt=" Another decision tree explaining the girls_rule_maths variable" />
<p class="caption">Figure 5.4:  Another decision tree explaining the <code>girls_rule_maths</code> variable</p>
</div>
<p>Performance metrics:</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb209-1" title="1">Y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(tree, edstats, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb209-2" title="2"><span class="kw">get_metrics</span>(Y_pred, edstats_subset<span class="op">$</span>girls_rule_maths)</a></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.79012  0.69231  0.66667  0.67925 46.00000  9.00000  8.00000 18.00000</code></pre>
<p>It’s interesting to note that some of the
goodness-of-fit measures are actually higher now.</p>
<p>The variables included in the model are:</p>
</details>
</div>
<div id="edstats-and-world-factbook-joining-forces" class="section level3">
<h3><span class="header-section-number">5.3.2</span> EdStats and World Factbook – Joining Forces</h3>
<p>In the course of our data science journey, we have considered two datasets
dealing with country-level indicators: the World Factbook and
World Bank’s EdStats.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb211-1" title="1">factbook &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/world_factbook_2020.csv&quot;</span>,</a>
<a class="sourceLine" id="cb211-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a>
<a class="sourceLine" id="cb211-3" title="3">edstats &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/edstats_2019_wide.csv&quot;</span>,</a>
<a class="sourceLine" id="cb211-4" title="4">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a></code></pre></div>
<p>Let’s combine the information they provide
and see if we come up with a better model of where
girls’ math scores are higher.</p>
<!--
#factbook$country[!(factbook$country %in% edstats$CountryName)]
#edstats$CountryName[!(edstats$CountryName %in% factbook$country)]
-->
<div class="exercise"><strong>Exercise.</strong>
<p>Some country names in one dataset don’t match those in the other
one, for instance: Czech Republic vs. Czechia,
Myanmar vs. Burma, etc. Resolve these conflicts as best you can.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>To get a list of the mismatched country names, we can call either:</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb212-1" title="1">factbook<span class="op">$</span>country[<span class="op">!</span>(factbook<span class="op">$</span>country <span class="op">%in%</span><span class="st"> </span>edstats<span class="op">$</span>CountryName)]</a></code></pre></div>
<p>or:</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb213-1" title="1">edstats<span class="op">$</span>CountryName[<span class="op">!</span>(edstats<span class="op">$</span>CountryName <span class="op">%in%</span><span class="st"> </span>factbook<span class="op">$</span>country)]</a></code></pre></div>
<p>Unfortunately, the data need to be cleaned manually – it’s a tedious task.
The following consists of what we hope are the best matches
between the two datasets (yet, the list is not perfect;
in particular, the Republic of North Macedonia
is completely missing in one of the datasets):</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb214-1" title="1">from_to &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>, <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb214-2" title="2"><span class="co"># FROM (edstats)                  # TO (factbook)</span></a>
<a class="sourceLine" id="cb214-3" title="3"><span class="st">&quot;Brunei Darussalam&quot;</span>             , <span class="st">&quot;Brunei&quot;</span>                            ,</a>
<a class="sourceLine" id="cb214-4" title="4"><span class="st">&quot;Congo, Dem. Rep.&quot;</span>              , <span class="st">&quot;Congo, Democratic Republic of the&quot;</span> ,</a>
<a class="sourceLine" id="cb214-5" title="5"><span class="st">&quot;Congo, Rep.&quot;</span>                   , <span class="st">&quot;Congo, Republic of the&quot;</span>            ,</a>
<a class="sourceLine" id="cb214-6" title="6"><span class="st">&quot;Czech Republic&quot;</span>                , <span class="st">&quot;Czechia&quot;</span>                           ,</a>
<a class="sourceLine" id="cb214-7" title="7"><span class="st">&quot;Egypt, Arab Rep.&quot;</span>              , <span class="st">&quot;Egypt&quot;</span>                             ,</a>
<a class="sourceLine" id="cb214-8" title="8"><span class="st">&quot;Hong Kong SAR, China&quot;</span>          , <span class="st">&quot;Hong Kong&quot;</span>                         ,</a>
<a class="sourceLine" id="cb214-9" title="9"><span class="st">&quot;Iran, Islamic Rep.&quot;</span>            , <span class="st">&quot;Iran&quot;</span>                              ,</a>
<a class="sourceLine" id="cb214-10" title="10"><span class="st">&quot;Korea, Dem. People’s Rep.&quot;</span>     , <span class="st">&quot;Korea, North&quot;</span>                      ,</a>
<a class="sourceLine" id="cb214-11" title="11"><span class="st">&quot;Korea, Rep.&quot;</span>                   , <span class="st">&quot;Korea, South&quot;</span>                      ,</a>
<a class="sourceLine" id="cb214-12" title="12"><span class="st">&quot;Kyrgyz Republic&quot;</span>               , <span class="st">&quot;Kyrgyzstan&quot;</span>                        ,</a>
<a class="sourceLine" id="cb214-13" title="13"><span class="st">&quot;Lao PDR&quot;</span>                       , <span class="st">&quot;Laos&quot;</span>                              ,</a>
<a class="sourceLine" id="cb214-14" title="14"><span class="st">&quot;Macao SAR, China&quot;</span>              , <span class="st">&quot;Macau&quot;</span>                             ,</a>
<a class="sourceLine" id="cb214-15" title="15"><span class="st">&quot;Micronesia, Fed. Sts.&quot;</span>         , <span class="st">&quot;Micronesia, Federated States of&quot;</span>   ,</a>
<a class="sourceLine" id="cb214-16" title="16"><span class="st">&quot;Myanmar&quot;</span>                       , <span class="st">&quot;Burma&quot;</span>                             ,</a>
<a class="sourceLine" id="cb214-17" title="17"><span class="st">&quot;Russian Federation&quot;</span>            , <span class="st">&quot;Russia&quot;</span>                            ,</a>
<a class="sourceLine" id="cb214-18" title="18"><span class="st">&quot;Slovak Republic&quot;</span>               , <span class="st">&quot;Slovakia&quot;</span>                          ,</a>
<a class="sourceLine" id="cb214-19" title="19"><span class="st">&quot;St. Kitts and Nevis&quot;</span>           , <span class="st">&quot;Saint Kitts and Nevis&quot;</span>             ,</a>
<a class="sourceLine" id="cb214-20" title="20"><span class="st">&quot;St. Lucia&quot;</span>                     , <span class="st">&quot;Saint Lucia&quot;</span>                       ,</a>
<a class="sourceLine" id="cb214-21" title="21"><span class="st">&quot;St. Martin (French part)&quot;</span>      , <span class="st">&quot;Saint Martin&quot;</span>                      ,</a>
<a class="sourceLine" id="cb214-22" title="22"><span class="st">&quot;St. Vincent and the Grenadines&quot;</span>, <span class="st">&quot;Saint Vincent and the Grenadines&quot;</span>  ,</a>
<a class="sourceLine" id="cb214-23" title="23"><span class="st">&quot;Syrian Arab Republic&quot;</span>          , <span class="st">&quot;Syria&quot;</span>                             ,</a>
<a class="sourceLine" id="cb214-24" title="24"><span class="st">&quot;Venezuela, RB&quot;</span>                 , <span class="st">&quot;Venezuela&quot;</span>                         ,</a>
<a class="sourceLine" id="cb214-25" title="25"><span class="st">&quot;Virgin Islands (U.S.)&quot;</span>         , <span class="st">&quot;Virgin Islands&quot;</span>                    ,</a>
<a class="sourceLine" id="cb214-26" title="26"><span class="st">&quot;Yemen, Rep.&quot;</span>                   , <span class="st">&quot;Yemen&quot;</span></a>
<a class="sourceLine" id="cb214-27" title="27">))</a></code></pre></div>
<p>Conversion of the names:</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb215-1" title="1"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(from_to)) {</a>
<a class="sourceLine" id="cb215-2" title="2">    edstats<span class="op">$</span>CountryName[edstats<span class="op">$</span>CountryName<span class="op">==</span>from_to[i,<span class="dv">1</span>]] &lt;-<span class="st"> </span>from_to[i,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb215-3" title="3">}</a></code></pre></div>
<p>On a side note (*), this could be done with a single call to
a function in the <code>stringi</code> package:</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb216-1" title="1"><span class="kw">library</span>(<span class="st">&quot;stringi&quot;</span>)</a>
<a class="sourceLine" id="cb216-2" title="2">edstats<span class="op">$</span>CountryName &lt;-<span class="st"> </span><span class="kw">stri_replace_all_fixed</span>(edstats<span class="op">$</span>CountryName,</a>
<a class="sourceLine" id="cb216-3" title="3">    from_to[,<span class="dv">1</span>], from_to[,<span class="dv">2</span>], <span class="dt">vectorize_all=</span><span class="ot">FALSE</span>)</a></code></pre></div>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Merge (join) the two datasets based on the country names.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>This can be done by means of the <code>merge()</code> function.</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb217-1" title="1">edbook &lt;-<span class="st"> </span><span class="kw">merge</span>(edstats, factbook, <span class="dt">by.x=</span><span class="st">&quot;CountryName&quot;</span>, <span class="dt">by.y=</span><span class="st">&quot;country&quot;</span>)</a>
<a class="sourceLine" id="cb217-2" title="2"><span class="kw">ncol</span>(edbook) <span class="co"># how many columns we have now</span></a></code></pre></div>
<pre><code>## [1] 157</code></pre>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Learn a decision tree that distinguishes between the countries where
girls are better at maths than boys and assess the quality of this classifier.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>We proceed as in one of the previous exercises:</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb219-1" title="1">edbook<span class="op">$</span>girls_rule_maths &lt;-</a>
<a class="sourceLine" id="cb219-2" title="2"><span class="st">    </span><span class="kw">factor</span>(<span class="kw">as.numeric</span>(</a>
<a class="sourceLine" id="cb219-3" title="3">        edbook<span class="op">$</span>LO.PISA.MAT.FE<span class="op">&gt;</span>edbook<span class="op">$</span>LO.PISA.MAT.MA</a>
<a class="sourceLine" id="cb219-4" title="4">    ))</a>
<a class="sourceLine" id="cb219-5" title="5">edbook_subset &lt;-<span class="st"> </span>edbook[<span class="op">!</span>(<span class="kw">names</span>(edbook) <span class="op">%in%</span></a>
<a class="sourceLine" id="cb219-6" title="6"><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;CountryName&quot;</span>, <span class="st">&quot;LO.PISA.MAT.FE&quot;</span>, <span class="st">&quot;LO.PISA.MAT.MA&quot;</span>))]</a></code></pre></div>
<p>Fitting and plotting (see Figure <a href="chap-trees.html#fig:joinedstats9">5.5</a>):</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb220-1" title="1"><span class="kw">library</span>(<span class="st">&quot;rpart&quot;</span>)</a>
<a class="sourceLine" id="cb220-2" title="2"><span class="kw">library</span>(<span class="st">&quot;rpart.plot&quot;</span>)</a>
<a class="sourceLine" id="cb220-3" title="3">tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(girls_rule_maths<span class="op">~</span>., <span class="dt">data=</span>edbook_subset,</a>
<a class="sourceLine" id="cb220-4" title="4">    <span class="dt">method=</span><span class="st">&quot;class&quot;</span>, <span class="dt">model=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb220-5" title="5"><span class="kw">rpart.plot</span>(tree)</a></code></pre></div>
<div class="figure"><span id="fig:joinedstats9"></span>
<img src="040-trees-figures/joinedstats9-1.svg" alt=" Yet another decision tree explaining the girls_rule_maths variable" />
<p class="caption">Figure 5.5:  Yet another decision tree explaining the <code>girls_rule_maths</code> variable</p>
</div>
<p>Performance metrics:</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb221-1" title="1">Y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(tree, edbook_subset, <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb221-2" title="2"><span class="kw">get_metrics</span>(Y_pred, edbook_subset<span class="op">$</span>girls_rule_maths)</a></code></pre></div>
<pre><code>##      Acc     Prec      Rec        F       TN       FN       FP       TP 
##  0.82716  0.78261  0.66667  0.72000 49.00000  9.00000  5.00000 18.00000</code></pre>
<p>The variables included in the model are:</p>
<p>This is… not at all enlightening.
Rest assured that experts in education
or econometrics for whom we work in this (imaginary) project
would raise many questions at this very point. Merely applying
some computational procedure on a dataset doesn’t cut it;
it’s too early to ask for a paycheque.
Classifiers are just blind <em>tools</em> in our gentle yet firm hands;
new questions are risen, new answers must be sought. Further
explorations are of course left as an exercise to the kind reader.</p>
</details>
</div>
<div id="wine-quality-random-forest-and-xgboost" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Wine Quality – Random Forest and XGBoost (*)</h3>
<p>Let’s consider the Wine Quality dataset:</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb223-1" title="1">wine_quality &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/wine_quality_all.csv&quot;</span>,</a>
<a class="sourceLine" id="cb223-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a>
<a class="sourceLine" id="cb223-3" title="3"><span class="kw">head</span>(wine_quality, <span class="dv">3</span>)</a></code></pre></div>
<pre><code>##   fixed.acidity volatile.acidity citric.acid residual.sugar chlorides
## 1           7.4             0.70        0.00            1.9     0.076
## 2           7.8             0.88        0.00            2.6     0.098
## 3           7.8             0.76        0.04            2.3     0.092
##   free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates
## 1                  11                   34  0.9978 3.51      0.56
## 2                  25                   67  0.9968 3.20      0.68
## 3                  15                   54  0.9970 3.26      0.65
##   alcohol response color
## 1     9.4        5   red
## 2     9.8        5   red
## 3     9.8        5   red</code></pre>
<p>Recall that there are 11 physicochemical features of wines reported
(columns 1-11).
Moreover, there is a wine rating (variable <code>response</code>)
on the scale 0 (bad) to 10 (excellent) given by wine experts.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Add a new column named <code>quality</code>. A wine should get a <code>quality</code> of <code>1</code>
if its rating is greater than or equal to 7 (a good wine)
and a quality of <code>0</code> otherwise.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Perform a random train-test split of size 60-40%:
create the matrices <code>X_train</code> and <code>X_test</code> containing
the 11 physicochemical wine features and
the corresponding label vectors <code>Y_train</code> and <code>Y_test</code>
that inform on the wines’ <code>quality</code>.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Construct (of course, on the train set)
a decision tree that models wine quality as a function
the 11 physicochemical features.
Assess the quality of the obtained model (of course, based on the test set)
by computing the basic
performance metrics for this classifier (accuracy, precision, recall,
F-measure). Also, print the confusion matrix (see <code>table()</code>).
Discuss the obtained results. Is this a good model?</p>
</div>
<dl>
<dt>Ensemble learning, bagging and random forest.</dt>
<dd><p><em>Ensemble learning</em> is a rather simple yet appealing
idea that emphasises on the fact
that no single model will likely ever be omniscient. Instead,
we can construct numerous diverse models explaining a given dependent variable
and then properly aggregate the obtained classifier committee
to obtain a single answer.</p>
<p><em>Bagging</em> (bootstrap aggregating)
is one of the most popular ensemble types:
here, we fit numerous models, each time to a different
randomly selected subset of observations in the input dataset.
In order to classify a new point, we ask each classifier
what it “thinks” the corresponding label should be and
report the mode (majority vote) of the candidate outputs.</p>
<p>The famous <em>random forest</em> algorithm is an example application
of the bagging technique that is based on decision trees
(with an extension that not only random subsets of observations,
but also random columns in the dataset are used to learn the
underlying models).</p>
</dd>
</dl>
<div class="exercise"><strong>Exercise.</strong>
<p>Ronstruct a random forest that models <code>quality</code> as a function
of the 11 physicochemical features – see R package <code>randomForest</code>.
Assess and discuss the classifier’s performance.</p>
</div>
<dl>
<dt>Boosting.</dt>
<dd><p>Another powerful ensemble technique is called <em>boosting</em>.
Here, we construct a supervised learner in an iterative manner;
in each step a simple (“weak”) classifier is added to the ensemble
by learning “something new” about the dataset, e.g., by uplifting
its performance in the cases where frequent misclassifications occur.
When determining the final outcome, each underlying model
might be assigned a different weight.</p>
</dd>
</dl>
<div class="exercise"><strong>Exercise.</strong>
<p>The XGBoost algorithm is a modern implementation of
the gradient boosting approach based on decision trees.
Fit a model with XGBoost (see package <code>xgboost</code>) that describes
<code>quality</code> as a function of the 11 physicochemical features.
Assess and discuss the classifier’s performance.</p>
</div>
</div>
</div>
<div id="outro" class="section level2">
<h2><span class="header-section-number">5.4</span> Outro</h2>
<p>The state-of-the art classifiers called
<em>Random Forests</em> and <em>XGBoost</em> (see also: <em>AdaBoost</em>) are based on decision trees.
They tend to be more accurate but – at the same time – they fail to
exhibit the decision trees’ important feature: interpretability.</p>
<p>Trees can also be used for regression tasks, see R package <code>rpart</code>.</p>
<div style="margin-top: 1em">

</div>
<p><strong>TODO</strong> …..</p>
<div style="margin-top: 1em">

</div>
<p>Recommended further reading: <span class="citation">(James et al. <a href="references.html#ref-islr">2017</a>: Chapters 4 and 8)</span> <!-- TODO: update --></p>
<p>Other: <span class="citation">(Hastie et al. <a href="references.html#ref-esl">2017</a>: Chapters 4 and 7 as well as (*) Chapters 9, 10, 13, 15)</span> <!-- TODO: update --></p>
<p>Next Chapter</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap-feature-engineering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-regression-simple.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
