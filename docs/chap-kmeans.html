<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Clustering with K-Means | Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Clustering with K-Means | Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Clustering with K-Means | Machine Learning Classics with R" />
  
  <meta name="twitter:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  

<meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap-optimisation-continuous.html"/>
<link rel="next" href="chap-optimisation-discrete.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

div.remark {
    border-left: 2px solid gray;
    padding-left: 1em;
}

div.definition {
    font-style: italic;
    border-left: 2px solid #550000;
    padding-left: 1em;
}

</style>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="chap-introduction.html"><a href="chap-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="chap-introduction.html"><a href="chap-introduction.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a><ul>
<li class="chapter" data-level="1.1.1" data-path="chap-introduction.html"><a href="chap-introduction.html#data-sources"><i class="fa fa-check"></i><b>1.1.1</b> Data Sources</a></li>
<li class="chapter" data-level="1.1.2" data-path="chap-introduction.html"><a href="chap-introduction.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chap-introduction.html"><a href="chap-introduction.html#input-data-x"><i class="fa fa-check"></i><b>1.2</b> Input Data, <strong>X</strong></a><ul>
<li class="chapter" data-level="1.2.1" data-path="chap-introduction.html"><a href="chap-introduction.html#abstract-formalism"><i class="fa fa-check"></i><b>1.2.1</b> Abstract Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="chap-introduction.html"><a href="chap-introduction.html#concrete-example"><i class="fa fa-check"></i><b>1.2.2</b> Concrete Example</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="chap-introduction.html"><a href="chap-introduction.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.3</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="1.3.1" data-path="chap-introduction.html"><a href="chap-introduction.html#dimensionality-reduction"><i class="fa fa-check"></i><b>1.3.1</b> Dimensionality Reduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="chap-introduction.html"><a href="chap-introduction.html#anomaly-detection"><i class="fa fa-check"></i><b>1.3.2</b> Anomaly Detection</a></li>
<li class="chapter" data-level="1.3.3" data-path="chap-introduction.html"><a href="chap-introduction.html#clustering"><i class="fa fa-check"></i><b>1.3.3</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="chap-introduction.html"><a href="chap-introduction.html#supervised-learning"><i class="fa fa-check"></i><b>1.4</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.4.1" data-path="chap-introduction.html"><a href="chap-introduction.html#desired-outputs-y"><i class="fa fa-check"></i><b>1.4.1</b> Desired Outputs, <strong>y</strong></a></li>
<li class="chapter" data-level="1.4.2" data-path="chap-introduction.html"><a href="chap-introduction.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.4.2</b> Types of Supervised Learning Problems</a></li>
<li class="chapter" data-level="1.4.3" data-path="chap-introduction.html"><a href="chap-introduction.html#one-dataset-many-problems"><i class="fa fa-check"></i><b>1.4.3</b> One Dataset – Many Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-hclust.html"><a href="chap-hclust.html"><i class="fa fa-check"></i><b>2</b> Agglomerative Hierarchical Clustering</a><ul>
<li class="chapter" data-level="2.1" data-path="chap-hclust.html"><a href="chap-hclust.html#dataset-partitions"><i class="fa fa-check"></i><b>2.1</b> Dataset Partitions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="chap-hclust.html"><a href="chap-hclust.html#label-vectors"><i class="fa fa-check"></i><b>2.1.1</b> Label Vectors</a></li>
<li class="chapter" data-level="2.1.2" data-path="chap-hclust.html"><a href="chap-hclust.html#k-partitions"><i class="fa fa-check"></i><b>2.1.2</b> K-Partitions</a></li>
<li class="chapter" data-level="2.1.3" data-path="chap-hclust.html"><a href="chap-hclust.html#interesting-partitions"><i class="fa fa-check"></i><b>2.1.3</b> “Interesting” Partitions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chap-hclust.html"><a href="chap-hclust.html#sec:euclidean"><i class="fa fa-check"></i><b>2.2</b> Euclidean Distance</a></li>
<li class="chapter" data-level="2.3" data-path="chap-hclust.html"><a href="chap-hclust.html#hierarchical-clustering-at-a-glance"><i class="fa fa-check"></i><b>2.3</b> Hierarchical Clustering at a Glance</a></li>
<li class="chapter" data-level="2.4" data-path="chap-hclust.html"><a href="chap-hclust.html#cluster-dendrograms"><i class="fa fa-check"></i><b>2.4</b> Cluster Dendrograms</a></li>
<li class="chapter" data-level="2.5" data-path="chap-hclust.html"><a href="chap-hclust.html#linkage-functions"><i class="fa fa-check"></i><b>2.5</b> Linkage Functions</a></li>
<li class="chapter" data-level="2.6" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
<li class="chapter" data-level="2.7" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>2.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-knn.html"><a href="chap-knn.html"><i class="fa fa-check"></i><b>3</b> Classification with Nearest Neighbours</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="chap-knn.html"><a href="chap-knn.html#k-nearest-neighbours-classifier"><i class="fa fa-check"></i><b>3.2</b> K-Nearest Neighbours Classifier</a></li>
<li class="chapter" data-level="3.3" data-path="chap-knn.html"><a href="chap-knn.html#example-in-r"><i class="fa fa-check"></i><b>3.3</b> Example in R</a></li>
<li class="chapter" data-level="3.4" data-path="chap-knn.html"><a href="chap-knn.html#classifier-assessment"><i class="fa fa-check"></i><b>3.4</b> Classifier Assessment</a></li>
<li class="chapter" data-level="3.5" data-path="chap-knn.html"><a href="chap-knn.html#classifier-selection"><i class="fa fa-check"></i><b>3.5</b> Classifier Selection</a></li>
<li class="chapter" data-level="3.6" data-path="chap-knn.html"><a href="chap-knn.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.6</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="3.6.1" data-path="chap-knn.html"><a href="chap-knn.html#main-routine"><i class="fa fa-check"></i><b>3.6.1</b> Main Routine</a></li>
<li class="chapter" data-level="3.6.2" data-path="chap-knn.html"><a href="chap-knn.html#sec:mode"><i class="fa fa-check"></i><b>3.6.2</b> Mode</a></li>
<li class="chapter" data-level="3.6.3" data-path="chap-knn.html"><a href="chap-knn.html#nn-search-methods"><i class="fa fa-check"></i><b>3.6.3</b> NN Search Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="chap-knn.html"><a href="chap-knn.html#remarks-1"><i class="fa fa-check"></i><b>3.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html"><i class="fa fa-check"></i><b>4</b> Feature Engineering</a><ul>
<li class="chapter" data-level="4.0.1" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#feature-engineering"><i class="fa fa-check"></i><b>4.0.1</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.0.2" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#different-metrics"><i class="fa fa-check"></i><b>4.0.2</b> Different Metrics (*)</a></li>
<li class="chapter" data-level="4.1" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#exercises-1"><i class="fa fa-check"></i><b>4.1</b> Exercises</a><ul>
<li class="chapter" data-level="4.1.1" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#wine-quality-best-k-nn-parameters-via-cross-validation"><i class="fa fa-check"></i><b>4.1.1</b> Wine Quality – Best K-NN Parameters via Cross-Validation (*)</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="chap-feature-engineering.html"><a href="chap-feature-engineering.html#remarks-2"><i class="fa fa-check"></i><b>4.2</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-trees.html"><a href="chap-trees.html"><i class="fa fa-check"></i><b>5</b> Classification with Decision Trees</a><ul>
<li class="chapter" data-level="5.1" data-path="chap-trees.html"><a href="chap-trees.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="chap-trees.html"><a href="chap-trees.html#classification-task"><i class="fa fa-check"></i><b>5.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="5.1.2" data-path="chap-trees.html"><a href="chap-trees.html#data"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="chap-trees.html"><a href="chap-trees.html#decision-trees"><i class="fa fa-check"></i><b>5.2</b> Decision Trees</a><ul>
<li class="chapter" data-level="5.2.1" data-path="chap-trees.html"><a href="chap-trees.html#introduction-2"><i class="fa fa-check"></i><b>5.2.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2.2" data-path="chap-trees.html"><a href="chap-trees.html#example-in-r-1"><i class="fa fa-check"></i><b>5.2.2</b> Example in R</a></li>
<li class="chapter" data-level="5.2.3" data-path="chap-trees.html"><a href="chap-trees.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>5.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="chap-trees.html"><a href="chap-trees.html#exercises-2"><i class="fa fa-check"></i><b>5.3</b> Exercises</a><ul>
<li class="chapter" data-level="5.3.1" data-path="chap-trees.html"><a href="chap-trees.html#edstats-where-girls-are-better-at-maths-than-boys"><i class="fa fa-check"></i><b>5.3.1</b> EdStats – Where Girls Are Better at Maths Than Boys?</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap-trees.html"><a href="chap-trees.html#edstats-and-world-factbook-joining-forces"><i class="fa fa-check"></i><b>5.3.2</b> EdStats and World Factbook – Joining Forces</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap-trees.html"><a href="chap-trees.html#wine-quality-random-forest-and-xgboost"><i class="fa fa-check"></i><b>5.3.3</b> Wine Quality – Random Forest and XGBoost (*)</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap-trees.html"><a href="chap-trees.html#outro"><i class="fa fa-check"></i><b>5.4</b> Outro</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html"><i class="fa fa-check"></i><b>6</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#simple-regression"><i class="fa fa-check"></i><b>6.1</b> Simple Regression</a><ul>
<li class="chapter" data-level="6.1.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#introduction-3"><i class="fa fa-check"></i><b>6.1.1</b> Introduction</a></li>
<li class="chapter" data-level="6.1.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>6.1.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#search-space-and-objective"><i class="fa fa-check"></i><b>6.1.3</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#introduction-4"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#solution-in-r"><i class="fa fa-check"></i><b>6.2.2</b> Solution in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#analytic-solution"><i class="fa fa-check"></i><b>6.2.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="6.2.4" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>6.2.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#exercises-3"><i class="fa fa-check"></i><b>6.3</b> Exercises</a><ul>
<li class="chapter" data-level="6.3.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>6.3.1</b> The Anscombe Quartet</a></li>
<li class="chapter" data-level="6.3.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#median-house-value-in-boston"><i class="fa fa-check"></i><b>6.3.2</b> Median House Value in Boston</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#outro-1"><i class="fa fa-check"></i><b>6.4</b> Outro</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html"><i class="fa fa-check"></i><b>7</b> Multiple Regression</a><ul>
<li class="chapter" data-level="7.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a><ul>
<li class="chapter" data-level="7.1.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#formalism"><i class="fa fa-check"></i><b>7.1.1</b> Formalism</a></li>
<li class="chapter" data-level="7.1.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>7.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#multiple-linear-regression"><i class="fa fa-check"></i><b>7.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#problem-formulation"><i class="fa fa-check"></i><b>7.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="7.2.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>7.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#finding-the-best-model"><i class="fa fa-check"></i><b>7.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="7.3.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#model-diagnostics"><i class="fa fa-check"></i><b>7.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="7.3.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#variable-selection"><i class="fa fa-check"></i><b>7.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="7.3.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#variable-transformation"><i class="fa fa-check"></i><b>7.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="7.3.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>7.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#exercises-4"><i class="fa fa-check"></i><b>7.4</b> Exercises</a><ul>
<li class="chapter" data-level="7.4.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>7.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="7.4.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>7.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="7.4.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>7.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="7.4.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>7.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="7.4.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>7.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
<li class="chapter" data-level="7.4.6" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#median-house-value-in-boston-continued"><i class="fa fa-check"></i><b>7.4.6</b> Median House Value in Boston (Continued)</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#outro-2"><i class="fa fa-check"></i><b>7.5</b> Outro</a><ul>
<li class="chapter" data-level="7.5.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#remarks-3"><i class="fa fa-check"></i><b>7.5.1</b> Remarks</a></li>
<li class="chapter" data-level="7.5.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#other-methods-for-regression"><i class="fa fa-check"></i><b>7.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="7.5.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>7.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="7.5.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>7.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="7.5.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>7.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-logistic.html"><a href="chap-logistic.html"><i class="fa fa-check"></i><b>8</b> Classification with Linear Models</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-logistic.html"><a href="chap-logistic.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="chap-logistic.html"><a href="chap-logistic.html#classification-task-1"><i class="fa fa-check"></i><b>8.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="8.1.2" data-path="chap-logistic.html"><a href="chap-logistic.html#data-1"><i class="fa fa-check"></i><b>8.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="chap-logistic.html"><a href="chap-logistic.html#binary-logistic-regression"><i class="fa fa-check"></i><b>8.2</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="8.2.1" data-path="chap-logistic.html"><a href="chap-logistic.html#motivation"><i class="fa fa-check"></i><b>8.2.1</b> Motivation</a></li>
<li class="chapter" data-level="8.2.2" data-path="chap-logistic.html"><a href="chap-logistic.html#logistic-model"><i class="fa fa-check"></i><b>8.2.2</b> Logistic Model</a></li>
<li class="chapter" data-level="8.2.3" data-path="chap-logistic.html"><a href="chap-logistic.html#example-in-r-2"><i class="fa fa-check"></i><b>8.2.3</b> Example in R</a></li>
<li class="chapter" data-level="8.2.4" data-path="chap-logistic.html"><a href="chap-logistic.html#loss-function-cross-entropy"><i class="fa fa-check"></i><b>8.2.4</b> Loss Function: Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="chap-logistic.html"><a href="chap-logistic.html#exercises-5"><i class="fa fa-check"></i><b>8.3</b> Exercises</a><ul>
<li class="chapter" data-level="8.3.1" data-path="chap-logistic.html"><a href="chap-logistic.html#edstats-fitting-of-binary-logistic-regression-models"><i class="fa fa-check"></i><b>8.3.1</b> EdStats – Fitting of Binary Logistic Regression Models</a></li>
<li class="chapter" data-level="8.3.2" data-path="chap-logistic.html"><a href="chap-logistic.html#edstats-variable-selection-in-binary-logistic-regression"><i class="fa fa-check"></i><b>8.3.2</b> EdStats – Variable Selection in Binary Logistic Regression (*)</a></li>
<li class="chapter" data-level="8.3.3" data-path="chap-logistic.html"><a href="chap-logistic.html#currency-exchange-rates-growthfall"><i class="fa fa-check"></i><b>8.3.3</b> Currency Exchange Rates Growth/Fall</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="chap-logistic.html"><a href="chap-logistic.html#outro-3"><i class="fa fa-check"></i><b>8.4</b> Outro</a><ul>
<li class="chapter" data-level="8.4.1" data-path="chap-logistic.html"><a href="chap-logistic.html#remarks-4"><i class="fa fa-check"></i><b>8.4.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html"><i class="fa fa-check"></i><b>9</b> Continuous Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="9.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#optimisation-problems"><i class="fa fa-check"></i><b>9.1.1</b> Optimisation Problems</a></li>
<li class="chapter" data-level="9.1.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>9.1.2</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="9.1.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>9.1.3</b> Example Objective over a 2D Domain</a></li>
<li class="chapter" data-level="9.1.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>9.1.4</b> Example Optimisation Problems in Machine Learning</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#iterative-methods"><i class="fa fa-check"></i><b>9.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="9.2.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#introduction-8"><i class="fa fa-check"></i><b>9.2.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-in-r-3"><i class="fa fa-check"></i><b>9.2.2</b> Example in R</a></li>
<li class="chapter" data-level="9.2.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>9.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="9.2.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#random-restarts"><i class="fa fa-check"></i><b>9.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#gradient-descent"><i class="fa fa-check"></i><b>9.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="9.3.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#function-gradient"><i class="fa fa-check"></i><b>9.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="9.3.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>9.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="9.3.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>9.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="9.3.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-mnist"><i class="fa fa-check"></i><b>9.3.4</b> Example: MNIST (*)</a></li>
<li class="chapter" data-level="9.3.5" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>9.3.5</b> Stochastic Gradient Descent (SGD) (*)</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>9.4</b> A Note on Convex Optimisation (*)</a></li>
<li class="chapter" data-level="9.5" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#outro-4"><i class="fa fa-check"></i><b>9.5</b> Outro</a><ul>
<li class="chapter" data-level="9.5.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#remarks-5"><i class="fa fa-check"></i><b>9.5.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-kmeans.html"><a href="chap-kmeans.html"><i class="fa fa-check"></i><b>10</b> Clustering with K-Means</a><ul>
<li class="chapter" data-level="10.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#k-means-clustering"><i class="fa fa-check"></i><b>10.1</b> K-means Clustering</a><ul>
<li class="chapter" data-level="10.1.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#example-in-r-4"><i class="fa fa-check"></i><b>10.1.1</b> Example in R</a></li>
<li class="chapter" data-level="10.1.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#problem-statement"><i class="fa fa-check"></i><b>10.1.2</b> Problem Statement</a></li>
<li class="chapter" data-level="10.1.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>10.1.3</b> Algorithms for the K-means Problem</a></li>
<li class="chapter" data-level="10.1.4" data-path="chap-kmeans.html"><a href="chap-kmeans.html#k-means-revisited"><i class="fa fa-check"></i><b>10.1.4</b> K-means Revisited</a></li>
<li class="chapter" data-level="10.1.5" data-path="chap-kmeans.html"><a href="chap-kmeans.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>10.1.5</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#exercises-6"><i class="fa fa-check"></i><b>10.2</b> Exercises</a><ul>
<li class="chapter" data-level="10.2.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#clustering-of-the-world-factbook"><i class="fa fa-check"></i><b>10.2.1</b> Clustering of the World Factbook</a></li>
<li class="chapter" data-level="10.2.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#unbalance-dataset-k-means-needs-multiple-starts"><i class="fa fa-check"></i><b>10.2.2</b> Unbalance Dataset – K-Means Needs Multiple Starts</a></li>
<li class="chapter" data-level="10.2.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#clustering-of-typical-2d-benchmark-datasets"><i class="fa fa-check"></i><b>10.2.3</b> Clustering of Typical 2D Benchmark Datasets</a></li>
<li class="chapter" data-level="10.2.4" data-path="chap-kmeans.html"><a href="chap-kmeans.html#wine-quality-volatile.acidity-and-sulphates"><i class="fa fa-check"></i><b>10.2.4</b> Wine Quality – <code>volatile.acidity</code> and <code>sulphates</code></a></li>
<li class="chapter" data-level="10.2.5" data-path="chap-kmeans.html"><a href="chap-kmeans.html#wine-quality-chlorides-and-total.sulfur.dioxide"><i class="fa fa-check"></i><b>10.2.5</b> Wine Quality – <code>chlorides</code> and <code>total.sulfur.dioxide</code></a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#outro-5"><i class="fa fa-check"></i><b>10.3</b> Outro</a><ul>
<li class="chapter" data-level="10.3.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#remarks-6"><i class="fa fa-check"></i><b>10.3.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html"><i class="fa fa-check"></i><b>11</b> Discrete Optimisation</a><ul>
<li class="chapter" data-level="11.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#introduction-9"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#recap"><i class="fa fa-check"></i><b>11.1.1</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#outro-6"><i class="fa fa-check"></i><b>11.2</b> Outro</a><ul>
<li class="chapter" data-level="11.2.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#remarks-7"><i class="fa fa-check"></i><b>11.2.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-feature-selection.html"><a href="chap-feature-selection.html"><i class="fa fa-check"></i><b>12</b> Feature Selection</a><ul>
<li class="chapter" data-level="12.1" data-path="chap-feature-selection.html"><a href="chap-feature-selection.html#introduction-10"><i class="fa fa-check"></i><b>12.1</b> Introduction</a><ul>
<li class="chapter" data-level="12.1.1" data-path="chap-feature-selection.html"><a href="chap-feature-selection.html#recap-1"><i class="fa fa-check"></i><b>12.1.1</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="chap-feature-selection.html"><a href="chap-feature-selection.html#outro-7"><i class="fa fa-check"></i><b>12.2</b> Outro</a><ul>
<li class="chapter" data-level="12.2.1" data-path="chap-feature-selection.html"><a href="chap-feature-selection.html#remarks-8"><i class="fa fa-check"></i><b>12.2.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="chap-images.html"><a href="chap-images.html"><i class="fa fa-check"></i><b>13</b> Shallow and Deep Neural Networks</a><ul>
<li class="chapter" data-level="13.1" data-path="chap-images.html"><a href="chap-images.html#introduction-11"><i class="fa fa-check"></i><b>13.1</b> Introduction</a><ul>
<li class="chapter" data-level="13.1.1" data-path="chap-images.html"><a href="chap-images.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>13.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="13.1.2" data-path="chap-images.html"><a href="chap-images.html#data-2"><i class="fa fa-check"></i><b>13.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="chap-images.html"><a href="chap-images.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>13.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="13.2.1" data-path="chap-images.html"><a href="chap-images.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>13.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="13.2.2" data-path="chap-images.html"><a href="chap-images.html#extending-logistic-regression"><i class="fa fa-check"></i><b>13.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="13.2.3" data-path="chap-images.html"><a href="chap-images.html#softmax-function"><i class="fa fa-check"></i><b>13.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="13.2.4" data-path="chap-images.html"><a href="chap-images.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>13.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="13.2.5" data-path="chap-images.html"><a href="chap-images.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>13.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="13.2.6" data-path="chap-images.html"><a href="chap-images.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>13.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="chap-images.html"><a href="chap-images.html#artificial-neural-networks"><i class="fa fa-check"></i><b>13.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="13.3.1" data-path="chap-images.html"><a href="chap-images.html#artificial-neuron"><i class="fa fa-check"></i><b>13.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="13.3.2" data-path="chap-images.html"><a href="chap-images.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>13.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="13.3.3" data-path="chap-images.html"><a href="chap-images.html#example-in-r-5"><i class="fa fa-check"></i><b>13.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="chap-images.html"><a href="chap-images.html#deep-neural-networks"><i class="fa fa-check"></i><b>13.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="13.4.1" data-path="chap-images.html"><a href="chap-images.html#introduction-12"><i class="fa fa-check"></i><b>13.4.1</b> Introduction</a></li>
<li class="chapter" data-level="13.4.2" data-path="chap-images.html"><a href="chap-images.html#activation-functions"><i class="fa fa-check"></i><b>13.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="13.4.3" data-path="chap-images.html"><a href="chap-images.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>13.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="13.4.4" data-path="chap-images.html"><a href="chap-images.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>13.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="chap-images.html"><a href="chap-images.html#preprocessing-of-data"><i class="fa fa-check"></i><b>13.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="13.5.1" data-path="chap-images.html"><a href="chap-images.html#introduction-13"><i class="fa fa-check"></i><b>13.5.1</b> Introduction</a></li>
<li class="chapter" data-level="13.5.2" data-path="chap-images.html"><a href="chap-images.html#image-deskewing"><i class="fa fa-check"></i><b>13.5.2</b> Image Deskewing</a></li>
<li class="chapter" data-level="13.5.3" data-path="chap-images.html"><a href="chap-images.html#summary-of-all-the-models-considered"><i class="fa fa-check"></i><b>13.5.3</b> Summary of All the Models Considered</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="chap-images.html"><a href="chap-images.html#outro-8"><i class="fa fa-check"></i><b>13.6</b> Outro</a><ul>
<li class="chapter" data-level="13.6.1" data-path="chap-images.html"><a href="chap-images.html#remarks-9"><i class="fa fa-check"></i><b>13.6.1</b> Remarks</a></li>
<li class="chapter" data-level="13.6.2" data-path="chap-images.html"><a href="chap-images.html#beyond-mnist"><i class="fa fa-check"></i><b>13.6.2</b> Beyond MNIST</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="chap-recommenders.html"><a href="chap-recommenders.html"><i class="fa fa-check"></i><b>14</b> Recommender Systems</a><ul>
<li class="chapter" data-level="14.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#introduction-14"><i class="fa fa-check"></i><b>14.1</b> Introduction</a><ul>
<li class="chapter" data-level="14.1.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#the-netflix-prize"><i class="fa fa-check"></i><b>14.1.1</b> The Netflix Prize</a></li>
<li class="chapter" data-level="14.1.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#main-approaches"><i class="fa fa-check"></i><b>14.1.2</b> Main Approaches</a></li>
<li class="chapter" data-level="14.1.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#formalism-1"><i class="fa fa-check"></i><b>14.1.3</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#collaborative-filtering"><i class="fa fa-check"></i><b>14.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="14.2.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#example"><i class="fa fa-check"></i><b>14.2.1</b> Example</a></li>
<li class="chapter" data-level="14.2.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#similarity-measures"><i class="fa fa-check"></i><b>14.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="14.2.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>14.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="14.2.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>14.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#exercise-the-movielens-dataset"><i class="fa fa-check"></i><b>14.3</b> Exercise: The MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="14.3.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#dataset"><i class="fa fa-check"></i><b>14.3.1</b> Dataset</a></li>
<li class="chapter" data-level="14.3.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#data-cleansing"><i class="fa fa-check"></i><b>14.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="14.3.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#item-item-similarities"><i class="fa fa-check"></i><b>14.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="14.3.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#example-recommendations"><i class="fa fa-check"></i><b>14.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="14.3.5" data-path="chap-recommenders.html"><a href="chap-recommenders.html#clustering-1"><i class="fa fa-check"></i><b>14.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#outro-9"><i class="fa fa-check"></i><b>14.4</b> Outro</a><ul>
<li class="chapter" data-level="14.4.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#remarks-10"><i class="fa fa-check"></i><b>14.4.1</b> Remarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="chap-text.html"><a href="chap-text.html"><i class="fa fa-check"></i><b>15</b> Natural Language Processing</a><ul>
<li class="chapter" data-level="15.1" data-path="chap-text.html"><a href="chap-text.html#to-do"><i class="fa fa-check"></i><b>15.1</b> TO DO</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-convention.html"><a href="appendix-convention.html"><i class="fa fa-check"></i><b>A</b> Notation Convention</a></li>
<li class="chapter" data-level="B" data-path="appendix-rintro.html"><a href="appendix-rintro.html"><i class="fa fa-check"></i><b>B</b> Setting Up the R Environment</a><ul>
<li class="chapter" data-level="B.1" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-r"><i class="fa fa-check"></i><b>B.1</b> Installing R</a></li>
<li class="chapter" data-level="B.2" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-an-ide"><i class="fa fa-check"></i><b>B.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="B.3" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-recommended-packages"><i class="fa fa-check"></i><b>B.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="B.4" data-path="appendix-rintro.html"><a href="appendix-rintro.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>B.4</b> First R Script in RStudio</a></li>
<li class="chapter" data-level="B.5" data-path="appendix-rintro.html"><a href="appendix-rintro.html#exercises-7"><i class="fa fa-check"></i><b>B.5</b> Exercises</a><ul>
<li class="chapter" data-level="B.5.1" data-path="appendix-rintro.html"><a href="appendix-rintro.html#first-steps-with-vectors"><i class="fa fa-check"></i><b>B.5.1</b> First Steps with Vectors</a></li>
<li class="chapter" data-level="B.5.2" data-path="appendix-rintro.html"><a href="appendix-rintro.html#basic-plotting"><i class="fa fa-check"></i><b>B.5.2</b> Basic Plotting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appendix-rvector.html"><a href="appendix-rvector.html"><i class="fa fa-check"></i><b>C</b> Vector Algebra in R</a><ul>
<li class="chapter" data-level="C.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#motivation-1"><i class="fa fa-check"></i><b>C.1</b> Motivation</a></li>
<li class="chapter" data-level="C.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#numeric-vectors"><i class="fa fa-check"></i><b>C.2</b> Numeric Vectors</a><ul>
<li class="chapter" data-level="C.2.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>C.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="C.2.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-vector-operations"><i class="fa fa-check"></i><b>C.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#aggregation-functions"><i class="fa fa-check"></i><b>C.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="C.2.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#special-functions"><i class="fa fa-check"></i><b>C.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="C.2.6" data-path="appendix-rvector.html"><a href="appendix-rvector.html#norms-and-distances"><i class="fa fa-check"></i><b>C.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="C.2.7" data-path="appendix-rvector.html"><a href="appendix-rvector.html#dot-product"><i class="fa fa-check"></i><b>C.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="C.2.8" data-path="appendix-rvector.html"><a href="appendix-rvector.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>C.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#logical-vectors"><i class="fa fa-check"></i><b>C.3</b> Logical Vectors</a><ul>
<li class="chapter" data-level="C.3.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-logical-vectors"><i class="fa fa-check"></i><b>C.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="C.3.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#logical-operations"><i class="fa fa-check"></i><b>C.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="C.3.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#comparison-operations"><i class="fa fa-check"></i><b>C.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="C.3.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#aggregation-functions-1"><i class="fa fa-check"></i><b>C.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#character-vectors"><i class="fa fa-check"></i><b>C.4</b> Character Vectors</a><ul>
<li class="chapter" data-level="C.4.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-character-vectors"><i class="fa fa-check"></i><b>C.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="C.4.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>C.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="C.4.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>C.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-subsetting"><i class="fa fa-check"></i><b>C.5</b> Vector Subsetting</a><ul>
<li class="chapter" data-level="C.5.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>C.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="C.5.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>C.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="C.5.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>C.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="C.5.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#replacing-elements"><i class="fa fa-check"></i><b>C.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="C.5.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#other-functions"><i class="fa fa-check"></i><b>C.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="appendix-rvector.html"><a href="appendix-rvector.html#named-vectors"><i class="fa fa-check"></i><b>C.6</b> Named Vectors</a><ul>
<li class="chapter" data-level="C.6.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-named-vectors"><i class="fa fa-check"></i><b>C.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="C.6.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>C.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="C.7" data-path="appendix-rvector.html"><a href="appendix-rvector.html#sec:factor"><i class="fa fa-check"></i><b>C.7</b> Factors</a><ul>
<li class="chapter" data-level="C.7.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-factors"><i class="fa fa-check"></i><b>C.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="C.7.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#levels"><i class="fa fa-check"></i><b>C.7.2</b> Levels</a></li>
<li class="chapter" data-level="C.7.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#internal-representation"><i class="fa fa-check"></i><b>C.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.8" data-path="appendix-rvector.html"><a href="appendix-rvector.html#lists"><i class="fa fa-check"></i><b>C.8</b> Lists</a><ul>
<li class="chapter" data-level="C.8.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-lists"><i class="fa fa-check"></i><b>C.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="C.8.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#named-lists"><i class="fa fa-check"></i><b>C.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="C.8.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>C.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="C.8.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#common-operations"><i class="fa fa-check"></i><b>C.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="appendix-rvector.html"><a href="appendix-rvector.html#exercises-8"><i class="fa fa-check"></i><b>C.9</b> Exercises</a><ul>
<li class="chapter" data-level="C.9.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#audeur-exchange-rates"><i class="fa fa-check"></i><b>C.9.1</b> AUD/EUR Exchange Rates</a></li>
</ul></li>
<li class="chapter" data-level="C.10" data-path="appendix-rvector.html"><a href="appendix-rvector.html#further-reading"><i class="fa fa-check"></i><b>C.10</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html"><i class="fa fa-check"></i><b>D</b> Matrix Algebra in R</a><ul>
<li class="chapter" data-level="D.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#creating-matrices"><i class="fa fa-check"></i><b>D.1</b> Creating Matrices</a><ul>
<li class="chapter" data-level="D.1.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix"><i class="fa fa-check"></i><b>D.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="D.1.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#stacking-vectors"><i class="fa fa-check"></i><b>D.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="D.1.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>D.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="D.1.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>D.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="D.1.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#other-methods"><i class="fa fa-check"></i><b>D.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="D.1.6" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#internal-representation-1"><i class="fa fa-check"></i><b>D.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#common-operations-1"><i class="fa fa-check"></i><b>D.2</b> Common Operations</a><ul>
<li class="chapter" data-level="D.2.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-transpose"><i class="fa fa-check"></i><b>D.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="D.2.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>D.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="D.2.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>D.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="D.2.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-multiplication"><i class="fa fa-check"></i><b>D.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="D.2.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>D.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="D.2.6" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#vectorised-special-functions"><i class="fa fa-check"></i><b>D.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="D.2.7" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-vector-operations"><i class="fa fa-check"></i><b>D.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-subsetting"><i class="fa fa-check"></i><b>D.3</b> Matrix Subsetting</a><ul>
<li class="chapter" data-level="D.3.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-individual-elements"><i class="fa fa-check"></i><b>D.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="D.3.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>D.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="D.3.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-submatrices"><i class="fa fa-check"></i><b>D.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="D.3.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>D.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="D.3.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>D.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#exercises-9"><i class="fa fa-check"></i><b>D.4</b> Exercises</a><ul>
<li class="chapter" data-level="D.4.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#currency-exchange-rates"><i class="fa fa-check"></i><b>D.4.1</b> Currency Exchange Rates</a></li>
<li class="chapter" data-level="D.4.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#currency-exchange-rates-relative-to-1999"><i class="fa fa-check"></i><b>D.4.2</b> Currency Exchange Rates Relative to 1999</a></li>
</ul></li>
<li class="chapter" data-level="D.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#further-reading-1"><i class="fa fa-check"></i><b>D.5</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="appendix-rdf.html"><a href="appendix-rdf.html"><i class="fa fa-check"></i><b>E</b> Data Frame Wrangling in R</a><ul>
<li class="chapter" data-level="E.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#creating-data-frames"><i class="fa fa-check"></i><b>E.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="E.2" data-path="appendix-rdf.html"><a href="appendix-rdf.html#importing-data-frames"><i class="fa fa-check"></i><b>E.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="E.3" data-path="appendix-rdf.html"><a href="appendix-rdf.html#data-frame-subsetting"><i class="fa fa-check"></i><b>E.3</b> Data Frame Subsetting</a><ul>
<li class="chapter" data-level="E.3.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>E.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="E.3.2" data-path="appendix-rdf.html"><a href="appendix-rdf.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>E.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="appendix-rdf.html"><a href="appendix-rdf.html#common-operations-2"><i class="fa fa-check"></i><b>E.4</b> Common Operations</a></li>
<li class="chapter" data-level="E.5" data-path="appendix-rdf.html"><a href="appendix-rdf.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>E.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="E.6" data-path="appendix-rdf.html"><a href="appendix-rdf.html#exercises-10"><i class="fa fa-check"></i><b>E.6</b> Exercises</a><ul>
<li class="chapter" data-level="E.6.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#urban-forest"><i class="fa fa-check"></i><b>E.6.1</b> Urban Forest</a></li>
</ul></li>
<li class="chapter" data-level="E.7" data-path="appendix-rdf.html"><a href="appendix-rdf.html#air-quality"><i class="fa fa-check"></i><b>E.7</b> Air Quality</a></li>
<li class="chapter" data-level="E.8" data-path="appendix-rdf.html"><a href="appendix-rdf.html#further-reading-2"><i class="fa fa-check"></i><b>E.8</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="appendix-datasets.html"><a href="appendix-datasets.html"><i class="fa fa-check"></i><b>F</b> Datasets</a><ul>
<li class="chapter" data-level="F.1" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:ssi"><i class="fa fa-check"></i><b>F.1</b> Sustainable Society Indices</a></li>
<li class="chapter" data-level="F.2" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:air-quality"><i class="fa fa-check"></i><b>F.2</b> Air Quality</a></li>
<li class="chapter" data-level="F.3" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:currency-exchange"><i class="fa fa-check"></i><b>F.3</b> Currency Exchange Rates</a></li>
<li class="chapter" data-level="F.4" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:urban-forest"><i class="fa fa-check"></i><b>F.4</b> Urban Forest</a></li>
<li class="chapter" data-level="F.5" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sec:wine-quality"><i class="fa fa-check"></i><b>F.5</b> Wine Quality</a></li>
<li class="chapter" data-level="F.6" data-path="appendix-datasets.html"><a href="appendix-datasets.html#the-world-factbook-countries-of-the-world"><i class="fa fa-check"></i><b>F.6</b> The World Factbook (Countries of the World)</a></li>
<li class="chapter" data-level="F.7" data-path="appendix-datasets.html"><a href="appendix-datasets.html#edstats-country-level-education-statistics"><i class="fa fa-check"></i><b>F.7</b> EdStats (Country-Level Education Statistics)</a></li>
<li class="chapter" data-level="F.8" data-path="appendix-datasets.html"><a href="appendix-datasets.html#food-and-nutrient-database-for-dietary-studies-fndds"><i class="fa fa-check"></i><b>F.8</b> Food and Nutrient Database for Dietary Studies (FNDDS)</a></li>
<li class="chapter" data-level="F.9" data-path="appendix-datasets.html"><a href="appendix-datasets.html#clustering-benchmarks"><i class="fa fa-check"></i><b>F.9</b> Clustering Benchmarks</a></li>
<li class="chapter" data-level="F.10" data-path="appendix-datasets.html"><a href="appendix-datasets.html#movie-lens-todo"><i class="fa fa-check"></i><b>F.10</b> Movie Lens (TODO)</a></li>
<li class="chapter" data-level="F.11" data-path="appendix-datasets.html"><a href="appendix-datasets.html#other-todo"><i class="fa fa-check"></i><b>F.11</b> Other (TODO)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.3 2020-06-25 22:37 (b3c4ab0)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap:kmeans" class="section level1">
<h1><span class="header-section-number">10</span> Clustering with K-Means</h1>
<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->
<p><strong>TODO</strong> In this chapter, we will:</p>
<ul>
<li><p>…</p></li>
<li><p>…</p></li>
</ul>
<!-- TODO: citations


centroid linkage

k-nearest centroids

exercise: Ward linkage




Note that we have studied "crisp" (disjoint) partitions only.



-->
<div id="k-means-clustering" class="section level2">
<h2><span class="header-section-number">10.1</span> K-means Clustering</h2>
<div id="example-in-r-4" class="section level3">
<h3><span class="header-section-number">10.1.1</span> Example in R</h3>
<p>..hierarchical clustering is nice, because it outputs a whole
hierarchy of nested partitions and it works in arbitrary spaces
equipped with a distance, but most algorithms are slow for large datasets</p>
<p>Let’s begin our clustering adventure
by applying the <span class="math inline">\(K\)</span>-means clustering method to find <span class="math inline">\(K=3\)</span> groups
in the famous Fisher’s <code>iris</code> data set (variables <code>Sepal.Width</code>
and <code>Petal.Length</code> variables only):</p>
<div class="sourceCode" id="cb604"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb604-1" title="1">X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(iris[,<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">2</span>)])</a>
<a class="sourceLine" id="cb604-2" title="2"><span class="co"># never forget to set nstart&gt;&gt;1!</span></a>
<a class="sourceLine" id="cb604-3" title="3">km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, <span class="dt">centers=</span><span class="dv">3</span>, <span class="dt">nstart=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb604-4" title="4">km<span class="op">$</span>cluster <span class="co"># labels assigned to each of 150 points:</span></a></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [35] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [69] 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [ reached getOption(&quot;max.print&quot;) -- omitted 51 entries ]</code></pre>
<dl>
<dt>Remark.</dt>
<dd><p>Later we’ll see that <code>nstart</code> is responsible for random restarting the
(local) optimisation procedure, just as we did in the previous chapter.</p>
</dd>
</dl>
<p>Let’s draw a scatter plot that depicts the detected clusters:</p>
<div class="sourceCode" id="cb606"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb606-1" title="1"><span class="kw">plot</span>(X, <span class="dt">col=</span>km<span class="op">$</span>cluster)</a></code></pre></div>
<div class="figure"><span id="fig:kmeans12"></span>
<img src="090-kmeans-figures/kmeans12-1.svg" alt=" 3-means clustering on a projection of the Iris dataset" />
<p class="caption">Figure 10.1:  3-means clustering on a projection of the Iris dataset</p>
</div>
<p>The colours in Figure <a href="chap-kmeans.html#fig:kmeans12">10.1</a> indicate the detected clusters.
The left group is clearly well-separated from the other two.</p>
<p>What can we do with this information? Well, if we were experts on plants
(in the 1930s), that’d definitely be something ground-breaking.
Figure <a href="chap-kmeans.html#fig:kmeans123">10.2</a> is a version of the aforementioned scatter plot
now with the true iris species added.</p>
<div class="sourceCode" id="cb607"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb607-1" title="1"><span class="kw">plot</span>(X, <span class="dt">col=</span>km<span class="op">$</span>cluster, <span class="dt">pch=</span><span class="kw">as.numeric</span>(iris<span class="op">$</span>Species))</a></code></pre></div>
<div class="figure"><span id="fig:kmeans123"></span>
<img src="090-kmeans-figures/kmeans123-1.svg" alt=" 3-means clustering (colours) vs true Iris species (shapes)" />
<p class="caption">Figure 10.2:  3-means clustering (colours) vs true Iris species (shapes)</p>
</div>
<p>Here is a contingency table for detected clusters vs. true iris species:</p>
<div class="sourceCode" id="cb608"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb608-1" title="1">(C &lt;-<span class="st"> </span><span class="kw">table</span>(km<span class="op">$</span>cluster, iris<span class="op">$</span>Species))</a></code></pre></div>
<pre><code>##    
##     setosa versicolor virginica
##   1     50          0         0
##   2      0          2        41
##   3      0         48         9</code></pre>
<p>It turns out that the discovered partition matches the original iris
species very well. We have just made a “discovery” in the field of
botany (actually some research fields classify their objects of study
into families, genres etc. by means of such tools).</p>
<p>Were the actual Iris species what we had hoped to match?
Was that our aim? Well, surely we have had begun our journey with
“clear minds” (yet with hungry eyes). Note that the true class labels
were not used during the clustering procedure – we’re dealing with
an unsupervised learning problem here. The result turned useful,
it’s a win.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) There are several indices that assess the
similarity of two partitions, for example the Adjusted Rand Index (ARI)
the Normalised Mutual Information Score (NMI)
or set matching-based measures,
see, e.g., <span class="citation">(Hubert &amp; Arabie <a href="references.html#ref-comparing_paritions">1985</a>)</span>, <span class="citation">(Rezaei &amp; Fränti <a href="references.html#ref-external_cluster_validity">2016</a>)</span>.</p>
</dd>
</dl>
<!-- genieclust.compare_partitions ? -->
<!-- OK, this is not the best measure:

sum(apply(C, 1, max))/sum(C) # "accuracy"

genieclust.compare_partitions ?

-->
</div>
<div id="problem-statement" class="section level3">
<h3><span class="header-section-number">10.1.2</span> Problem Statement</h3>
<p>The aim of <em><span class="math inline">\(K\)</span>-means clustering</em> is to find <span class="math inline">\(K\)</span> “good” cluster centres
<span class="math inline">\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)</span>.</p>
<p>Then, a point <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> will be assigned to the
cluster represented by the closest centre. Here, by <em>closest</em>
we mean the <em>squared</em> Euclidean distance.</p>
<p>More formally,
assuming all the points are in a <span class="math inline">\(p\)</span>-dimensional space, <span class="math inline">\(\mathbb{R}^p\)</span>,
we define the distance between the <span class="math inline">\(i\)</span>-th point and the <span class="math inline">\(k\)</span>-th
centre as:</p>
<p><span class="math display">\[
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}) = \| \mathbf{x}_{i,\cdot} - \boldsymbol\mu_{k,\cdot} \|^2 =  \sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\]</span></p>
<p>Then the <span class="math inline">\(i\)</span>-th point’s cluster is determined by:</p>
<p><span class="math display">\[
\mathrm{C}(i) = \mathrm{arg}\min_{k=1,\dots,K}
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}),
\]</span></p>
<p>where, as usual, <span class="math inline">\(\mathrm{arg}\min\)</span> (argument minimum) is the index
<span class="math inline">\(k\)</span> that minimises the given expression.</p>
<p>In the previous example, the three identified cluster centres in <span class="math inline">\(\mathbb{R}^2\)</span>
are given by (see Figure @ref(fig:kmeans_problem1) for illustration):</p>
<div class="sourceCode" id="cb610"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb610-1" title="1">km<span class="op">$</span>centers</a></code></pre></div>
<pre><code>##   Petal.Length Sepal.Width
## 1       1.4620      3.4280
## 2       5.6721      3.0326
## 3       4.3281      2.7509</code></pre>
<div class="sourceCode" id="cb612"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb612-1" title="1"><span class="kw">plot</span>(X, <span class="dt">col=</span>km<span class="op">$</span>cluster, <span class="dt">asp=</span><span class="dv">1</span>) <span class="co"># asp=1 gives the same scale on both axes</span></a>
<a class="sourceLine" id="cb612-2" title="2"><span class="kw">points</span>(km<span class="op">$</span>centers, <span class="dt">cex=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="dv">4</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a></code></pre></div>
<div class="figure">
<img src="090-kmeans-figures/kmeans_problem1-1.svg" alt="(#fig:kmeans_problem1) Cluster centres (blue dots) identified by the 3-means algorithm" />
<p class="caption">(#fig:kmeans_problem1) Cluster centres (blue dots) identified by the 3-means algorithm</p>
</div>
<p>Figure @ref(fig:kmeans_problem2) depicts the partition
of the whole <span class="math inline">\(\mathbb{R}^2\)</span> space
into clusters based on the closeness to the three cluster centres.</p>
<div class="figure">
<img src="090-kmeans-figures/kmeans_problem2-1.svg" alt="(#fig:kmeans_problem2) The division of the whole space into three sets based on the proximity to cluster centres (a so-called Voronoi diagram)" />
<p class="caption">(#fig:kmeans_problem2) The division of the whole space into three sets based on the proximity to cluster centres (a so-called Voronoi diagram)</p>
</div>
<p>To compute the distances between all the points and the cluster centres,
we may call <code>pdist::pdist()</code>:</p>
<div class="sourceCode" id="cb613"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb613-1" title="1"><span class="kw">library</span>(<span class="st">&quot;pdist&quot;</span>)</a>
<a class="sourceLine" id="cb613-2" title="2">D &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">pdist</span>(X, km<span class="op">$</span>centers))<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb613-3" title="3"><span class="kw">head</span>(D)</a></code></pre></div>
<pre><code>##          [,1]   [,2]   [,3]
## [1,] 0.009028 18.469 9.1348
## [2,] 0.187028 18.252 8.6357
## [3,] 0.078228 19.143 9.3709
## [4,] 0.109028 17.411 8.1199
## [5,] 0.033428 18.573 9.2946
## [6,] 0.279428 16.530 8.2272</code></pre>
<p>where <code>D[i,k]</code> gives the squared Euclidean distance between
<span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> and <span class="math inline">\(\boldsymbol\mu_{k,\cdot}\)</span>.</p>
<p>The cluster memberships the (<span class="math inline">\(\mathrm{arg}\min\)</span>s)
can now be determined by:</p>
<div class="sourceCode" id="cb615"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb615-1" title="1">(idx &lt;-<span class="st"> </span><span class="kw">apply</span>(D, <span class="dv">1</span>, which.min)) <span class="co"># for every row of D...</span></a></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [35] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [69] 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [ reached getOption(&quot;max.print&quot;) -- omitted 51 entries ]</code></pre>
<div class="sourceCode" id="cb617"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb617-1" title="1"><span class="kw">all</span>(km<span class="op">$</span>cluster <span class="op">==</span><span class="st"> </span>idx) <span class="co"># sanity check</span></a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="algorithms-for-the-k-means-problem" class="section level3">
<h3><span class="header-section-number">10.1.3</span> Algorithms for the K-means Problem</h3>
<p>All good, but how do we find “good” cluster centres?
Good, better, best… yet again we are in a need for a goodness-of-fit metric.
In the <span class="math inline">\(K\)</span>-means clustering, we determine
<span class="math inline">\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)</span>
that minimise the total within-cluster distances (distances from each point
to each own cluster centre):</p>
<p><span class="math display">\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n
d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{C(i),\cdot}),
\]</span></p>
<p>Note that the <span class="math inline">\(\boldsymbol\mu\)</span>s are also “hidden” inside the point-to-cluster
belongingness mapping, <span class="math inline">\(\mathrm{C}\)</span>.
Expanding the above yields:</p>
<p><span class="math display">\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{k=1,\dots,K}
\sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\right).
\]</span></p>
<p>Unfortunately, the <span class="math inline">\(\min\)</span> operator in the objective function
makes this optimisation problem not tractable
with the methods discussed in the previous chapter.</p>
<p>The above problem is <em>hard</em> to solve (* more precisely,
it is an NP-hard problem).
Therefore, in practice we use various heuristics to solve it.
The <code>kmeans()</code> function itself implements 3 of them:
the Hartigan-Wong, Lloyd (a.k.a. Lloyd-Forgy) and MacQueen algorithms.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Technically, there is no such thing as “<em>the</em> K-means algorithm” –
all the aforementioned methods are particular heuristic
approaches to solving the K-means
clustering problem formalised as the above optimisation task.
By setting <code>nstart = 10</code> above, we ask the (Hartigan-Wong, which is
the default one in <code>kmeans()</code>) algorithm
to find 10 solution candidates obtained by considering different random
initial clusterings and choose the best one (with respect to the sum
of within-cluster distances) amongst them. This does not guarantee
finding the optimal solution, especially for very unbalanced datasets,
but increases the likelihood of such.</p>
</dd>
<dt>Remark.</dt>
<dd><p>The <em>squared</em> Euclidean distance was of course chosen to make computations
easier. It turns out that for any given subset of input points
<span class="math inline">\(\mathbf{x}_{i_1,\cdot},\dots,\mathbf{x}_{i_m,\cdot}\)</span>,
the point <span class="math inline">\(\boldsymbol\mu_{k,\cdot}\)</span> that minimises the total distances
to all of them, i.e.,</p>
<p><span class="math display">\[
    \min_{\boldsymbol\mu_{k,\cdot}\in \mathbb{R}^p}
    \sum_{\ell=1}^m \left(
    \sum_{j=1}^p \left(x_{i_\ell,j}-\mu_{k,j}\right)^2
    \right),
\]</span></p>
<p>is exactly these points’ <em>centroid</em> – which is given by
the componentwise arithmetic means of their coordinates.</p>
<p>For example:</p>
<div class="sourceCode" id="cb619"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb619-1" title="1"><span class="kw">colMeans</span>(X[km<span class="op">$</span>cluster <span class="op">==</span><span class="st"> </span><span class="dv">1</span>,]) <span class="co"># centroid of the points in the 1st cluster</span></a></code></pre></div>
<pre><code>## Petal.Length  Sepal.Width 
##        1.462        3.428</code></pre>
<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb621-1" title="1">km<span class="op">$</span>centers[<span class="dv">1</span>,] <span class="co"># the centre of the 1st cluster</span></a></code></pre></div>
<pre><code>## Petal.Length  Sepal.Width 
##        1.462        3.428</code></pre>
</dd>
</dl>
<div style="margin-top: 1em">

</div>
<p>Among the various heuristics to solve the K-means problem,
Lloyd’s algorithm (1957) is perhaps the simplest.
This is probably the reason why it is sometimes referred
to as “the” K-means algorithm:</p>
<ol style="list-style-type: decimal">
<li><p>Start with random cluster centres <span class="math inline">\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)</span>.</p></li>
<li><p>For each point <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span>, determine its closest centre <span class="math inline">\(C(i)\in\{1,\dots,K\}\)</span>:</p>
<p><span class="math display">\[
 \mathrm{C}(i) = \mathrm{arg}\min_{k=1,\dots,K}
 d(\mathbf{x}_{i,\cdot}, \boldsymbol\mu_{k,\cdot}).
 \]</span></p></li>
<li><p>For each cluster <span class="math inline">\(k\in\{1,\dots,K\}\)</span>, compute the new cluster centre <span class="math inline">\(\boldsymbol\mu_{k,\cdot}\)</span> as the centroid of
all the point indices <span class="math inline">\(i\)</span> such that <span class="math inline">\(C(i)=k\)</span>.</p></li>
<li><p>If the cluster centres changed since the last iteration, go to step 2,
otherwise stop and return the result.</p></li>
</ol>
<div style="margin-top: 1em">

</div>
<p>(*) Here’s an example implementation.
As the initial cluster centres, let’s pick some “noisy” versions
of <span class="math inline">\(K\)</span> randomly chosen points in <span class="math inline">\(\mathbf{X}\)</span>.</p>
<div class="sourceCode" id="cb623"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb623-1" title="1"><span class="kw">set.seed</span>(<span class="dv">12345</span>)</a>
<a class="sourceLine" id="cb623-2" title="2">K &lt;-<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb623-3" title="3"></a>
<a class="sourceLine" id="cb623-4" title="4"><span class="co"># Random initial cluster centres:</span></a>
<a class="sourceLine" id="cb623-5" title="5">M &lt;-<span class="st"> </span><span class="kw">jitter</span>(X[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(X), K),])</a>
<a class="sourceLine" id="cb623-6" title="6">M</a></code></pre></div>
<pre><code>##      Petal.Length Sepal.Width
## [1,]       5.1004      3.0814
## [2,]       4.7091      3.1861
## [3,]       3.3196      2.4094</code></pre>
<p>In what follows, we will be maintaining a matrix
such that <code>D[i,k]</code> is the distance between the <span class="math inline">\(i\)</span>-th point
and the <span class="math inline">\(k\)</span>-th centre and a vector such that <code>idx[i]</code>
denotes the index of the cluster centre closest to the <code>i</code>-th point.</p>
<div class="sourceCode" id="cb625"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb625-1" title="1">D &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">pdist</span>(X, M))<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb625-2" title="2">idx &lt;-<span class="st"> </span><span class="kw">apply</span>(D, <span class="dv">1</span>, which.min)</a></code></pre></div>
<div class="sourceCode" id="cb626"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb626-1" title="1"><span class="cf">repeat</span> {</a>
<a class="sourceLine" id="cb626-2" title="2">    <span class="co"># Determine the new cluster centres:</span></a>
<a class="sourceLine" id="cb626-3" title="3">    M &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>K, <span class="cf">function</span>(k) {</a>
<a class="sourceLine" id="cb626-4" title="4">        <span class="co"># the centroid of all points in the k-th cluster:</span></a>
<a class="sourceLine" id="cb626-5" title="5">        <span class="kw">colMeans</span>(X[idx<span class="op">==</span>k,])</a>
<a class="sourceLine" id="cb626-6" title="6">    }))</a>
<a class="sourceLine" id="cb626-7" title="7"></a>
<a class="sourceLine" id="cb626-8" title="8">    <span class="co"># Store the previous cluster belongingness info:</span></a>
<a class="sourceLine" id="cb626-9" title="9">    old_idx &lt;-<span class="st"> </span>idx</a>
<a class="sourceLine" id="cb626-10" title="10"></a>
<a class="sourceLine" id="cb626-11" title="11">    <span class="co"># Recompute D and idx:</span></a>
<a class="sourceLine" id="cb626-12" title="12">    D &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">pdist</span>(X, M))<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb626-13" title="13">    idx &lt;-<span class="st"> </span><span class="kw">apply</span>(D, <span class="dv">1</span>, which.min)</a>
<a class="sourceLine" id="cb626-14" title="14"></a>
<a class="sourceLine" id="cb626-15" title="15">    <span class="co"># Check if converged already:</span></a>
<a class="sourceLine" id="cb626-16" title="16">    <span class="cf">if</span> (<span class="kw">all</span>(idx <span class="op">==</span><span class="st"> </span>old_idx)) <span class="cf">break</span></a>
<a class="sourceLine" id="cb626-17" title="17">}</a></code></pre></div>
<p>Let’s compare the obtained cluster centres with the ones returned
by <code>kmeans()</code>:</p>
<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb627-1" title="1">M <span class="co"># our result</span></a></code></pre></div>
<pre><code>##      Petal.Length Sepal.Width
## [1,]       5.6721      3.0326
## [2,]       4.3281      2.7509
## [3,]       1.4620      3.4280</code></pre>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb629-1" title="1">km<span class="op">$</span>center <span class="co"># result of kmeans()</span></a></code></pre></div>
<pre><code>##   Petal.Length Sepal.Width
## 1       1.4620      3.4280
## 2       5.6721      3.0326
## 3       4.3281      2.7509</code></pre>
<p>These two represent exactly the same 3-partitions
(note that the actual labels (the order of centres) are not important).</p>
<p>The value of the objective function (total within-cluster distances)
at the identified candidate solution
is equal to:</p>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb631-1" title="1"><span class="kw">sum</span>(D[<span class="kw">cbind</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(X),idx)]) <span class="co"># indexing with a 2-column matrix!</span></a></code></pre></div>
<pre><code>## [1] 40.737</code></pre>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb633-1" title="1">km<span class="op">$</span>tot.withinss <span class="co"># as reported by kmeans()</span></a></code></pre></div>
<pre><code>## [1] 40.737</code></pre>
<p>We would need it if we were to implement the <code>nstart</code> functionality,
which is left as an:</p>
<div class="exercise"><strong>Exercise.</strong>
<p>(*) Wrap the implementation of the Lloyd algorithm into a standalone R function,
with a similar look-and-feel as the original <code>kmeans()</code>.</p>
</div>
<div class="figure">
<img src="090-kmeans-figures/kmeanimpl_plot-1.svg" alt="(#fig:kmeanimpl_plot) The arrows denote the cluster centres in each iteration of the Lloyd algorithm" />
<p class="caption">(#fig:kmeanimpl_plot) The arrows denote the cluster centres in each iteration of the Lloyd algorithm</p>
</div>
<p>On a side note, our algorithm needed <code>4</code>
iterations to identify the (locally optimal) cluster centres.
Figure @ref(fig:kmeanimpl_plot) depicts its quest for the clustering grail.</p>
<!--

# TODO: k-medoids???

## TODO




TODO maybe k-medoids, nice for optimisation
plus they allow for different metrics

alternatively, DBSCAN or PCA or MDS or ...



-->
</div>
<div id="k-means-revisited" class="section level3">
<h3><span class="header-section-number">10.1.4</span> K-means Revisited</h3>
<p>In <strong>K-means clustering</strong> we are minimising the squared Euclidean distance
to each point’s cluster centre:
<span class="math display">\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{k=1,\dots,K}
\sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\right).
\]</span></p>
<p>This is an (NP-)hard problem!
There is no efficient exact algorithm.</p>
<p>We need approximations. In the last chapter, we have
discussed the iterative Lloyd’s algorithm (1957),
which is amongst a few procedures implemented in the <code>kmeans()</code> function.</p>
<p>To recall, Lloyd’s algorithm (1957) is sometimes referred to as “the” K-means algorithm:</p>
<ol style="list-style-type: decimal">
<li><p>Start with random cluster centres <span class="math inline">\(\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}\)</span>.</p></li>
<li><p>For each point <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span>, determine its closest centre <span class="math inline">\(C(i)\in\{1,\dots,K\}\)</span>.</p></li>
<li><p>For each cluster <span class="math inline">\(k\in\{1,\dots,K\}\)</span>, compute the new cluster centre <span class="math inline">\(\boldsymbol\mu_{k,\cdot}\)</span> as the componentwise arithmetic mean
of the coordinates of all the point indices <span class="math inline">\(i\)</span> such that <span class="math inline">\(C(i)=k\)</span>.</p></li>
<li><p>If the cluster centres changed since last iteration, go to step 2, otherwise stop and return the result.</p></li>
</ol>
<p> <br />
</p>
<p>As the procedure might get stuck in a local minimum,
a few restarts are recommended (as usual).</p>
<p>Hence, we are used to calling:</p>
<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb635-1" title="1"><span class="kw">kmeans</span>(X, <span class="dt">centers=</span>k, <span class="dt">nstart=</span><span class="dv">10</span>)</a></code></pre></div>
</div>
<div id="optim-vs.-kmeans" class="section level3">
<h3><span class="header-section-number">10.1.5</span> optim() vs. kmeans()</h3>
<p>Let us compare how a general-purpose optimiser such as the BFGS algorithm
implemented in <code>optim()</code> compares with a customised, problem-specific solver.</p>
<p>We will need some benchmark data.</p>
<div class="sourceCode" id="cb636"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb636-1" title="1">gen_cluster &lt;-<span class="st"> </span><span class="cf">function</span>(n, p, m, s) {</a>
<a class="sourceLine" id="cb636-2" title="2">    vectors &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p)</a>
<a class="sourceLine" id="cb636-3" title="3">    unit_vectors &lt;-<span class="st"> </span>vectors<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">rowSums</span>(vectors<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb636-4" title="4">    unit_vectors<span class="op">*</span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, s)<span class="op">+</span><span class="kw">rep</span>(m, <span class="dt">each=</span>n)</a>
<a class="sourceLine" id="cb636-5" title="5">}</a></code></pre></div>
<p>The above function generates <span class="math inline">\(n\)</span> points in <span class="math inline">\(\mathbb{R}^p\)</span>
from a distribution centred at <span class="math inline">\(\mathbf{m}\in\mathbb{R}^p\)</span>,
spread randomly in every possible direction with scale factor <span class="math inline">\(s\)</span>.</p>
<p>Two example clusters in <span class="math inline">\(\mathbb{R}^2\)</span>:</p>
<div class="sourceCode" id="cb637"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb637-1" title="1"><span class="co"># plot the &quot;black&quot; cluster</span></a>
<a class="sourceLine" id="cb637-2" title="2"><span class="kw">plot</span>(<span class="kw">gen_cluster</span>(<span class="dv">500</span>, <span class="dv">2</span>, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="dv">1</span>), <span class="dt">col=</span><span class="st">&quot;#00000022&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>,</a>
<a class="sourceLine" id="cb637-3" title="3">    <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">4</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">4</span>), <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">ann=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb637-4" title="4"><span class="co"># plot the &quot;red&quot; cluster</span></a>
<a class="sourceLine" id="cb637-5" title="5"><span class="kw">points</span>(<span class="kw">gen_cluster</span>(<span class="dv">250</span>, <span class="dv">2</span>, <span class="kw">c</span>(<span class="fl">1.5</span>, <span class="dv">1</span>), <span class="fl">0.5</span>), <span class="dt">col=</span><span class="st">&quot;#ff000022&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a></code></pre></div>
<div class="figure">
<img src="090-kmeans-figures/gendata_example-1.svg" alt="(#fig:gendata_example) plot of chunk gendata_example" />
<p class="caption">(#fig:gendata_example) plot of chunk gendata_example</p>
</div>
<p>Let’s generate the benchmark dataset <span class="math inline">\(\mathbf{X}\)</span>
that consists of three clusters in a high-dimensional space.</p>
<div class="sourceCode" id="cb638"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb638-1" title="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb638-2" title="2">p  &lt;-<span class="st"> </span><span class="dv">32</span></a>
<a class="sourceLine" id="cb638-3" title="3">Ns &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb638-4" title="4">Ms &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb638-5" title="5">s  &lt;-<span class="st"> </span><span class="fl">1.5</span><span class="op">*</span>p</a>
<a class="sourceLine" id="cb638-6" title="6">K  &lt;-<span class="st"> </span><span class="kw">length</span>(Ns)</a>
<a class="sourceLine" id="cb638-7" title="7"></a>
<a class="sourceLine" id="cb638-8" title="8">X &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>K, <span class="cf">function</span>(k)</a>
<a class="sourceLine" id="cb638-9" title="9">    <span class="kw">gen_cluster</span>(Ns[k], p, <span class="kw">rep</span>(Ms[k], p), s))</a>
<a class="sourceLine" id="cb638-10" title="10">X &lt;-<span class="st"> </span><span class="kw">do.call</span>(rbind, X) <span class="co"># rbind(X[[1]], X[[2]], X[[3]])</span></a></code></pre></div>
<!--
X <- as.matrix(read.csv("datasets/sipu_unbalance.csv",
    header=FALSE, sep=" ", comment.char="#"))
X <- X/10000-30 # a more user-friendly scale
K <- 8
p <- 2

library("FNN")
get_fitness <- function(mu, X) {
    # For each point in X,
    # get the index of the closest point in mu:
    memb <- FNN::get.knnx(mu, X, 1)$nn.index

    # compute the sum of squared distances
    # between each point and its closes cluster centre:
    sum((X-mu[memb,])^2)
}

Storn R, Price K (1997). “Differential Evolution – A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces.”Journal of Global Optimization,11(4), 341–359.

Mullen K, Ardia D, Gil D, Windover D, Cline J (2011). “DEoptim:  An R Package for GlobalOptimization by Differential Evolution.”Journal of Statistical Software,40(6), 1–26. URLhttp://www.jstatsoft.org/v40/i06/

library("DEoptim")
obj <- function(mu) {
            get_fitness(matrix(mu, nrow=K), X)
        }
res <- DEoptim(fn=obj,
        lower=rep(apply(X, 2, min), each=K),
        upper=rep(apply(X, 2, max), each=K)
        #control=list(itermax=1000)
        )

mu_res <- matrix(res$optim$bestmem, nrow=K)
plot(X)
points(mu_res, col=2,cex=3)
get_fitness(mu_res, X)
km <- kmeans(X, mu_res)
get_fitness(km$centers, X)




res <- optim(X[sample(nrow(X), K),],
    fn=obj,
        #lower=rep(apply(X, 2, min), each=K),
        #upper=rep(apply(X, 2, max), each=K),
        method="SANN",
        control = list(maxit = 20000)
        )

mu_res <- matrix(res$par, nrow=K)
plot(X)
points(mu_res, col=2,cex=3)
get_fitness(mu_res, X)
km <- kmeans(X, mu_res)
get_fitness(km$centers, X)

Zambrano-Bigiarini, M., Rojas, R. (2013). “A model-independent Particle Swarm Optimisation software for model calibration.” Environmental Modelling & Software, 43, 5-25. doi: 10.1016/j.envsoft.2013.01.004, http://dx.doi.org/10.1016/j.envsoft.2013.01.004.

library("hydroPSO")
res <- hydroPSO(fn=obj,
        lower=rep(apply(X, 2, min), each=K),
        upper=rep(apply(X, 2, max), each=K)
        #control=list(itermax=1000)
        )

mu_res <- matrix(res$par, nrow=K)
plot(X)
points(mu_res, col=2,cex=3)
get_fitness(mu_res, X)
km <- kmeans(X, mu_res)
get_fitness(km$centers, X)




cntr <- matrix(ncol=2, byrow=TRUE, c( # initial guess
   -15,   5,
   -12,   10,
   -10,   5,
    15,   0,
    15,   10,
    20,   5,
    25,   0,
    25,   10))
km <- kmeans(X, cntr)
get_fitness(km$centers, X)

km <- kmeans(X, K, nstart=10)
get_fitness(km$centers, X)

-->
<p>The objective function for the K-means clustering problem:</p>
<div class="sourceCode" id="cb639"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb639-1" title="1"><span class="kw">library</span>(<span class="st">&quot;FNN&quot;</span>)</a>
<a class="sourceLine" id="cb639-2" title="2">get_fitness &lt;-<span class="st"> </span><span class="cf">function</span>(mu, X) {</a>
<a class="sourceLine" id="cb639-3" title="3">    <span class="co"># For each point in X,</span></a>
<a class="sourceLine" id="cb639-4" title="4">    <span class="co"># get the index of the closest point in mu:</span></a>
<a class="sourceLine" id="cb639-5" title="5">    memb &lt;-<span class="st"> </span>FNN<span class="op">::</span><span class="kw">get.knnx</span>(mu, X, <span class="dv">1</span>)<span class="op">$</span>nn.index</a>
<a class="sourceLine" id="cb639-6" title="6"></a>
<a class="sourceLine" id="cb639-7" title="7">    <span class="co"># compute the sum of squared distances</span></a>
<a class="sourceLine" id="cb639-8" title="8">    <span class="co"># between each point and its closes cluster centre:</span></a>
<a class="sourceLine" id="cb639-9" title="9">    <span class="kw">sum</span>((X<span class="op">-</span>mu[memb,])<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb639-10" title="10">}</a></code></pre></div>
<p>Setting up the solvers:</p>
<div class="sourceCode" id="cb640"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb640-1" title="1">min_HartiganWong &lt;-<span class="st"> </span><span class="cf">function</span>(mu0, X)</a>
<a class="sourceLine" id="cb640-2" title="2">    <span class="kw">get_fitness</span>(</a>
<a class="sourceLine" id="cb640-3" title="3">        <span class="co"># algorithm=&quot;Hartigan-Wong&quot;</span></a>
<a class="sourceLine" id="cb640-4" title="4">        <span class="kw">kmeans</span>(X, mu0, <span class="dt">iter.max=</span><span class="dv">100</span>)<span class="op">$</span>centers,</a>
<a class="sourceLine" id="cb640-5" title="5">    X)</a>
<a class="sourceLine" id="cb640-6" title="6">min_Lloyd &lt;-<span class="st"> </span><span class="cf">function</span>(mu0, X)</a>
<a class="sourceLine" id="cb640-7" title="7">    <span class="kw">get_fitness</span>(</a>
<a class="sourceLine" id="cb640-8" title="8">        <span class="kw">kmeans</span>(X, mu0, <span class="dt">iter.max=</span><span class="dv">100</span>, <span class="dt">algorithm=</span><span class="st">&quot;Lloyd&quot;</span>)<span class="op">$</span>centers,</a>
<a class="sourceLine" id="cb640-9" title="9">    X)</a>
<a class="sourceLine" id="cb640-10" title="10">min_optim &lt;-<span class="st"> </span><span class="cf">function</span>(mu0, X)</a>
<a class="sourceLine" id="cb640-11" title="11">    <span class="kw">optim</span>(mu0,</a>
<a class="sourceLine" id="cb640-12" title="12">        <span class="cf">function</span>(mu, X) {</a>
<a class="sourceLine" id="cb640-13" title="13">            <span class="kw">get_fitness</span>(<span class="kw">matrix</span>(mu, <span class="dt">nrow=</span><span class="kw">nrow</span>(mu0)), X)</a>
<a class="sourceLine" id="cb640-14" title="14">        }, <span class="dt">X=</span>X, <span class="dt">method=</span><span class="st">&quot;BFGS&quot;</span>, <span class="dt">control=</span><span class="kw">list</span>(<span class="dt">reltol=</span><span class="fl">1e-16</span>)</a>
<a class="sourceLine" id="cb640-15" title="15">    )<span class="op">$</span>val</a></code></pre></div>
<p>Running the simulation:</p>
<div class="sourceCode" id="cb641"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb641-1" title="1">nstart &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb641-2" title="2"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb641-3" title="3">res &lt;-<span class="st"> </span><span class="kw">replicate</span>(nstart, {</a>
<a class="sourceLine" id="cb641-4" title="4">  mu0 &lt;-<span class="st"> </span>X[<span class="kw">sample</span>(<span class="kw">nrow</span>(X), K),]</a>
<a class="sourceLine" id="cb641-5" title="5">    <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb641-6" title="6">        <span class="dt">HartiganWong=</span><span class="kw">min_HartiganWong</span>(mu0, X),</a>
<a class="sourceLine" id="cb641-7" title="7">        <span class="dt">Lloyd=</span><span class="kw">min_Lloyd</span>(mu0, X),</a>
<a class="sourceLine" id="cb641-8" title="8">        <span class="dt">optim=</span><span class="kw">min_optim</span>(mu0, X)</a>
<a class="sourceLine" id="cb641-9" title="9">    )</a>
<a class="sourceLine" id="cb641-10" title="10">})</a></code></pre></div>
<p>Notice a considerable variability of the
objective function at the local minima found:</p>
<div class="sourceCode" id="cb642"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb642-1" title="1"><span class="kw">boxplot</span>(<span class="kw">as.data.frame</span>(<span class="kw">t</span>(res)), <span class="dt">horizontal=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="st">&quot;white&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:gendata5"></span>
<img src="090-kmeans-figures/gendata5-1.svg" alt=" plot of chunk gendata5" />
<p class="caption">Figure 10.3:  plot of chunk gendata5</p>
</div>
<div class="sourceCode" id="cb643"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb643-1" title="1"><span class="kw">print</span>(<span class="kw">apply</span>(res, <span class="dv">1</span>, <span class="cf">function</span>(x)</a>
<a class="sourceLine" id="cb643-2" title="2">    <span class="kw">c</span>(<span class="kw">summary</span>(x), <span class="dt">sd=</span><span class="kw">sd</span>(x))</a>
<a class="sourceLine" id="cb643-3" title="3">))</a></code></pre></div>
<pre><code>##         HartiganWong    Lloyd  optim
## Min.          421889 425119.5 422989
## 1st Qu.       424663 433669.3 432446
## Median        427129 438502.2 440033
## Mean          426557 438075.0 440635
## 3rd Qu.       428243 441381.3 446614
## Max.          431869 450469.7 466303
## sd              2301   5709.3  10888</code></pre>
<p>Of course, we are interested in the smallest value of the objective,
because we’re trying to pinpoint the global minimum.</p>
<div class="sourceCode" id="cb645"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb645-1" title="1"><span class="kw">print</span>(<span class="kw">apply</span>(res, <span class="dv">1</span>, min))</a></code></pre></div>
<pre><code>## HartiganWong        Lloyd        optim 
##       421889       425119       422989</code></pre>
<p>The Hartigan-Wong algorithm (the default one in <code>kmeans()</code>)
is the most reliable one of the three:</p>
<ul>
<li>it gives the best solution (low bias)</li>
<li>the solutions have the lowest degree of variability (low variance)</li>
<li>it is the fastest:</li>
</ul>
<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb647-1" title="1"><span class="kw">library</span>(<span class="st">&quot;microbenchmark&quot;</span>)</a>
<a class="sourceLine" id="cb647-2" title="2"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb647-3" title="3">mu0 &lt;-<span class="st"> </span>X[<span class="kw">sample</span>(<span class="kw">nrow</span>(X), K),]</a>
<a class="sourceLine" id="cb647-4" title="4"><span class="kw">summary</span>(<span class="kw">microbenchmark</span>(</a>
<a class="sourceLine" id="cb647-5" title="5">    <span class="dt">HartiganWong=</span><span class="kw">min_HartiganWong</span>(mu0, X),</a>
<a class="sourceLine" id="cb647-6" title="6">    <span class="dt">Lloyd=</span><span class="kw">min_Lloyd</span>(mu0, X),</a>
<a class="sourceLine" id="cb647-7" title="7">    <span class="dt">optim=</span><span class="kw">min_optim</span>(mu0, X),</a>
<a class="sourceLine" id="cb647-8" title="8">    <span class="dt">times=</span><span class="dv">10</span></a>
<a class="sourceLine" id="cb647-9" title="9">), <span class="dt">unit=</span><span class="st">&quot;relative&quot;</span>)</a></code></pre></div>
<pre><code>##           expr       min        lq      mean    median        uq    max
## 1 HartiganWong    1.1026    1.1198    1.1888    1.2083    1.2776    1.3
## 2        Lloyd    1.0000    1.0000    1.0000    1.0000    1.0000    1.0
## 3        optim 1651.0947 1647.9260 1609.1389 1639.2033 1620.5961 1515.4
##   neval
## 1    10
## 2    10
## 3    10</code></pre>
<div class="sourceCode" id="cb649"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb649-1" title="1"><span class="kw">print</span>(<span class="kw">min</span>(res))</a></code></pre></div>
<pre><code>## [1] 421889</code></pre>
<p>Is it the global minimum?</p>
<blockquote>
<p>We don’t know, we just didn’t happen to find anything better (yet).</p>
</blockquote>
<p>Did we put enough effort to find it?</p>
<blockquote>
<p>Well, maybe. We can try more random restarts:</p>
</blockquote>
<div class="sourceCode" id="cb651"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb651-1" title="1">res_tried_very_hard &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, K, <span class="dt">nstart=</span><span class="dv">100000</span>, <span class="dt">iter.max=</span><span class="dv">10000</span>)<span class="op">$</span>centers</a>
<a class="sourceLine" id="cb651-2" title="2"><span class="kw">print</span>(<span class="kw">get_fitness</span>(res_tried_very_hard, X))</a></code></pre></div>
<pre><code>## [1] 421889</code></pre>
<p>Is it good enough?</p>
<blockquote>
<p>It depends what we’d like to do with this. Does it make your boss happy?
Does it generate revenue? Does it help solve any other problem?
Is it useful anyhow?
Are you really looking for the global minimum?</p>
</blockquote>
</div>
</div>
<div id="exercises-6" class="section level2">
<h2><span class="header-section-number">10.2</span> Exercises</h2>
<div id="clustering-of-the-world-factbook" class="section level3">
<h3><span class="header-section-number">10.2.1</span> Clustering of the World Factbook</h3>
<p>Let’s perform a cluster analysis of countries
based on the information contained in the World Factbook dataset:</p>
<div class="sourceCode" id="cb653"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb653-1" title="1">factbook &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/world_factbook_2020.csv&quot;</span>,</a>
<a class="sourceLine" id="cb653-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a></code></pre></div>
<div class="exercise"><strong>Exercise.</strong>
<p>Remove all the columns that consist of more than 40 missing values.
Then remove all the rows with at least 1 missing value.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>To remove appropriate columns, we must first count the number
of <code>NA</code>s in them.</p>
<div class="sourceCode" id="cb654"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb654-1" title="1">count_na_in_columns &lt;-<span class="st"> </span><span class="kw">sapply</span>(factbook, <span class="cf">function</span>(x) <span class="kw">sum</span>(<span class="kw">is.na</span>(x)))</a>
<a class="sourceLine" id="cb654-2" title="2">factbook &lt;-<span class="st"> </span>factbook[count_na_in_columns <span class="op">&lt;=</span><span class="st"> </span><span class="dv">40</span>] <span class="co"># column removal</span></a></code></pre></div>
<p>Getting rid of the rows plagued by missing values is as simple as calling
the <code>na.omit()</code> function:</p>
<div class="sourceCode" id="cb655"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb655-1" title="1">factbook &lt;-<span class="st"> </span><span class="kw">na.omit</span>(factbook) <span class="co"># row removal</span></a>
<a class="sourceLine" id="cb655-2" title="2"><span class="kw">dim</span>(factbook) <span class="co"># how many rows and cols remained</span></a></code></pre></div>
<pre><code>## [1] 203  23</code></pre>
<p>Missing value removal is necessary for metric-based
clustering methods, especially K-means. Otherwise, some of the computed distances
would be not available.</p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Standardise all the numeric columns.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Distance-based
methods are very sensitive to the order of magnitude of
the variables, and our dataset is a mess with regards to this
(population, GDP, birth rate, oil production etc.) – standardisation
of variables is definitely a good idea:</p>
<div class="sourceCode" id="cb657"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb657-1" title="1"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="kw">ncol</span>(factbook)) <span class="co"># skip `country`</span></a>
<a class="sourceLine" id="cb657-2" title="2">    factbook[[i]] &lt;-<span class="st"> </span>(factbook[[i]]<span class="op">-</span><span class="kw">mean</span>(factbook[[i]]))<span class="op">/</span></a>
<a class="sourceLine" id="cb657-3" title="3"><span class="st">                        </span><span class="kw">sd</span>(factbook[[i]])</a></code></pre></div>
<p>Recall that Z-scores (values of the standardised variables)
have a very intuitive interpretation: <span class="math inline">\(0\)</span> is the value equal to the column
mean, <span class="math inline">\(1\)</span> is one standard deviation above the mean, <span class="math inline">\(-2\)</span> is two standard deviations
below the mean etc.</p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the 2-means algorithm, i.e., K-means with <span class="math inline">\(K=2\)</span>.
Analyse the results.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Calling <code>kmeans()</code>:</p>
<div class="sourceCode" id="cb658"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb658-1" title="1">km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(factbook[<span class="op">-</span><span class="dv">1</span>], <span class="dv">2</span>, <span class="dt">nstart=</span><span class="dv">10</span>)</a></code></pre></div>
<p>Let’s split the country list w.r.t. the obtained cluster labels.
It turns out that the obtained partition is heavily imbalanced, so we’ll
print only the contents of the first group:</p>
<div class="sourceCode" id="cb659"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb659-1" title="1">km_countries &lt;-<span class="st"> </span><span class="kw">split</span>(factbook[[<span class="dv">1</span>]], km<span class="op">$</span>cluster)</a>
<a class="sourceLine" id="cb659-2" title="2">km_countries[[<span class="dv">1</span>]]</a></code></pre></div>
<pre><code>## [1] &quot;China&quot;         &quot;India&quot;         &quot;United States&quot;</code></pre>
<p>With regards to which criteria has the K-means algorithm distinguished
the countries? Let’s inspect the cluster centres to check the average
Z-scores of all the countries in each cluster:</p>
<div class="sourceCode" id="cb661"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb661-1" title="1"><span class="kw">t</span>(km<span class="op">$</span>centers) <span class="co"># transposed for readability</span></a></code></pre></div>
<pre><code>##                                        1          2
## area                            3.661581 -0.0549237
## population                      6.987279 -0.1048092
## median_age                      0.477991 -0.0071699
## population_growth_rate         -0.252774  0.0037916
## birth_rate                     -0.501030  0.0075155
## death_rate                      0.153915 -0.0023087
## net_migration_rate              0.236449 -0.0035467
## infant_mortality_rate          -0.139577  0.0020937
## life_expectancy_at_birth        0.251541 -0.0037731
## total_fertility_rate           -0.472716  0.0070907
## gdp_purchasing_power_parity     7.213681 -0.1082052
## gdp_real_growth_rate            0.369499 -0.0055425
## gdp_per_capita_ppp              0.298103 -0.0044715
## labor_force                     6.914319 -0.1037148
## taxes_and_other_revenues       -0.922735  0.0138410
## budget_surplus_or_deficit      -0.012627  0.0001894
## inflation_rate_consumer_prices -0.096626  0.0014494
## exports                         5.341178 -0.0801177
## imports                         5.956538 -0.0893481
## telephones_fixed_lines          5.989858 -0.0898479
## internet_users                  6.997126 -0.1049569
## airports                        4.551832 -0.0682775</code></pre>
<p>Countries in Cluster 2 are… average (Z-scores <span class="math inline">\(\simeq 0\)</span>).
On the other hand, the three countries in Cluster 1
dominate the others w.r.t. area, population, GDP PPP, labour force etc.</p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the complete linkage agglomerative hierarchical clustering algorithm.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Recall that the complete linkage-based method is implemented in the
<code>hclust()</code> function:</p>
<div class="sourceCode" id="cb663"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb663-1" title="1">d &lt;-<span class="st"> </span><span class="kw">dist</span>(factbook[<span class="op">-</span><span class="dv">1</span>]) <span class="co"># skip `country`</span></a>
<a class="sourceLine" id="cb663-2" title="2">h &lt;-<span class="st"> </span><span class="kw">hclust</span>(d, <span class="dt">method=</span><span class="st">&quot;complete&quot;</span>)</a></code></pre></div>
<p>A “nice” number of clusters to divide our dataset into
can be read from the dendrogram, see Figure @ref(fig:clustering_factbook7).</p>
<div class="sourceCode" id="cb664"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb664-1" title="1"><span class="kw">plot</span>(h, <span class="dt">labels=</span><span class="ot">FALSE</span>, <span class="dt">ann=</span><span class="ot">FALSE</span>); <span class="kw">box</span>()</a></code></pre></div>
<div class="figure">
<img src="090-kmeans-figures/clustering_factbook7-1.svg" alt="(#fig:clustering_factbook7) Cluster dendrogram for the World Factbook dataset – Complete linkage" />
<p class="caption">(#fig:clustering_factbook7) Cluster dendrogram for the World Factbook dataset – Complete linkage</p>
</div>
<p>It seems that a 9-partition might reveal something interesting,
because it will distinguish two larger country groups.
However, there will be many singletons if we do so either way.</p>
<div class="sourceCode" id="cb665"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb665-1" title="1">y &lt;-<span class="st"> </span><span class="kw">cutree</span>(h, <span class="dv">9</span>)</a>
<a class="sourceLine" id="cb665-2" title="2">h_countries &lt;-<span class="st"> </span><span class="kw">split</span>(factbook[[<span class="dv">1</span>]], y)</a>
<a class="sourceLine" id="cb665-3" title="3"><span class="kw">sapply</span>(h_countries, length) <span class="co"># number of elements in each cluster</span></a></code></pre></div>
<pre><code>##   1   2   3   4   5   6   7   8   9 
## 138  56   1   1   3   1   1   1   1</code></pre>
<p>Most likely this is not an interesting partitioning of this dataset,
therefore we’ll not be exploring it any further.</p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the Genie clustering algorithm (package <code>genieclust</code>).</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>The Genie algorithm <span class="citation">(Gagolewski et al. <a href="references.html#ref-genie">2016</a>)</span> is a hierarchical clustering algorithm
implemented in R package <code>genieclust</code>.
Its interface is compatible with <code>hclust()</code>.</p>
<div class="sourceCode" id="cb667"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb667-1" title="1"><span class="kw">library</span>(<span class="st">&quot;genieclust&quot;</span>)</a>
<a class="sourceLine" id="cb667-2" title="2">d &lt;-<span class="st"> </span><span class="kw">dist</span>(factbook[<span class="op">-</span><span class="dv">1</span>])</a>
<a class="sourceLine" id="cb667-3" title="3">g &lt;-<span class="st"> </span><span class="kw">gclust</span>(d)</a></code></pre></div>
<p>The cluster dendrogram in Figure @ref(fig:clustering_factbook10)
reveals 3 evident clusters.</p>
<div class="sourceCode" id="cb668"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb668-1" title="1"><span class="kw">plot</span>(g, <span class="dt">labels=</span><span class="ot">FALSE</span>, <span class="dt">ann=</span><span class="ot">FALSE</span>); <span class="kw">box</span>()</a></code></pre></div>
<div class="figure">
<img src="090-kmeans-figures/clustering_factbook10-1.svg" alt="(#fig:clustering_factbook10) Cluster dendrogram for the World Factbook dataset – Genie algorithm" />
<p class="caption">(#fig:clustering_factbook10) Cluster dendrogram for the World Factbook dataset – Genie algorithm</p>
</div>
<p>Let’s determine the 3-partition of the data set.</p>
<div class="sourceCode" id="cb669"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb669-1" title="1">y &lt;-<span class="st"> </span><span class="kw">cutree</span>(g, <span class="dv">3</span>)</a></code></pre></div>
<p>Here are few countries in each cluster:</p>
<div class="sourceCode" id="cb670"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb670-1" title="1">y &lt;-<span class="st"> </span><span class="kw">cutree</span>(g, <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb670-2" title="2"><span class="kw">sapply</span>(<span class="kw">split</span>(factbook<span class="op">$</span>countr, y), sample, <span class="dv">6</span>)</a></code></pre></div>
<pre><code>##      1             2          3                       
## [1,] &quot;Curacao&quot;     &quot;Uganda&quot;   &quot;Netherlands&quot;           
## [2,] &quot;Puerto Rico&quot; &quot;Senegal&quot;  &quot;Bosnia and Herzegovina&quot;
## [3,] &quot;Israel&quot;      &quot;Niger&quot;    &quot;United Kingdom&quot;        
## [4,] &quot;Barbados&quot;    &quot;Djibouti&quot; &quot;Norway&quot;                
## [5,] &quot;Mongolia&quot;    &quot;Haiti&quot;    &quot;Denmark&quot;               
## [6,] &quot;Guam&quot;        &quot;Kenya&quot;    &quot;Palau&quot;</code></pre>
<p>We can draw the countries in each cluster
on a map by using the <code>rworldmap</code> package (see its documentation for more details),
see Figure @ref(fig:clustering_factbook12).</p>
<div class="sourceCode" id="cb672"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb672-1" title="1"><span class="kw">library</span>(<span class="st">&quot;rworldmap&quot;</span>)</a>
<a class="sourceLine" id="cb672-2" title="2">mapdata &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">country=</span>factbook<span class="op">$</span>country, <span class="dt">cluster=</span>y)</a>
<a class="sourceLine" id="cb672-3" title="3"><span class="co"># 3 country names must be adjusted to get a match</span></a>
<a class="sourceLine" id="cb672-4" title="4">mapdata<span class="op">$</span>country[mapdata<span class="op">$</span>country <span class="op">==</span><span class="st"> &quot;Czechia&quot;</span>] &lt;-<span class="st"> &quot;Czech Republic&quot;</span></a>
<a class="sourceLine" id="cb672-5" title="5">mapdata<span class="op">$</span>country[mapdata<span class="op">$</span>country <span class="op">==</span><span class="st"> &quot;Eswatini&quot;</span>] &lt;-<span class="st"> &quot;Swaziland&quot;</span></a>
<a class="sourceLine" id="cb672-6" title="6">mapdata<span class="op">$</span>country[mapdata<span class="op">$</span>country <span class="op">==</span><span class="st"> &quot;Cabo Verde&quot;</span>] &lt;-<span class="st"> &quot;Cape Verde&quot;</span></a>
<a class="sourceLine" id="cb672-7" title="7">mapdata &lt;-<span class="st"> </span><span class="kw">joinCountryData2Map</span>(mapdata, <span class="dt">joinCode=</span><span class="st">&quot;NAME&quot;</span>,</a>
<a class="sourceLine" id="cb672-8" title="8">  <span class="dt">nameJoinColumn=</span><span class="st">&quot;country&quot;</span>)</a></code></pre></div>
<pre><code>## 203 codes from your data successfully matched countries in the map
## 0 codes from your data failed to match with a country code in the map
## 40 codes from the map weren&#39;t represented in your data</code></pre>
<div class="sourceCode" id="cb674"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb674-1" title="1"><span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb674-2" title="2"><span class="kw">mapCountryData</span>(mapdata, <span class="dt">nameColumnToPlot=</span><span class="st">&quot;cluster&quot;</span>,</a>
<a class="sourceLine" id="cb674-3" title="3">    <span class="dt">catMethod=</span><span class="st">&quot;categorical&quot;</span>, <span class="dt">missingCountryCol=</span><span class="st">&quot;gray&quot;</span>,</a>
<a class="sourceLine" id="cb674-4" title="4">    <span class="dt">colourPalette=</span><span class="kw">palette</span>()[<span class="dv">2</span><span class="op">:</span><span class="dv">4</span>],</a>
<a class="sourceLine" id="cb674-5" title="5">    <span class="dt">mapTitle=</span><span class="st">&quot;&quot;</span>, <span class="dt">addLegend=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<div class="figure">
<img src="090-kmeans-figures/clustering_factbook12-1.svg" alt="(#fig:clustering_factbook12) 3 clusters discovered by the Genie algorithm" />
<p class="caption">(#fig:clustering_factbook12) 3 clusters discovered by the Genie algorithm</p>
</div>
<p>Here are the average Z-scores in each cluster:</p>
<div class="sourceCode" id="cb675"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb675-1" title="1"><span class="kw">round</span>(<span class="kw">sapply</span>(<span class="kw">split</span>(factbook[<span class="op">-</span><span class="dv">1</span>], y), colMeans), <span class="dv">3</span>)</a></code></pre></div>
<pre><code>##                                     1      2      3
## area                            0.124 -0.068 -0.243
## population                      0.077 -0.058 -0.130
## median_age                      0.118 -1.219  1.261
## population_growth_rate         -0.227  1.052 -0.757
## birth_rate                     -0.316  1.370 -0.930
## death_rate                     -0.439  0.071  1.075
## net_migration_rate             -0.123  0.053  0.260
## infant_mortality_rate          -0.366  1.399 -0.835
## life_expectancy_at_birth        0.354 -1.356  0.812
## total_fertility_rate           -0.363  1.332 -0.758
## gdp_purchasing_power_parity     0.084 -0.213  0.052
## gdp_real_growth_rate           -0.062  0.126  0.002
## gdp_per_capita_ppp              0.021 -0.744  0.905
## labor_force                     0.087 -0.096 -0.107
## taxes_and_other_revenues       -0.095 -0.584  1.006
## budget_surplus_or_deficit      -0.113 -0.188  0.543
## inflation_rate_consumer_prices  0.044 -0.013 -0.099
## exports                        -0.013 -0.318  0.447
## imports                         0.007 -0.308  0.379
## telephones_fixed_lines          0.048 -0.244  0.186
## internet_users                  0.093 -0.178 -0.016
## airports                        0.104 -0.131 -0.107</code></pre>
<p>That is really interesting! The interpretation of the above is left
to the reader.</p>
<!--
# factbook$country[!(factbook$country %in% mapdata$country)]
-->
</details>
</div>
<div id="unbalance-dataset-k-means-needs-multiple-starts" class="section level3">
<h3><span class="header-section-number">10.2.2</span> Unbalance Dataset – K-Means Needs Multiple Starts</h3>
<p>Let us consider a benchmark (artificial) dataset
proposed in <span class="citation">(Rezaei &amp; Fränti <a href="references.html#ref-external_cluster_validity">2016</a>)</span>:</p>
<div class="sourceCode" id="cb677"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb677-1" title="1">unbalance &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">read.csv</span>(<span class="st">&quot;datasets/sipu_unbalance.csv&quot;</span>,</a>
<a class="sourceLine" id="cb677-2" title="2">    <span class="dt">header=</span><span class="ot">FALSE</span>, <span class="dt">sep=</span><span class="st">&quot; &quot;</span>, <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>))</a>
<a class="sourceLine" id="cb677-3" title="3">unbalance &lt;-<span class="st"> </span>unbalance<span class="op">/</span><span class="dv">10000-30</span> <span class="co"># a more user-friendly scale</span></a></code></pre></div>
<p>According to its authors, this dataset is comprised of 8 clusters:
there are 3 groups on the lefthand side (2000 points each)
and 5 on the right side (100 each).</p>
<div class="sourceCode" id="cb678"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb678-1" title="1"><span class="kw">plot</span>(unbalance, <span class="dt">asp=</span><span class="dv">1</span>)</a></code></pre></div>
<div class="figure">
<img src="090-kmeans-figures/sipu_unbalance2-1.svg" alt="(#fig:sipu_unbalance2) sipu_unbalance dataset" />
<p class="caption">(#fig:sipu_unbalance2) <code>sipu_unbalance</code> dataset</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the K-means algorithm with <span class="math inline">\(K=8\)</span>.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Of course, here by “the” K-means we mean the default method
available in the <code>kmeans()</code> function.
The clustering results are depicted in Figure @ref(fig:sipu_unbalance3a).</p>
<div class="sourceCode" id="cb679"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb679-1" title="1">km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(unbalance, <span class="dv">8</span>, <span class="dt">nstart=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb679-2" title="2"><span class="kw">plot</span>(unbalance, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>km<span class="op">$</span>cluster)</a></code></pre></div>
<div class="figure">
<img src="090-kmeans-figures/sipu_unbalance3a-1.svg" alt="(#fig:sipu_unbalance3a) Results of K-means on the sipu_unbalance dataset" />
<p class="caption">(#fig:sipu_unbalance3a) Results of K-means on the <code>sipu_unbalance</code> dataset</p>
</div>
<p>This is far from what we expected.
The total within-cluster distances are equal to:</p>
<div class="sourceCode" id="cb680"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb680-1" title="1">km<span class="op">$</span>tot.withinss</a></code></pre></div>
<pre><code>## [1] 21713</code></pre>
<p>Increasing the number of restarts even further improves the solution,
but the local minimum is still far from the global one,
compare Figure @ref(fig:sipu_unbalance3b).</p>
<div class="sourceCode" id="cb682"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb682-1" title="1">km &lt;-<span class="st"> </span><span class="kw">suppressWarnings</span>(<span class="kw">kmeans</span>(unbalance, <span class="dv">8</span>, <span class="dt">nstart=</span><span class="dv">1000</span>, <span class="dt">iter.max=</span><span class="dv">1000</span>))</a>
<a class="sourceLine" id="cb682-2" title="2"><span class="kw">plot</span>(unbalance, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>km<span class="op">$</span>cluster)</a></code></pre></div>
<div class="figure">
<img src="090-kmeans-figures/sipu_unbalance3b-1.svg" alt="(#fig:sipu_unbalance3b) Results of K-means on the sipu_unbalance dataset – many more restarts" />
<p class="caption">(#fig:sipu_unbalance3b) Results of K-means on the <code>sipu_unbalance</code> dataset – many more restarts</p>
</div>
<div class="sourceCode" id="cb683"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb683-1" title="1">km<span class="op">$</span>tot.withinss</a></code></pre></div>
<pre><code>## [1] 4378</code></pre>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the K-means algorithm starting from a “good” initial guess
on the true cluster centres.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Clustering is – in its essence – an unsupervised learning method,
so what we’re going to do now could be called, let’s be blunt about it, cheating.
Luckily, we have an oracle at our disposal – it has provided us
with the following educated guesses (by looking at the scatter plot)
about the localisation of the cluster centres:</p>
<div class="sourceCode" id="cb685"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb685-1" title="1">cntr &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>, <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb685-2" title="2">   <span class="dv">-15</span>,   <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb685-3" title="3">   <span class="dv">-12</span>,   <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb685-4" title="4">   <span class="dv">-10</span>,   <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb685-5" title="5">    <span class="dv">15</span>,   <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb685-6" title="6">    <span class="dv">15</span>,   <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb685-7" title="7">    <span class="dv">20</span>,   <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb685-8" title="8">    <span class="dv">25</span>,   <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb685-9" title="9">    <span class="dv">25</span>,   <span class="dv">10</span>))</a></code></pre></div>
<p>Running <code>kmeans()</code> yields the clustering depicted in Figure @ref(fig:sipu_unbalance6).</p>
<div class="sourceCode" id="cb686"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb686-1" title="1">km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(unbalance, cntr)</a>
<a class="sourceLine" id="cb686-2" title="2"><span class="kw">plot</span>(unbalance, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>km<span class="op">$</span>cluster)</a></code></pre></div>
<div class="figure">
<img src="090-kmeans-figures/sipu_unbalance6-1.svg" alt="(#fig:sipu_unbalance6) Results of K-means on the sipu_unbalance dataset – an educated guess on the cluster centres’ locations" />
<p class="caption">(#fig:sipu_unbalance6) Results of K-means on the <code>sipu_unbalance</code> dataset – an educated guess on the cluster centres’ locations</p>
</div>
<p>The total within-cluster distances are now equal to:</p>
<div class="sourceCode" id="cb687"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb687-1" title="1">km<span class="op">$</span>tot.withinss</a></code></pre></div>
<pre><code>## [1] 2144.9</code></pre>
<p>This is finally the globally optimal solution to the K-means problem
we were asked to solve. Recall that the algorithms implemented in the <code>kmeans()</code>
function are just fast heuristics that are supposed to find
local optima of the K-means objective function, which is given
by the within-cluster sum of squared Euclidean distances.</p>
<!--
http://cs.joensuu.fi/sipu/datasets/:

cntr <- matrix(ncol=2, byrow=TRUE, c(
    209948,   349963,
    539379,   299653,
    440134,   400135,
    440754,   298283,
    491036,   349798,
    150007,   350104,
    538884,   400947,
    179955,   380008))
-->
</details>
</div>
<div id="clustering-of-typical-2d-benchmark-datasets" class="section level3">
<h3><span class="header-section-number">10.2.3</span> Clustering of Typical 2D Benchmark Datasets</h3>
<p>Let us consider a few clustering benchmark datasets
available at <a href="https://github.com/gagolews/clustering_benchmarks_v1" class="uri">https://github.com/gagolews/clustering_benchmarks_v1</a>
and <a href="http://cs.joensuu.fi/sipu/datasets/" class="uri">http://cs.joensuu.fi/sipu/datasets/</a>.
Here is a list of file names together with the
corresponding numbers of clusters (as given by datasets’ authors):</p>
<div class="sourceCode" id="cb689"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb689-1" title="1">files &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;datasets/wut_isolation.csv&quot;</span>,</a>
<a class="sourceLine" id="cb689-2" title="2">           <span class="st">&quot;datasets/wut_mk2.csv&quot;</span>,</a>
<a class="sourceLine" id="cb689-3" title="3">           <span class="st">&quot;datasets/wut_z3.csv&quot;</span>,</a>
<a class="sourceLine" id="cb689-4" title="4">           <span class="st">&quot;datasets/sipu_aggregation.csv&quot;</span>,</a>
<a class="sourceLine" id="cb689-5" title="5">           <span class="st">&quot;datasets/sipu_pathbased.csv&quot;</span>,</a>
<a class="sourceLine" id="cb689-6" title="6">           <span class="st">&quot;datasets/sipu_unbalance.csv&quot;</span>)</a>
<a class="sourceLine" id="cb689-7" title="7">Ks &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">3</span>, <span class="dv">8</span>)</a></code></pre></div>
<p>All the datasets are two-dimensional, hence we’ll be able to visualise
the obtained results and assess the sensibility of the obtained clusterings.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the K-means, the single, average and complete linkage
and the Genie algorithm (from package <code>genieclust</code>)
on the aforementioned datasets and discuss the results.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>Apart from a call to the Genie algorithm with the default parameters,
we will also look at the results it generates when we set the
<code>gini_threshold</code> parameter to 0.5 (default is 0.3; smaller thresholds
lead to clusters of more balanced sizes as measured by the Gini index).</p>
<p>The following function is our workhorse that will perform all the computations
and will draw all the figures for a single dataset:</p>
<div class="sourceCode" id="cb690"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb690-1" title="1">clusterise &lt;-<span class="st"> </span><span class="cf">function</span>(file, K) {</a>
<a class="sourceLine" id="cb690-2" title="2">    X &lt;-<span class="st"> </span><span class="kw">read.csv</span>(file,</a>
<a class="sourceLine" id="cb690-3" title="3">        <span class="dt">header=</span><span class="ot">FALSE</span>, <span class="dt">sep=</span><span class="st">&quot; &quot;</span>, <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a>
<a class="sourceLine" id="cb690-4" title="4">    d &lt;-<span class="st"> </span><span class="kw">dist</span>(X)</a>
<a class="sourceLine" id="cb690-5" title="5">    <span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb690-6" title="6">    <span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">2</span>, <span class="fl">0.5</span>))</a>
<a class="sourceLine" id="cb690-7" title="7"></a>
<a class="sourceLine" id="cb690-8" title="8">    y &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, K, <span class="dt">nstart=</span><span class="dv">10</span>)<span class="op">$</span>cluster</a>
<a class="sourceLine" id="cb690-9" title="9">    <span class="kw">plot</span>(X, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>y, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">axes=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb690-10" title="10">    <span class="kw">mtext</span>(<span class="st">&quot;K-means&quot;</span>, <span class="dt">line=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb690-11" title="11"></a>
<a class="sourceLine" id="cb690-12" title="12">    y &lt;-<span class="st"> </span><span class="kw">cutree</span>(<span class="kw">hclust</span>(d, <span class="st">&quot;complete&quot;</span>), K)</a>
<a class="sourceLine" id="cb690-13" title="13">    <span class="kw">plot</span>(X, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>y, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">axes=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb690-14" title="14">    <span class="kw">mtext</span>(<span class="st">&quot;Complete Linkage&quot;</span>, <span class="dt">line=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb690-15" title="15"></a>
<a class="sourceLine" id="cb690-16" title="16">    y &lt;-<span class="st"> </span><span class="kw">cutree</span>(<span class="kw">hclust</span>(d, <span class="st">&quot;average&quot;</span>), K)</a>
<a class="sourceLine" id="cb690-17" title="17">    <span class="kw">plot</span>(X, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>y, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">axes=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb690-18" title="18">    <span class="kw">mtext</span>(<span class="st">&quot;Average Linkage&quot;</span>, <span class="dt">line=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb690-19" title="19"></a>
<a class="sourceLine" id="cb690-20" title="20">    y &lt;-<span class="st"> </span><span class="kw">cutree</span>(<span class="kw">hclust</span>(d, <span class="st">&quot;single&quot;</span>), K)</a>
<a class="sourceLine" id="cb690-21" title="21">    <span class="kw">plot</span>(X, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>y, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">axes=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb690-22" title="22">    <span class="kw">mtext</span>(<span class="st">&quot;Single Linkage&quot;</span>, <span class="dt">line=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb690-23" title="23"></a>
<a class="sourceLine" id="cb690-24" title="24">    y &lt;-<span class="st"> </span><span class="kw">genie</span>(d, K) <span class="co"># gini_threshold=0.3</span></a>
<a class="sourceLine" id="cb690-25" title="25">    <span class="kw">plot</span>(X, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>y, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">axes=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb690-26" title="26">    <span class="kw">mtext</span>(<span class="st">&quot;Genie (default)&quot;</span>, <span class="dt">line=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb690-27" title="27"></a>
<a class="sourceLine" id="cb690-28" title="28">    y &lt;-<span class="st"> </span><span class="kw">genie</span>(d, K, <span class="dt">gini_threshold=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb690-29" title="29">    <span class="kw">plot</span>(X, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">col=</span>y, <span class="dt">ann=</span><span class="ot">FALSE</span>, <span class="dt">axes=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb690-30" title="30">    <span class="kw">mtext</span>(<span class="st">&quot;Genie (g=0.5)&quot;</span>, <span class="dt">line=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb690-31" title="31">}</a></code></pre></div>
<p>Applying the above as <code>clusterise(files[i], Ks[i])</code> yields
Figures @ref(fig:clustering_benchmarks_plot1)-@ref(fig:clustering_benchmarks_plot6).</p>
<div class="figure">
<img src="090-kmeans-figures/clustering_benchmarks_plot1-1.svg" alt="(#fig:clustering_benchmarks_plot1) Clustering of the wut_isolation dataset" />
<p class="caption">(#fig:clustering_benchmarks_plot1) Clustering of the <code>wut_isolation</code> dataset</p>
</div>
<div class="figure">
<img src="090-kmeans-figures/clustering_benchmarks_plot2-1.svg" alt="(#fig:clustering_benchmarks_plot2) Clustering of the wut_mk2 dataset" />
<p class="caption">(#fig:clustering_benchmarks_plot2) Clustering of the <code>wut_mk2</code> dataset</p>
</div>
<div class="figure">
<img src="090-kmeans-figures/clustering_benchmarks_plot3-1.svg" alt="(#fig:clustering_benchmarks_plot3) Clustering of the wut_z3 dataset" />
<p class="caption">(#fig:clustering_benchmarks_plot3) Clustering of the <code>wut_z3</code> dataset</p>
</div>
<div class="figure">
<img src="090-kmeans-figures/clustering_benchmarks_plot4-1.svg" alt="(#fig:clustering_benchmarks_plot4) Clustering of the sipu_aggregation dataset" />
<p class="caption">(#fig:clustering_benchmarks_plot4) Clustering of the <code>sipu_aggregation</code> dataset</p>
</div>
<div class="figure">
<img src="090-kmeans-figures/clustering_benchmarks_plot5-1.svg" alt="(#fig:clustering_benchmarks_plot5) Clustering of the sipu_pathbased dataset" />
<p class="caption">(#fig:clustering_benchmarks_plot5) Clustering of the <code>sipu_pathbased</code> dataset</p>
</div>
<div class="figure">
<img src="090-kmeans-figures/clustering_benchmarks_plot6-1.svg" alt="(#fig:clustering_benchmarks_plot6) Clustering of the sipu_unbalance dataset" />
<p class="caption">(#fig:clustering_benchmarks_plot6) Clustering of the <code>sipu_unbalance</code> dataset</p>
</div>
<p>Note that, by definition, K-means is only able to detect clusters
of convex shapes. The Genie algorithm, on the other hand, might
fail to detect clusters of very small sizes amongst the more populous ones.
Single linkage is very sensitive to outliers in data – it often outputs
clusters of cardinality 1.</p>
</details>
</div>
<div id="wine-quality-volatile.acidity-and-sulphates" class="section level3">
<h3><span class="header-section-number">10.2.4</span> Wine Quality – <code>volatile.acidity</code> and <code>sulphates</code></h3>
<p>Let’s consider the Wine Quality dataset:</p>
<div class="sourceCode" id="cb691"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb691-1" title="1">wine_quality &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/wine_quality_all.csv&quot;</span>,</a>
<a class="sourceLine" id="cb691-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a></code></pre></div>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the 2-means clustering algorithm (i.e., K-means with <span class="math inline">\(K=2\)</span>)
on a subset of <code>wine_quality</code> consisting only of
the <code>volatile.acidity</code> and <code>sulphates</code> columns.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Print the contingency table (see the <code>table()</code> function)
for the discovered clusters vs. the wine colour (see the <code>color</code>
column in <code>wine_quality</code>).</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Draw two scatter plots of <code>volatile.acidity</code> vs. <code>sulphates</code>:</p>
<ul>
<li>one where points’ colours correspond to the discovered cluster labels
(1st cluster = black symbols, 2nd cluster = red symbols),</li>
<li>the other one such that points’ colours correspond to the true wine colours
(white wines = black symbols, red wines = red symbols).</li>
</ul>
<p>Is there are a good match between the discovered clusters
and the true wine colours? Discuss.</p>
</div>
<!--
(n <- nrow(wines)) # number of samples
for (i in 1:11)
    wines[[i]] <- (wines[[i]]-mean(wines[[i]]))/sd(wines[[i]])
names(wines)

ari_best <- 0
for (i in 1:10) {
    for (j in (i+1):11) {
        X <- as.matrix(wines[c(i,j)])
        y_pred <- kmeans(X, 2, nstart=10)$cluster
        y_true <- as.integer(factor(wines[["color"]]))
        #plot(X, pch=y_pred, col=y_true, asp=1)
        #
        ari <- mclust::adjustedRandIndex(y_pred, y_true)
        if (ari > ari_best[1])
            ari_best <- c(ari, i, j)
    }
}

print(ari_best)
X <- as.matrix(wines[ari_best[-1]])
y_pred <- kmeans(X, 2, nstart=10)$cluster
par(mfrow=c(1,2))
plot(X, pch=y_pred, col=y_true, asp=1)
plot(X, col=y_pred, pch=y_true, asp=1)
table(y_pred, y_true)
-->
</div>
<div id="wine-quality-chlorides-and-total.sulfur.dioxide" class="section level3">
<h3><span class="header-section-number">10.2.5</span> Wine Quality – <code>chlorides</code> and <code>total.sulfur.dioxide</code></h3>
<p>The Wine Quality dataset again:</p>
<div class="sourceCode" id="cb692"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb692-1" title="1">wine_quality &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/wine_quality_all.csv&quot;</span>,</a>
<a class="sourceLine" id="cb692-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a></code></pre></div>
<div class="exercise"><strong>Exercise.</strong>
<p>Create a matrix <code>X</code> by extracting the <code>chlorides</code>
and <code>total.sulfur.dioxide</code> columns from <code>wines</code>.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Standardise both columns in <code>X</code> (i.e., from each column,
subtract its mean and then divide by its standard deviation).</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the 2-means clustering algorithm (i.e., K-means with <span class="math inline">\(K=2\)</span>) on <code>X</code>.
Store the obtained cluster labels in a vector named <code>y_kmeans</code>.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the average linkage hierarchical clustering method.
Cut the obtained hierarchy (see the <code>cutree()</code> function)
so as to obtain 2 clusters and store the results in a vector named <code>y_average</code>.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the single linkage hierarchical clustering method.
Cut the obtained hierarchy into two groups and store the results in
a vector named <code>y_single</code>.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Apply the complete linkage hierarchical clustering method.
Cut the obtained hierarchy (see the <code>cutree()</code> function)
into two groups and store the results in a vector named <code>y_complete</code>.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Print the 4 contingency tables (see the <code>table()</code> function)
for each of the 4 discovered partitions (<code>y_kmeans</code>, …, <code>y_complete</code>)
vs. the wine colour (see the <code>color</code> column in <code>wines</code>).
Is there are a good match between the discovered clusters
and the true wine colours? Discuss.</p>
</div>
</div>
</div>
<div id="outro-5" class="section level2">
<h2><span class="header-section-number">10.3</span> Outro</h2>
<div id="remarks-6" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Remarks</h3>
<p>In K-means, we need to specify the number of clusters, <span class="math inline">\(K\)</span>, in advance.
What if we don’t have any idea how to choose this parameter (which is often
the case)?</p>
<p>Also, the problem with K-means is that there is no guarantee that a
<span class="math inline">\(K\)</span>-partition is any “similar” to the <span class="math inline">\(K&#39;\)</span>-one for <span class="math inline">\(K\neq K&#39;\)</span>,
see Figure @ref(fig:kmeans_different_K).</p>
<div class="sourceCode" id="cb693"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb693-1" title="1">X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(iris[,<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">2</span>)])</a>
<a class="sourceLine" id="cb693-2" title="2"><span class="co"># never forget to set nstart&gt;&gt;1!</span></a>
<a class="sourceLine" id="cb693-3" title="3">km &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, <span class="dt">centers=</span><span class="dv">3</span>, <span class="dt">nstart=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb693-4" title="4">km<span class="op">$</span>cluster <span class="co"># labels assigned to each of 150 points:</span></a></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
## [35] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [69] 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [ reached getOption(&quot;max.print&quot;) -- omitted 51 entries ]</code></pre>
<div class="sourceCode" id="cb695"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb695-1" title="1">km1 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, <span class="dv">3</span>, <span class="dt">nstart=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb695-2" title="2">km2 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X, <span class="dv">4</span>, <span class="dt">nstart=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb695-3" title="3"><span class="kw">plot</span>(X, <span class="dt">col=</span>km1<span class="op">$</span>cluster, <span class="dt">pch=</span>km2<span class="op">$</span>cluster, <span class="dt">asp=</span><span class="dv">1</span>)</a></code></pre></div>
<div class="figure">
<img src="090-kmeans-figures/kmeans_different_K-1.svg" alt="(#fig:kmeans_different_K) 3-means (colours) vs. 4-means (symbols) on example data; the “circle” cluster cannot decide if it likes the green or the black one more" />
<p class="caption">(#fig:kmeans_different_K) 3-means (colours) vs. 4-means (symbols) on example data; the “circle” cluster cannot decide if it likes the green or the black one more</p>
</div>
<p>Unsupervised learning is often performed during the data pre-processing
and exploration stage.
Assessing the quality of clustering is particularly challenging as,
unlike in a supervised setting,
we have no access to “ground truth” information.</p>
<p>In practice, we often apply different clustering algorithms
and just see where they lead us. There’s no teacher that would
tell us what we should do, so whatever we do is awesome, right?
Well, not precisely. Most frequently, you, my dear reader, will work
for some party that’s genuinely
interested in your explaining why did you spent the last month coming up
with nothing useful at all. Thus, the main body of work related to proving
the use-ful<em>l</em>/less-ness will be on you.</p>
<p>Clustering methods can aid us in supervised tasks
– instead of fitting a single “large model”, it might be
useful to fit separate models to each cluster.</p>
<div style="margin-top: 1em">

</div>
<p>To sum up, the aim of K-means is to find
<span class="math inline">\(K\)</span> clusters based on the notion of the points’ closeness
to the cluster centres. Remember that <span class="math inline">\(K\)</span> must be set in advance.
By definition (* via its relation to Voronoi diagrams),
all clusters will be of convex shapes.</p>
<p>However, we may try applying <span class="math inline">\(K&#39;\)</span>-means for <span class="math inline">\(K&#39; \gg K\)</span>
to obtain a “fine grained” compressed representation of data and then
combine the (sub)clusters into more meaningful groups
using other methods (such as the hierarchical ones).</p>
<p>Iterative K-means algorithms are very fast (e.g., a mini-batch
version of the algorithm can be implement to speed up the optimisation
process) even for large data sets,
but they may fail to find a desirable solution, especially if clusters
are unbalanced.</p>
<div style="margin-top: 1em">

</div>
<p>Hierarchical methods, on the other hand, output
a whole family of mutually nested partitions, which may provide us
with insight into the underlying structure of data data.
Unfortunately, there is no easy way to assign new points
to existing clusters; yet, you can always build a classifier
(e.g., a decision tree or a neural network) that learns
the discovered labels.</p>
<p>A linkage scheme must be chosen with care, for instance, single linkage
can be sensitive to outliers. However, it is generally the fastest.
The methods implemented in <code>hclust()</code> are generally slow; they have
time complexity between <span class="math inline">\(O(n^2)\)</span> and <span class="math inline">\(O(n^3)\)</span>.</p>
<dl>
<dt>Remark.</dt>
<dd><p>Note that the <code>fastcluster</code> package provides a more efficient
and memory-saving
implementation of some methods available via a call to <code>hclust()</code>.
See also the <code>genieclust</code> package for a super-robust version
of the single linkage
algorithm based on the datasets’s Euclidean minimum spanning tree,
which can be computed quite quickly.</p>
</dd>
</dl>
<p>Finally, note that all the discussed clustering methods are based on the
notion of pairwise distances. These of course tend
to behave weirdly in high-dimensional
spaces (“the curse of dimensionality”). Moreover, some hardcore
feature engineering might be needed to obtain meaningful results.</p>
<div style="margin-top: 1em">

</div>
<p><strong>TODO</strong> …..</p>
<div style="margin-top: 1em">

</div>
<p>Recommended further reading: <span class="citation">(James et al. <a href="references.html#ref-islr">2017</a>: Section 10.3)</span></p>
<p>Other: <span class="citation">(Hastie et al. <a href="references.html#ref-esl">2017</a>: Section 14.3)</span></p>
<p>Additionally, check out other noteworthy clustering approaches:</p>
<ul>
<li>Genie (see R package <code>genieclust</code>) <span class="citation">(Gagolewski et al. <a href="references.html#ref-genie">2016</a>)</span></li>
<li>ITM <span class="citation">(Müller et al. <a href="references.html#ref-itm">2012</a>)</span></li>
<li>DBSCAN, HDBSCAN* <span class="citation">(Ling <a href="references.html#ref-pre_dbscan">1973</a>, Ester et al. <a href="references.html#ref-dbscan">1996</a>, Campello et al. <a href="references.html#ref-hdbscan">2015</a>)</span></li>
<li>K-medoids, K-medians</li>
<li>Fuzzy C-means (a.k.a. weighted K-means) <span class="citation">(Bezdek et al. <a href="references.html#ref-cmeans">1984</a>)</span></li>
<li>Spectral clustering; e.g., <span class="citation">(Ng et al. <a href="references.html#ref-spectral_nips">2001</a>)</span></li>
<li>BIRCH <span class="citation">(Zhang et al. <a href="references.html#ref-birch">1996</a>)</span></li>
</ul>
<p>Next chapter…</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap-optimisation-continuous.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-optimisation-discrete.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
