<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.3 Finding the Best Model | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="2.3 Finding the Best Model | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.3 Finding the Best Model | Lightweight Machine Learning Classics with R" />
  
  <meta name="twitter:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  

<meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multiple-linear-regression.html"/>
<link rel="next" href="outro-1.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>{</a></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a><ul>
<li class="chapter" data-level="1.1.1" data-path="machine-learning.html"><a href="machine-learning.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.1.2" data-path="machine-learning.html"><a href="machine-learning.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.2.1" data-path="supervised-learning.html"><a href="supervised-learning.html#formalism"><i class="fa fa-check"></i><b>1.2.1</b> Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="supervised-learning.html"><a href="supervised-learning.html#desired-outputs"><i class="fa fa-check"></i><b>1.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="1.2.3" data-path="supervised-learning.html"><a href="supervised-learning.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple-regression.html"><a href="simple-regression.html"><i class="fa fa-check"></i><b>1.3</b> Simple Regression</a><ul>
<li class="chapter" data-level="1.3.1" data-path="simple-regression.html"><a href="simple-regression.html#introduction"><i class="fa fa-check"></i><b>1.3.1</b> Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-regression.html"><a href="simple-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>1.3.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.4.1" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> Introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html#solution-in-r"><i class="fa fa-check"></i><b>1.4.2</b> Solution in R</a></li>
<li class="chapter" data-level="1.4.3" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html#analytic-solution"><i class="fa fa-check"></i><b>1.4.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="1.4.4" data-path="simple-linear-regression-1.html"><a href="simple-linear-regression-1.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>1.4.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="outro.html"><a href="outro.html"><i class="fa fa-check"></i><b>1.5</b> Outro</a><ul>
<li class="chapter" data-level="1.5.1" data-path="outro.html"><a href="outro.html#remarks"><i class="fa fa-check"></i><b>1.5.1</b> Remarks</a></li>
<li class="chapter" data-level="1.5.2" data-path="outro.html"><a href="outro.html#further-reading"><i class="fa fa-check"></i><b>1.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>2.1</b> Introduction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-2.html"><a href="introduction-2.html#formalism-1"><i class="fa fa-check"></i><b>2.1.1</b> Formalism</a></li>
<li class="chapter" data-level="2.1.2" data-path="introduction-2.html"><a href="introduction-2.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>2.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#problem-formulation"><i class="fa fa-check"></i><b>2.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>2.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html"><i class="fa fa-check"></i><b>2.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="2.3.1" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#model-diagnostics"><i class="fa fa-check"></i><b>2.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.3.2" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#variable-selection"><i class="fa fa-check"></i><b>2.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="2.3.3" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#variable-transformation"><i class="fa fa-check"></i><b>2.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="2.3.4" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>2.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="outro-1.html"><a href="outro-1.html"><i class="fa fa-check"></i><b>2.4</b> Outro</a><ul>
<li class="chapter" data-level="2.4.1" data-path="outro-1.html"><a href="outro-1.html#remarks-1"><i class="fa fa-check"></i><b>2.4.1</b> Remarks</a></li>
<li class="chapter" data-level="2.4.2" data-path="outro-1.html"><a href="outro-1.html#other-methods-for-regression"><i class="fa fa-check"></i><b>2.4.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="2.4.3" data-path="outro-1.html"><a href="outro-1.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>2.4.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="2.4.4" data-path="outro-1.html"><a href="outro-1.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>2.4.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="2.4.5" data-path="outro-1.html"><a href="outro-1.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>2.4.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="2.4.6" data-path="outro-1.html"><a href="outro-1.html#further-reading-1"><i class="fa fa-check"></i><b>2.4.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-3.html"><a href="introduction-3.html"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-3.html"><a href="introduction-3.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction-3.html"><a href="introduction-3.html#data"><i class="fa fa-check"></i><b>3.1.2</b> Data</a></li>
<li class="chapter" data-level="3.1.3" data-path="introduction-3.html"><a href="introduction-3.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.3</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.4" data-path="introduction-3.html"><a href="introduction-3.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.4</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a><ul>
<li class="chapter" data-level="3.2.1" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#introduction-4"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#feature-engineering"><i class="fa fa-check"></i><b>3.2.3</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html"><i class="fa fa-check"></i><b>3.3</b> Model Assessment and Selection</a><ul>
<li class="chapter" data-level="3.3.1" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html#performance-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>3.3.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="3.3.3" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>3.3.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html"><i class="fa fa-check"></i><b>3.4</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#factor-data-type"><i class="fa fa-check"></i><b>3.4.1</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.4.2" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#main-routine"><i class="fa fa-check"></i><b>3.4.2</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.4.3" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#mode"><i class="fa fa-check"></i><b>3.4.3</b> Mode</a></li>
<li class="chapter" data-level="3.4.4" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#nn-search-routines"><i class="fa fa-check"></i><b>3.4.4</b> NN Search Routines (*)</a></li>
<li class="chapter" data-level="3.4.5" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#different-metrics"><i class="fa fa-check"></i><b>3.4.5</b> Different Metrics (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="outro-2.html"><a href="outro-2.html"><i class="fa fa-check"></i><b>3.5</b> Outro</a><ul>
<li class="chapter" data-level="3.5.1" data-path="outro-2.html"><a href="outro-2.html#remarks-2"><i class="fa fa-check"></i><b>3.5.1</b> Remarks</a></li>
<li class="chapter" data-level="3.5.2" data-path="outro-2.html"><a href="outro-2.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.5.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.5.3" data-path="outro-2.html"><a href="outro-2.html#further-reading-2"><i class="fa fa-check"></i><b>3.5.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html"><i class="fa fa-check"></i><b>4</b> Classification with Trees and Linear Models</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction-5.html"><a href="introduction-5.html"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="introduction-5.html"><a href="introduction-5.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="introduction-5.html"><a href="introduction-5.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>4.2</b> Decision Trees</a><ul>
<li class="chapter" data-level="4.2.1" data-path="decision-trees.html"><a href="decision-trees.html#introduction-6"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="decision-trees.html"><a href="decision-trees.html#example-in-r-1"><i class="fa fa-check"></i><b>4.2.2</b> Example in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="decision-trees.html"><a href="decision-trees.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html"><i class="fa fa-check"></i><b>4.3</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="4.3.1" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#motivation"><i class="fa fa-check"></i><b>4.3.1</b> Motivation</a></li>
<li class="chapter" data-level="4.3.2" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>4.3.2</b> Logistic Model</a></li>
<li class="chapter" data-level="4.3.3" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#example-in-r-2"><i class="fa fa-check"></i><b>4.3.3</b> Example in R</a></li>
<li class="chapter" data-level="4.3.4" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#loss-function"><i class="fa fa-check"></i><b>4.3.4</b> Loss Function</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="outro-3.html"><a href="outro-3.html"><i class="fa fa-check"></i><b>4.4</b> Outro</a><ul>
<li class="chapter" data-level="4.4.1" data-path="outro-3.html"><a href="outro-3.html#remarks-3"><i class="fa fa-check"></i><b>4.4.1</b> Remarks</a></li>
<li class="chapter" data-level="4.4.2" data-path="outro-3.html"><a href="outro-3.html#further-reading-3"><i class="fa fa-check"></i><b>4.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Shallow and Deep Neural Networks</a><ul>
<li class="chapter" data-level="5.1" data-path="introduction-7.html"><a href="introduction-7.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="introduction-7.html"><a href="introduction-7.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>5.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="5.1.2" data-path="introduction-7.html"><a href="introduction-7.html#data-2"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>5.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="5.2.2" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#extending-logistic-regression"><i class="fa fa-check"></i><b>5.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#softmax-function"><i class="fa fa-check"></i><b>5.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="5.2.4" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>5.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="5.2.5" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>5.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="5.2.6" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>5.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html"><i class="fa fa-check"></i><b>5.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="5.3.1" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>5.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="5.3.2" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>5.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="5.3.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#example-in-r-3"><i class="fa fa-check"></i><b>5.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html"><i class="fa fa-check"></i><b>5.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="5.4.1" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#introduction-8"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>5.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="5.4.3" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>5.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="5.4.4" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>5.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="preprocessing-of-data.html"><a href="preprocessing-of-data.html"><i class="fa fa-check"></i><b>5.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="5.5.1" data-path="preprocessing-of-data.html"><a href="preprocessing-of-data.html#introduction-9"><i class="fa fa-check"></i><b>5.5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.5.2" data-path="preprocessing-of-data.html"><a href="preprocessing-of-data.html#image-deskewing"><i class="fa fa-check"></i><b>5.5.2</b> Image Deskewing</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="outro-4.html"><a href="outro-4.html"><i class="fa fa-check"></i><b>5.6</b> Outro</a><ul>
<li class="chapter" data-level="5.6.1" data-path="outro-4.html"><a href="outro-4.html#remarks-4"><i class="fa fa-check"></i><b>5.6.1</b> Remarks</a></li>
<li class="chapter" data-level="5.6.2" data-path="outro-4.html"><a href="outro-4.html#beyond-mnist"><i class="fa fa-check"></i><b>5.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="5.6.3" data-path="outro-4.html"><a href="outro-4.html#further-reading-4"><i class="fa fa-check"></i><b>5.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>6</b> Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-10.html"><a href="introduction-10.html"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="introduction-10.html"><a href="introduction-10.html#optimisation-problem"><i class="fa fa-check"></i><b>6.1.1</b> Optimisation Problem</a></li>
<li class="chapter" data-level="6.1.2" data-path="introduction-10.html"><a href="introduction-10.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>6.1.2</b> Example Optimisation Problems in Machine Learning</a></li>
<li class="chapter" data-level="6.1.3" data-path="introduction-10.html"><a href="introduction-10.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>6.1.3</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="6.1.4" data-path="introduction-10.html"><a href="introduction-10.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>6.1.4</b> Example Objective over a 2D Domain</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="iterative-methods.html"><a href="iterative-methods.html"><i class="fa fa-check"></i><b>6.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="iterative-methods.html"><a href="iterative-methods.html#introduction-11"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="iterative-methods.html"><a href="iterative-methods.html#example-in-r-4"><i class="fa fa-check"></i><b>6.2.2</b> Example in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="iterative-methods.html"><a href="iterative-methods.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>6.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="6.2.4" data-path="iterative-methods.html"><a href="iterative-methods.html#random-restarts"><i class="fa fa-check"></i><b>6.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gradient-descent.html"><a href="gradient-descent.html"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.3.1" data-path="gradient-descent.html"><a href="gradient-descent.html#function-gradient"><i class="fa fa-check"></i><b>6.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="6.3.2" data-path="gradient-descent.html"><a href="gradient-descent.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>6.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="6.3.3" data-path="gradient-descent.html"><a href="gradient-descent.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>6.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="6.3.4" data-path="gradient-descent.html"><a href="gradient-descent.html#example-mnist"><i class="fa fa-check"></i><b>6.3.4</b> Example: MNIST</a></li>
<li class="chapter" data-level="6.3.5" data-path="gradient-descent.html"><a href="gradient-descent.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>6.3.5</b> Stochastic Gradient Descent (SGD)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="outro-5.html"><a href="outro-5.html"><i class="fa fa-check"></i><b>6.4</b> Outro</a><ul>
<li class="chapter" data-level="6.4.1" data-path="outro-5.html"><a href="outro-5.html#remarks-5"><i class="fa fa-check"></i><b>6.4.1</b> Remarks</a></li>
<li class="chapter" data-level="6.4.2" data-path="outro-5.html"><a href="outro-5.html#optimisers-in-keras"><i class="fa fa-check"></i><b>6.4.2</b> Optimisers in Keras</a></li>
<li class="chapter" data-level="6.4.3" data-path="outro-5.html"><a href="outro-5.html#note-on-search-spaces"><i class="fa fa-check"></i><b>6.4.3</b> Note on Search Spaces</a></li>
<li class="chapter" data-level="6.4.4" data-path="outro-5.html"><a href="outro-5.html#further-reading-5"><i class="fa fa-check"></i><b>6.4.4</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a><ul>
<li class="chapter" data-level="7.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="7.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#introduction-12"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>7.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="7.1.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering-1"><i class="fa fa-check"></i><b>7.1.3</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>7.2</b> K-means Clustering</a><ul>
<li class="chapter" data-level="7.2.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>7.2.1</b> Example in R</a></li>
<li class="chapter" data-level="7.2.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#problem-statement"><i class="fa fa-check"></i><b>7.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="7.2.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>7.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html"><i class="fa fa-check"></i><b>7.3</b> Hierarchical Methods</a><ul>
<li class="chapter" data-level="7.3.1" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#introduction-13"><i class="fa fa-check"></i><b>7.3.1</b> Introduction</a></li>
<li class="chapter" data-level="7.3.2" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#example-in-r-6"><i class="fa fa-check"></i><b>7.3.2</b> Example in R</a></li>
<li class="chapter" data-level="7.3.3" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>7.3.3</b> Agglomerative Hierarchical Clustering</a></li>
<li class="chapter" data-level="7.3.4" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#linkage-functions"><i class="fa fa-check"></i><b>7.3.4</b> Linkage Functions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="outro-6.html"><a href="outro-6.html"><i class="fa fa-check"></i><b>7.4</b> Outro</a><ul>
<li class="chapter" data-level="7.4.1" data-path="outro-6.html"><a href="outro-6.html#remarks-6"><i class="fa fa-check"></i><b>7.4.1</b> Remarks</a></li>
<li class="chapter" data-level="7.4.2" data-path="outro-6.html"><a href="outro-6.html#other-noteworthy-clustering-algorithms"><i class="fa fa-check"></i><b>7.4.2</b> Other Noteworthy Clustering Algorithms</a></li>
<li class="chapter" data-level="7.4.3" data-path="outro-6.html"><a href="outro-6.html#further-reading-6"><i class="fa fa-check"></i><b>7.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html"><i class="fa fa-check"></i><b>8</b> Optimisation with Genetic Algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="introduction-14.html"><a href="introduction-14.html"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="introduction-14.html"><a href="introduction-14.html#recap"><i class="fa fa-check"></i><b>8.1.1</b> Recap</a></li>
<li class="chapter" data-level="8.1.2" data-path="introduction-14.html"><a href="introduction-14.html#k-means-revisited"><i class="fa fa-check"></i><b>8.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="8.1.3" data-path="introduction-14.html"><a href="introduction-14.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>8.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html"><i class="fa fa-check"></i><b>8.2</b> A Note on Convex Optimisation (*)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#introduction-15"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#convex-combinations"><i class="fa fa-check"></i><b>8.2.2</b> Convex Combinations (*)</a></li>
<li class="chapter" data-level="8.2.3" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#convex-functions"><i class="fa fa-check"></i><b>8.2.3</b> Convex Functions (*)</a></li>
<li class="chapter" data-level="8.2.4" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#examples"><i class="fa fa-check"></i><b>8.2.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html"><i class="fa fa-check"></i><b>8.3</b> Genetic Algorithms</a><ul>
<li class="chapter" data-level="8.3.1" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html#introduction-16"><i class="fa fa-check"></i><b>8.3.1</b> Introduction</a></li>
<li class="chapter" data-level="8.3.2" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html#overview-of-the-method"><i class="fa fa-check"></i><b>8.3.2</b> Overview of the Method</a></li>
<li class="chapter" data-level="8.3.3" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html#example-implementation---ga-for-k-means"><i class="fa fa-check"></i><b>8.3.3</b> Example Implementation - GA for K-means</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="outro-7.html"><a href="outro-7.html"><i class="fa fa-check"></i><b>8.4</b> Outro</a><ul>
<li class="chapter" data-level="8.4.1" data-path="outro-7.html"><a href="outro-7.html#remarks-7"><i class="fa fa-check"></i><b>8.4.1</b> Remarks</a></li>
<li class="chapter" data-level="8.4.2" data-path="outro-7.html"><a href="outro-7.html#further-reading-7"><i class="fa fa-check"></i><b>8.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>9</b> Recommender Systems</a><ul>
<li class="chapter" data-level="9.1" data-path="introduction-17.html"><a href="introduction-17.html"><i class="fa fa-check"></i><b>9.1</b> Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="introduction-17.html"><a href="introduction-17.html#what-is-a-recommender-system"><i class="fa fa-check"></i><b>9.1.1</b> What is a Recommender System?</a></li>
<li class="chapter" data-level="9.1.2" data-path="introduction-17.html"><a href="introduction-17.html#the-netflix-prize"><i class="fa fa-check"></i><b>9.1.2</b> The Netflix Prize</a></li>
<li class="chapter" data-level="9.1.3" data-path="introduction-17.html"><a href="introduction-17.html#main-approaches"><i class="fa fa-check"></i><b>9.1.3</b> Main Approaches</a></li>
<li class="chapter" data-level="9.1.4" data-path="introduction-17.html"><a href="introduction-17.html#formalism-2"><i class="fa fa-check"></i><b>9.1.4</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html"><i class="fa fa-check"></i><b>9.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="9.2.1" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#example"><i class="fa fa-check"></i><b>9.2.1</b> Example</a></li>
<li class="chapter" data-level="9.2.2" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#similarity-measures"><i class="fa fa-check"></i><b>9.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="9.2.3" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="9.2.4" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="movielens-dataset.html"><a href="movielens-dataset.html"><i class="fa fa-check"></i><b>9.3</b> MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="9.3.1" data-path="movielens-dataset.html"><a href="movielens-dataset.html#dataset"><i class="fa fa-check"></i><b>9.3.1</b> Dataset</a></li>
<li class="chapter" data-level="9.3.2" data-path="movielens-dataset.html"><a href="movielens-dataset.html#data-cleansing"><i class="fa fa-check"></i><b>9.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="9.3.3" data-path="movielens-dataset.html"><a href="movielens-dataset.html#item-item-similarities"><i class="fa fa-check"></i><b>9.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="9.3.4" data-path="movielens-dataset.html"><a href="movielens-dataset.html#example-recommendations"><i class="fa fa-check"></i><b>9.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="9.3.5" data-path="movielens-dataset.html"><a href="movielens-dataset.html#clustering-2"><i class="fa fa-check"></i><b>9.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="outro-8.html"><a href="outro-8.html"><i class="fa fa-check"></i><b>9.4</b> Outro</a><ul>
<li class="chapter" data-level="9.4.1" data-path="outro-8.html"><a href="outro-8.html#remarks-8"><i class="fa fa-check"></i><b>9.4.1</b> Remarks</a></li>
<li class="chapter" data-level="9.4.2" data-path="outro-8.html"><a href="outro-8.html#issues"><i class="fa fa-check"></i><b>9.4.2</b> Issues</a></li>
<li class="chapter" data-level="9.4.3" data-path="outro-8.html"><a href="outro-8.html#further-reading-8"><i class="fa fa-check"></i><b>9.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="section-10.html"><a href="section-10.html"><i class="fa fa-check"></i>}</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html"><i class="fa fa-check"></i><b>A</b> Setting Up the R Environment</a><ul>
<li class="chapter" data-level="A.1" data-path="installing-r.html"><a href="installing-r.html"><i class="fa fa-check"></i><b>A.1</b> Installing R</a></li>
<li class="chapter" data-level="A.2" data-path="installing-an-ide.html"><a href="installing-an-ide.html"><i class="fa fa-check"></i><b>A.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="A.3" data-path="installing-recommended-packages.html"><a href="installing-recommended-packages.html"><i class="fa fa-check"></i><b>A.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="A.4" data-path="first-r-script.html"><a href="first-r-script.html"><i class="fa fa-check"></i><b>A.4</b> First R Script</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html"><i class="fa fa-check"></i><b>B</b> Vector Algebra in R</a><ul>
<li class="chapter" data-level="B.1" data-path="motivation-1.html"><a href="motivation-1.html"><i class="fa fa-check"></i><b>B.1</b> Motivation</a></li>
<li class="chapter" data-level="B.2" data-path="numeric-vectors.html"><a href="numeric-vectors.html"><i class="fa fa-check"></i><b>B.2</b> Numeric Vectors</a><ul>
<li class="chapter" data-level="B.2.1" data-path="numeric-vectors.html"><a href="numeric-vectors.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>B.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="B.2.2" data-path="numeric-vectors.html"><a href="numeric-vectors.html#vector-scalar-operations"><i class="fa fa-check"></i><b>B.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="B.2.3" data-path="numeric-vectors.html"><a href="numeric-vectors.html#vector-vector-operations"><i class="fa fa-check"></i><b>B.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="B.2.4" data-path="numeric-vectors.html"><a href="numeric-vectors.html#aggregation-functions"><i class="fa fa-check"></i><b>B.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="B.2.5" data-path="numeric-vectors.html"><a href="numeric-vectors.html#special-functions"><i class="fa fa-check"></i><b>B.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="B.2.6" data-path="numeric-vectors.html"><a href="numeric-vectors.html#norms-and-distances"><i class="fa fa-check"></i><b>B.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="B.2.7" data-path="numeric-vectors.html"><a href="numeric-vectors.html#dot-product"><i class="fa fa-check"></i><b>B.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="B.2.8" data-path="numeric-vectors.html"><a href="numeric-vectors.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>B.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="logical-vectors.html"><a href="logical-vectors.html"><i class="fa fa-check"></i><b>B.3</b> Logical Vectors</a><ul>
<li class="chapter" data-level="B.3.1" data-path="logical-vectors.html"><a href="logical-vectors.html#creating-logical-vectors"><i class="fa fa-check"></i><b>B.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="B.3.2" data-path="logical-vectors.html"><a href="logical-vectors.html#logical-operations"><i class="fa fa-check"></i><b>B.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="B.3.3" data-path="logical-vectors.html"><a href="logical-vectors.html#comparison-operations"><i class="fa fa-check"></i><b>B.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="B.3.4" data-path="logical-vectors.html"><a href="logical-vectors.html#aggregation-functions-1"><i class="fa fa-check"></i><b>B.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="character-vectors.html"><a href="character-vectors.html"><i class="fa fa-check"></i><b>B.4</b> Character Vectors</a><ul>
<li class="chapter" data-level="B.4.1" data-path="character-vectors.html"><a href="character-vectors.html#creating-character-vectors"><i class="fa fa-check"></i><b>B.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="B.4.2" data-path="character-vectors.html"><a href="character-vectors.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>B.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="B.4.3" data-path="character-vectors.html"><a href="character-vectors.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>B.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="vector-subsetting.html"><a href="vector-subsetting.html"><i class="fa fa-check"></i><b>B.5</b> Vector Subsetting</a><ul>
<li class="chapter" data-level="B.5.1" data-path="vector-subsetting.html"><a href="vector-subsetting.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>B.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="B.5.2" data-path="vector-subsetting.html"><a href="vector-subsetting.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>B.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="B.5.3" data-path="vector-subsetting.html"><a href="vector-subsetting.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>B.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="B.5.4" data-path="vector-subsetting.html"><a href="vector-subsetting.html#replacing-elements"><i class="fa fa-check"></i><b>B.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="B.5.5" data-path="vector-subsetting.html"><a href="vector-subsetting.html#other-functions"><i class="fa fa-check"></i><b>B.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="B.6" data-path="named-vectors.html"><a href="named-vectors.html"><i class="fa fa-check"></i><b>B.6</b> Named Vectors</a><ul>
<li class="chapter" data-level="B.6.1" data-path="named-vectors.html"><a href="named-vectors.html#creating-named-vectors"><i class="fa fa-check"></i><b>B.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="B.6.2" data-path="named-vectors.html"><a href="named-vectors.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>B.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="B.7" data-path="factors.html"><a href="factors.html"><i class="fa fa-check"></i><b>B.7</b> Factors</a><ul>
<li class="chapter" data-level="B.7.1" data-path="factors.html"><a href="factors.html#creating-factors"><i class="fa fa-check"></i><b>B.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="B.7.2" data-path="factors.html"><a href="factors.html#levels"><i class="fa fa-check"></i><b>B.7.2</b> Levels</a></li>
<li class="chapter" data-level="B.7.3" data-path="factors.html"><a href="factors.html#internal-representation"><i class="fa fa-check"></i><b>B.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="B.8" data-path="lists.html"><a href="lists.html"><i class="fa fa-check"></i><b>B.8</b> Lists</a><ul>
<li class="chapter" data-level="B.8.1" data-path="lists.html"><a href="lists.html#creating-lists"><i class="fa fa-check"></i><b>B.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="B.8.2" data-path="lists.html"><a href="lists.html#named-lists"><i class="fa fa-check"></i><b>B.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="B.8.3" data-path="lists.html"><a href="lists.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>B.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="B.8.4" data-path="lists.html"><a href="lists.html#common-operations"><i class="fa fa-check"></i><b>B.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="B.9" data-path="further-reading-9.html"><a href="further-reading-9.html"><i class="fa fa-check"></i><b>B.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html"><i class="fa fa-check"></i><b>C</b> Matrix Algebra in R</a><ul>
<li class="chapter" data-level="C.1" data-path="creating-matrices.html"><a href="creating-matrices.html"><i class="fa fa-check"></i><b>C.1</b> Creating Matrices</a><ul>
<li class="chapter" data-level="C.1.1" data-path="creating-matrices.html"><a href="creating-matrices.html#matrix"><i class="fa fa-check"></i><b>C.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="C.1.2" data-path="creating-matrices.html"><a href="creating-matrices.html#stacking-vectors"><i class="fa fa-check"></i><b>C.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="C.1.3" data-path="creating-matrices.html"><a href="creating-matrices.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>C.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="C.1.4" data-path="creating-matrices.html"><a href="creating-matrices.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>C.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="C.1.5" data-path="creating-matrices.html"><a href="creating-matrices.html#other-methods"><i class="fa fa-check"></i><b>C.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="C.1.6" data-path="creating-matrices.html"><a href="creating-matrices.html#internal-representation-1"><i class="fa fa-check"></i><b>C.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="algebraic-operations.html"><a href="algebraic-operations.html"><i class="fa fa-check"></i><b>C.2</b> Algebraic Operations</a><ul>
<li class="chapter" data-level="C.2.1" data-path="algebraic-operations.html"><a href="algebraic-operations.html#matrix-transpose"><i class="fa fa-check"></i><b>C.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="C.2.2" data-path="algebraic-operations.html"><a href="algebraic-operations.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="algebraic-operations.html"><a href="algebraic-operations.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>C.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="algebraic-operations.html"><a href="algebraic-operations.html#matrix-multiplication"><i class="fa fa-check"></i><b>C.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="C.2.5" data-path="algebraic-operations.html"><a href="algebraic-operations.html#matrix-vector-operations"><i class="fa fa-check"></i><b>C.2.5</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="matrix-subsetting.html"><a href="matrix-subsetting.html"><i class="fa fa-check"></i><b>C.3</b> Matrix Subsetting</a><ul>
<li class="chapter" data-level="C.3.1" data-path="matrix-subsetting.html"><a href="matrix-subsetting.html#selecting-individual-elements"><i class="fa fa-check"></i><b>C.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="C.3.2" data-path="matrix-subsetting.html"><a href="matrix-subsetting.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>C.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="C.3.3" data-path="matrix-subsetting.html"><a href="matrix-subsetting.html#selecting-submatrices"><i class="fa fa-check"></i><b>C.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="C.3.4" data-path="matrix-subsetting.html"><a href="matrix-subsetting.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>C.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="C.3.5" data-path="matrix-subsetting.html"><a href="matrix-subsetting.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>C.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="further-reading-10.html"><a href="further-reading-10.html"><i class="fa fa-check"></i><b>C.4</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html"><i class="fa fa-check"></i><b>D</b> Data Frame Wrangling in R</a><ul>
<li class="chapter" data-level="D.1" data-path="creating-data-frames.html"><a href="creating-data-frames.html"><i class="fa fa-check"></i><b>D.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="D.2" data-path="importing-data-frames.html"><a href="importing-data-frames.html"><i class="fa fa-check"></i><b>D.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="D.3" data-path="data-frame-subsetting.html"><a href="data-frame-subsetting.html"><i class="fa fa-check"></i><b>D.3</b> Data Frame Subsetting</a><ul>
<li class="chapter" data-level="D.3.1" data-path="data-frame-subsetting.html"><a href="data-frame-subsetting.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>D.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="D.3.2" data-path="data-frame-subsetting.html"><a href="data-frame-subsetting.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>D.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="common-operations-1.html"><a href="common-operations-1.html"><i class="fa fa-check"></i><b>D.4</b> Common Operations</a></li>
<li class="chapter" data-level="D.5" data-path="metaprogramming-and-formulas.html"><a href="metaprogramming-and-formulas.html"><i class="fa fa-check"></i><b>D.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="D.6" data-path="further-reading-11.html"><a href="further-reading-11.html"><i class="fa fa-check"></i><b>D.6</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.2 2020-03-31 00:14 (b57874d)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="finding-the-best-model" class="section level2">
<h2><span class="header-section-number">2.3</span> Finding the Best Model</h2>
<div id="model-diagnostics" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Model Diagnostics</h3>
<hr />
<p>Consider the three following models.</p>
<table>
<thead>
<tr class="header">
<th>Formula</th>
<th>Equation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rating ~ Balance + Income</td>
<td><span class="math inline">\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span></td>
</tr>
<tr class="even">
<td>Rating ~ Balance</td>
<td><span class="math inline">\(Y=a X_1 + b\)</span> (<span class="math inline">\(\beta_0=b, \beta_1=a, \beta_2=0\)</span>)</td>
</tr>
<tr class="odd">
<td>Rating ~ Income</td>
<td><span class="math inline">\(Y=a X_2 + b\)</span> (<span class="math inline">\(\beta_0=b, \beta_1=0, \beta_2=a\)</span>)</td>
</tr>
</tbody>
</table>
<hr />
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" title="1">f12 &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1<span class="op">+</span>X2) <span class="co"># Rating ~ Balance + Income</span></a>
<a class="sourceLine" id="cb67-2" title="2">f12<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>## (Intercept)          X1          X2 
## 172.5586670   0.1828011   2.1976461</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" title="1">f1  &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1)    <span class="co"># Rating ~ Balance</span></a>
<a class="sourceLine" id="cb69-2" title="2">f1<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>## (Intercept)          X1 
## 226.4711446   0.2661459</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" title="1">f2  &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X2)    <span class="co"># Rating ~ Income</span></a>
<a class="sourceLine" id="cb71-2" title="2">f2<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>## (Intercept)          X2 
##  253.851416    3.025286</code></pre>
<p>Which of the three models is the best?
Of course, by using the word “best”,
we need to answer the question “best?… but with respect to what kind of measure?”</p>
<p>So far we were fitting w.r.t. SSR,
as the multiple regression model generalises the two simple ones,
the former must yield a not-worse SSR.
This is because in the case of <span class="math inline">\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>,
setting <span class="math inline">\(\beta_1\)</span> to 0 (just one of uncountably many possible <span class="math inline">\(\beta_1\)</span>s,
if it happens to be the <em>best</em> one, good for us)
gives <span class="math inline">\(Y=a X_2 + b\)</span>
whereas by setting <span class="math inline">\(\beta_2\)</span> to 0 we obtain <span class="math inline">\(Y=a X_1 + b\)</span>.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" title="1"><span class="kw">sum</span>(f12<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 358260.6</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" title="1"><span class="kw">sum</span>(f1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 2132108</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" title="1"><span class="kw">sum</span>(f2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 1823473</code></pre>
<p>We get that <span class="math inline">\(f_{12} \succeq f_{2} \succeq f_{1}\)</span> but these error values
are meaningless.</p>
<blockquote>
<p>Interpretability in ML has always been an important issue, think the EU
General Data Protection Regulation (GDPR), amongst others.</p>
</blockquote>
<hr />
<p>The quality of fit can be assessed by performing some <em>descriptive
statistical analysis of the residuals</em>, <span class="math inline">\(\hat{y}_i-y_i\)</span>,
for <span class="math inline">\(i=1,\dots,n\)</span>.</p>
<p>I know how to summarise data on the residuals!
Of course I should compute their arithmetic mean and I’m done with that shtask!
Interestingly, the mean of residuals (this can be shown analytically)
in the least squared fit is always equal to <span class="math inline">\(0\)</span>:
<span class="math display">\[
 \frac{1}{n} \sum_{i=1}^n (\hat{y}_i-y_i)=0.
\]</span>
Therefore, we need a different metric.</p>
<blockquote>
<p>(*) A proof of this fact is left as an exercise to the curious;
assume <span class="math inline">\(p=1\)</span> just as in the previous chapter and note that <span class="math inline">\(\hat{y}_i=a x_i+b\)</span>.</p>
</blockquote>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" title="1"><span class="kw">mean</span>(f12<span class="op">$</span>residuals) <span class="co"># almost zero numerically</span></a></code></pre></div>
<pre><code>## [1] 4.444473e-16</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" title="1"><span class="kw">all.equal</span>(<span class="kw">mean</span>(f12<span class="op">$</span>residuals), <span class="dv">0</span>)</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<hr />
<p>We noted that sum of squared residuals (SSR) is not interpretable,
but the mean squared residuals
(MSR) – also called mean squared error (MSE) regression loss – is a little better.
Recall that mean is defined as the sum divided by number of samples.</p>
<p><span class="math display">\[
 \mathrm{MSE}(f) = \frac{1}{n} \sum_{i=1}^n (f(\mathbf{x}_{i,\cdot})-y_i)^2.
\]</span></p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" title="1"><span class="kw">mean</span>(f12<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 1155.679</code></pre>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" title="1"><span class="kw">mean</span>(f1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 6877.768</code></pre>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" title="1"><span class="kw">mean</span>(f2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 5882.171</code></pre>
<p>This gives an information of how much do we err <em>per sample</em>,
so at least this measure does not depend on <span class="math inline">\(n\)</span> anymore.
However, if the original <span class="math inline">\(Y\)</span>s are, say, in metres <span class="math inline">\([\mathrm{m}]\)</span>,
MSE is expressed in metres squared <span class="math inline">\([\mathrm{m}^2]\)</span>.</p>
<p>To account for that, we may consider the root mean squared error (RMSE):
<span class="math display">\[
 \mathrm{RMSE}(f) = \sqrt{\frac{1}{n} \sum_{i=1}^n (f(\mathbf{x}_{i,\cdot})-y_i)^2}.
\]</span>
This is just like with the sample variance vs. standard deviation –
recall the latter is defined as the square root of the former.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" title="1"><span class="kw">sqrt</span>(<span class="kw">mean</span>(f12<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 33.99528</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" title="1"><span class="kw">sqrt</span>(<span class="kw">mean</span>(f1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 82.93231</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" title="1"><span class="kw">sqrt</span>(<span class="kw">mean</span>(f2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 76.69531</code></pre>
<p>The interpretation of the RMSE is rather quirky;
it is some-sort-of-averaged <em>deviance</em> from the true rating
(which is on the scale 0–1000, hence we see that the first model is not
that bad). Recall that the square function is sensitive to large observations,
hence, it penalises notable deviations more heavily.</p>
<p>As still we have a problem with finding something easily interpretable
(your non-technical boss or client may ask you: but what do these numbers mean??),
we suggest here that the mean absolute error (MAE;
also called mean absolute deviations, MAD)
might be a better idea than the above:
<span class="math display">\[
 \mathrm{MAE}(f) = \frac{1}{n} \sum_{i=1}^n |f(\mathbf{x}_{i,\cdot})-y_i|.
\]</span></p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" title="1"><span class="kw">mean</span>(<span class="kw">abs</span>(f12<span class="op">$</span>residuals))</a></code></pre></div>
<pre><code>## [1] 22.86342</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" title="1"><span class="kw">mean</span>(<span class="kw">abs</span>(f1<span class="op">$</span>residuals))</a></code></pre></div>
<pre><code>## [1] 61.48892</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" title="1"><span class="kw">mean</span>(<span class="kw">abs</span>(f2<span class="op">$</span>residuals))</a></code></pre></div>
<pre><code>## [1] 64.1506</code></pre>
<p>With the above we may say “On average, the predicted rating differs from the
observed one by”. That is good enough.</p>
<blockquote>
<p>(*) You may ask why don’t we fit models so as to minimise the MAE
and we minimise the RMSE instead (note that minimising RMSE is the same as
minimising the SSR, one is a strictly monotone transformation of the other
and do not affect the solution). Well, it is possible.
It turns out that, however, minimising MAE is more computationally expensive
and the solution may be numerically unstable.
So it’s rarely an analyst’s first choice (assuming they are well-educated
enough to know about the MAD regression task). However, it may be worth
trying it out sometimes.</p>
<p>Sometimes we might prefer MAD regression to the classic one
if our data is heavily contaminated by outliers. But
in such cases it is worth checking if proper data cleansing does
the trick.</p>
</blockquote>
<p>If we are not happy with single numerical aggregated of the residuals
or their absolute values, we can (and should) always compute a whole
bunch of descriptive statistics:</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb101-1" title="1"><span class="kw">summary</span>(f12<span class="op">$</span>residuals)</a></code></pre></div>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -108.100   -1.940    7.812    0.000   20.249   50.623</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" title="1"><span class="kw">summary</span>(f1<span class="op">$</span>residuals)</a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -226.75  -48.30  -10.08    0.00   42.58  268.74</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" title="1"><span class="kw">summary</span>(f2<span class="op">$</span>residuals)</a></code></pre></div>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -195.156  -57.341   -1.284    0.000   64.013  175.344</code></pre>
<p>The outputs generated by <code>summary()</code> include:</p>
<ul>
<li><code>Min.</code> – sample minimum</li>
<li><code>1st Qu.</code> – 1st quartile == 25th percentile == quantile of order 0.25</li>
<li><code>Median</code> – median == 50th percentile == quantile of order 0.5</li>
<li><code>3rd Qu.</code> – 3rd quartile = 75th percentile == quantile of order 0.75</li>
<li><code>Max.</code> – sample maximum</li>
</ul>
<p>For example, 1st quartile is the observation <span class="math inline">\(q\)</span> such that
25% values are <span class="math inline">\(\le q\)</span> and 75% values are <span class="math inline">\(\ge q\)</span>,
see <code>?quantile</code> in R.</p>
<p>Graphically, it is nice to summarise the empirical distribution of the residuals
on a <strong>box and whisker plot</strong>:</p>
<ul>
<li>IQR == Interquartile range == Q3<span class="math inline">\(-\)</span>Q1 (box width)</li>
<li>The box contains 50% of the “most typical” observations</li>
<li>Box and whiskers altogether have width <span class="math inline">\(\le\)</span> 4 IQR</li>
<li>Outliers == observations potentially worth inspecting (is it a bug or a feature?)</li>
</ul>
<p><img src="02-regression-multiple-figures/unnamed-chunk-13-1.png" alt="plot of chunk unnamed-chunk-13" style="width:50.0%" /></p>
<hr />
<p>A picture is worth a thousand words:</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb107-1" title="1"><span class="kw">boxplot</span>(<span class="dt">las=</span><span class="dv">1</span>, <span class="dt">horizontal=</span><span class="ot">TRUE</span>, <span class="dt">xlab=</span><span class="st">&quot;residuals&quot;</span>,</a>
<a class="sourceLine" id="cb107-2" title="2">  <span class="kw">list</span>(<span class="dt">f12=</span>f12<span class="op">$</span>residuals, <span class="dt">f1=</span>f1<span class="op">$</span>residuals, <span class="dt">f2=</span>f2<span class="op">$</span>residuals))</a>
<a class="sourceLine" id="cb107-3" title="3"><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a></code></pre></div>
<p><img src="02-regression-multiple-figures/unnamed-chunk-14-1.png" alt="plot of chunk unnamed-chunk-14" style="width:50.0%" /></p>
<hr />
<p><em>Violin plot</em> – a blend of a box plot and a (kernel) density estimator (histogram-like):</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" title="1"><span class="kw">library</span>(<span class="st">&quot;vioplot&quot;</span>)</a>
<a class="sourceLine" id="cb108-2" title="2"><span class="kw">vioplot</span>(<span class="dt">las=</span><span class="dv">1</span>, <span class="dt">horizontal=</span><span class="ot">TRUE</span>, <span class="dt">xlab=</span><span class="st">&quot;residuals&quot;</span>,</a>
<a class="sourceLine" id="cb108-3" title="3">  <span class="kw">list</span>(<span class="dt">f12=</span>f12<span class="op">$</span>residuals, <span class="dt">f1=</span>f1<span class="op">$</span>residuals, <span class="dt">f2=</span>f2<span class="op">$</span>residuals))</a>
<a class="sourceLine" id="cb108-4" title="4"><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a></code></pre></div>
<p><img src="02-regression-multiple-figures/unnamed-chunk-15-1.png" alt="plot of chunk unnamed-chunk-15" style="width:50.0%" /></p>
<hr />
<p>By the way, this is Rating (<span class="math inline">\(Y\)</span>) as function of Balance (<span class="math inline">\(X_1\)</span>, top subfigure)
and Income (<span class="math inline">\(X_2\)</span>, bottom subfigure).</p>
<p><img src="02-regression-multiple-figures/unnamed-chunk-16-1.png" alt="plot of chunk unnamed-chunk-16" style="width:50.0%" /></p>
<hr />
<p>Descriptive statistics for absolute values of residuals:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb109-1" title="1"><span class="kw">summary</span>(<span class="kw">abs</span>(f12<span class="op">$</span>residuals))</a></code></pre></div>
<pre><code>##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
##   0.06457   6.46397  14.07055  22.86342  26.41772 108.09995</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" title="1"><span class="kw">summary</span>(<span class="kw">abs</span>(f1<span class="op">$</span>residuals))</a></code></pre></div>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
##   0.5056  19.6640  45.0716  61.4889  80.1239 268.7377</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" title="1"><span class="kw">summary</span>(<span class="kw">abs</span>(f2<span class="op">$</span>residuals))</a></code></pre></div>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
##   0.6545  29.8540  59.6756  64.1506  95.7384 195.1557</code></pre>
<hr />
<p>This picture is worth $1000:</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" title="1"><span class="kw">boxplot</span>(<span class="dt">las=</span><span class="dv">1</span>, <span class="dt">horizontal=</span><span class="ot">TRUE</span>, <span class="dt">xlab=</span><span class="st">&quot;abs(residuals)&quot;</span>,</a>
<a class="sourceLine" id="cb115-2" title="2">  <span class="kw">list</span>(<span class="dt">f12=</span><span class="kw">abs</span>(f12<span class="op">$</span>residuals), <span class="dt">f1=</span><span class="kw">abs</span>(f1<span class="op">$</span>residuals),</a>
<a class="sourceLine" id="cb115-3" title="3">       <span class="dt">f2=</span><span class="kw">abs</span>(f2<span class="op">$</span>residuals)))</a>
<a class="sourceLine" id="cb115-4" title="4"><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a></code></pre></div>
<p><img src="02-regression-multiple-figures/unnamed-chunk-18-1.png" alt="plot of chunk unnamed-chunk-18" style="width:50.0%" /></p>
<hr />
<p>If we didn’t know the range of the dependent variable
(in our case we do know that the credit rating is on the scale 0–1000),
the RMSE or MAE would be hard to interpret.</p>
<p>It turns out that there is a popular <em>normalised</em> (unit-less) measure
that is somehow easy to interpret with no domain-specific knowledge
of the modelled problem.
Namely, the (unadjusted) <strong><span class="math inline">\(R^2\)</span> score</strong> (the coefficient of determination)
is given by:</p>
<p><span class="math display">\[
R^2(f) = 1 - \frac{\sum_{i=1}^{n} \left(y_i-f(\mathbf{x}_{i,\cdot})\right)^2}{\sum_{i=1}^{n} \left(y_i-\bar{y}\right)^2},
\]</span>
where <span class="math inline">\(\bar{y}\)</span> is the arithmetic mean <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n y_i\)</span>.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" title="1">(r12 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(f12<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>((Y<span class="op">-</span><span class="kw">mean</span>(Y))<span class="op">^</span><span class="dv">2</span>) )</a></code></pre></div>
<pre><code>## [1] 0.9390901</code></pre>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" title="1">(r1  &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(f1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>((Y<span class="op">-</span><span class="kw">mean</span>(Y))<span class="op">^</span><span class="dv">2</span>)  )</a></code></pre></div>
<pre><code>## [1] 0.6375085</code></pre>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" title="1">(r2  &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(f2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>((Y<span class="op">-</span><span class="kw">mean</span>(Y))<span class="op">^</span><span class="dv">2</span>)  )</a></code></pre></div>
<pre><code>## [1] 0.6899812</code></pre>
<p>The coefficient of determination gives the proportion of variance of the
dependent variable explained by independent variables in the model;
<span class="math inline">\(R^2(f)\simeq 1\)</span> indicates a perfect fit</p>
<p>Unfortunately, <span class="math inline">\(R^2\)</span> tends to automatically increase as the number of independent variables
increase (recall that the more variables in the model,
the better the SSR must be).
To correct for this phenomenon, we sometimes consider the <strong>adjusted <span class="math inline">\(R^2\)</span></strong>:</p>
<p><span class="math display">\[
\bar{R}^2(f) = 1 - (1-{R}^2(f))\frac{n-1}{n-p-1}
\]</span></p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" title="1">n &lt;-<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb122-2" title="2"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>r12)<span class="op">*</span>(n<span class="dv">-1</span>)<span class="op">/</span>(n<span class="dv">-3</span>)</a></code></pre></div>
<pre><code>## [1] 0.9386933</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" title="1"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>r1 )<span class="op">*</span>(n<span class="dv">-1</span>)<span class="op">/</span>(n<span class="dv">-2</span>)</a></code></pre></div>
<pre><code>## [1] 0.6363316</code></pre>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" title="1"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>r2 )<span class="op">*</span>(n<span class="dv">-1</span>)<span class="op">/</span>(n<span class="dv">-2</span>)</a></code></pre></div>
<pre><code>## [1] 0.6889747</code></pre>
<p>In other words, the adjusted <span class="math inline">\(R^2\)</span> penalises for more complex models.</p>
<hr />
<blockquote>
<p>(*) Side note – results of some statistical tests (e.g., significance of coefficients)
are reported by calling <code>summary(f12)</code> etc. — refer to a more advanced source to obtain more information.
These, however, require the verification of some assumptions regarding the input data
and the residuals.</p>
</blockquote>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" title="1"><span class="kw">summary</span>(f12)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 + X2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -108.100   -1.940    7.812   20.249   50.623 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.726e+02  3.950e+00   43.69   &lt;2e-16 ***
## X1          1.828e-01  5.159e-03   35.43   &lt;2e-16 ***
## X2          2.198e+00  5.637e-02   38.99   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 34.16 on 307 degrees of freedom
## Multiple R-squared:  0.9391, Adjusted R-squared:  0.9387 
## F-statistic:  2367 on 2 and 307 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="variable-selection" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Variable Selection</h3>
<p>Okay, up to now we were considering the problem of modelling
the <code>Rating</code> variable as a function of <code>Balance</code> and/or <code>Income</code>.
However, it the <code>Credit</code> data set there are other variables
possibly worth inspecting.</p>
<p>Consider all quantitative (numeric-continuous) variables in the <code>Credit</code> data set.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb130-1" title="1">C &lt;-<span class="st"> </span>Credit[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>,</a>
<a class="sourceLine" id="cb130-2" title="2">    <span class="kw">c</span>(<span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Limit&quot;</span>, <span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Age&quot;</span>,</a>
<a class="sourceLine" id="cb130-3" title="3">      <span class="st">&quot;Education&quot;</span>, <span class="st">&quot;Balance&quot;</span>)]</a>
<a class="sourceLine" id="cb130-4" title="4"><span class="kw">head</span>(C)</a></code></pre></div>
<pre><code>##   Rating Limit  Income Age Education Balance
## 1    283  3606  14.891  34        11     333
## 2    483  6645 106.025  82        15     903
## 3    514  7075 104.593  71        11     580
## 4    681  9504 148.924  36        11     964
## 5    357  4897  55.882  68        16     331
## 6    569  8047  80.180  77        10    1151</code></pre>
<p>Obviously there are many possible combinations of the variables
upon which regression models can be constructed
(precisely, for <span class="math inline">\(p\)</span> variables there are <span class="math inline">\(2^p\)</span> such models).
How do we choose the <em>best</em> set of inputs?</p>
<blockquote>
<p>We should already be suspicious at this point:
wait… <em>best</em> requires some sort of criterion, right?</p>
</blockquote>
<p>First, however, let’s draw a <em>pair plot</em> – a matrix of scatter plots for every pair of variables
– so as to get an impression of how individual variables interact with each other.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb132-1" title="1"><span class="kw">pairs</span>(C)</a></code></pre></div>
<hr />
<p><img src="02-regression-multiple-figures/unnamed-chunk-24-1.png" alt="plot of chunk unnamed-chunk-24" style="width:50.0%" /></p>
<hr />
<p>Seems like <code>Rating</code> depends on <code>Limit</code> almost linearly…</p>
<p>Pearson’s <span class="math inline">\(r\)</span> – linear correlation coefficient:</p>
<p><span class="math display">\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y})
}{
    \sqrt{\sum_{i=1}^n (x_i-\bar{x})^2} \sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}
}.
\]</span></p>
<p>It holds <span class="math inline">\(r\in[-1,1]\)</span>, where:</p>
<ul>
<li><span class="math inline">\(r=1\)</span> – positive linear dependence (<span class="math inline">\(y\)</span> increases as <span class="math inline">\(x\)</span> increases)</li>
<li><span class="math inline">\(r=-1\)</span> – negative linear dependence (<span class="math inline">\(y\)</span> decreases as <span class="math inline">\(x\)</span> increases)</li>
<li><span class="math inline">\(r\simeq 0\)</span> – uncorrelated or non-linearly dependent</li>
</ul>
<!-- TODO anscombe -->
<hr />
<p>Interpretation:</p>
<p><img src="02-regression-multiple-figures/unnamed-chunk-25-1.png" alt="plot of chunk unnamed-chunk-25" style="width:50.0%" /></p>
<hr />
<p>Compute Pearson’s <span class="math inline">\(r\)</span> between all pairs of variables:</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb133-1" title="1"><span class="kw">round</span>(<span class="kw">cor</span>(C), <span class="dv">3</span>)</a></code></pre></div>
<pre><code>##           Rating  Limit Income   Age Education Balance
## Rating     1.000  0.996  0.831 0.167    -0.040   0.798
## Limit      0.996  1.000  0.834 0.164    -0.032   0.796
## Income     0.831  0.834  1.000 0.227    -0.033   0.414
## Age        0.167  0.164  0.227 1.000     0.024   0.008
## Education -0.040 -0.032 -0.033 0.024     1.000   0.001
## Balance    0.798  0.796  0.414 0.008     0.001   1.000</code></pre>
<p><code>Rating</code> and <code>Limit</code> are almost perfectly linearly correlated,
and both seem to describe the same thing.</p>
<p>For practical purposes, we’d rather model <code>Rating</code> as a function of the other variables.</p>
<p>For simple linear regression models, we’d choose either <code>Income</code> or <code>Balance</code>.</p>
<blockquote>
<p>How about multiple regression?</p>
</blockquote>
<hr />
<p>The best model:</p>
<ul>
<li>has high predictive power,</li>
<li>is simple.</li>
</ul>
<p>These are often mutually exclusive.</p>
<p>Which variables should be included in the optimal model?</p>
<hr />
<p>Again, the definition of the “best” object needs a <em>fitness</em> function.</p>
<p>For fitting a single model to data, we use the SSR.</p>
<p>We need a metric that takes the number of dependent variables into account.</p>
<blockquote>
<p>(*) Unfortunately, the adjusted <span class="math inline">\(R^2\)</span>, despite its interpretability,
is not really suitable for this task. It does not penalise complex models
heavily enough to be really useful.</p>
</blockquote>
<hr />
<p>Here we’ll be using <strong>the Akaike Information Criterion</strong> (AIC).</p>
<p>For a model <span class="math inline">\(f\)</span> with <span class="math inline">\(p&#39;\)</span> independent variables:
<span class="math display">\[
\mathrm{AIC}(f) = 2(p&#39;+1)+n\log(\mathrm{SSR}(f))-n\log n
\]</span></p>
<p>Our task is to find the combination of independent variables
that minimises the AIC.</p>
<blockquote>
<p>(*) Note that this is a bi-level optimisation problem – for every
considered combination of variables (which we look for),
we must solve another problem of finding the best model
involving these variables – the one that minimises the SSR.
<span class="math display">\[
\min_{s_1,s_2,\dots,s_p\in\{0, 1\}}
\left(
\begin{array}{l}
2\left(\displaystyle\sum_{j=1}^p s_j +1\right)+\\
n\log\left(
\displaystyle\min_{\beta_0,\beta_1,\dots,\beta_p\in\mathbb{R}}
\sum_{i=1}^n \left(
\beta_0 + s_1\beta_1 x_{i,1} + \dots + s_p\beta_p x_{i,p}
-y_i
\right)^2
\right)
\end{array}
\right)
\]</span>
We dropped the <span class="math inline">\(n\log n\)</span> term, because it is always constant
and hence doesn’t affect the solution.
If <span class="math inline">\(s_j=0\)</span>, then the <span class="math inline">\(s_j\beta_j x_{i,j}\)</span> term is equal to
<span class="math inline">\(0\)</span>, and hence is not considered in the model.
This plays the role of including <span class="math inline">\(s_j=1\)</span> or omitting <span class="math inline">\(s_j=0\)</span> the <span class="math inline">\(j\)</span>-th
variable in the model building exercise.</p>
</blockquote>
<p>For <span class="math inline">\(p\)</span> variables, the number of their possible
combinations is equal to <span class="math inline">\(2^p\)</span>
(grows exponentially with <span class="math inline">\(p\)</span>).
For large <span class="math inline">\(p\)</span> (think big data), an extensive search is impractical
(in our case we could get away with this though – left as an exercise
to a slightly more advanced reader).
Therefore, to find the variable combination minimising the AIC,
we often rely on one of the two following greedy heuristics:</p>
<ul>
<li><p>forward selection:</p>
<ol style="list-style-type: decimal">
<li>start with an empty model</li>
<li>find an independent variable
whose addition to the current model would yield the highest decrease in the AIC and add it to the model</li>
<li>go to step 2 until AIC decreases</li>
</ol></li>
<li><p>backward elimination:</p>
<ol style="list-style-type: decimal">
<li>start with the full model</li>
<li>find an independent variable
whose removal from the current model would decrease the AIC the most and eliminate it from the model</li>
<li>go to step 2 until AIC decreases</li>
</ol></li>
</ul>
<blockquote>
<p>(**) The above bi-level optimisation problem
can be solved by implementing a genetic algorithm – see further chapter for more details.</p>
</blockquote>
<blockquote>
<p>(*) There are of course many other methods which also perform
some form of variable selection, e.g., lasso regression.
But these minimise a different objective.</p>
</blockquote>
<p>Forward selection example:</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb135-1" title="1">C &lt;-<span class="st"> </span>Credit[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>,</a>
<a class="sourceLine" id="cb135-2" title="2">    <span class="kw">c</span>(<span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Age&quot;</span>,</a>
<a class="sourceLine" id="cb135-3" title="3">      <span class="st">&quot;Education&quot;</span>, <span class="st">&quot;Balance&quot;</span>)]</a>
<a class="sourceLine" id="cb135-4" title="4"><span class="kw">step</span>(<span class="kw">lm</span>(Rating<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>C), <span class="co"># empty model</span></a>
<a class="sourceLine" id="cb135-5" title="5">    <span class="dt">scope=</span><span class="kw">formula</span>(<span class="kw">lm</span>(Rating<span class="op">~</span>., <span class="dt">data=</span>C)), <span class="co"># full model</span></a>
<a class="sourceLine" id="cb135-6" title="6">    <span class="dt">direction=</span><span class="st">&quot;forward&quot;</span>)</a></code></pre></div>
<pre><code>## Start:  AIC=3055.75
## Rating ~ 1
## 
##             Df Sum of Sq     RSS    AIC
## + Income     1   4058342 1823473 2694.7
## + Balance    1   3749707 2132108 2743.2
## + Age        1    164567 5717248 3048.9
## &lt;none&gt;                   5881815 3055.8
## + Education  1      9631 5872184 3057.2
## 
## Step:  AIC=2694.7
## Rating ~ Income
## 
##             Df Sum of Sq     RSS    AIC
## + Balance    1   1465212  358261 2192.3
## &lt;none&gt;                   1823473 2694.7
## + Age        1      2836 1820637 2696.2
## + Education  1      1063 1822410 2696.5
## 
## Step:  AIC=2192.26
## Rating ~ Income + Balance
## 
##             Df Sum of Sq    RSS    AIC
## + Age        1    4119.1 354141 2190.7
## + Education  1    2692.1 355568 2191.9
## &lt;none&gt;                   358261 2192.3
## 
## Step:  AIC=2190.67
## Rating ~ Income + Balance + Age
## 
##             Df Sum of Sq    RSS    AIC
## + Education  1    2925.7 351216 2190.1
## &lt;none&gt;                   354141 2190.7
## 
## Step:  AIC=2190.1
## Rating ~ Income + Balance + Age + Education</code></pre>
<pre><code>## 
## Call:
## lm(formula = Rating ~ Income + Balance + Age + Education, data = C)
## 
## Coefficients:
## (Intercept)       Income      Balance          Age  
##    173.8300       2.1668       0.1839       0.2234  
##   Education  
##     -0.9601</code></pre>
<p>The full model has been selected.</p>
<p>Backward elimination example:</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" title="1"><span class="kw">step</span>(<span class="kw">lm</span>(Rating<span class="op">~</span>., <span class="dt">data=</span>C), <span class="co"># full model</span></a>
<a class="sourceLine" id="cb138-2" title="2">     <span class="dt">scope=</span><span class="kw">formula</span>(<span class="kw">lm</span>(Rating<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>C)), <span class="co"># empty model</span></a>
<a class="sourceLine" id="cb138-3" title="3">     <span class="dt">direction=</span><span class="st">&quot;backward&quot;</span>)</a></code></pre></div>
<pre><code>## Start:  AIC=2190.1
## Rating ~ Income + Age + Education + Balance
## 
##             Df Sum of Sq     RSS    AIC
## &lt;none&gt;                    351216 2190.1
## - Education  1      2926  354141 2190.7
## - Age        1      4353  355568 2191.9
## - Balance    1   1468466 1819682 2698.1
## - Income     1   1617191 1968406 2722.4</code></pre>
<pre><code>## 
## Call:
## lm(formula = Rating ~ Income + Age + Education + Balance, data = C)
## 
## Coefficients:
## (Intercept)       Income          Age    Education  
##    173.8300       2.1668       0.2234      -0.9601  
##     Balance  
##      0.1839</code></pre>
<p>The full model is considered the best.</p>
<p>Forward selection example – full dataset:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb141-1" title="1">C &lt;-<span class="st"> </span>Credit[,  <span class="co"># do not restrict to Credit$Balance&gt;0</span></a>
<a class="sourceLine" id="cb141-2" title="2">    <span class="kw">c</span>(<span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Age&quot;</span>,</a>
<a class="sourceLine" id="cb141-3" title="3">      <span class="st">&quot;Education&quot;</span>, <span class="st">&quot;Balance&quot;</span>)]</a>
<a class="sourceLine" id="cb141-4" title="4"><span class="kw">step</span>(<span class="kw">lm</span>(Rating<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>C), <span class="co"># empty model</span></a>
<a class="sourceLine" id="cb141-5" title="5">    <span class="dt">scope=</span><span class="kw">formula</span>(<span class="kw">lm</span>(Rating<span class="op">~</span>., <span class="dt">data=</span>C)), <span class="co"># full model</span></a>
<a class="sourceLine" id="cb141-6" title="6">    <span class="dt">direction=</span><span class="st">&quot;forward&quot;</span>)</a></code></pre></div>
<pre><code>## Start:  AIC=4034.31
## Rating ~ 1
## 
##             Df Sum of Sq     RSS    AIC
## + Balance    1   7124258 2427627 3488.4
## + Income     1   5982140 3569744 3642.6
## + Age        1    101661 9450224 4032.0
## &lt;none&gt;                   9551885 4034.3
## + Education  1      8675 9543210 4036.0
## 
## Step:  AIC=3488.38
## Rating ~ Balance
## 
##             Df Sum of Sq     RSS    AIC
## + Income     1   1859749  567878 2909.3
## + Age        1     98562 2329065 3473.8
## &lt;none&gt;                   2427627 3488.4
## + Education  1      5130 2422497 3489.5
## 
## Step:  AIC=2909.28
## Rating ~ Balance + Income
## 
##             Df Sum of Sq    RSS    AIC
## &lt;none&gt;                   567878 2909.3
## + Age        1    2142.4 565735 2909.8
## + Education  1    1208.6 566669 2910.4</code></pre>
<pre><code>## 
## Call:
## lm(formula = Rating ~ Balance + Income, data = C)
## 
## Coefficients:
## (Intercept)      Balance       Income  
##    145.3506       0.2129       2.1863</code></pre>
<p>This procedure suggests including only the <code>Balance</code> and <code>Income</code> variables.</p>
<p>Backward elimination example – full dataset:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb144-1" title="1"><span class="kw">step</span>(<span class="kw">lm</span>(Rating<span class="op">~</span>., <span class="dt">data=</span>C), <span class="co"># full model</span></a>
<a class="sourceLine" id="cb144-2" title="2">     <span class="dt">scope=</span><span class="kw">formula</span>(<span class="kw">lm</span>(Rating<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>C)), <span class="co"># empty model</span></a>
<a class="sourceLine" id="cb144-3" title="3">     <span class="dt">direction=</span><span class="st">&quot;backward&quot;</span>)</a></code></pre></div>
<pre><code>## Start:  AIC=2910.89
## Rating ~ Income + Age + Education + Balance
## 
##             Df Sum of Sq     RSS    AIC
## - Education  1      1238  565735 2909.8
## - Age        1      2172  566669 2910.4
## &lt;none&gt;                    564497 2910.9
## - Income     1   1759273 2323770 3474.9
## - Balance    1   2992164 3556661 3645.1
## 
## Step:  AIC=2909.77
## Rating ~ Income + Age + Balance
## 
##           Df Sum of Sq     RSS    AIC
## - Age      1      2142  567878 2909.3
## &lt;none&gt;                  565735 2909.8
## - Income   1   1763329 2329065 3473.8
## - Balance  1   2991523 3557259 3643.2
## 
## Step:  AIC=2909.28
## Rating ~ Income + Balance
## 
##           Df Sum of Sq     RSS    AIC
## &lt;none&gt;                  567878 2909.3
## - Income   1   1859749 2427627 3488.4
## - Balance  1   3001866 3569744 3642.6</code></pre>
<pre><code>## 
## Call:
## lm(formula = Rating ~ Income + Balance, data = C)
## 
## Coefficients:
## (Intercept)       Income      Balance  
##    145.3506       2.1863       0.2129</code></pre>
<p>This procedure gives the same results as forward selection
(however, for other data sets that is not necessarily the case).</p>
</div>
<div id="variable-transformation" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Variable Transformation</h3>
<hr />
<p>So far we have been fitting linear models of the form:
<span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p.
\]</span></p>
<p>What about some non-linear models such as polynomials etc.? For example:
<span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_1^3 + \beta_4 X_2.
\]</span></p>
<p>Solution: pre-process inputs by setting
<span class="math inline">\(X_1&#39; := X_1\)</span>, <span class="math inline">\(X_2&#39; := X_1^2\)</span>, <span class="math inline">\(X_3&#39; := X_1^3\)</span>, <span class="math inline">\(X_4&#39; := X_2\)</span>
and fit a linear model:</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1 X_1&#39; + \beta_2 X_2&#39; + \beta_3 X_3&#39; + \beta_4 X_4&#39;.
\]</span></p>
<p>This trick works for every model of the form
<span class="math inline">\(Y=\sum_{i=1}^k \sum_{j=1}^p \varphi_{i,j}(X_j)\)</span> for any <span class="math inline">\(k\)</span>
and any univariate functions <span class="math inline">\(\varphi_{i,j}\)</span>.</p>
<hr />
<p>Also, with a little creativity (and maths), we might be able to transform
a few other models to a linear one, e.g.,</p>
<p><span class="math display">\[
Y = b e^{aX} \qquad \to \qquad \log Y = \log b + aX \qquad\to\qquad Y&#39;=aX+b&#39;
\]</span></p>
<p>This is an example of a model’s <strong>linearisation</strong>.
However, not every model can be linearised.
In particular, one that involves functions that are not invertible.</p>
<hr />
<p>For example, here’s a series of simple (<span class="math inline">\(p=1\)</span>) degree-<span class="math inline">\(d\)</span>
polynomial regression models
of the form:
<span class="math display">\[
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \dots + \beta_d X^d.
\]</span></p>
<p>Such models can be fit with the <code>lm()</code> function based on the formula
of the form <code>Y~poly(X, d)</code> or <code>Y~X+I(X^2)+I(X^3)+...</code></p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb147-1" title="1">f1_<span class="dv">1</span>  &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1)</a>
<a class="sourceLine" id="cb147-2" title="2">f1_<span class="dv">3</span>  &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1<span class="op">+</span><span class="kw">I</span>(X1<span class="op">^</span><span class="dv">2</span>)<span class="op">+</span><span class="kw">I</span>(X1<span class="op">^</span><span class="dv">3</span>)) <span class="co"># also: Y~poly(X1, 3)</span></a>
<a class="sourceLine" id="cb147-3" title="3">f1_<span class="dv">14</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span><span class="kw">poly</span>(X1, <span class="dv">14</span>))</a></code></pre></div>
<p>Above we have fit polynomials of degrees 1, 3 and 14.
Note that a polynomial of degree 1 is just a line.</p>
<p>Let us depict the three models:</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb148-1" title="1"><span class="kw">plot</span>(X1, Y, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;#000000aa&quot;</span>)</a>
<a class="sourceLine" id="cb148-2" title="2">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(X1), <span class="kw">max</span>(X1), <span class="dt">length.out=</span><span class="dv">101</span>)</a>
<a class="sourceLine" id="cb148-3" title="3"><span class="kw">lines</span>(x, <span class="kw">predict</span>(f1_<span class="dv">1</span>, <span class="kw">data.frame</span>(<span class="dt">X1=</span>x)), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb148-4" title="4"><span class="kw">lines</span>(x, <span class="kw">predict</span>(f1_<span class="dv">3</span>, <span class="kw">data.frame</span>(<span class="dt">X1=</span>x)), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb148-5" title="5"><span class="kw">lines</span>(x, <span class="kw">predict</span>(f1_<span class="dv">14</span>, <span class="kw">data.frame</span>(<span class="dt">X1=</span>x)), <span class="dt">col=</span><span class="st">&quot;darkgreen&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a></code></pre></div>
<p><img src="02-regression-multiple-figures/unnamed-chunk-32-1.png" alt="plot of chunk unnamed-chunk-32" style="width:50.0%" /></p>
<p>There’s clearly a problem with the degree-14 polynomial.</p>
</div>
<div id="predictive-vs.-descriptive-power" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Predictive vs. Descriptive Power</h3>
<hr />
<p>The above high-degree polynomial model (<code>f1_14</code>) is a typical instance of a phenomenon called
an <strong>overfit</strong>.</p>
<p>Clearly (based on our expert knowledge), the <code>Rating</code> shouldn’t decrease as <code>Balance</code> increases.</p>
<p>In other words, <code>f1_14</code> gives a better fit to data actually observed,
but fails to produce good results for the points that are yet to come.</p>
<p>We say that it <strong>generalises</strong> poorly to unseen data.</p>
<hr />
<p>Assume our true model is of the form:</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb149-1" title="1">true_model &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="dv">3</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span><span class="op">+</span><span class="dv">5</span></a></code></pre></div>
<p>And we generate the following random sample from this model (with <span class="math inline">\(Y\)</span> subject
to error):</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb150-1" title="1">n &lt;-<span class="st"> </span><span class="dv">25</span></a>
<a class="sourceLine" id="cb150-2" title="2">X &lt;-<span class="st"> </span><span class="kw">runif</span>(n, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb150-3" title="3">Y &lt;-<span class="st"> </span><span class="kw">true_model</span>(X)<span class="op">+</span><span class="kw">rnorm</span>(n, <span class="dt">sd=</span><span class="fl">0.1</span>)</a></code></pre></div>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb151-1" title="1"><span class="kw">plot</span>(X, Y, <span class="dt">las=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb151-2" title="2">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">101</span>)</a>
<a class="sourceLine" id="cb151-3" title="3"><span class="kw">lines</span>(x, <span class="kw">true_model</span>(x), <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<hr />
<p><img src="02-regression-multiple-figures/unnamed-chunk-35-1.png" alt="plot of chunk unnamed-chunk-35" style="width:50.0%" /></p>
<hr />
<p>Let’s fit polynomials of different degrees:</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb152-1" title="1"><span class="kw">plot</span>(X, Y, <span class="dt">las=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb152-2" title="2"><span class="kw">lines</span>(x, <span class="kw">true_model</span>(x), <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb152-3" title="3"></a>
<a class="sourceLine" id="cb152-4" title="4">dmax &lt;-<span class="st"> </span><span class="dv">15</span> <span class="co"># maximal polynomial degree</span></a>
<a class="sourceLine" id="cb152-5" title="5">MSE_train &lt;-<span class="st"> </span><span class="kw">numeric</span>(dmax)</a>
<a class="sourceLine" id="cb152-6" title="6">MSE_test  &lt;-<span class="st"> </span><span class="kw">numeric</span>(dmax)</a>
<a class="sourceLine" id="cb152-7" title="7"><span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>dmax) { <span class="co"># for every polynomial degree</span></a>
<a class="sourceLine" id="cb152-8" title="8">    f &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span><span class="kw">poly</span>(X, d)) <span class="co"># fit a d-degree polynomial</span></a>
<a class="sourceLine" id="cb152-9" title="9">    y &lt;-<span class="st"> </span><span class="kw">predict</span>(f, <span class="kw">data.frame</span>(<span class="dt">X=</span>x))</a>
<a class="sourceLine" id="cb152-10" title="10">    <span class="kw">lines</span>(x, y, <span class="dt">col=</span>d)</a>
<a class="sourceLine" id="cb152-11" title="11">    <span class="co"># MSE on given random X,Y:</span></a>
<a class="sourceLine" id="cb152-12" title="12">    MSE_train[d] &lt;-<span class="st"> </span><span class="kw">mean</span>(f<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb152-13" title="13">    <span class="co"># MSE on many more points:</span></a>
<a class="sourceLine" id="cb152-14" title="14">    MSE_test[d]  &lt;-<span class="st"> </span><span class="kw">mean</span>((y<span class="op">-</span><span class="kw">true_model</span>(x))<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb152-15" title="15">}</a></code></pre></div>
<hr />
<p><img src="02-regression-multiple-figures/unnamed-chunk-36-1.png" alt="plot of chunk unnamed-chunk-36" style="width:50.0%" /></p>
<hr />
<p>Compare the mean squared error (MSE) for the observed vs. future data points:</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb153-1" title="1"><span class="kw">matplot</span>(<span class="dv">1</span><span class="op">:</span>dmax, <span class="kw">cbind</span>(MSE_train, MSE_test), <span class="dt">type=</span><span class="st">&#39;b&#39;</span>,</a>
<a class="sourceLine" id="cb153-2" title="2">    <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="fl">1e-3</span>, <span class="fl">1e3</span>), <span class="dt">log=</span><span class="st">&quot;y&quot;</span>, <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb153-3" title="3">    <span class="dt">xlab=</span><span class="st">&#39;Model complexity (degree of polynomial)&#39;</span>,</a>
<a class="sourceLine" id="cb153-4" title="4">    <span class="dt">ylab=</span><span class="st">&quot;MSE&quot;</span>)</a>
<a class="sourceLine" id="cb153-5" title="5"><span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;MSE train&quot;</span>, <span class="st">&quot;MSE test&quot;</span>),</a>
<a class="sourceLine" id="cb153-6" title="6">    <span class="dt">lty=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)</a></code></pre></div>
<p>Note the logarithmic scale on the <span class="math inline">\(y\)</span> axis.</p>
<hr />
<p><img src="02-regression-multiple-figures/unnamed-chunk-37-1.png" alt="plot of chunk unnamed-chunk-37" style="width:50.0%" /></p>
<hr />
<p>This is a very typical behaviour!</p>
<ul>
<li><p>A model’s fit to observed data improves as the model’s complexity increases.</p></li>
<li><p>A model’s generalisation to unseen data initially improves, but then becomes worse.</p></li>
<li><p>In the above example, the sweet spot is at a polynomial of degree 3, which is exactly
our true underlying model.</p></li>
</ul>
<hr />
<p>Hence, most often <strong>we should be interested in the accuracy of the predictions
made in the case of unobserved data.</strong></p>
<p>If we have a data set of a considerable size,
we can divide it (randomly) into two parts:</p>
<ul>
<li><em>training sample</em> (say, 60% or 80%) – used to fit a model</li>
<li><em>test sample</em> (the remaining 40% or 20%) – used to assess its quality
(e.g., using MSE)</li>
</ul>
<p>More on this issue in the chapter on Classification.</p>
<blockquote>
<p>(*) We shall see that sometimes a train-test-validate split will be necessary,
e.g., 60-20-20%.</p>
</blockquote>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multiple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="outro-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
