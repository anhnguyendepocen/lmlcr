<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.3 Finding the Best Model | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="3.3 Finding the Best Model | Lightweight Machine Learning Classics with R" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="3.3 Finding the Best Model | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.3 Finding the Best Model | Lightweight Machine Learning Classics with R" />
  
  
  

<meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multiple-linear-regression.html"/>
<link rel="next" href="outro-1.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> {</a></li>
<li class="chapter" data-level="2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>2.1</b> Machine Learning</a><ul>
<li class="chapter" data-level="2.1.1" data-path="machine-learning.html"><a href="machine-learning.html#what-is-machine-learning"><i class="fa fa-check"></i><b>2.1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="2.1.2" data-path="machine-learning.html"><a href="machine-learning.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>2.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>2.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="2.2.1" data-path="supervised-learning.html"><a href="supervised-learning.html#formalism"><i class="fa fa-check"></i><b>2.2.1</b> Formalism</a></li>
<li class="chapter" data-level="2.2.2" data-path="supervised-learning.html"><a href="supervised-learning.html#desired-outputs"><i class="fa fa-check"></i><b>2.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="2.2.3" data-path="supervised-learning.html"><a href="supervised-learning.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>2.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="simple-regression.html"><a href="simple-regression.html"><i class="fa fa-check"></i><b>2.3</b> Simple Regression</a><ul>
<li class="chapter" data-level="2.3.1" data-path="simple-regression.html"><a href="simple-regression.html#introduction"><i class="fa fa-check"></i><b>2.3.1</b> Introduction</a></li>
<li class="chapter" data-level="2.3.2" data-path="simple-regression.html"><a href="simple-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>2.3.2</b> Search Space and Objective</a></li>
<li class="chapter" data-level="2.3.3" data-path="simple-regression.html"><a href="simple-regression.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>2.3.3</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="2.3.4" data-path="simple-regression.html"><a href="simple-regression.html#solution-in-r"><i class="fa fa-check"></i><b>2.3.4</b> Solution in R</a></li>
<li class="chapter" data-level="2.3.5" data-path="simple-regression.html"><a href="simple-regression.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>2.3.5</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="vector-and-matrix-algebra-basics.html"><a href="vector-and-matrix-algebra-basics.html"><i class="fa fa-check"></i><b>2.4</b> Vector and Matrix Algebra Basics</a><ul>
<li class="chapter" data-level="2.4.1" data-path="vector-and-matrix-algebra-basics.html"><a href="vector-and-matrix-algebra-basics.html#motivation"><i class="fa fa-check"></i><b>2.4.1</b> Motivation</a></li>
<li class="chapter" data-level="2.4.2" data-path="vector-and-matrix-algebra-basics.html"><a href="vector-and-matrix-algebra-basics.html#vector-scalar-operations"><i class="fa fa-check"></i><b>2.4.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="2.4.3" data-path="vector-and-matrix-algebra-basics.html"><a href="vector-and-matrix-algebra-basics.html#vector-vector-operations"><i class="fa fa-check"></i><b>2.4.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="2.4.4" data-path="vector-and-matrix-algebra-basics.html"><a href="vector-and-matrix-algebra-basics.html#other-vector-operations"><i class="fa fa-check"></i><b>2.4.4</b> Other Vector Operations</a></li>
<li class="chapter" data-level="2.4.5" data-path="vector-and-matrix-algebra-basics.html"><a href="vector-and-matrix-algebra-basics.html#matrices"><i class="fa fa-check"></i><b>2.4.5</b> Matrices</a></li>
<li class="chapter" data-level="2.4.6" data-path="vector-and-matrix-algebra-basics.html"><a href="vector-and-matrix-algebra-basics.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>2.4.6</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="2.4.7" data-path="vector-and-matrix-algebra-basics.html"><a href="vector-and-matrix-algebra-basics.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>2.4.7</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="2.4.8" data-path="vector-and-matrix-algebra-basics.html"><a href="vector-and-matrix-algebra-basics.html#matrix-multiplication"><i class="fa fa-check"></i><b>2.4.8</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="2.4.9" data-path="vector-and-matrix-algebra-basics.html"><a href="vector-and-matrix-algebra-basics.html#matrix-vector-operations"><i class="fa fa-check"></i><b>2.4.9</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="outro.html"><a href="outro.html"><i class="fa fa-check"></i><b>2.5</b> Outro</a><ul>
<li class="chapter" data-level="2.5.1" data-path="outro.html"><a href="outro.html#remarks"><i class="fa fa-check"></i><b>2.5.1</b> Remarks</a></li>
<li class="chapter" data-level="2.5.2" data-path="outro.html"><a href="outro.html#further-reading"><i class="fa fa-check"></i><b>2.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>3</b> Multiple Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="introduction-1.html"><a href="introduction-1.html#formalism-1"><i class="fa fa-check"></i><b>3.1.1</b> Formalism</a></li>
<li class="chapter" data-level="3.1.2" data-path="introduction-1.html"><a href="introduction-1.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>3.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>3.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#problem-formulation"><i class="fa fa-check"></i><b>3.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="3.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html"><i class="fa fa-check"></i><b>3.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="3.3.1" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#model-diagnostics"><i class="fa fa-check"></i><b>3.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="3.3.2" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#variable-selection"><i class="fa fa-check"></i><b>3.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="3.3.3" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#variable-transformation"><i class="fa fa-check"></i><b>3.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="3.3.4" data-path="finding-the-best-model.html"><a href="finding-the-best-model.html#predictive-vs.descriptive-power"><i class="fa fa-check"></i><b>3.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="outro-1.html"><a href="outro-1.html"><i class="fa fa-check"></i><b>3.4</b> Outro</a><ul>
<li class="chapter" data-level="3.4.1" data-path="outro-1.html"><a href="outro-1.html#remarks-1"><i class="fa fa-check"></i><b>3.4.1</b> Remarks</a></li>
<li class="chapter" data-level="3.4.2" data-path="outro-1.html"><a href="outro-1.html#other-methods-for-regression"><i class="fa fa-check"></i><b>3.4.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="3.4.3" data-path="outro-1.html"><a href="outro-1.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>3.4.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="3.4.4" data-path="outro-1.html"><a href="outro-1.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>3.4.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="3.4.5" data-path="outro-1.html"><a href="outro-1.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>3.4.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="3.4.6" data-path="outro-1.html"><a href="outro-1.html#further-reading-1"><i class="fa fa-check"></i><b>3.4.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>4</b> Classification with K-Nearest Neighbours</a><ul>
<li class="chapter" data-level="4.1" data-path="introduction-2.html"><a href="introduction-2.html"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="introduction-2.html"><a href="introduction-2.html#classification-task"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="introduction-2.html"><a href="introduction-2.html#factor-data-type"><i class="fa fa-check"></i><b>4.1.2</b> Factor Data Type</a></li>
<li class="chapter" data-level="4.1.3" data-path="introduction-2.html"><a href="introduction-2.html#data"><i class="fa fa-check"></i><b>4.1.3</b> Data</a></li>
<li class="chapter" data-level="4.1.4" data-path="introduction-2.html"><a href="introduction-2.html#training-and-test-sets"><i class="fa fa-check"></i><b>4.1.4</b> Training and Test Sets</a></li>
<li class="chapter" data-level="4.1.5" data-path="introduction-2.html"><a href="introduction-2.html#discussed-methods"><i class="fa fa-check"></i><b>4.1.5</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html"><i class="fa fa-check"></i><b>4.2</b> K-nearest Neighbour Classifier</a><ul>
<li class="chapter" data-level="4.2.1" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#introduction-3"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#example-in-r"><i class="fa fa-check"></i><b>4.2.2</b> Example in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#different-metrics"><i class="fa fa-check"></i><b>4.2.3</b> Different Metrics (*)</a></li>
<li class="chapter" data-level="4.2.4" data-path="k-nearest-neighbour-classifier.html"><a href="k-nearest-neighbour-classifier.html#standardisation-of-independent-variables"><i class="fa fa-check"></i><b>4.2.4</b> Standardisation of Independent Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html"><i class="fa fa-check"></i><b>4.3</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="4.3.1" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#main-routine"><i class="fa fa-check"></i><b>4.3.1</b> Main Routine (*)</a></li>
<li class="chapter" data-level="4.3.2" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#mode"><i class="fa fa-check"></i><b>4.3.2</b> Mode</a></li>
<li class="chapter" data-level="4.3.3" data-path="implementing-a-k-nn-classifier.html"><a href="implementing-a-k-nn-classifier.html#nn-search-routines"><i class="fa fa-check"></i><b>4.3.3</b> NN Search Routines (*)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="outro-2.html"><a href="outro-2.html"><i class="fa fa-check"></i><b>4.4</b> Outro</a><ul>
<li class="chapter" data-level="4.4.1" data-path="outro-2.html"><a href="outro-2.html#remarks-2"><i class="fa fa-check"></i><b>4.4.1</b> Remarks</a></li>
<li class="chapter" data-level="4.4.2" data-path="outro-2.html"><a href="outro-2.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>4.4.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="4.4.3" data-path="outro-2.html"><a href="outro-2.html#further-reading-2"><i class="fa fa-check"></i><b>4.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html"><i class="fa fa-check"></i><b>5</b> Classification with Trees and Linear Models</a><ul>
<li class="chapter" data-level="5.1" data-path="introduction-4.html"><a href="introduction-4.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="introduction-4.html"><a href="introduction-4.html#classification-task-1"><i class="fa fa-check"></i><b>5.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="5.1.2" data-path="introduction-4.html"><a href="introduction-4.html#data-1"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
<li class="chapter" data-level="5.1.3" data-path="introduction-4.html"><a href="introduction-4.html#discussed-methods-1"><i class="fa fa-check"></i><b>5.1.3</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html"><i class="fa fa-check"></i><b>5.2</b> Model Assessment and Selection</a><ul>
<li class="chapter" data-level="5.2.1" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html#performance-metrics"><i class="fa fa-check"></i><b>5.2.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="5.2.2" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>5.2.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="5.2.3" data-path="model-assessment-and-selection.html"><a href="model-assessment-and-selection.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>5.2.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>5.3</b> Decision Trees</a><ul>
<li class="chapter" data-level="5.3.1" data-path="decision-trees.html"><a href="decision-trees.html#introduction-5"><i class="fa fa-check"></i><b>5.3.1</b> Introduction</a></li>
<li class="chapter" data-level="5.3.2" data-path="decision-trees.html"><a href="decision-trees.html#example-in-r-1"><i class="fa fa-check"></i><b>5.3.2</b> Example in R</a></li>
<li class="chapter" data-level="5.3.3" data-path="decision-trees.html"><a href="decision-trees.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>5.3.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html"><i class="fa fa-check"></i><b>5.4</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="5.4.1" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#motivation-1"><i class="fa fa-check"></i><b>5.4.1</b> Motivation</a></li>
<li class="chapter" data-level="5.4.2" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>5.4.2</b> Logistic Model</a></li>
<li class="chapter" data-level="5.4.3" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#example-in-r-2"><i class="fa fa-check"></i><b>5.4.3</b> Example in R</a></li>
<li class="chapter" data-level="5.4.4" data-path="binary-logistic-regression.html"><a href="binary-logistic-regression.html#loss-function"><i class="fa fa-check"></i><b>5.4.4</b> Loss Function</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="outro-3.html"><a href="outro-3.html"><i class="fa fa-check"></i><b>5.5</b> Outro</a><ul>
<li class="chapter" data-level="5.5.1" data-path="outro-3.html"><a href="outro-3.html#remarks-3"><i class="fa fa-check"></i><b>5.5.1</b> Remarks</a></li>
<li class="chapter" data-level="5.5.2" data-path="outro-3.html"><a href="outro-3.html#further-reading-3"><i class="fa fa-check"></i><b>5.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>6</b> Neural Networks</a><ul>
<li class="chapter" data-level="6.1" data-path="introduction-6.html"><a href="introduction-6.html"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="introduction-6.html"><a href="introduction-6.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>6.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="6.1.2" data-path="introduction-6.html"><a href="introduction-6.html#data-2"><i class="fa fa-check"></i><b>6.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html"><i class="fa fa-check"></i><b>6.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>6.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="6.2.2" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#extending-logistic-regression"><i class="fa fa-check"></i><b>6.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="6.2.3" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#softmax-function"><i class="fa fa-check"></i><b>6.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="6.2.4" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>6.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="6.2.5" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>6.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="6.2.6" data-path="multinomial-logistic-regression.html"><a href="multinomial-logistic-regression.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>6.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html"><i class="fa fa-check"></i><b>6.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="6.3.1" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>6.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="6.3.2" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>6.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="6.3.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#example-in-r-3"><i class="fa fa-check"></i><b>6.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html"><i class="fa fa-check"></i><b>6.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="6.4.1" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#introduction-7"><i class="fa fa-check"></i><b>6.4.1</b> Introduction</a></li>
<li class="chapter" data-level="6.4.2" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>6.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="6.4.3" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>6.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="6.4.4" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>6.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="preprocessing-of-data.html"><a href="preprocessing-of-data.html"><i class="fa fa-check"></i><b>6.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="6.5.1" data-path="preprocessing-of-data.html"><a href="preprocessing-of-data.html#introduction-8"><i class="fa fa-check"></i><b>6.5.1</b> Introduction</a></li>
<li class="chapter" data-level="6.5.2" data-path="preprocessing-of-data.html"><a href="preprocessing-of-data.html#image-deskewing"><i class="fa fa-check"></i><b>6.5.2</b> Image Deskewing</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="outro-4.html"><a href="outro-4.html"><i class="fa fa-check"></i><b>6.6</b> Outro</a><ul>
<li class="chapter" data-level="6.6.1" data-path="outro-4.html"><a href="outro-4.html#remarks-4"><i class="fa fa-check"></i><b>6.6.1</b> Remarks</a></li>
<li class="chapter" data-level="6.6.2" data-path="outro-4.html"><a href="outro-4.html#beyond-mnist"><i class="fa fa-check"></i><b>6.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="6.6.3" data-path="outro-4.html"><a href="outro-4.html#further-reading-4"><i class="fa fa-check"></i><b>6.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>7</b> Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction-9.html"><a href="introduction-9.html"><i class="fa fa-check"></i><b>7.1</b> Introduction</a><ul>
<li class="chapter" data-level="7.1.1" data-path="introduction-9.html"><a href="introduction-9.html#optimisation-problem"><i class="fa fa-check"></i><b>7.1.1</b> Optimisation Problem</a></li>
<li class="chapter" data-level="7.1.2" data-path="introduction-9.html"><a href="introduction-9.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>7.1.2</b> Example Optimisation Problems in Machine Learning</a></li>
<li class="chapter" data-level="7.1.3" data-path="introduction-9.html"><a href="introduction-9.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>7.1.3</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="7.1.4" data-path="introduction-9.html"><a href="introduction-9.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>7.1.4</b> Example Objective over a 2D Domain</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="iterative-methods.html"><a href="iterative-methods.html"><i class="fa fa-check"></i><b>7.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="iterative-methods.html"><a href="iterative-methods.html#introduction-10"><i class="fa fa-check"></i><b>7.2.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2.2" data-path="iterative-methods.html"><a href="iterative-methods.html#example-in-r-4"><i class="fa fa-check"></i><b>7.2.2</b> Example in R</a></li>
<li class="chapter" data-level="7.2.3" data-path="iterative-methods.html"><a href="iterative-methods.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>7.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="7.2.4" data-path="iterative-methods.html"><a href="iterative-methods.html#random-restarts"><i class="fa fa-check"></i><b>7.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="gradient-descent.html"><a href="gradient-descent.html"><i class="fa fa-check"></i><b>7.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="7.3.1" data-path="gradient-descent.html"><a href="gradient-descent.html#function-gradient"><i class="fa fa-check"></i><b>7.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="7.3.2" data-path="gradient-descent.html"><a href="gradient-descent.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>7.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="7.3.3" data-path="gradient-descent.html"><a href="gradient-descent.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>7.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="7.3.4" data-path="gradient-descent.html"><a href="gradient-descent.html#example-mnist"><i class="fa fa-check"></i><b>7.3.4</b> Example: MNIST</a></li>
<li class="chapter" data-level="7.3.5" data-path="gradient-descent.html"><a href="gradient-descent.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>7.3.5</b> Stochastic Gradient Descent (SGD)</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="outro-5.html"><a href="outro-5.html"><i class="fa fa-check"></i><b>7.4</b> Outro</a><ul>
<li class="chapter" data-level="7.4.1" data-path="outro-5.html"><a href="outro-5.html#remarks-5"><i class="fa fa-check"></i><b>7.4.1</b> Remarks</a></li>
<li class="chapter" data-level="7.4.2" data-path="outro-5.html"><a href="outro-5.html#optimisers-in-keras"><i class="fa fa-check"></i><b>7.4.2</b> Optimisers in Keras</a></li>
<li class="chapter" data-level="7.4.3" data-path="outro-5.html"><a href="outro-5.html#note-on-search-spaces"><i class="fa fa-check"></i><b>7.4.3</b> Note on Search Spaces</a></li>
<li class="chapter" data-level="7.4.4" data-path="outro-5.html"><a href="outro-5.html#further-reading-5"><i class="fa fa-check"></i><b>7.4.4</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>8</b> Clustering</a><ul>
<li class="chapter" data-level="8.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>8.1</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="8.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#introduction-11"><i class="fa fa-check"></i><b>8.1.1</b> Introduction</a></li>
<li class="chapter" data-level="8.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>8.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="8.1.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#clustering-1"><i class="fa fa-check"></i><b>8.1.3</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>8.2</b> K-means Clustering</a><ul>
<li class="chapter" data-level="8.2.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>8.2.1</b> Example in R</a></li>
<li class="chapter" data-level="8.2.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#problem-statement"><i class="fa fa-check"></i><b>8.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="8.2.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>8.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html"><i class="fa fa-check"></i><b>8.3</b> Hierarchical Methods</a><ul>
<li class="chapter" data-level="8.3.1" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#introduction-12"><i class="fa fa-check"></i><b>8.3.1</b> Introduction</a></li>
<li class="chapter" data-level="8.3.2" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#example-in-r-6"><i class="fa fa-check"></i><b>8.3.2</b> Example in R</a></li>
<li class="chapter" data-level="8.3.3" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>8.3.3</b> Agglomerative Hierarchical Clustering</a></li>
<li class="chapter" data-level="8.3.4" data-path="hierarchical-methods.html"><a href="hierarchical-methods.html#linkage-functions"><i class="fa fa-check"></i><b>8.3.4</b> Linkage Functions</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="outro-6.html"><a href="outro-6.html"><i class="fa fa-check"></i><b>8.4</b> Outro</a><ul>
<li class="chapter" data-level="8.4.1" data-path="outro-6.html"><a href="outro-6.html#remarks-6"><i class="fa fa-check"></i><b>8.4.1</b> Remarks</a></li>
<li class="chapter" data-level="8.4.2" data-path="outro-6.html"><a href="outro-6.html#other-noteworthy-clustering-algorithms"><i class="fa fa-check"></i><b>8.4.2</b> Other Noteworthy Clustering Algorithms</a></li>
<li class="chapter" data-level="8.4.3" data-path="outro-6.html"><a href="outro-6.html#further-reading-6"><i class="fa fa-check"></i><b>8.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html"><i class="fa fa-check"></i><b>9</b> Optimisation with Genetic Algorithms</a><ul>
<li class="chapter" data-level="9.1" data-path="introduction-13.html"><a href="introduction-13.html"><i class="fa fa-check"></i><b>9.1</b> Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="introduction-13.html"><a href="introduction-13.html#recap"><i class="fa fa-check"></i><b>9.1.1</b> Recap</a></li>
<li class="chapter" data-level="9.1.2" data-path="introduction-13.html"><a href="introduction-13.html#k-means-revisited"><i class="fa fa-check"></i><b>9.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="9.1.3" data-path="introduction-13.html"><a href="introduction-13.html#optim-vs.kmeans"><i class="fa fa-check"></i><b>9.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html"><i class="fa fa-check"></i><b>9.2</b> A Note on Convex Optimisation (*)</a><ul>
<li class="chapter" data-level="9.2.1" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#introduction-14"><i class="fa fa-check"></i><b>9.2.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2.2" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#convex-combinations"><i class="fa fa-check"></i><b>9.2.2</b> Convex Combinations (*)</a></li>
<li class="chapter" data-level="9.2.3" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#convex-functions"><i class="fa fa-check"></i><b>9.2.3</b> Convex Functions (*)</a></li>
<li class="chapter" data-level="9.2.4" data-path="a-note-on-convex-optimisation.html"><a href="a-note-on-convex-optimisation.html#examples"><i class="fa fa-check"></i><b>9.2.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html"><i class="fa fa-check"></i><b>9.3</b> Genetic Algorithms</a><ul>
<li class="chapter" data-level="9.3.1" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html#introduction-15"><i class="fa fa-check"></i><b>9.3.1</b> Introduction</a></li>
<li class="chapter" data-level="9.3.2" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html#overview-of-the-method"><i class="fa fa-check"></i><b>9.3.2</b> Overview of the Method</a></li>
<li class="chapter" data-level="9.3.3" data-path="genetic-algorithms.html"><a href="genetic-algorithms.html#example-implementation---ga-for-k-means"><i class="fa fa-check"></i><b>9.3.3</b> Example Implementation - GA for K-means</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="outro-7.html"><a href="outro-7.html"><i class="fa fa-check"></i><b>9.4</b> Outro</a><ul>
<li class="chapter" data-level="9.4.1" data-path="outro-7.html"><a href="outro-7.html#remarks-7"><i class="fa fa-check"></i><b>9.4.1</b> Remarks</a></li>
<li class="chapter" data-level="9.4.2" data-path="outro-7.html"><a href="outro-7.html#further-reading-7"><i class="fa fa-check"></i><b>9.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>10</b> Recommender Systems</a><ul>
<li class="chapter" data-level="10.1" data-path="introduction-16.html"><a href="introduction-16.html"><i class="fa fa-check"></i><b>10.1</b> Introduction</a><ul>
<li class="chapter" data-level="10.1.1" data-path="introduction-16.html"><a href="introduction-16.html#what-is-a-recommender-system"><i class="fa fa-check"></i><b>10.1.1</b> What is a Recommender System?</a></li>
<li class="chapter" data-level="10.1.2" data-path="introduction-16.html"><a href="introduction-16.html#the-netflix-prize"><i class="fa fa-check"></i><b>10.1.2</b> The Netflix Prize</a></li>
<li class="chapter" data-level="10.1.3" data-path="introduction-16.html"><a href="introduction-16.html#main-approaches"><i class="fa fa-check"></i><b>10.1.3</b> Main Approaches</a></li>
<li class="chapter" data-level="10.1.4" data-path="introduction-16.html"><a href="introduction-16.html#formalism-2"><i class="fa fa-check"></i><b>10.1.4</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html"><i class="fa fa-check"></i><b>10.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="10.2.1" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#example"><i class="fa fa-check"></i><b>10.2.1</b> Example</a></li>
<li class="chapter" data-level="10.2.2" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#similarity-measures"><i class="fa fa-check"></i><b>10.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="10.2.3" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>10.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="10.2.4" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>10.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="movielens-dataset.html"><a href="movielens-dataset.html"><i class="fa fa-check"></i><b>10.3</b> MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="10.3.1" data-path="movielens-dataset.html"><a href="movielens-dataset.html#dataset"><i class="fa fa-check"></i><b>10.3.1</b> Dataset</a></li>
<li class="chapter" data-level="10.3.2" data-path="movielens-dataset.html"><a href="movielens-dataset.html#data-cleansing"><i class="fa fa-check"></i><b>10.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="10.3.3" data-path="movielens-dataset.html"><a href="movielens-dataset.html#item-item-similarities"><i class="fa fa-check"></i><b>10.3.3</b> Item-item Similarities</a></li>
<li class="chapter" data-level="10.3.4" data-path="movielens-dataset.html"><a href="movielens-dataset.html#example-recommendations"><i class="fa fa-check"></i><b>10.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="10.3.5" data-path="movielens-dataset.html"><a href="movielens-dataset.html#clustering-2"><i class="fa fa-check"></i><b>10.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="outro-8.html"><a href="outro-8.html"><i class="fa fa-check"></i><b>10.4</b> Outro</a><ul>
<li class="chapter" data-level="10.4.1" data-path="outro-8.html"><a href="outro-8.html#remarks-8"><i class="fa fa-check"></i><b>10.4.1</b> Remarks</a></li>
<li class="chapter" data-level="10.4.2" data-path="outro-8.html"><a href="outro-8.html#issues"><i class="fa fa-check"></i><b>10.4.2</b> Issues</a></li>
<li class="chapter" data-level="10.4.3" data-path="outro-8.html"><a href="outro-8.html#further-reading-8"><i class="fa fa-check"></i><b>10.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="section-15.html"><a href="section-15.html"><i class="fa fa-check"></i><b>11</b> }</a></li>
<li class="chapter" data-level="12" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.1 2020-02-23 11:47 (94b2505)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="finding-the-best-model" class="section level2">
<h2><span class="header-section-number">3.3</span> Finding the Best Model</h2>
<div id="model-diagnostics" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Model Diagnostics</h3>
<hr />
<p>Consider the three following models.</p>
<table>
<thead>
<tr class="header">
<th>Formula</th>
<th>Equation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rating ~ Balance + Income</td>
<td><span class="math inline">\(Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span></td>
</tr>
<tr class="even">
<td>Rating ~ Balance</td>
<td><span class="math inline">\(Y=a X_1 + b\)</span> (<span class="math inline">\(\beta_0=b, \beta_1=a, \beta_2=0\)</span>)</td>
</tr>
<tr class="odd">
<td>Rating ~ Income</td>
<td><span class="math inline">\(Y=a X_2 + b\)</span> (<span class="math inline">\(\beta_0=b, \beta_1=0, \beta_2=a\)</span>)</td>
</tr>
</tbody>
</table>
<hr />
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb181-1" data-line-number="1">f12 &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1<span class="op">+</span>X2) <span class="co"># Rating ~ Balance + Income</span></a>
<a class="sourceLine" id="cb181-2" data-line-number="2">f12<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>## (Intercept)          X1          X2 
## 172.5586670   0.1828011   2.1976461</code></pre>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" data-line-number="1">f1  &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1)    <span class="co"># Rating ~ Balance</span></a>
<a class="sourceLine" id="cb183-2" data-line-number="2">f1<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>## (Intercept)          X1 
## 226.4711446   0.2661459</code></pre>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb185-1" data-line-number="1">f2  &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X2)    <span class="co"># Rating ~ Income</span></a>
<a class="sourceLine" id="cb185-2" data-line-number="2">f2<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>## (Intercept)          X2 
##  253.851416    3.025286</code></pre>
<p>Which of the three models is the best?</p>
<hr />
<p>“Best” — with respect to what kind of measure?</p>
<p>So far we were fitting w.r.t. SSR,
as the multiple regression model generalise the two simple ones,
the former must yield a not-worse SSR.</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" data-line-number="1"><span class="kw">sum</span>(f12<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 358260.6</code></pre>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb189-1" data-line-number="1"><span class="kw">sum</span>(f1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 2132108</code></pre>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb191-1" data-line-number="1"><span class="kw">sum</span>(f2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 1823473</code></pre>
<p>We get that <span class="math inline">\(f_{12} \succeq f_{2} \succeq f_{1}\)</span> but these error values
are meaningless.</p>
<blockquote>
<p>Interpretability in ML has always been an important issue, think the EU GDPR, amongst others.</p>
</blockquote>
<hr />
<p>The quality of fit can be assessed by performing some descriptive
statistical analysis of the residuals, <span class="math inline">\(\hat{y}_i-y_i\)</span>.</p>
<p>Interestingly, the mean of residuals (this can be shown analytically)
in the least squared fit is always equal to <span class="math inline">\(0\)</span>:
<span class="math display">\[
 \frac{1}{n} \sum_{i=1}^n (\hat{y}_i-y_i)=0.
\]</span></p>
<blockquote>
<p>(*) A proof of this fact is left as an exercise to the curious;
assume <span class="math inline">\(p=1\)</span> just as in the previous chapter and note that <span class="math inline">\(\hat{y}_i=a x_i+b\)</span>.</p>
</blockquote>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb193-1" data-line-number="1"><span class="kw">mean</span>(f12<span class="op">$</span>residuals) <span class="co"># almost zero numerically</span></a></code></pre></div>
<pre><code>## [1] -2.086704e-16</code></pre>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb195-1" data-line-number="1"><span class="kw">all.equal</span>(<span class="kw">mean</span>(f12<span class="op">$</span>residuals), <span class="dv">0</span>)</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<hr />
<p>Sum of squared residuals (SSR) is not interpretable, but the mean squared residuals
(MSR) – also called mean squared error (MSE) regression loss – is a little better.</p>
<p>Recall that mean == sum / number of samples.</p>
<p><span class="math display">\[
 \mathrm{MSE}(f) = \frac{1}{n} \sum_{i=1}^n (f(\mathbf{x}_{i,\cdot})-y_i)^2.
\]</span></p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb197-1" data-line-number="1"><span class="kw">mean</span>(f12<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 1155.679</code></pre>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb199-1" data-line-number="1"><span class="kw">mean</span>(f1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 6877.768</code></pre>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb201-1" data-line-number="1"><span class="kw">mean</span>(f2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 5882.171</code></pre>
<hr />
<p>However, if original <span class="math inline">\(Y\)</span>s are, say, in metres <span class="math inline">\([\mathrm{m}]\)</span>,
MSR is expressed in metres squared <span class="math inline">\([\mathrm{m}^2]\)</span>.</p>
<p>Root mean squared error (RMSE):
<span class="math display">\[
 \mathrm{RMSE}(f) = \sqrt{\frac{1}{n} \sum_{i=1}^n (f(\mathbf{x}_{i,\cdot})-y_i)^2}.
\]</span></p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb203-1" data-line-number="1"><span class="kw">sqrt</span>(<span class="kw">mean</span>(f12<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 33.99528</code></pre>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb205-1" data-line-number="1"><span class="kw">sqrt</span>(<span class="kw">mean</span>(f1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 82.93231</code></pre>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb207-1" data-line-number="1"><span class="kw">sqrt</span>(<span class="kw">mean</span>(f2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 76.69531</code></pre>
<p>(compare variance vs. standard deviation)</p>
<hr />
<p>Still there’s a problem with interpreting this values.</p>
<p>Mean absolute error (MAE):
<span class="math display">\[
 \mathrm{MSE}(f) = \frac{1}{n} \sum_{i=1}^n |f(\mathbf{x}_{i,\cdot})-y_i|.
\]</span></p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb209-1" data-line-number="1"><span class="kw">mean</span>(<span class="kw">abs</span>(f12<span class="op">$</span>residuals))</a></code></pre></div>
<pre><code>## [1] 22.86342</code></pre>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb211-1" data-line-number="1"><span class="kw">mean</span>(<span class="kw">abs</span>(f1<span class="op">$</span>residuals))</a></code></pre></div>
<pre><code>## [1] 61.48892</code></pre>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb213-1" data-line-number="1"><span class="kw">mean</span>(<span class="kw">abs</span>(f2<span class="op">$</span>residuals))</a></code></pre></div>
<pre><code>## [1] 64.1506</code></pre>
<p>“On average, the predicted rating differs from the observed one by”</p>
<hr />
<p>Descriptive statistics for residuals:</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb215-1" data-line-number="1"><span class="kw">summary</span>(f12<span class="op">$</span>residuals)</a></code></pre></div>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -108.100   -1.940    7.812    0.000   20.249   50.623</code></pre>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb217-1" data-line-number="1"><span class="kw">summary</span>(f1<span class="op">$</span>residuals)</a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -226.75  -48.30  -10.08    0.00   42.58  268.74</code></pre>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb219-1" data-line-number="1"><span class="kw">summary</span>(f2<span class="op">$</span>residuals)</a></code></pre></div>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -195.156  -57.341   -1.284    0.000   64.013  175.344</code></pre>
<hr />
<p>The outputs include:</p>
<ul>
<li><code>Min.</code> – sample minimum</li>
<li><code>1st Qu.</code> – 1st quartile == 25th percentile == quantile of order 0.25</li>
<li><code>Median</code> – median == 50th percentile == quantile of order 0.5</li>
<li><code>3rd Qu.</code> – 3rd quartile = 75th percentile == quantile of order 0.75</li>
<li><code>Max.</code> – sample maximum</li>
</ul>
<p>See <code>?quantile</code> in R.</p>
<p>For example 1st quartile is the observation <span class="math inline">\(q\)</span> such that
25% values are <span class="math inline">\(\le q\)</span> and 75% values are <span class="math inline">\(\ge q\)</span>.</p>
<hr />
<p>A picture is worth a thousand words:</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb221-1" data-line-number="1"><span class="kw">boxplot</span>(<span class="dt">las=</span><span class="dv">1</span>, <span class="dt">horizontal=</span><span class="ot">TRUE</span>, <span class="dt">xlab=</span><span class="st">&quot;residuals&quot;</span>,</a>
<a class="sourceLine" id="cb221-2" data-line-number="2">  <span class="kw">list</span>(<span class="dt">f12=</span>f12<span class="op">$</span>residuals, <span class="dt">f1=</span>f1<span class="op">$</span>residuals, <span class="dt">f2=</span>f2<span class="op">$</span>residuals))</a>
<a class="sourceLine" id="cb221-3" data-line-number="3"><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a></code></pre></div>
<p><img src="02-regression-multiple-figures/unnamed-chunk-13-1.png" alt="plot of chunk unnamed-chunk-13" style="width:50.0%" /></p>
<hr />
<p><strong>Box and whisker plot</strong>:</p>
<p><img src="02-regression-multiple-figures/unnamed-chunk-14-1.png" alt="plot of chunk unnamed-chunk-14" style="width:50.0%" /></p>
<ul>
<li>IQR == Interquartile range == Q3<span class="math inline">\(-\)</span>Q1 (box width)</li>
<li>The box contains 50% of the “most typical” observations</li>
<li>Box and whiskers altogether have width <span class="math inline">\(\le\)</span> 4 IQR</li>
<li>Outliers == observations potentially worth inspecting (is it a bug or a feature?)</li>
</ul>
<hr />
<p><em>Violin plot</em> – a blend of a box plot and a (kernel) density estimator (histogram-like):</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb222-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;vioplot&quot;</span>)</a>
<a class="sourceLine" id="cb222-2" data-line-number="2"><span class="kw">vioplot</span>(<span class="dt">las=</span><span class="dv">1</span>, <span class="dt">horizontal=</span><span class="ot">TRUE</span>, <span class="dt">xlab=</span><span class="st">&quot;residuals&quot;</span>,</a>
<a class="sourceLine" id="cb222-3" data-line-number="3">  <span class="kw">list</span>(<span class="dt">f12=</span>f12<span class="op">$</span>residuals, <span class="dt">f1=</span>f1<span class="op">$</span>residuals, <span class="dt">f2=</span>f2<span class="op">$</span>residuals))</a>
<a class="sourceLine" id="cb222-4" data-line-number="4"><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a></code></pre></div>
<p><img src="02-regression-multiple-figures/unnamed-chunk-15-1.png" alt="plot of chunk unnamed-chunk-15" style="width:50.0%" /></p>
<hr />
<p>By the way, this is Rating (<span class="math inline">\(Y\)</span>) as function of Balance (<span class="math inline">\(X_1\)</span>, top subfigure)
and Income (<span class="math inline">\(X_2\)</span>, bottom subfigure).</p>
<p><img src="02-regression-multiple-figures/unnamed-chunk-16-1.png" alt="plot of chunk unnamed-chunk-16" style="width:50.0%" /></p>
<hr />
<p>Descriptive statistics for absolute values of residuals:</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb223-1" data-line-number="1"><span class="kw">summary</span>(<span class="kw">abs</span>(f12<span class="op">$</span>residuals))</a></code></pre></div>
<pre><code>##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
##   0.06457   6.46397  14.07055  22.86342  26.41772 108.09995</code></pre>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb225-1" data-line-number="1"><span class="kw">summary</span>(<span class="kw">abs</span>(f1<span class="op">$</span>residuals))</a></code></pre></div>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
##   0.5056  19.6640  45.0716  61.4889  80.1239 268.7377</code></pre>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb227-1" data-line-number="1"><span class="kw">summary</span>(<span class="kw">abs</span>(f2<span class="op">$</span>residuals))</a></code></pre></div>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
##   0.6545  29.8540  59.6756  64.1506  95.7384 195.1557</code></pre>
<hr />
<p>This picture is worth $1000:</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb229-1" data-line-number="1"><span class="kw">boxplot</span>(<span class="dt">las=</span><span class="dv">1</span>, <span class="dt">horizontal=</span><span class="ot">TRUE</span>, <span class="dt">xlab=</span><span class="st">&quot;abs(residuals)&quot;</span>,</a>
<a class="sourceLine" id="cb229-2" data-line-number="2">  <span class="kw">list</span>(<span class="dt">f12=</span><span class="kw">abs</span>(f12<span class="op">$</span>residuals), <span class="dt">f1=</span><span class="kw">abs</span>(f1<span class="op">$</span>residuals),</a>
<a class="sourceLine" id="cb229-3" data-line-number="3">       <span class="dt">f2=</span><span class="kw">abs</span>(f2<span class="op">$</span>residuals)))</a>
<a class="sourceLine" id="cb229-4" data-line-number="4"><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a></code></pre></div>
<p><img src="02-regression-multiple-figures/unnamed-chunk-18-1.png" alt="plot of chunk unnamed-chunk-18" style="width:50.0%" /></p>
<hr />
<p>The (unadjusted) <strong><span class="math inline">\(R^2\)</span> score</strong> (the coefficient of determination):</p>
<p><span class="math display">\[
R^2(f) = 1 - \frac{\sum_{i=1}^{n} \left(y_i-f(\mathbf{x}_{i,\cdot})\right)^2}{\sum_{i=1}^{n} \left(y_i-\bar{y}\right)^2},
\]</span>
where <span class="math inline">\(\bar{y}\)</span> is the arithmetic mean <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n y_i\)</span>.</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb230-1" data-line-number="1">(r12 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(f12<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>((Y<span class="op">-</span><span class="kw">mean</span>(Y))<span class="op">^</span><span class="dv">2</span>) )</a></code></pre></div>
<pre><code>## [1] 0.9390901</code></pre>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb232-1" data-line-number="1">(r1  &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(f1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>((Y<span class="op">-</span><span class="kw">mean</span>(Y))<span class="op">^</span><span class="dv">2</span>)  )</a></code></pre></div>
<pre><code>## [1] 0.6375085</code></pre>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb234-1" data-line-number="1">(r2  &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(f2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>((Y<span class="op">-</span><span class="kw">mean</span>(Y))<span class="op">^</span><span class="dv">2</span>)  )</a></code></pre></div>
<pre><code>## [1] 0.6899812</code></pre>
<p><span class="math inline">\(R^2(f)\simeq 1\)</span> indicates a perfect fit – it is the
proportion of variance of the dependent variable explained by independent variables in the model.</p>
<hr />
<p>Unfortunately, <span class="math inline">\(R^2\)</span> tends to automatically increase as the number of independent variables
increase.</p>
<p>To correct for this phenomenon, we can consider the <strong>adjusted <span class="math inline">\(R^2\)</span></strong>:</p>
<p><span class="math display">\[
\bar{R}^2(f) = 1 - (1-{R}^2(f))\frac{n-1}{n-p-1}
\]</span></p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb236-1" data-line-number="1">n &lt;-<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb236-2" data-line-number="2"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>r12)<span class="op">*</span>(n<span class="dv">-1</span>)<span class="op">/</span>(n<span class="dv">-3</span>)</a></code></pre></div>
<pre><code>## [1] 0.9386933</code></pre>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb238-1" data-line-number="1"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>r1 )<span class="op">*</span>(n<span class="dv">-1</span>)<span class="op">/</span>(n<span class="dv">-2</span>)</a></code></pre></div>
<pre><code>## [1] 0.6363316</code></pre>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb240-1" data-line-number="1"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>r2 )<span class="op">*</span>(n<span class="dv">-1</span>)<span class="op">/</span>(n<span class="dv">-2</span>)</a></code></pre></div>
<pre><code>## [1] 0.6889747</code></pre>
<p>The adjusted <span class="math inline">\(R^2\)</span> penalises for more complex models.</p>
<hr />
<blockquote>
<p>(*) Side note – results of some statistical tests (e.g., significance of coefficients)
are reported by the <code>summary()</code> function — refer to a more advanced source to obtain more information.
These, however, require the verification of some assumptions regarding the input data
and the residuals.</p>
</blockquote>
<hr />
<div id="section-2" class="section level4 allowframebreaks unnumbered">
<h4></h4>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb242-1" data-line-number="1"><span class="kw">summary</span>(f12)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 + X2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -108.100   -1.940    7.812   20.249   50.623 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.726e+02  3.950e+00   43.69   &lt;2e-16 ***
## X1          1.828e-01  5.159e-03   35.43   &lt;2e-16 ***
## X2          2.198e+00  5.637e-02   38.99   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 34.16 on 307 degrees of freedom
## Multiple R-squared:  0.9391, Adjusted R-squared:  0.9387 
## F-statistic:  2367 on 2 and 307 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<div id="variable-selection" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Variable Selection</h3>
<hr />
<p>Consider all quantitative (numeric-continuous) variables in the <code>Credit</code> data set.</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb244-1" data-line-number="1">C &lt;-<span class="st"> </span>Credit[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>,</a>
<a class="sourceLine" id="cb244-2" data-line-number="2">    <span class="kw">c</span>(<span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Limit&quot;</span>, <span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Age&quot;</span>,</a>
<a class="sourceLine" id="cb244-3" data-line-number="3">      <span class="st">&quot;Education&quot;</span>, <span class="st">&quot;Balance&quot;</span>)]</a>
<a class="sourceLine" id="cb244-4" data-line-number="4"><span class="kw">head</span>(C)</a></code></pre></div>
<pre><code>##   Rating Limit  Income Age Education Balance
## 1    283  3606  14.891  34        11     333
## 2    483  6645 106.025  82        15     903
## 3    514  7075 104.593  71        11     580
## 4    681  9504 148.924  36        11     964
## 5    357  4897  55.882  68        16     331
## 6    569  8047  80.180  77        10    1151</code></pre>
<p>Let’s draw a <em>pair plot</em> – a matrix of scatter plots for every pair of variables:</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb246-1" data-line-number="1"><span class="kw">pairs</span>(C)</a></code></pre></div>
<hr />
<p><img src="02-regression-multiple-figures/unnamed-chunk-24-1.png" alt="plot of chunk unnamed-chunk-24" style="width:50.0%" /></p>
<hr />
<p>Seems like <code>Rating</code> almost linearly depends on <code>Limit</code>…</p>
<p>Pearson’s <span class="math inline">\(r\)</span> – linear correlation coefficient:</p>
<p><span class="math display">\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y})
}{
    \sqrt{\sum_{i=1}^n (x_i-\bar{x})^2} \sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}
}.
\]</span></p>
<p>It holds <span class="math inline">\(r\in[-1,1]\)</span>, where:</p>
<ul>
<li><span class="math inline">\(r=1\)</span> – positive linear dependence (<span class="math inline">\(y\)</span> increases as <span class="math inline">\(x\)</span> increases)</li>
<li><span class="math inline">\(r=1\)</span> – negative linear dependence (<span class="math inline">\(y\)</span> decreases as <span class="math inline">\(x\)</span> increases)</li>
<li><span class="math inline">\(r\simeq 0\)</span> – uncorrelated or non-linearly dependent</li>
</ul>
<!-- TODO anscombe -->
<hr />
<p>Interpretation:</p>
<p><img src="02-regression-multiple-figures/unnamed-chunk-25-1.png" alt="plot of chunk unnamed-chunk-25" style="width:50.0%" /></p>
<hr />
<p>Compute Pearson’s <span class="math inline">\(r\)</span> between all pairs of variables:</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb247-1" data-line-number="1"><span class="kw">round</span>(<span class="kw">cor</span>(C), <span class="dv">3</span>)</a></code></pre></div>
<pre><code>##           Rating  Limit Income   Age Education Balance
## Rating     1.000  0.996  0.831 0.167    -0.040   0.798
## Limit      0.996  1.000  0.834 0.164    -0.032   0.796
## Income     0.831  0.834  1.000 0.227    -0.033   0.414
## Age        0.167  0.164  0.227 1.000     0.024   0.008
## Education -0.040 -0.032 -0.033 0.024     1.000   0.001
## Balance    0.798  0.796  0.414 0.008     0.001   1.000</code></pre>
<p><code>Rating</code> and <code>Limit</code> are almost perfectly linearly correlated,
but both seem to describe the same thing.</p>
<p>For practical purposes, you’d rather model <code>Rating</code> as a function of the other variables.</p>
<p>For simple linear regression models, we’d choose either <code>Income</code> or <code>Balance</code>.</p>
<blockquote>
<p>How about multiple regression?</p>
</blockquote>
<hr />
<p>The best model:</p>
<ul>
<li>has high predictive power,</li>
<li>is simple.</li>
</ul>
<p>These are often mutually exclusive.</p>
<p>Which variables should be included in the optimal model?</p>
<hr />
<p>Again, the definition of the “best” object needs a <em>fitness</em> function.</p>
<p>For fitting a single model to data, we use the RSS.</p>
<p>We need a metric that takes the number of dependent variables into account.</p>
<p>It turns out that the adjusted <span class="math inline">\(R^2\)</span>, despite its interpretability,
is not suitable for this task.</p>
<hr />
<p>Instead, here we’ll be using <strong>the Akaike Information Criterion</strong> (AIC).</p>
<p>For a model <span class="math inline">\(f\)</span> with <span class="math inline">\(p\)</span> independent variables:
<span class="math display">\[
\mathrm{AIC}(f) = 2(p+1)+n\log(\mathrm{RSS}(f)/n)
\]</span></p>
<p>Our task is to find the combination of independent variables
that minimises the AIC.</p>
<p>For <span class="math inline">\(p\)</span> variables, the number of possible combinations is <span class="math inline">\(2^p\)</span>
(grows exponentially with <span class="math inline">\(p\)</span>).</p>
<p>For large <span class="math inline">\(p\)</span>, an extensive search is impractical.</p>
<hr />
<p>Therefore, to find the variable combination minimising the AIC,
we often rely on one of the two following greedy heuristics:</p>
<ul>
<li><p>forward selection:</p>
<ol style="list-style-type: decimal">
<li>start with an empty model</li>
<li>find an independent variable
whose addition to the current model would yield the highest decrease in the AIC and add it to the model</li>
<li>go to 2 until AIC decreases</li>
</ol></li>
<li><p>backward elimination:</p>
<ol style="list-style-type: decimal">
<li>start with the full model</li>
<li>find an independent variable
whose removal from the current model would decrease the AIC the most and eliminate it from the model</li>
<li>go to 2 until AIC decreases</li>
</ol></li>
</ul>
<blockquote>
<p>(*) There are of course many other methods, e.g., lasso regression
whose side effect is variable selection.</p>
</blockquote>
<hr />
<div id="section-3" class="section level4 allowframebreaks unnumbered">
<h4></h4>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb249-1" data-line-number="1">C &lt;-<span class="st"> </span>Credit[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>,</a>
<a class="sourceLine" id="cb249-2" data-line-number="2">    <span class="kw">c</span>(<span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Age&quot;</span>,</a>
<a class="sourceLine" id="cb249-3" data-line-number="3">      <span class="st">&quot;Education&quot;</span>, <span class="st">&quot;Balance&quot;</span>)]</a>
<a class="sourceLine" id="cb249-4" data-line-number="4"><span class="kw">step</span>(<span class="kw">lm</span>(Rating<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>C), <span class="co"># empty model</span></a>
<a class="sourceLine" id="cb249-5" data-line-number="5">    <span class="dt">scope=</span><span class="kw">formula</span>(<span class="kw">lm</span>(Rating<span class="op">~</span>., <span class="dt">data=</span>C)), <span class="co"># full model</span></a>
<a class="sourceLine" id="cb249-6" data-line-number="6">    <span class="dt">direction=</span><span class="st">&quot;forward&quot;</span>)</a></code></pre></div>
<pre><code>## Start:  AIC=3055.75
## Rating ~ 1
## 
##             Df Sum of Sq     RSS    AIC
## + Income     1   4058342 1823473 2694.7
## + Balance    1   3749707 2132108 2743.2
## + Age        1    164567 5717248 3048.9
## &lt;none&gt;                   5881815 3055.8
## + Education  1      9631 5872184 3057.2
## 
## Step:  AIC=2694.7
## Rating ~ Income
## 
##             Df Sum of Sq     RSS    AIC
## + Balance    1   1465212  358261 2192.3
## &lt;none&gt;                   1823473 2694.7
## + Age        1      2836 1820637 2696.2
## + Education  1      1063 1822410 2696.5
## 
## Step:  AIC=2192.26
## Rating ~ Income + Balance
## 
##             Df Sum of Sq    RSS    AIC
## + Age        1    4119.1 354141 2190.7
## + Education  1    2692.1 355568 2191.9
## &lt;none&gt;                   358261 2192.3
## 
## Step:  AIC=2190.67
## Rating ~ Income + Balance + Age
## 
##             Df Sum of Sq    RSS    AIC
## + Education  1    2925.7 351216 2190.1
## &lt;none&gt;                   354141 2190.7
## 
## Step:  AIC=2190.1
## Rating ~ Income + Balance + Age + Education</code></pre>
<pre><code>## 
## Call:
## lm(formula = Rating ~ Income + Balance + Age + Education, data = C)
## 
## Coefficients:
## (Intercept)       Income      Balance          Age  
##    173.8300       2.1668       0.1839       0.2234  
##   Education  
##     -0.9601</code></pre>
</div>
<div id="section-4" class="section level4 allowframebreaks unnumbered">
<h4></h4>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb252-1" data-line-number="1"><span class="kw">step</span>(<span class="kw">lm</span>(Rating<span class="op">~</span>., <span class="dt">data=</span>C), <span class="co"># full model</span></a>
<a class="sourceLine" id="cb252-2" data-line-number="2">     <span class="dt">scope=</span><span class="kw">formula</span>(<span class="kw">lm</span>(Rating<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>C)), <span class="co"># empty model</span></a>
<a class="sourceLine" id="cb252-3" data-line-number="3">     <span class="dt">direction=</span><span class="st">&quot;backward&quot;</span>)</a></code></pre></div>
<pre><code>## Start:  AIC=2190.1
## Rating ~ Income + Age + Education + Balance
## 
##             Df Sum of Sq     RSS    AIC
## &lt;none&gt;                    351216 2190.1
## - Education  1      2926  354141 2190.7
## - Age        1      4353  355568 2191.9
## - Balance    1   1468466 1819682 2698.1
## - Income     1   1617191 1968406 2722.4</code></pre>
<pre><code>## 
## Call:
## lm(formula = Rating ~ Income + Age + Education + Balance, data = C)
## 
## Coefficients:
## (Intercept)       Income          Age    Education  
##    173.8300       2.1668       0.2234      -0.9601  
##     Balance  
##      0.1839</code></pre>
</div>
<div id="section-5" class="section level4 allowframebreaks unnumbered">
<h4></h4>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb255-1" data-line-number="1">C &lt;-<span class="st"> </span>Credit[,  <span class="co"># do not restrict to Credit$Balance&gt;0</span></a>
<a class="sourceLine" id="cb255-2" data-line-number="2">    <span class="kw">c</span>(<span class="st">&quot;Rating&quot;</span>, <span class="st">&quot;Income&quot;</span>, <span class="st">&quot;Age&quot;</span>,</a>
<a class="sourceLine" id="cb255-3" data-line-number="3">      <span class="st">&quot;Education&quot;</span>, <span class="st">&quot;Balance&quot;</span>)]</a>
<a class="sourceLine" id="cb255-4" data-line-number="4"><span class="kw">step</span>(<span class="kw">lm</span>(Rating<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>C), <span class="co"># empty model</span></a>
<a class="sourceLine" id="cb255-5" data-line-number="5">    <span class="dt">scope=</span><span class="kw">formula</span>(<span class="kw">lm</span>(Rating<span class="op">~</span>., <span class="dt">data=</span>C)), <span class="co"># full model</span></a>
<a class="sourceLine" id="cb255-6" data-line-number="6">    <span class="dt">direction=</span><span class="st">&quot;forward&quot;</span>)</a></code></pre></div>
<pre><code>## Start:  AIC=4034.31
## Rating ~ 1
## 
##             Df Sum of Sq     RSS    AIC
## + Balance    1   7124258 2427627 3488.4
## + Income     1   5982140 3569744 3642.6
## + Age        1    101661 9450224 4032.0
## &lt;none&gt;                   9551885 4034.3
## + Education  1      8675 9543210 4036.0
## 
## Step:  AIC=3488.38
## Rating ~ Balance
## 
##             Df Sum of Sq     RSS    AIC
## + Income     1   1859749  567878 2909.3
## + Age        1     98562 2329065 3473.8
## &lt;none&gt;                   2427627 3488.4
## + Education  1      5130 2422497 3489.5
## 
## Step:  AIC=2909.28
## Rating ~ Balance + Income
## 
##             Df Sum of Sq    RSS    AIC
## &lt;none&gt;                   567878 2909.3
## + Age        1    2142.4 565735 2909.8
## + Education  1    1208.6 566669 2910.4</code></pre>
<pre><code>## 
## Call:
## lm(formula = Rating ~ Balance + Income, data = C)
## 
## Coefficients:
## (Intercept)      Balance       Income  
##    145.3506       0.2129       2.1863</code></pre>
</div>
<div id="section-6" class="section level4 allowframebreaks unnumbered">
<h4></h4>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb258-1" data-line-number="1"><span class="kw">step</span>(<span class="kw">lm</span>(Rating<span class="op">~</span>., <span class="dt">data=</span>C), <span class="co"># full model</span></a>
<a class="sourceLine" id="cb258-2" data-line-number="2">     <span class="dt">scope=</span><span class="kw">formula</span>(<span class="kw">lm</span>(Rating<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>C)), <span class="co"># empty model</span></a>
<a class="sourceLine" id="cb258-3" data-line-number="3">     <span class="dt">direction=</span><span class="st">&quot;backward&quot;</span>)</a></code></pre></div>
<pre><code>## Start:  AIC=2910.89
## Rating ~ Income + Age + Education + Balance
## 
##             Df Sum of Sq     RSS    AIC
## - Education  1      1238  565735 2909.8
## - Age        1      2172  566669 2910.4
## &lt;none&gt;                    564497 2910.9
## - Income     1   1759273 2323770 3474.9
## - Balance    1   2992164 3556661 3645.1
## 
## Step:  AIC=2909.77
## Rating ~ Income + Age + Balance
## 
##           Df Sum of Sq     RSS    AIC
## - Age      1      2142  567878 2909.3
## &lt;none&gt;                  565735 2909.8
## - Income   1   1763329 2329065 3473.8
## - Balance  1   2991523 3557259 3643.2
## 
## Step:  AIC=2909.28
## Rating ~ Income + Balance
## 
##           Df Sum of Sq     RSS    AIC
## &lt;none&gt;                  567878 2909.3
## - Income   1   1859749 2427627 3488.4
## - Balance  1   3001866 3569744 3642.6</code></pre>
<pre><code>## 
## Call:
## lm(formula = Rating ~ Income + Balance, data = C)
## 
## Coefficients:
## (Intercept)       Income      Balance  
##    145.3506       2.1863       0.2129</code></pre>
</div>
</div>
<div id="variable-transformation" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Variable Transformation</h3>
<hr />
<p>So far we have been fitting linear models of the form:
<span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p.
\]</span></p>
<p>What about some non-linear models such as polynomials etc.? For example:
<span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_1^3 + \beta_4 X_2.
\]</span></p>
<p>Solution: pre-process inputs by setting
<span class="math inline">\(X_1&#39; := X_1\)</span>, <span class="math inline">\(X_2&#39; := X_1^2\)</span>, <span class="math inline">\(X_3&#39; := X_1^3\)</span>, <span class="math inline">\(X_4&#39; := X_2\)</span>
and fit a linear model:</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1 X_1&#39; + \beta_2 X_2&#39; + \beta_3 X_3&#39; + \beta_4 X_4&#39;.
\]</span></p>
<p>This trick works for every model of the form
<span class="math inline">\(Y=\sum_{i=1}^k \sum_{j=1}^p \varphi_{i,j}(X_j)\)</span> for any <span class="math inline">\(k\)</span>
and any univariate functions <span class="math inline">\(\varphi_{i,j}\)</span>.</p>
<hr />
<p>Also, with a little creativity (and maths), we might be able to transform
a few other models to a linear one, e.g.,</p>
<p><span class="math display">\[
Y = b e^{aX} \qquad \to \qquad \log Y = \log b + aX \qquad\to\qquad Y&#39;=aX+b&#39;
\]</span></p>
<p>This is an example of a model’s <strong>linearisation</strong>.</p>
<hr />
<p>For example, here’s a series of simple polynomial regression models
of the form <code>Rating~poly(Balance)</code>:</p>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb261-1" data-line-number="1">f1_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1)</a>
<a class="sourceLine" id="cb261-2" data-line-number="2">f1_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X1<span class="op">+</span><span class="kw">I</span>(X1<span class="op">^</span><span class="dv">2</span>)<span class="op">+</span><span class="kw">I</span>(X1<span class="op">^</span><span class="dv">3</span>)) <span class="co"># also: Y~poly(X1, 3)</span></a>
<a class="sourceLine" id="cb261-3" data-line-number="3">f1_<span class="dv">14</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span><span class="kw">poly</span>(X1, <span class="dv">14</span>))</a></code></pre></div>
<hr />
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb262-1" data-line-number="1"><span class="kw">plot</span>(X1, Y, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;#000000aa&quot;</span>)</a>
<a class="sourceLine" id="cb262-2" data-line-number="2">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(X1), <span class="kw">max</span>(X1), <span class="dt">length.out=</span><span class="dv">101</span>)</a>
<a class="sourceLine" id="cb262-3" data-line-number="3"><span class="kw">lines</span>(x, <span class="kw">predict</span>(f1_<span class="dv">1</span>, <span class="kw">data.frame</span>(<span class="dt">X1=</span>x)), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb262-4" data-line-number="4"><span class="kw">lines</span>(x, <span class="kw">predict</span>(f1_<span class="dv">3</span>, <span class="kw">data.frame</span>(<span class="dt">X1=</span>x)), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb262-5" data-line-number="5"><span class="kw">lines</span>(x, <span class="kw">predict</span>(f1_<span class="dv">14</span>, <span class="kw">data.frame</span>(<span class="dt">X1=</span>x)), <span class="dt">col=</span><span class="st">&quot;darkgreen&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a></code></pre></div>
<p><img src="02-regression-multiple-figures/unnamed-chunk-32-1.png" alt="plot of chunk unnamed-chunk-32" style="width:50.0%" /></p>
</div>
<div id="predictive-vs.descriptive-power" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Predictive vs. Descriptive Power</h3>
<hr />
<p>The above high-degree polynomial model (<code>f1_14</code>) is a clear example of an <strong>overfit</strong>.</p>
<p>Clearly (based on our expert knowledge), the <code>Rating</code> shouldn’t decrease as Balance increases.</p>
<p>In other words, <code>f1_14</code> gives a better fit to actually observed data,
but fails to produce good results for the points that are yet to come.</p>
<p>We say that it <strong>generalises</strong> poorly to unseen data.</p>
<hr />
<p>Assume our true model is of the form:</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb263-1" data-line-number="1">true_model &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="dv">3</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span><span class="op">+</span><span class="dv">5</span></a></code></pre></div>
<p>And we generate the following random sample from this model (with <span class="math inline">\(Y\)</span> subject
to error):</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb264-1" data-line-number="1">n &lt;-<span class="st"> </span><span class="dv">25</span></a>
<a class="sourceLine" id="cb264-2" data-line-number="2">X &lt;-<span class="st"> </span><span class="kw">runif</span>(n, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb264-3" data-line-number="3">Y &lt;-<span class="st"> </span><span class="kw">true_model</span>(X)<span class="op">+</span><span class="kw">rnorm</span>(n, <span class="dt">sd=</span><span class="fl">0.1</span>)</a></code></pre></div>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb265-1" data-line-number="1"><span class="kw">plot</span>(X, Y, <span class="dt">las=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb265-2" data-line-number="2">x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">101</span>)</a>
<a class="sourceLine" id="cb265-3" data-line-number="3"><span class="kw">lines</span>(x, <span class="kw">true_model</span>(x), <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<hr />
<p><img src="02-regression-multiple-figures/unnamed-chunk-35-1.png" alt="plot of chunk unnamed-chunk-35" style="width:50.0%" /></p>
<hr />
<p>Let’s fit polynomials of different degrees:</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb266-1" data-line-number="1"><span class="kw">plot</span>(X, Y, <span class="dt">las=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb266-2" data-line-number="2"><span class="kw">lines</span>(x, <span class="kw">true_model</span>(x), <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb266-3" data-line-number="3"></a>
<a class="sourceLine" id="cb266-4" data-line-number="4">dmax &lt;-<span class="st"> </span><span class="dv">15</span></a>
<a class="sourceLine" id="cb266-5" data-line-number="5">MSE_train &lt;-<span class="st"> </span><span class="kw">numeric</span>(dmax)</a>
<a class="sourceLine" id="cb266-6" data-line-number="6">MSE_test  &lt;-<span class="st"> </span><span class="kw">numeric</span>(dmax)</a>
<a class="sourceLine" id="cb266-7" data-line-number="7"><span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>dmax) {</a>
<a class="sourceLine" id="cb266-8" data-line-number="8">    f &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span><span class="kw">poly</span>(X, d))</a>
<a class="sourceLine" id="cb266-9" data-line-number="9">    y &lt;-<span class="st"> </span><span class="kw">predict</span>(f, <span class="kw">data.frame</span>(<span class="dt">X=</span>x))</a>
<a class="sourceLine" id="cb266-10" data-line-number="10">    <span class="kw">lines</span>(x, y, <span class="dt">col=</span>d)</a>
<a class="sourceLine" id="cb266-11" data-line-number="11"></a>
<a class="sourceLine" id="cb266-12" data-line-number="12">    MSE_train[d] &lt;-<span class="st"> </span><span class="kw">mean</span>(f<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb266-13" data-line-number="13">    MSE_test[d]  &lt;-<span class="st"> </span><span class="kw">mean</span>((y<span class="op">-</span><span class="kw">true_model</span>(x))<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb266-14" data-line-number="14">}</a></code></pre></div>
<hr />
<p><img src="02-regression-multiple-figures/unnamed-chunk-36-1.png" alt="plot of chunk unnamed-chunk-36" style="width:50.0%" /></p>
<hr />
<p>Compare the mean squared error (MSE) for the observed vs. future data points:</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb267-1" data-line-number="1"><span class="kw">matplot</span>(<span class="dv">1</span><span class="op">:</span>dmax, <span class="kw">cbind</span>(MSE_train, MSE_test), <span class="dt">type=</span><span class="st">&#39;b&#39;</span>,</a>
<a class="sourceLine" id="cb267-2" data-line-number="2">    <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="fl">1e-3</span>, <span class="fl">1e3</span>), <span class="dt">log=</span><span class="st">&quot;y&quot;</span>, <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb267-3" data-line-number="3">    <span class="dt">xlab=</span><span class="st">&#39;Model complexity (degree of polynomial)&#39;</span>,</a>
<a class="sourceLine" id="cb267-4" data-line-number="4">    <span class="dt">ylab=</span><span class="st">&quot;MSE&quot;</span>)</a>
<a class="sourceLine" id="cb267-5" data-line-number="5"><span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;MSE train&quot;</span>, <span class="st">&quot;MSE test&quot;</span>),</a>
<a class="sourceLine" id="cb267-6" data-line-number="6">    <span class="dt">lty=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)</a></code></pre></div>
<p>Note the logarithmic scale on the <span class="math inline">\(y\)</span> axis.</p>
<hr />
<p><img src="02-regression-multiple-figures/unnamed-chunk-37-1.png" alt="plot of chunk unnamed-chunk-37" style="width:50.0%" /></p>
<hr />
<p>This is a very typical behaviour!</p>
<ul>
<li><p>A model’s fit to observed data improves as the model’s complexity increases.</p></li>
<li><p>A model’s generalisation to unseen data initially improves, but then becomes worse.</p></li>
<li><p>In the above example, the sweet spot is at a polynomial of degree 3, which is exactly
our true underlying model.</p></li>
</ul>
<hr />
<p>Hence, most often <strong>we should be interested in the accuracy of the predictions
made in the case of unobserved data.</strong></p>
<p>If we have a data set of a considerable size,
we can divide it (randomly) into two parts:</p>
<ul>
<li><em>training sample</em> (say, 60% or 80%) – used to fit a model</li>
<li><em>test sample</em> (the remaining 40% or 20%) – used to assess its quality
(e.g., using MSE)</li>
</ul>
<p>More on this issue in the chapter on Classification.</p>
<blockquote>
<p>(*) We shall see that sometimes a train-test-validate split will be necessary,
e.g., 60-20-20%.</p>
</blockquote>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multiple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="outro-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
