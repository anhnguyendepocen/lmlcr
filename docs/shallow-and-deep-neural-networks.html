<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Shallow and Deep Neural Networks | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Shallow and Deep Neural Networks | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Shallow and Deep Neural Networks | Lightweight Machine Learning Classics with R" />
  
  <meta name="twitter:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  

<meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-with-trees-and-linear-models.html"/>
<link rel="next" href="optimisation-with-iterative-algorithms.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>{</a></li>
<li class="chapter" data-level="1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a><ul>
<li class="chapter" data-level="1.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#supervised-learning"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#formalism"><i class="fa fa-check"></i><b>1.2.1</b> Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#desired-outputs"><i class="fa fa-check"></i><b>1.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="1.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-regression"><i class="fa fa-check"></i><b>1.3</b> Simple Regression</a><ul>
<li class="chapter" data-level="1.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>1.3.1</b> Introduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>1.3.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>1.4.1</b> Introduction</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#solution-in-r"><i class="fa fa-check"></i><b>1.4.2</b> Solution in R</a></li>
<li class="chapter" data-level="1.4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analytic-solution"><i class="fa fa-check"></i><b>1.4.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="1.4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>1.4.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-in-r"><i class="fa fa-check"></i><b>1.5</b> Exercises in R</a><ul>
<li class="chapter" data-level="1.5.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>1.5.1</b> The Anscombe Quartet</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outro"><i class="fa fa-check"></i><b>1.6</b> Outro</a><ul>
<li class="chapter" data-level="1.6.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#remarks"><i class="fa fa-check"></i><b>1.6.1</b> Remarks</a></li>
<li class="chapter" data-level="1.6.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#further-reading"><i class="fa fa-check"></i><b>1.6.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#formalism-1"><i class="fa fa-check"></i><b>2.1.1</b> Formalism</a></li>
<li class="chapter" data-level="2.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>2.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>2.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#problem-formulation"><i class="fa fa-check"></i><b>2.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>2.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="multiple-regression.html"><a href="multiple-regression.html#finding-the-best-model"><i class="fa fa-check"></i><b>2.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>2.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.3.2" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-selection"><i class="fa fa-check"></i><b>2.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="2.3.3" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-transformation"><i class="fa fa-check"></i><b>2.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="2.3.4" data-path="multiple-regression.html"><a href="multiple-regression.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>2.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multiple-regression.html"><a href="multiple-regression.html#exercises-in-r-1"><i class="fa fa-check"></i><b>2.4</b> Exercises in R</a><ul>
<li class="chapter" data-level="2.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>2.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="2.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="2.4.3" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>2.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="2.4.4" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>2.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="2.4.5" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>2.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="multiple-regression.html"><a href="multiple-regression.html#outro-1"><i class="fa fa-check"></i><b>2.5</b> Outro</a><ul>
<li class="chapter" data-level="2.5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#remarks-1"><i class="fa fa-check"></i><b>2.5.1</b> Remarks</a></li>
<li class="chapter" data-level="2.5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#other-methods-for-regression"><i class="fa fa-check"></i><b>2.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="2.5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>2.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="2.5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>2.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="2.5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>2.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="2.5.6" data-path="multiple-regression.html"><a href="multiple-regression.html#further-reading-1"><i class="fa fa-check"></i><b>2.5.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>3</b> Classification with K-Nearest Neighbours</a><ul>
<li class="chapter" data-level="3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#data"><i class="fa fa-check"></i><b>3.1.2</b> Data</a></li>
<li class="chapter" data-level="3.1.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.3</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.4</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#k-nearest-neighbour-classifier"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a><ul>
<li class="chapter" data-level="3.2.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-4"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#feature-engineering"><i class="fa fa-check"></i><b>3.2.3</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#model-assessment-and-selection"><i class="fa fa-check"></i><b>3.3</b> Model Assessment and Selection</a><ul>
<li class="chapter" data-level="3.3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#performance-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>3.3.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="3.3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>3.3.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.4</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#factor-data-type"><i class="fa fa-check"></i><b>3.4.1</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.4.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#main-routine"><i class="fa fa-check"></i><b>3.4.2</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.4.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#mode"><i class="fa fa-check"></i><b>3.4.3</b> Mode</a></li>
<li class="chapter" data-level="3.4.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#nn-search-routines"><i class="fa fa-check"></i><b>3.4.4</b> NN Search Routines (*)</a></li>
<li class="chapter" data-level="3.4.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#different-metrics"><i class="fa fa-check"></i><b>3.4.5</b> Different Metrics (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#outro-2"><i class="fa fa-check"></i><b>3.5</b> Outro</a><ul>
<li class="chapter" data-level="3.5.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#remarks-2"><i class="fa fa-check"></i><b>3.5.1</b> Remarks</a></li>
<li class="chapter" data-level="3.5.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.5.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.5.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#further-reading-2"><i class="fa fa-check"></i><b>3.5.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html"><i class="fa fa-check"></i><b>4</b> Classification with Trees and Linear Models</a><ul>
<li class="chapter" data-level="4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision Trees</a><ul>
<li class="chapter" data-level="4.2.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#introduction-6"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-1"><i class="fa fa-check"></i><b>4.2.2</b> Example in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>4.3</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="4.3.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#motivation"><i class="fa fa-check"></i><b>4.3.1</b> Motivation</a></li>
<li class="chapter" data-level="4.3.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#logistic-model"><i class="fa fa-check"></i><b>4.3.2</b> Logistic Model</a></li>
<li class="chapter" data-level="4.3.3" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#example-in-r-2"><i class="fa fa-check"></i><b>4.3.3</b> Example in R</a></li>
<li class="chapter" data-level="4.3.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#loss-function"><i class="fa fa-check"></i><b>4.3.4</b> Loss Function</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#outro-3"><i class="fa fa-check"></i><b>4.4</b> Outro</a><ul>
<li class="chapter" data-level="4.4.1" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#remarks-3"><i class="fa fa-check"></i><b>4.4.1</b> Remarks</a></li>
<li class="chapter" data-level="4.4.2" data-path="classification-with-trees-and-linear-models.html"><a href="classification-with-trees-and-linear-models.html#further-reading-3"><i class="fa fa-check"></i><b>4.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Shallow and Deep Neural Networks</a><ul>
<li class="chapter" data-level="5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-7"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>5.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="5.1.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#data-2"><i class="fa fa-check"></i><b>5.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>5.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="5.2.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#extending-logistic-regression"><i class="fa fa-check"></i><b>5.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="5.2.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#softmax-function"><i class="fa fa-check"></i><b>5.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="5.2.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>5.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="5.2.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>5.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="5.2.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>5.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neural-networks"><i class="fa fa-check"></i><b>5.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="5.3.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>5.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="5.3.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>5.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="5.3.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r-3"><i class="fa fa-check"></i><b>5.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>5.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="5.4.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-8"><i class="fa fa-check"></i><b>5.4.1</b> Introduction</a></li>
<li class="chapter" data-level="5.4.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>5.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="5.4.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>5.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="5.4.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>5.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#preprocessing-of-data"><i class="fa fa-check"></i><b>5.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="5.5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-9"><i class="fa fa-check"></i><b>5.5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#image-deskewing"><i class="fa fa-check"></i><b>5.5.2</b> Image Deskewing</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#outro-4"><i class="fa fa-check"></i><b>5.6</b> Outro</a><ul>
<li class="chapter" data-level="5.6.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#remarks-4"><i class="fa fa-check"></i><b>5.6.1</b> Remarks</a></li>
<li class="chapter" data-level="5.6.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#beyond-mnist"><i class="fa fa-check"></i><b>5.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="5.6.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#further-reading-4"><i class="fa fa-check"></i><b>5.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>6</b> Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="6.1" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#introduction-10"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#optimisation-problem"><i class="fa fa-check"></i><b>6.1.1</b> Optimisation Problem</a></li>
<li class="chapter" data-level="6.1.2" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>6.1.2</b> Example Optimisation Problems in Machine Learning</a></li>
<li class="chapter" data-level="6.1.3" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>6.1.3</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="6.1.4" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>6.1.4</b> Example Objective over a 2D Domain</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#iterative-methods"><i class="fa fa-check"></i><b>6.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="6.2.1" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#introduction-11"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#example-in-r-4"><i class="fa fa-check"></i><b>6.2.2</b> Example in R</a></li>
<li class="chapter" data-level="6.2.3" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>6.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="6.2.4" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#random-restarts"><i class="fa fa-check"></i><b>6.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>6.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.3.1" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#function-gradient"><i class="fa fa-check"></i><b>6.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="6.3.2" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>6.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="6.3.3" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>6.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="6.3.4" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#example-mnist"><i class="fa fa-check"></i><b>6.3.4</b> Example: MNIST</a></li>
<li class="chapter" data-level="6.3.5" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>6.3.5</b> Stochastic Gradient Descent (SGD)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#outro-5"><i class="fa fa-check"></i><b>6.4</b> Outro</a><ul>
<li class="chapter" data-level="6.4.1" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#remarks-5"><i class="fa fa-check"></i><b>6.4.1</b> Remarks</a></li>
<li class="chapter" data-level="6.4.2" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#optimisers-in-keras"><i class="fa fa-check"></i><b>6.4.2</b> Optimisers in Keras</a></li>
<li class="chapter" data-level="6.4.3" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#note-on-search-spaces"><i class="fa fa-check"></i><b>6.4.3</b> Note on Search Spaces</a></li>
<li class="chapter" data-level="6.4.4" data-path="optimisation-with-iterative-algorithms.html"><a href="optimisation-with-iterative-algorithms.html#further-reading-5"><i class="fa fa-check"></i><b>6.4.4</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a><ul>
<li class="chapter" data-level="7.1" data-path="clustering.html"><a href="clustering.html#unsupervised-learning"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="7.1.1" data-path="clustering.html"><a href="clustering.html#introduction-12"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="clustering.html"><a href="clustering.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>7.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="7.1.3" data-path="clustering.html"><a href="clustering.html#clustering-1"><i class="fa fa-check"></i><b>7.1.3</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>7.2</b> K-means Clustering</a><ul>
<li class="chapter" data-level="7.2.1" data-path="clustering.html"><a href="clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>7.2.1</b> Example in R</a></li>
<li class="chapter" data-level="7.2.2" data-path="clustering.html"><a href="clustering.html#problem-statement"><i class="fa fa-check"></i><b>7.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="7.2.3" data-path="clustering.html"><a href="clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>7.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="clustering.html"><a href="clustering.html#hierarchical-methods"><i class="fa fa-check"></i><b>7.3</b> Hierarchical Methods</a><ul>
<li class="chapter" data-level="7.3.1" data-path="clustering.html"><a href="clustering.html#introduction-13"><i class="fa fa-check"></i><b>7.3.1</b> Introduction</a></li>
<li class="chapter" data-level="7.3.2" data-path="clustering.html"><a href="clustering.html#example-in-r-6"><i class="fa fa-check"></i><b>7.3.2</b> Example in R</a></li>
<li class="chapter" data-level="7.3.3" data-path="clustering.html"><a href="clustering.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>7.3.3</b> Agglomerative Hierarchical Clustering</a></li>
<li class="chapter" data-level="7.3.4" data-path="clustering.html"><a href="clustering.html#linkage-functions"><i class="fa fa-check"></i><b>7.3.4</b> Linkage Functions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="clustering.html"><a href="clustering.html#outro-6"><i class="fa fa-check"></i><b>7.4</b> Outro</a><ul>
<li class="chapter" data-level="7.4.1" data-path="clustering.html"><a href="clustering.html#remarks-6"><i class="fa fa-check"></i><b>7.4.1</b> Remarks</a></li>
<li class="chapter" data-level="7.4.2" data-path="clustering.html"><a href="clustering.html#other-noteworthy-clustering-algorithms"><i class="fa fa-check"></i><b>7.4.2</b> Other Noteworthy Clustering Algorithms</a></li>
<li class="chapter" data-level="7.4.3" data-path="clustering.html"><a href="clustering.html#further-reading-6"><i class="fa fa-check"></i><b>7.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html"><i class="fa fa-check"></i><b>8</b> Optimisation with Genetic Algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-14"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#recap"><i class="fa fa-check"></i><b>8.1.1</b> Recap</a></li>
<li class="chapter" data-level="8.1.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#k-means-revisited"><i class="fa fa-check"></i><b>8.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="8.1.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>8.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>8.2</b> A Note on Convex Optimisation (*)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-15"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#convex-combinations"><i class="fa fa-check"></i><b>8.2.2</b> Convex Combinations (*)</a></li>
<li class="chapter" data-level="8.2.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#convex-functions"><i class="fa fa-check"></i><b>8.2.3</b> Convex Functions (*)</a></li>
<li class="chapter" data-level="8.2.4" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#examples"><i class="fa fa-check"></i><b>8.2.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#genetic-algorithms"><i class="fa fa-check"></i><b>8.3</b> Genetic Algorithms</a><ul>
<li class="chapter" data-level="8.3.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#introduction-16"><i class="fa fa-check"></i><b>8.3.1</b> Introduction</a></li>
<li class="chapter" data-level="8.3.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#overview-of-the-method"><i class="fa fa-check"></i><b>8.3.2</b> Overview of the Method</a></li>
<li class="chapter" data-level="8.3.3" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#example-implementation---ga-for-k-means"><i class="fa fa-check"></i><b>8.3.3</b> Example Implementation - GA for K-means</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#outro-7"><i class="fa fa-check"></i><b>8.4</b> Outro</a><ul>
<li class="chapter" data-level="8.4.1" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#remarks-7"><i class="fa fa-check"></i><b>8.4.1</b> Remarks</a></li>
<li class="chapter" data-level="8.4.2" data-path="optimisation-with-genetic-algorithms.html"><a href="optimisation-with-genetic-algorithms.html#further-reading-7"><i class="fa fa-check"></i><b>8.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>9</b> Recommender Systems</a><ul>
<li class="chapter" data-level="9.1" data-path="recommender-systems.html"><a href="recommender-systems.html#introduction-17"><i class="fa fa-check"></i><b>9.1</b> Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="recommender-systems.html"><a href="recommender-systems.html#what-is-a-recommender-system"><i class="fa fa-check"></i><b>9.1.1</b> What is a Recommender System?</a></li>
<li class="chapter" data-level="9.1.2" data-path="recommender-systems.html"><a href="recommender-systems.html#the-netflix-prize"><i class="fa fa-check"></i><b>9.1.2</b> The Netflix Prize</a></li>
<li class="chapter" data-level="9.1.3" data-path="recommender-systems.html"><a href="recommender-systems.html#main-approaches"><i class="fa fa-check"></i><b>9.1.3</b> Main Approaches</a></li>
<li class="chapter" data-level="9.1.4" data-path="recommender-systems.html"><a href="recommender-systems.html#formalism-2"><i class="fa fa-check"></i><b>9.1.4</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="recommender-systems.html"><a href="recommender-systems.html#collaborative-filtering"><i class="fa fa-check"></i><b>9.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="9.2.1" data-path="recommender-systems.html"><a href="recommender-systems.html#example"><i class="fa fa-check"></i><b>9.2.1</b> Example</a></li>
<li class="chapter" data-level="9.2.2" data-path="recommender-systems.html"><a href="recommender-systems.html#similarity-measures"><i class="fa fa-check"></i><b>9.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="9.2.3" data-path="recommender-systems.html"><a href="recommender-systems.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="9.2.4" data-path="recommender-systems.html"><a href="recommender-systems.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>9.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="recommender-systems.html"><a href="recommender-systems.html#movielens-dataset"><i class="fa fa-check"></i><b>9.3</b> MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="9.3.1" data-path="recommender-systems.html"><a href="recommender-systems.html#dataset"><i class="fa fa-check"></i><b>9.3.1</b> Dataset</a></li>
<li class="chapter" data-level="9.3.2" data-path="recommender-systems.html"><a href="recommender-systems.html#data-cleansing"><i class="fa fa-check"></i><b>9.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="9.3.3" data-path="recommender-systems.html"><a href="recommender-systems.html#item-item-similarities"><i class="fa fa-check"></i><b>9.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="9.3.4" data-path="recommender-systems.html"><a href="recommender-systems.html#example-recommendations"><i class="fa fa-check"></i><b>9.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="9.3.5" data-path="recommender-systems.html"><a href="recommender-systems.html#clustering-2"><i class="fa fa-check"></i><b>9.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="recommender-systems.html"><a href="recommender-systems.html#outro-8"><i class="fa fa-check"></i><b>9.4</b> Outro</a><ul>
<li class="chapter" data-level="9.4.1" data-path="recommender-systems.html"><a href="recommender-systems.html#remarks-8"><i class="fa fa-check"></i><b>9.4.1</b> Remarks</a></li>
<li class="chapter" data-level="9.4.2" data-path="recommender-systems.html"><a href="recommender-systems.html#issues"><i class="fa fa-check"></i><b>9.4.2</b> Issues</a></li>
<li class="chapter" data-level="9.4.3" data-path="recommender-systems.html"><a href="recommender-systems.html#further-reading-8"><i class="fa fa-check"></i><b>9.4.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="section-9.html"><a href="section-9.html"><i class="fa fa-check"></i>}</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html"><i class="fa fa-check"></i><b>A</b> Setting Up the R Environment</a><ul>
<li class="chapter" data-level="A.1" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-r"><i class="fa fa-check"></i><b>A.1</b> Installing R</a></li>
<li class="chapter" data-level="A.2" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-an-ide"><i class="fa fa-check"></i><b>A.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="A.3" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-recommended-packages"><i class="fa fa-check"></i><b>A.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="A.4" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>A.4</b> First R Script in RStudio</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html"><i class="fa fa-check"></i><b>B</b> Vector Algebra in R</a><ul>
<li class="chapter" data-level="B.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#motivation-1"><i class="fa fa-check"></i><b>B.1</b> Motivation</a></li>
<li class="chapter" data-level="B.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#numeric-vectors"><i class="fa fa-check"></i><b>B.2</b> Numeric Vectors</a><ul>
<li class="chapter" data-level="B.2.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>B.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="B.2.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-scalar-operations"><i class="fa fa-check"></i><b>B.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="B.2.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-vector-operations"><i class="fa fa-check"></i><b>B.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="B.2.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions"><i class="fa fa-check"></i><b>B.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="B.2.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#special-functions"><i class="fa fa-check"></i><b>B.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="B.2.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#norms-and-distances"><i class="fa fa-check"></i><b>B.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="B.2.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#dot-product"><i class="fa fa-check"></i><b>B.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="B.2.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>B.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-vectors"><i class="fa fa-check"></i><b>B.3</b> Logical Vectors</a><ul>
<li class="chapter" data-level="B.3.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-logical-vectors"><i class="fa fa-check"></i><b>B.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="B.3.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-operations"><i class="fa fa-check"></i><b>B.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="B.3.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#comparison-operations"><i class="fa fa-check"></i><b>B.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="B.3.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions-1"><i class="fa fa-check"></i><b>B.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#character-vectors"><i class="fa fa-check"></i><b>B.4</b> Character Vectors</a><ul>
<li class="chapter" data-level="B.4.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-character-vectors"><i class="fa fa-check"></i><b>B.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="B.4.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>B.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="B.4.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>B.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-subsetting"><i class="fa fa-check"></i><b>B.5</b> Vector Subsetting</a><ul>
<li class="chapter" data-level="B.5.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>B.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="B.5.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>B.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="B.5.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>B.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="B.5.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#replacing-elements"><i class="fa fa-check"></i><b>B.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="B.5.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#other-functions"><i class="fa fa-check"></i><b>B.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="B.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-vectors"><i class="fa fa-check"></i><b>B.6</b> Named Vectors</a><ul>
<li class="chapter" data-level="B.6.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-named-vectors"><i class="fa fa-check"></i><b>B.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="B.6.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>B.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="B.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#factors"><i class="fa fa-check"></i><b>B.7</b> Factors</a><ul>
<li class="chapter" data-level="B.7.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-factors"><i class="fa fa-check"></i><b>B.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="B.7.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#levels"><i class="fa fa-check"></i><b>B.7.2</b> Levels</a></li>
<li class="chapter" data-level="B.7.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#internal-representation"><i class="fa fa-check"></i><b>B.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="B.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#lists"><i class="fa fa-check"></i><b>B.8</b> Lists</a><ul>
<li class="chapter" data-level="B.8.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-lists"><i class="fa fa-check"></i><b>B.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="B.8.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-lists"><i class="fa fa-check"></i><b>B.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="B.8.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>B.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="B.8.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#common-operations"><i class="fa fa-check"></i><b>B.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="B.9" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#further-reading-9"><i class="fa fa-check"></i><b>B.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html"><i class="fa fa-check"></i><b>C</b> Matrix Algebra in R</a><ul>
<li class="chapter" data-level="C.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#creating-matrices"><i class="fa fa-check"></i><b>C.1</b> Creating Matrices</a><ul>
<li class="chapter" data-level="C.1.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix"><i class="fa fa-check"></i><b>C.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="C.1.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#stacking-vectors"><i class="fa fa-check"></i><b>C.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="C.1.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>C.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="C.1.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>C.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="C.1.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#other-methods"><i class="fa fa-check"></i><b>C.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="C.1.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#internal-representation-1"><i class="fa fa-check"></i><b>C.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#common-operations-1"><i class="fa fa-check"></i><b>C.2</b> Common Operations</a><ul>
<li class="chapter" data-level="C.2.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-transpose"><i class="fa fa-check"></i><b>C.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="C.2.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>C.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-multiplication"><i class="fa fa-check"></i><b>C.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="C.2.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>C.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="C.2.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#vectorised-special-functions"><i class="fa fa-check"></i><b>C.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="C.2.7" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-vector-operations"><i class="fa fa-check"></i><b>C.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-subsetting"><i class="fa fa-check"></i><b>C.3</b> Matrix Subsetting</a><ul>
<li class="chapter" data-level="C.3.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-individual-elements"><i class="fa fa-check"></i><b>C.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="C.3.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>C.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="C.3.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-submatrices"><i class="fa fa-check"></i><b>C.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="C.3.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>C.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="C.3.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>C.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#further-reading-10"><i class="fa fa-check"></i><b>C.4</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html"><i class="fa fa-check"></i><b>D</b> Data Frame Wrangling in R</a><ul>
<li class="chapter" data-level="D.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#creating-data-frames"><i class="fa fa-check"></i><b>D.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="D.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#importing-data-frames"><i class="fa fa-check"></i><b>D.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="D.3" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#data-frame-subsetting"><i class="fa fa-check"></i><b>D.3</b> Data Frame Subsetting</a><ul>
<li class="chapter" data-level="D.3.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>D.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="D.3.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>D.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#common-operations-2"><i class="fa fa-check"></i><b>D.4</b> Common Operations</a></li>
<li class="chapter" data-level="D.5" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>D.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="D.6" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#further-reading-11"><i class="fa fa-check"></i><b>D.6</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.2 2020-04-10 15:18 (d7e591e)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="shallow-and-deep-neural-networks" class="section level1">
<h1><span class="header-section-number">5</span> Shallow and Deep Neural Networks</h1>
<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->
<div id="introduction-7" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<div id="binary-logistic-regression-recap" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Binary Logistic Regression: Recap</h3>
<hr />
<p>Let <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span> be an input matrix
that consists of <span class="math inline">\(n\)</span> points in a <span class="math inline">\(p\)</span>-dimensional space.</p>
<p>In other words, we have a database on <span class="math inline">\(n\)</span> objects, each of which
being described by means of <span class="math inline">\(p\)</span> numerical features.</p>
<p><span class="math display">\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p} \\
x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p} \\
\end{array}
\right]
\]</span></p>
<p>With each input <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> we associate the desired output
<span class="math inline">\(y_i\)</span> which is a categorical label – hence we
will be dealing with <strong>classification</strong> tasks again.</p>
<hr />
<p>In <strong>binary logistic regression</strong> we were modelling the probabilities that
a given input belongs to either of the two classes:</p>
<p><span class="math display">\[
\begin{array}{ll}
\Pr(Y=1|\mathbf{X},\boldsymbol\beta)=&amp;\phantom{1-}\phi(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)\\
\Pr(Y=0|\mathbf{X},\boldsymbol\beta)=&amp;1-\phi(\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p)\\
\end{array}
\]</span>
where <span class="math inline">\(\phi(z) = \frac{1}{1+e^{-z}}\)</span> is the logistic sigmoid function.</p>
<p>It holds:
<span class="math display">\[
\begin{array}{ll}
\Pr(Y=1|\mathbf{X},\boldsymbol\beta)=&amp;\displaystyle\frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}\\
\Pr(Y=0|\mathbf{X},\boldsymbol\beta)=&amp;\displaystyle\frac{e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}{1+e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}\\
\end{array}
\]</span></p>
<hr />
<p>The fitting of the model was performed by minimising the cross-entropy (log-loss):
<span class="math display">\[
\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}
-\frac{1}{n} \sum_{i=1}^n
\left(y_i\log \Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta) + (1-y_i)\log \Pr(Y=0|\mathbf{x}_{i,\cdot},\boldsymbol\beta)\right).
\]</span></p>
<p>Note that for each <span class="math inline">\(i\)</span>,
either the left or the right term (in the bracketed expression) vanishes.</p>
<p>Hence, we may also write the above as:
<span class="math display">\[
\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}
-\frac{1}{n} \sum_{i=1}^n
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\boldsymbol\beta).
\]</span></p>
<!--Taking into account the fact that
$\log \frac{a}{b} = \log{a}-\log{b}$,
$\log e^{a} = a$ and $\log 1 = 0$, we can rewrite the above as:-->
<!--\[
-\frac{1}{n} \sum_{i=1}^n \left(
    y_i\log \Pr(Y=1|\mathbf{x}_{i,\cdot},\boldsymbol\beta) +
    \log \Pr(Y=0|\mathbf{x}_{i,\cdot},\boldsymbol\beta)
    - y_i \log \Pr(Y=0|\mathbf{x}_{i,\cdot},\boldsymbol\beta)
\right)
\]-->
<!--\[
-\frac{1}{n} \sum_{i=1}^n \left(
    y_i\log \frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}} +
    \log \frac{e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}{1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)}}
    - y_i \log \frac{e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}{1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)}}
\right)
\]-->
<!--\[
-\frac{1}{n} \sum_{i=1}^n \left(
    -y_i\log \left({1+e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}\right)
    +       \log \left({e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}\right)
    -       \log \left({1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)}}\right)
    -y_i\log \left({e^{-(\beta_0 + \beta_1 X_1 +  \dots + \beta_p X_p)}}\right)
    +y_i\log \left({1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)}}\right)
\right)
\]-->
<!--\[
\frac{1}{n} \sum_{i=1}^n \left(
    (1-y_i)    \left(\beta_0 + \beta_1 x_{i,1} +  \dots + \beta_p x_{i,p})\right)
    +       \log \left({1+e^{-(\beta_0 + \beta_1 x_{i,1}+ \dots + \beta_p x_{i,p})}}\right)
\right)
\]-->
<hr />
<p>In this chapter we will generalise the binary logistic regression model:</p>
<ul>
<li><p>First we will consider the case of multiclass classification.</p></li>
<li><p>Then we will note that multinomial logistic regression is a special
case of a feed-forward neural network.</p></li>
</ul>
</div>
<div id="data-2" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Data</h3>
<hr />
<p>We will study the famous classic – the MNIST image classification dataset.</p>
<blockquote>
<p>== Modified National Institute of Standards and Technology database,
see <a href="http://yann.lecun.com/exdb/mnist/" class="uri">http://yann.lecun.com/exdb/mnist/</a></p>
</blockquote>
<p>It consists of 28×28 pixel images of handwritten digits:</p>
<ul>
<li><code>train</code>: 60,000 training images,</li>
<li><code>t10k</code>: 10,000 testing images.</li>
</ul>
<p>There are 10 unique digits, so this is a multiclass classification problem.</p>
<blockquote>
<p>The dataset is already “too easy” for testing of the state-of-the-art
classifiers (see the notes below), but it’s a great educational example.</p>
</blockquote>
<hr />
<p>A few image instances from each class:</p>
<p><img src="05-classification-nnets-figures/mnist_demo-1.png" alt="plot of chunk mnist_demo" style="width:80.0%" /></p>
<hr />
<p>Accessing MNIST via the keras package
(which we will use throughout this chapter anyway) is easy:</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb437-1" title="1"><span class="kw">library</span>(<span class="st">&quot;keras&quot;</span>)</a>
<a class="sourceLine" id="cb437-2" title="2">mnist &lt;-<span class="st"> </span><span class="kw">dataset_mnist</span>()</a>
<a class="sourceLine" id="cb437-3" title="3">X_train &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>x</a>
<a class="sourceLine" id="cb437-4" title="4">Y_train &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>y</a>
<a class="sourceLine" id="cb437-5" title="5">X_test  &lt;-<span class="st"> </span>mnist<span class="op">$</span>test<span class="op">$</span>x</a>
<a class="sourceLine" id="cb437-6" title="6">Y_test  &lt;-<span class="st"> </span>mnist<span class="op">$</span>test<span class="op">$</span>y</a></code></pre></div>
<!--


```{cache=TRUE,mnist_download,warning=FALSE}
dir.create("mnist")
files <- c("train-images-idx3-ubyte.gz",
           "train-labels-idx1-ubyte.gz",
           "t10k-images-idx3-ubyte.gz",
           "t10k-labels-idx1-ubyte.gz")
for (file in files) {
    cat(sprintf("downloading %s...\n", file))
    download.file(sprintf("http://yann.lecun.com/exdb/mnist/%s", file),
              sprintf("mnist/%s", file))
}
```
-->
<hr />
<p><code>X_train</code> and <code>X_test</code> consist of 28×28 pixel images.</p>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb438-1" title="1"><span class="kw">dim</span>(X_train)</a></code></pre></div>
<pre><code>## [1] 60000    28    28</code></pre>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb440-1" title="1"><span class="kw">dim</span>(X_test)</a></code></pre></div>
<pre><code>## [1] 10000    28    28</code></pre>
<blockquote>
<p><code>X_train</code> and <code>X_test</code> are 3-dimensional arrays, think
of them as vectors of 60000 and 10000 matrices of size 28×28, respectively.</p>
</blockquote>
<p>These are greyscale images, with 0 = black, …, 255 = white:</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb442-1" title="1"><span class="kw">range</span>(X_train)</a></code></pre></div>
<pre><code>## [1]   0 255</code></pre>
<p>It is better to convert the colour values to 0.0 = black, …, 1.0 = white:</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb444-1" title="1">X_train &lt;-<span class="st"> </span>X_train<span class="op">/</span><span class="dv">255</span></a>
<a class="sourceLine" id="cb444-2" title="2">X_test  &lt;-<span class="st"> </span>X_test<span class="op">/</span><span class="dv">255</span></a></code></pre></div>
<hr />
<p><code>Y_train</code> and <code>Y_test</code> are the corresponding integer labels:</p>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb445-1" title="1"><span class="kw">length</span>(Y_train)</a></code></pre></div>
<pre><code>## [1] 60000</code></pre>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb447-1" title="1"><span class="kw">length</span>(Y_test)</a></code></pre></div>
<pre><code>## [1] 10000</code></pre>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb449-1" title="1"><span class="kw">table</span>(Y_train) <span class="co"># label distribution in train sample</span></a></code></pre></div>
<pre><code>## Y_train
##    0    1    2    3    4    5    6    7    8    9 
## 5923 6742 5958 6131 5842 5421 5918 6265 5851 5949</code></pre>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb451-1" title="1"><span class="kw">table</span>(Y_test)  <span class="co"># label distribution in test sample</span></a></code></pre></div>
<pre><code>## Y_test
##    0    1    2    3    4    5    6    7    8    9 
##  980 1135 1032 1010  982  892  958 1028  974 1009</code></pre>
<hr />
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb453-1" title="1">id &lt;-<span class="st"> </span><span class="dv">123</span> <span class="co"># which image to show</span></a>
<a class="sourceLine" id="cb453-2" title="2"><span class="kw">image</span>(<span class="dt">z=</span><span class="kw">t</span>(X_train[id,,]), <span class="dt">col=</span><span class="kw">grey.colors</span>(<span class="dv">256</span>, <span class="dv">0</span>, <span class="dv">1</span>),</a>
<a class="sourceLine" id="cb453-3" title="3">    <span class="dt">axes=</span><span class="ot">FALSE</span>, <span class="dt">asp=</span><span class="dv">1</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>))</a>
<a class="sourceLine" id="cb453-4" title="4"><span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">bg=</span><span class="st">&quot;white&quot;</span>,</a>
<a class="sourceLine" id="cb453-5" title="5">    <span class="dt">legend=</span><span class="kw">sprintf</span>(<span class="st">&quot;True label=%d&quot;</span>, Y_train[id]))</a></code></pre></div>
<p><img src="05-classification-nnets-figures/mnist_info2b-1.png" alt="plot of chunk mnist_info2b" style="width:80.0%" /></p>
</div>
</div>
<div id="multinomial-logistic-regression" class="section level2">
<h2><span class="header-section-number">5.2</span> Multinomial Logistic Regression</h2>
<div id="a-note-on-data-representation" class="section level3">
<h3><span class="header-section-number">5.2.1</span> A Note on Data Representation</h3>
<hr />
<p>So… you may now be wondering “how do we construct an image classifier,
this seems so complicated!”.</p>
<p>For a computer, (almost) everything is just numbers.</p>
<p>Instead of playing with <span class="math inline">\(n\)</span> matrices, each of size 28×28,
we may “flatten” the images so as to get
<span class="math inline">\(n\)</span> “long” vectors of length <span class="math inline">\(p=784\)</span>.</p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb454-1" title="1">X_train2 &lt;-<span class="st"> </span><span class="kw">matrix</span>(X_train, <span class="dt">ncol=</span><span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</a>
<a class="sourceLine" id="cb454-2" title="2">X_test2  &lt;-<span class="st"> </span><span class="kw">matrix</span>(X_test, <span class="dt">ncol=</span><span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</a></code></pre></div>
<p>The classifiers studied here do not take the “spatial” positioning of
the pixels into account anyway.</p>
<blockquote>
<p>(*) See, however, convolutional neural networks (CNNs),
e.g., in <span class="citation">(Goodfellow et al. <a href="references.html#ref-deeplearn">2016</a>)</span>.</p>
</blockquote>
<p>Hence, now we’re back to our “comfort zone”.</p>
</div>
<div id="extending-logistic-regression" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Extending Logistic Regression</h3>
<hr />
<p>Let us generalise the binary logistic regression model
to a 10-class one (or, more generally, <span class="math inline">\(K\)</span>-class one).</p>
<p>This time we will be modelling ten probabilities,
with
<span class="math inline">\(\Pr(Y=k|\mathbf{X},\mathbf{B})\)</span> denoting the <em>confidence</em> that a given image <span class="math inline">\(\mathbf{X}\)</span>
is in fact the <span class="math inline">\(k\)</span>-th digit:</p>
<p><span class="math display">\[
\begin{array}{lcl}
\Pr(Y=0|\mathbf{X},\mathbf{B})&amp;=&amp;\dots\\
\Pr(Y=1|\mathbf{X},\mathbf{B})&amp;=&amp;\dots\\
&amp;\vdots&amp;\\
\Pr(Y=9|\mathbf{X},\mathbf{B})&amp;=&amp;\dots\\
\end{array}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{B}\)</span> is the set of underlying model parameters
(to be determined soon).</p>
<hr />
<p>In binary logistic regression,
the class probabilities are obtained by “cleverly normalising”
the outputs of a linear model (so that we obtain a value in <span class="math inline">\([0,1]\)</span>).</p>
<p>In the multinomial case, we can use a separate linear model for each digit
so that <span class="math inline">\(\Pr(Y=k|\mathbf{X},\mathbf{B})\)</span>
is given as a function of
<span class="math display">\[\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p}.\]</span></p>
<p>Therefore, instead of a parameter vector of length <span class="math inline">\((p+1)\)</span>,
we will need a parameter matrix of size <span class="math inline">\((p+1)\times 10\)</span>
representing the model’s definition.</p>
<blockquote>
<p>Side note: upper case of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(B\)</span>.</p>
</blockquote>
<p>Then, these 10 numbers will have to be normalised
so as to they are positive and sum to <span class="math inline">\(1\)</span>.</p>
<hr />
<p>To maintain the spirit of the original model,
we can apply <span class="math inline">\(e^{-(\beta_{0,k} + \beta_{1,k} X_{1} + \dots + \beta_{p,k} X_{p})}\)</span>
to get a positive value,
because the co-domain of the exponential function <span class="math inline">\(t\mapsto e^t\)</span>
is <span class="math inline">\((0,\infty)\)</span>.</p>
<p>Then, dividing each output by the sum of all the outputs will guarantee that
the total sum equals 1.</p>
<p>This leads to:
<span class="math display">\[
\begin{array}{lcl}
\Pr(Y=0|\mathbf{X},\mathbf{B})&amp;=&amp;\displaystyle\frac{e^{-(\beta_{0,0} + \beta_{1,0} X_{1} +  \dots + \beta_{p,0} X_{p})}}{\sum_{k=0}^9 e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_p)}},\\
\Pr(Y=1|\mathbf{X},\mathbf{B})&amp;=&amp;\displaystyle\frac{e^{-(\beta_{0,1} + \beta_{1,1} X_{1} +  \dots + \beta_{p,1} X_{p})}}{\sum_{k=0}^9 e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p})}},\\
&amp;\vdots&amp;\\
\Pr(Y=9|\mathbf{X},\mathbf{B})&amp;=&amp;\displaystyle\frac{e^{-(\beta_{0,9} + \beta_{1,9} X_{1} +  \dots + \beta_{p,9} X_{p})}}{\sum_{k=0}^9 e^{-(\beta_{0,k} + \beta_{1,k} X_{1} +  \dots + \beta_{p,k} X_{p})}}.\\
\end{array}
\]</span></p>
<p>Note that we get the binary logistic regression
if we fix <span class="math inline">\(\beta_{0,0}=\beta_{1,0}=\dots=\beta_{p,0}=0\)</span>
as <span class="math inline">\(e^0=1\)</span> and consider only the classes <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p>
</div>
<div id="softmax-function" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Softmax Function</h3>
<hr />
<p>The above transformation (that maps 10 arbitrary real numbers
to positive ones that sum to 1)
is called the <strong>softmax</strong> function (or <em>softargmax</em>).</p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb455-1" title="1">softmax &lt;-<span class="st"> </span><span class="cf">function</span>(T) {</a>
<a class="sourceLine" id="cb455-2" title="2">    T2 &lt;-<span class="st"> </span><span class="kw">exp</span>(T) <span class="co"># ignore the minus sign above</span></a>
<a class="sourceLine" id="cb455-3" title="3">    T2<span class="op">/</span><span class="kw">sum</span>(T2)</a>
<a class="sourceLine" id="cb455-4" title="4">}</a>
<a class="sourceLine" id="cb455-5" title="5"><span class="kw">round</span>(<span class="kw">rbind</span>(</a>
<a class="sourceLine" id="cb455-6" title="6">    <span class="kw">softmax</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,  <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)),</a>
<a class="sourceLine" id="cb455-7" title="7">    <span class="kw">softmax</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)),</a>
<a class="sourceLine" id="cb455-8" title="8">    <span class="kw">softmax</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">9</span>,  <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)),</a>
<a class="sourceLine" id="cb455-9" title="9">    <span class="kw">softmax</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">9</span>,  <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">8</span>))), <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    0 1.00    0    0    0 0.00    0    0  0.00
## [2,]    0    0 0.50    0    0    0 0.50    0    0  0.00
## [3,]    0    0 0.73    0    0    0 0.27    0    0  0.00
## [4,]    0    0 0.67    0    0    0 0.24    0    0  0.09</code></pre>
</div>
<div id="one-hot-encoding-and-decoding" class="section level3">
<h3><span class="header-section-number">5.2.4</span> One-Hot Encoding and Decoding</h3>
<hr />
<p>The ten class-belongingness-degrees can be decoded
to obtain a single label by simply choosing
the class that is assigned the highest probability.</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb457-1" title="1">y_pred &lt;-<span class="st"> </span><span class="kw">softmax</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">9</span>,  <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">8</span>))</a>
<a class="sourceLine" id="cb457-2" title="2"><span class="kw">round</span>(y_pred, <span class="dv">2</span>) <span class="co"># probabilities of class 0, 1, 2, ..., 9</span></a></code></pre></div>
<pre><code>##  [1] 0.00 0.00 0.67 0.00 0.00 0.00 0.24 0.00 0.00 0.09</code></pre>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb459-1" title="1"><span class="kw">which.max</span>(y_pred)<span class="op">-</span><span class="dv">1</span> <span class="co"># 1..10 -&gt; 0..9</span></a></code></pre></div>
<pre><code>## [1] 2</code></pre>
<blockquote>
<p><code>which.max(y)</code> returns an index <code>k</code> such that
<code>y[k]==max(y)</code> (recall that in R the first element in a vector is at index <code>1</code>).
Mathematically, we denote this operation as <span class="math inline">\(\mathrm{arg}\max_{k=1,\dots,K} y_k\)</span>.</p>
</blockquote>
<hr />
<p>To make processing the outputs of a logistic regression model more convenient,
we will apply the <strong>one-hot-encoding</strong> of the labels.</p>
<p>Here, each label will be represented as a 0-1 probability vector
– with probability 1 corresponding to the true class only.</p>
<p>For example:</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb461-1" title="1">y &lt;-<span class="st"> </span><span class="dv">2</span> <span class="co"># true class (example)</span></a>
<a class="sourceLine" id="cb461-2" title="2">y2 &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb461-3" title="3">y2[y<span class="op">+</span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="dv">1</span> <span class="co"># +1 because we need 0..9 -&gt; 1..10</span></a>
<a class="sourceLine" id="cb461-4" title="4">y2  <span class="co"># one-hot-encoded y</span></a></code></pre></div>
<pre><code>##  [1] 0 0 1 0 0 0 0 0 0 0</code></pre>
<hr />
<p>To one-hot encode the reference outputs in R,
we start with a matrix of size <span class="math inline">\(n\times 10\)</span> populated with “0”s:</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb463-1" title="1">Y_train2 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span><span class="kw">length</span>(Y_train), <span class="dt">ncol=</span><span class="dv">10</span>)</a></code></pre></div>
<p>Next, for every <span class="math inline">\(i\)</span>, we insert a “1” in the <span class="math inline">\(i\)</span>-th row
and the (<code>Y_train[</code><span class="math inline">\(i\)</span><code>]+1</code>)-th column:</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb464-1" title="1"><span class="co"># Note the &quot;+1&quot; 0..9 -&gt; 1..10</span></a>
<a class="sourceLine" id="cb464-2" title="2">Y_train2[<span class="kw">cbind</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(Y_train), Y_train<span class="op">+</span><span class="dv">1</span>)] &lt;-<span class="st"> </span><span class="dv">1</span></a></code></pre></div>
<blockquote>
<p>In R, indexing a matrix <code>A</code> with a 2-column matrix <code>B</code>, i.e., <code>A[B]</code>,
allows for an easy access to
<code>A[B[1,1], B[1,2]]</code>, <code>A[B[2,1], B[2,2]]</code>, <code>A[B[3,1], B[3,2]]</code>, …</p>
</blockquote>
<hr />
<p>Sanity check:</p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb465-1" title="1"><span class="kw">head</span>(Y_train)</a></code></pre></div>
<pre><code>## [1] 5 0 4 1 9 2</code></pre>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb467-1" title="1"><span class="kw">head</span>(Y_train2)</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    0    0    0    0    1    0    0    0     0
## [2,]    1    0    0    0    0    0    0    0    0     0
## [3,]    0    0    0    0    1    0    0    0    0     0
## [4,]    0    1    0    0    0    0    0    0    0     0
## [5,]    0    0    0    0    0    0    0    0    0     1
## [6,]    0    0    1    0    0    0    0    0    0     0</code></pre>
<hr />
<p>Let us generalise the above idea and write a function
that can one-hot-encode any vector of integer labels:</p>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb469-1" title="1">one_hot_encode &lt;-<span class="st"> </span><span class="cf">function</span>(Y) {</a>
<a class="sourceLine" id="cb469-2" title="2">    <span class="kw">stopifnot</span>(<span class="kw">is.numeric</span>(Y))</a>
<a class="sourceLine" id="cb469-3" title="3">    c1 &lt;-<span class="st"> </span><span class="kw">min</span>(Y) <span class="co"># first class label</span></a>
<a class="sourceLine" id="cb469-4" title="4">    cK &lt;-<span class="st"> </span><span class="kw">max</span>(Y) <span class="co"># last class label</span></a>
<a class="sourceLine" id="cb469-5" title="5">    K &lt;-<span class="st"> </span>cK<span class="op">-</span>c1<span class="op">+</span><span class="dv">1</span> <span class="co"># number of classes</span></a>
<a class="sourceLine" id="cb469-6" title="6"></a>
<a class="sourceLine" id="cb469-7" title="7">    Y2 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span><span class="kw">length</span>(Y), <span class="dt">ncol=</span>K)</a>
<a class="sourceLine" id="cb469-8" title="8">    Y2[<span class="kw">cbind</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(Y), Y<span class="op">-</span>c1<span class="op">+</span><span class="dv">1</span>)] &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb469-9" title="9">    Y2</a>
<a class="sourceLine" id="cb469-10" title="10">}</a></code></pre></div>
<p>Encode <code>Y_train</code> and <code>Y_test</code>:</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb470-1" title="1">Y_train2 &lt;-<span class="st"> </span><span class="kw">one_hot_encode</span>(Y_train)</a>
<a class="sourceLine" id="cb470-2" title="2">Y_test2 &lt;-<span class="st"> </span><span class="kw">one_hot_encode</span>(Y_test)</a></code></pre></div>
</div>
<div id="cross-entropy-revisited" class="section level3">
<h3><span class="header-section-number">5.2.5</span> Cross-entropy Revisited</h3>
<hr />
<p>In essence, we will be comparing
the probability vectors as generated by a classifier, <span class="math inline">\(\hat{Y}\)</span>:</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb471-1" title="1"><span class="kw">round</span>(y_pred, <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##  [1] 0.00 0.00 0.67 0.00 0.00 0.00 0.24 0.00 0.00 0.09</code></pre>
<p>with the one-hot-encoded true probabilities, <span class="math inline">\(Y\)</span>:</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb473-1" title="1">y2</a></code></pre></div>
<pre><code>##  [1] 0 0 1 0 0 0 0 0 0 0</code></pre>
<hr />
<p>It turns out that one of the definitions of cross-entropy introduced above
already handles the case of multiclass classification:
<span class="math display">\[
E(\mathbf{B}) =
-\frac{1}{n} \sum_{i=1}^n
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\mathbf{B}).
\]</span>
The smaller the probability corresponding to the ground-truth class
outputted by the classifier, the higher the penalty:</p>
<p><img src="05-classification-nnets-figures/unnamed-chunk-7-1.png" alt="plot of chunk unnamed-chunk-7" style="width:80.0%" /></p>
<hr />
<p>To sum up, we will be solving the optimisation problem:
<span class="math display">\[
\min_{\mathbf{B}\in\mathbb{R}^{(p+1)\times 10}}
-\frac{1}{n} \sum_{i=1}^n
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\mathbf{B}).
\]</span>
This has no analytical solution,
but can be solved using iterative methods
(see the chapter on optimisation).</p>
<hr />
<p>(*) Side note: A single term in the above formula,
<span class="math display">\[
\log \Pr(Y=y_i|\mathbf{x}_{i,\cdot},\mathbf{B})
\]</span>
given:</p>
<ul>
<li><code>y_pred</code> – a vector of 10 probabilities
generated by the model:
<span class="math display">\[
\left[\Pr(Y=0|\mathbf{x}_{i,\cdot},\mathbf{B})\  \Pr(Y=1|\mathbf{x}_{i,\cdot},\mathbf{B})\ \cdots\ \Pr(Y=9|\mathbf{x}_{i,\cdot},\mathbf{B})\right]
\]</span></li>
<li><code>y2</code> – a one-hot-encoded version of the true label, <span class="math inline">\(y_i\)</span>, of the form
<span class="math display">\[
\left[0\ 0\ \cdots\ 0\ 1\ 0\ \cdots\ 0\right]
\]</span></li>
</ul>
<p>can be computed as:</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb475-1" title="1"><span class="kw">sum</span>(y2<span class="op">*</span><span class="kw">log</span>(y_pred))</a></code></pre></div>
<pre><code>## [1] -0.4078174</code></pre>
</div>
<div id="problem-formulation-in-matrix-form" class="section level3">
<h3><span class="header-section-number">5.2.6</span> Problem Formulation in Matrix Form (**)</h3>
<hr />
<p>The definition of a multinomial logistic regression
model for a multiclass classification task involving classes <span class="math inline">\(\{1,2,\dots,K\}\)</span>
is slightly bloated.</p>
<p>Assuming that <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span> is the input matrix,
to compute the <span class="math inline">\(K\)</span> predicted probabilities for the <span class="math inline">\(i\)</span>-th input,
<span class="math display">\[
\left[
\hat{y}_{i,1}\ \hat{y}_{i,2}\ \cdots\ \hat{y}_{i,K}
\right],
\]</span>
given a parameter matrix <span class="math inline">\(\mathbf{B}^{(p+1)\times K}\)</span>, we apply:
<span class="math display">\[
\begin{array}{lcl}
\hat{y}_{i,1}=\Pr(Y=1|\mathbf{x}_{i,\cdot},\mathbf{B})&amp;=&amp;\displaystyle\frac{e^{\beta_{0,1} + \beta_{1,1} x_{i,1} +  \dots + \beta_{p,1} x_{i,p}}}{\sum_{k=1}^K e^{\beta_{0,k} + \beta_{1,k} x_{i,1} +  \dots + \beta_{p,k} x_{i,p}}},\\
&amp;\vdots&amp;\\
\hat{y}_{i,K}=\Pr(Y=K|\mathbf{x}_{i,\cdot},\mathbf{B})&amp;=&amp;\displaystyle\frac{e^{\beta_{0,K} + \beta_{1,K} x_{i,1} +  \dots + \beta_{p,K} x_{i,p}}}{\sum_{k=1}^K e^{\beta_{0,k} + \beta_{1,k} x_{i,1} +  \dots + \beta_{p,k} x_{i,p}}}.\\
\end{array}
\]</span></p>
<blockquote>
<p>We have dropped the minus sign in the exponentiation for the brevity of notation.
Note that we can always map <span class="math inline">\(b_{j,k}&#39;=-b_{j,k}\)</span>.</p>
</blockquote>
<p>It turns out we can make use of matrix notation
to tidy the above formulas.</p>
<hr />
<p>Denote the linear combinations prior to computing the softmax function with:
<span class="math display">\[
\begin{array}{lcl}
t_{i,1}&amp;=&amp;\beta_{0,1} + \beta_{1,1} x_{i,1} +  \dots + \beta_{p,1} x_{i,p}\\
&amp;\vdots&amp;\\
t_{i,K}&amp;=&amp;\beta_{0,K} + \beta_{1,K} x_{i,1} +  \dots + \beta_{p,K} x_{i,p}\\
\end{array}
\]</span></p>
<p>We have:</p>
<ul>
<li><span class="math inline">\(x_{i,j}\)</span> – <span class="math inline">\(i\)</span>-th observation, <span class="math inline">\(j\)</span>-th feature;</li>
<li><span class="math inline">\(\hat{y}_{i,k}\)</span> – <span class="math inline">\(i\)</span>-th observation, <span class="math inline">\(k\)</span>-th class probability;</li>
<li><span class="math inline">\(\beta_{j,k}\)</span> – coefficient for the <span class="math inline">\(j\)</span>-th feature when computing the <span class="math inline">\(k\)</span>-th class.</li>
</ul>
<p>Note that by augmenting <span class="math inline">\(\mathbf{\dot{X}}=[\boldsymbol{1}\ \mathbf{X}]\in\mathbb{R}^{n\times (p+1)}\)</span>,
where <span class="math inline">\(\dot{x}_{i,0}=1\)</span> and <span class="math inline">\(\dot{x}_{i,j}=x_{i,j}\)</span> for all <span class="math inline">\(j\ge 1\)</span> and all <span class="math inline">\(i\)</span>, we can write the above as:
<span class="math display">\[
\begin{array}{lclcl}
t_{i,1}&amp;=&amp;\sum_{j=0}^p \dot{x}_{i,j}\, \beta_{j,1} &amp;=&amp; \dot{\mathbf{x}}_{i,\cdot}\, \boldsymbol\beta_{\cdot,1}\\
&amp;\vdots&amp;\\
t_{i,K}&amp;=&amp;\sum_{j=0}^p \dot{x}_{i,j}\, \beta_{j,K} &amp;=&amp; \dot{\mathbf{x}}_{i,\cdot}\, \boldsymbol\beta_{\cdot,K}\\
\end{array}
\]</span></p>
<hr />
<p>We can get the <span class="math inline">\(K\)</span> linear combinations all at once
in the form of a row vector by writing:
<span class="math display">\[
\left[
t_{i,1}\ t_{i,2}\ \cdots\ t_{i,K}
\right]
=
{\mathbf{x}_{i,\cdot}}\, \mathbf{B}
\]</span></p>
<p>Moreover, we can do that for all the <span class="math inline">\(n\)</span> inputs by writing:
<span class="math display">\[
\mathbf{T}=\dot{\mathbf{X}}\,\mathbf{B}
\]</span>
Yes, this is a single matrix multiplication,
we have <span class="math inline">\(\mathbf{T}\in\mathbb{R}^{n\times K}\)</span>.</p>
<p>To obtain <span class="math inline">\(\hat{\mathbf{Y}}\)</span>, we have to apply the softmax function
on every row of <span class="math inline">\(\mathbf{T}\)</span>:
<span class="math display">\[
\hat{\mathbf{Y}}=\mathrm{softmax}\left(
\dot{\mathbf{X}}\,\mathbf{B}
\right).
\]</span></p>
<p>That’s it. Take some time to appreciate the elegance of this notation.</p>
<p>Methods for minimising crossentropy expressed in matrix form
will be discussed in the next chapter.</p>
</div>
</div>
<div id="artificial-neural-networks" class="section level2">
<h2><span class="header-section-number">5.3</span> Artificial Neural Networks</h2>
<div id="artificial-neuron" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Artificial Neuron</h3>
<hr />
<p>A neuron as a mathematical function:</p>
<p><img src="figures/neuron.png" alt="Source: https://en.wikipedia.org/wiki/File:Neuron3.png by Egm4313.s12 at English Wikipedia, licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license" /></p>
<hr />
<p>The <strong>perceptron</strong> (Frank Rosenblatt, 1958) was amongst the first
models of artificial neurons:</p>
<p><img src="figures/perceptron.png" /></p>
</div>
<div id="logistic-regression-as-a-neural-network" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Logistic Regression as a Neural Network</h3>
<hr />
<p>The above resembles our binary logistic regression model!</p>
<p>We determine a linear combination (a weighted sum) of 784 inputs
and then transform it using the logistic sigmoid “activation” function.</p>
<p><img src="figures/logistic_regression_binary.png" /></p>
<hr />
<p>A multiclass logistic regression can be depicted as:</p>
<p><img src="figures/logistic_regression_multiclass.png" /></p>
<hr />
<p>This is an instance of a:</p>
<ul>
<li><strong>single layer</strong> (there is only one processing step that consists of 10 units),</li>
<li><strong>densely connected</strong> (all the inputs are connected to all the neurons),</li>
<li><strong>feed-forward</strong> (outputs are generated by processing the inputs directly,
there are no loops in the graph etc.)</li>
</ul>
<p><em>artificial</em> <strong>neural network</strong>
that uses the softmax as the activation function.</p>
</div>
<div id="example-in-r-3" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Example in R</h3>
<hr />
<p>To train such a neural network (fit a multinomial logistic regression model),
we will use the keras package,
a wrapper around the state-of-the-art, GPU-enabled TensorFlow library.</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb477-1" title="1"><span class="co"># Start with an empty model</span></a>
<a class="sourceLine" id="cb477-2" title="2">model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>()</a>
<a class="sourceLine" id="cb477-3" title="3"><span class="co"># Add a single layer with 10 units and softmax activation</span></a>
<a class="sourceLine" id="cb477-4" title="4"><span class="kw">layer_dense</span>(model, <span class="dt">units=</span><span class="dv">10</span>, <span class="dt">activation=</span><span class="st">&#39;softmax&#39;</span>)</a>
<a class="sourceLine" id="cb477-5" title="5"><span class="co"># We will be minimising the cross-entropy,</span></a>
<a class="sourceLine" id="cb477-6" title="6"><span class="co"># sgd == stochastic gradient descent, see the next chapter</span></a>
<a class="sourceLine" id="cb477-7" title="7"><span class="kw">compile</span>(model, <span class="dt">optimizer=</span><span class="st">&#39;sgd&#39;</span>,</a>
<a class="sourceLine" id="cb477-8" title="8">        <span class="dt">loss=</span><span class="st">&#39;categorical_crossentropy&#39;</span>)</a>
<a class="sourceLine" id="cb477-9" title="9"><span class="co"># Fit the model</span></a>
<a class="sourceLine" id="cb477-10" title="10"><span class="kw">fit</span>(model, X_train2, Y_train2, <span class="dt">epochs=</span><span class="dv">5</span>)</a></code></pre></div>
<hr />
<p>Predict over the test set and one-hot-decode the output probabilities:</p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb478-1" title="1">Y_pred2 &lt;-<span class="st"> </span><span class="kw">predict</span>(model, X_test2)</a>
<a class="sourceLine" id="cb478-2" title="2"><span class="kw">round</span>(<span class="kw">head</span>(Y_pred2), <span class="dv">2</span>) <span class="co"># predicted class probabilities</span></a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00  0.00
## [2,] 0.01 0.00 0.85 0.02 0.00 0.02 0.08 0.00 0.01  0.00
## [3,] 0.00 0.95 0.02 0.01 0.00 0.00 0.01 0.01 0.01  0.00
## [4,] 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00
## [5,] 0.00 0.00 0.01 0.00 0.87 0.00 0.01 0.02 0.02  0.07
## [6,] 0.00 0.97 0.00 0.01 0.00 0.00 0.00 0.00 0.01  0.00</code></pre>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb480-1" title="1">Y_pred &lt;-<span class="st"> </span><span class="kw">apply</span>(Y_pred2, <span class="dv">1</span>, which.max)<span class="op">-</span><span class="dv">1</span> <span class="co"># 1..10 -&gt; 0..9</span></a>
<a class="sourceLine" id="cb480-2" title="2"><span class="kw">head</span>(Y_pred, <span class="dv">20</span>) <span class="co"># predicted outputs</span></a></code></pre></div>
<pre><code>##  [1] 7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4</code></pre>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb482-1" title="1"><span class="kw">head</span>(Y_test, <span class="dv">20</span>) <span class="co"># true outputs</span></a></code></pre></div>
<pre><code>##  [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4</code></pre>
<hr />
<p>Accuracy on the test set:</p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb484-1" title="1"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_pred)</a></code></pre></div>
<pre><code>## [1] 0.9097</code></pre>
<hr />
<p>Performance metrics for each digit separately:</p>
<table>
<thead>
<tr class="header">
<th align="right">i</th>
<th align="right">Acc</th>
<th align="right">Prec</th>
<th align="right">Rec</th>
<th align="right">F</th>
<th align="right">TN</th>
<th align="right">FN</th>
<th align="right">FP</th>
<th align="right">TP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">0.9918</td>
<td align="right">0.9419291</td>
<td align="right">0.9765306</td>
<td align="right">0.9589178</td>
<td align="right">8961</td>
<td align="right">23</td>
<td align="right">59</td>
<td align="right">957</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0.9916</td>
<td align="right">0.9549784</td>
<td align="right">0.9718062</td>
<td align="right">0.9633188</td>
<td align="right">8813</td>
<td align="right">32</td>
<td align="right">52</td>
<td align="right">1103</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">0.9780</td>
<td align="right">0.9134420</td>
<td align="right">0.8691860</td>
<td align="right">0.8907646</td>
<td align="right">8883</td>
<td align="right">135</td>
<td align="right">85</td>
<td align="right">897</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.9789</td>
<td align="right">0.9007021</td>
<td align="right">0.8891089</td>
<td align="right">0.8948680</td>
<td align="right">8891</td>
<td align="right">112</td>
<td align="right">99</td>
<td align="right">898</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.9833</td>
<td align="right">0.9153925</td>
<td align="right">0.9144603</td>
<td align="right">0.9149261</td>
<td align="right">8935</td>
<td align="right">84</td>
<td align="right">83</td>
<td align="right">898</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">0.9769</td>
<td align="right">0.8948626</td>
<td align="right">0.8396861</td>
<td align="right">0.8663968</td>
<td align="right">9020</td>
<td align="right">143</td>
<td align="right">88</td>
<td align="right">749</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.9877</td>
<td align="right">0.9229990</td>
<td align="right">0.9509395</td>
<td align="right">0.9367609</td>
<td align="right">8966</td>
<td align="right">47</td>
<td align="right">76</td>
<td align="right">911</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.9819</td>
<td align="right">0.9180652</td>
<td align="right">0.9046693</td>
<td align="right">0.9113180</td>
<td align="right">8889</td>
<td align="right">98</td>
<td align="right">83</td>
<td align="right">930</td>
</tr>
<tr class="odd">
<td align="right">8</td>
<td align="right">0.9736</td>
<td align="right">0.8550000</td>
<td align="right">0.8778234</td>
<td align="right">0.8662614</td>
<td align="right">8881</td>
<td align="right">119</td>
<td align="right">145</td>
<td align="right">855</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="right">0.9757</td>
<td align="right">0.8711240</td>
<td align="right">0.8909812</td>
<td align="right">0.8809407</td>
<td align="right">8858</td>
<td align="right">110</td>
<td align="right">133</td>
<td align="right">899</td>
</tr>
</tbody>
</table>
<p>Note how misleading the individual accuracies are! Averages:</p>
<pre><code>##       Acc      Prec       Rec         F 
## 0.9819400 0.9088495 0.9085191 0.9084473</code></pre>
<hr />
<p><img src="05-classification-nnets-figures/logistic6-1.png" alt="plot of chunk logistic6" style="width:80.0%" /></p>
</div>
</div>
<div id="deep-neural-networks" class="section level2">
<h2><span class="header-section-number">5.4</span> Deep Neural Networks</h2>
<div id="introduction-8" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Introduction</h3>
<hr />
<p>In a brain, a neuron’s output is an input to another neuron.</p>
<p>We could try aligning neurons into many interconnected layers.</p>
<p><img src="figures/nnet.png" /></p>
</div>
<div id="activation-functions" class="section level3">
<h3><span class="header-section-number">5.4.2</span> Activation Functions</h3>
<hr />
<p>Each layer’s outputs should be transformed by some non-linear
activation function. Otherwise, we’d end up with linear combinations of linear combinations,
which are linear combinations themselves.</p>
<!-- Apart from `softmax` -->
<!--linear
Linear (i.e. identity) activation function.
Not for hidden layers - use for the output layer in regression tasks-->
<p>Example activation functions
that can be used in hidden (inner) layers:</p>
<ul>
<li><code>relu</code> – The rectified linear unit:
<span class="math display">\[\psi(t)=\max(t, 0),\]</span></li>
<li><code>sigmoid</code> – The logistic sigmoid:
<span class="math display">\[\phi(t)=1 / (1 + \exp(-t)),\]</span></li>
<li><code>tanh</code> – The hyperbolic function:
<span class="math display">\[\mathrm{tanh}(t) = (\exp(t) - \exp(-t)) / (\exp(t) + \exp(-t)).\]</span></li>
</ul>
<p>There is not much difference between them, but some might be more convenient
to handle numerically than the others, depending on the implementation.</p>
</div>
<div id="example-in-r---2-layers" class="section level3">
<h3><span class="header-section-number">5.4.3</span> Example in R - 2 Layers</h3>
<hr />
<p>2-layer Neural Network 784-800-10</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb487-1" title="1">model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>()</a>
<a class="sourceLine" id="cb487-2" title="2"><span class="kw">layer_dense</span>(model, <span class="dt">units=</span><span class="dv">800</span>, <span class="dt">activation=</span><span class="st">&#39;relu&#39;</span>)</a>
<a class="sourceLine" id="cb487-3" title="3"><span class="kw">layer_dense</span>(model, <span class="dt">units=</span><span class="dv">10</span>,  <span class="dt">activation=</span><span class="st">&#39;softmax&#39;</span>)</a>
<a class="sourceLine" id="cb487-4" title="4"><span class="kw">compile</span>(model, <span class="dt">optimizer=</span><span class="st">&#39;sgd&#39;</span>,</a>
<a class="sourceLine" id="cb487-5" title="5">        <span class="dt">loss=</span><span class="st">&#39;categorical_crossentropy&#39;</span>)</a>
<a class="sourceLine" id="cb487-6" title="6"><span class="kw">fit</span>(model, X_train2, Y_train2, <span class="dt">epochs=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb487-7" title="7"></a>
<a class="sourceLine" id="cb487-8" title="8">Y_pred2 &lt;-<span class="st"> </span><span class="kw">predict</span>(model, X_test2)</a>
<a class="sourceLine" id="cb487-9" title="9">Y_pred &lt;-<span class="st"> </span><span class="kw">apply</span>(Y_pred2, <span class="dv">1</span>, which.max)<span class="op">-</span><span class="dv">1</span> <span class="co"># 1..10 -&gt; 0..9</span></a>
<a class="sourceLine" id="cb487-10" title="10"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_pred) <span class="co"># accuracy on the test set</span></a></code></pre></div>
<pre><code>## [1] 0.9417</code></pre>
<hr />
<p>Performance metrics for each digit separately:</p>
<table>
<thead>
<tr class="header">
<th align="right">i</th>
<th align="right">Acc</th>
<th align="right">Prec</th>
<th align="right">Rec</th>
<th align="right">F</th>
<th align="right">TN</th>
<th align="right">FN</th>
<th align="right">FP</th>
<th align="right">TP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">0.9942</td>
<td align="right">0.9591633</td>
<td align="right">0.9826531</td>
<td align="right">0.9707661</td>
<td align="right">8979</td>
<td align="right">17</td>
<td align="right">41</td>
<td align="right">963</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0.9950</td>
<td align="right">0.9771328</td>
<td align="right">0.9788546</td>
<td align="right">0.9779930</td>
<td align="right">8839</td>
<td align="right">24</td>
<td align="right">26</td>
<td align="right">1111</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">0.9875</td>
<td align="right">0.9467980</td>
<td align="right">0.9312016</td>
<td align="right">0.9389350</td>
<td align="right">8914</td>
<td align="right">71</td>
<td align="right">54</td>
<td align="right">961</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.9863</td>
<td align="right">0.9241983</td>
<td align="right">0.9415842</td>
<td align="right">0.9328102</td>
<td align="right">8912</td>
<td align="right">59</td>
<td align="right">78</td>
<td align="right">951</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.9862</td>
<td align="right">0.9042146</td>
<td align="right">0.9613035</td>
<td align="right">0.9318855</td>
<td align="right">8918</td>
<td align="right">38</td>
<td align="right">100</td>
<td align="right">944</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">0.9867</td>
<td align="right">0.9407666</td>
<td align="right">0.9080717</td>
<td align="right">0.9241301</td>
<td align="right">9057</td>
<td align="right">82</td>
<td align="right">51</td>
<td align="right">810</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.9903</td>
<td align="right">0.9470405</td>
<td align="right">0.9519833</td>
<td align="right">0.9495055</td>
<td align="right">8991</td>
<td align="right">46</td>
<td align="right">51</td>
<td align="right">912</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.9877</td>
<td align="right">0.9423265</td>
<td align="right">0.9377432</td>
<td align="right">0.9400293</td>
<td align="right">8913</td>
<td align="right">64</td>
<td align="right">59</td>
<td align="right">964</td>
</tr>
<tr class="odd">
<td align="right">8</td>
<td align="right">0.9853</td>
<td align="right">0.9215087</td>
<td align="right">0.9281314</td>
<td align="right">0.9248082</td>
<td align="right">8949</td>
<td align="right">70</td>
<td align="right">77</td>
<td align="right">904</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="right">0.9842</td>
<td align="right">0.9512195</td>
<td align="right">0.8889990</td>
<td align="right">0.9190574</td>
<td align="right">8945</td>
<td align="right">112</td>
<td align="right">46</td>
<td align="right">897</td>
</tr>
</tbody>
</table>
<hr />
<p><img src="05-classification-nnets-figures/deep23-1.png" alt="plot of chunk deep23" style="width:80.0%" /></p>
</div>
<div id="example-in-r---6-layers" class="section level3">
<h3><span class="header-section-number">5.4.4</span> Example in R - 6 Layers</h3>
<hr />
<p>6-layer <em>Deep</em> Neural Network 784-2500-2000-1500-1000-500-10</p>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb489-1" title="1">model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>()</a>
<a class="sourceLine" id="cb489-2" title="2"><span class="kw">layer_dense</span>(model, <span class="dt">units=</span><span class="dv">2500</span>, <span class="dt">activation=</span><span class="st">&#39;relu&#39;</span>)</a>
<a class="sourceLine" id="cb489-3" title="3"><span class="kw">layer_dense</span>(model, <span class="dt">units=</span><span class="dv">2000</span>, <span class="dt">activation=</span><span class="st">&#39;relu&#39;</span>)</a>
<a class="sourceLine" id="cb489-4" title="4"><span class="kw">layer_dense</span>(model, <span class="dt">units=</span><span class="dv">1500</span>, <span class="dt">activation=</span><span class="st">&#39;relu&#39;</span>)</a>
<a class="sourceLine" id="cb489-5" title="5"><span class="kw">layer_dense</span>(model, <span class="dt">units=</span><span class="dv">1000</span>, <span class="dt">activation=</span><span class="st">&#39;relu&#39;</span>)</a>
<a class="sourceLine" id="cb489-6" title="6"><span class="kw">layer_dense</span>(model, <span class="dt">units=</span><span class="dv">500</span>,  <span class="dt">activation=</span><span class="st">&#39;relu&#39;</span>)</a>
<a class="sourceLine" id="cb489-7" title="7"><span class="kw">layer_dense</span>(model, <span class="dt">units=</span><span class="dv">10</span>,   <span class="dt">activation=</span><span class="st">&#39;softmax&#39;</span>)</a>
<a class="sourceLine" id="cb489-8" title="8"><span class="kw">compile</span>(model, <span class="dt">optimizer=</span><span class="st">&#39;sgd&#39;</span>,</a>
<a class="sourceLine" id="cb489-9" title="9">        <span class="dt">loss=</span><span class="st">&#39;categorical_crossentropy&#39;</span>)</a>
<a class="sourceLine" id="cb489-10" title="10"><span class="kw">fit</span>(model, X_train2, Y_train2, <span class="dt">epochs=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb489-11" title="11"></a>
<a class="sourceLine" id="cb489-12" title="12">Y_pred2 &lt;-<span class="st"> </span><span class="kw">predict</span>(model, X_test2)</a>
<a class="sourceLine" id="cb489-13" title="13">Y_pred &lt;-<span class="st"> </span><span class="kw">apply</span>(Y_pred2, <span class="dv">1</span>, which.max)<span class="op">-</span><span class="dv">1</span> <span class="co"># 1..10 -&gt; 0..9</span></a>
<a class="sourceLine" id="cb489-14" title="14"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_pred) <span class="co"># accuracy on the test set</span></a></code></pre></div>
<pre><code>## [1] 0.9729</code></pre>
<hr />
<p>Performance metrics for each digit separately:</p>
<table>
<thead>
<tr class="header">
<th align="right">i</th>
<th align="right">Acc</th>
<th align="right">Prec</th>
<th align="right">Rec</th>
<th align="right">F</th>
<th align="right">TN</th>
<th align="right">FN</th>
<th align="right">FP</th>
<th align="right">TP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">0.9965</td>
<td align="right">0.9846154</td>
<td align="right">0.9795918</td>
<td align="right">0.9820972</td>
<td align="right">9005</td>
<td align="right">20</td>
<td align="right">15</td>
<td align="right">960</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0.9967</td>
<td align="right">0.9799652</td>
<td align="right">0.9911894</td>
<td align="right">0.9855453</td>
<td align="right">8842</td>
<td align="right">10</td>
<td align="right">23</td>
<td align="right">1125</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">0.9943</td>
<td align="right">0.9841112</td>
<td align="right">0.9602713</td>
<td align="right">0.9720451</td>
<td align="right">8952</td>
<td align="right">41</td>
<td align="right">16</td>
<td align="right">991</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.9950</td>
<td align="right">0.9838710</td>
<td align="right">0.9663366</td>
<td align="right">0.9750250</td>
<td align="right">8974</td>
<td align="right">34</td>
<td align="right">16</td>
<td align="right">976</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.9948</td>
<td align="right">0.9678068</td>
<td align="right">0.9796334</td>
<td align="right">0.9736842</td>
<td align="right">8986</td>
<td align="right">20</td>
<td align="right">32</td>
<td align="right">962</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">0.9942</td>
<td align="right">0.9612832</td>
<td align="right">0.9742152</td>
<td align="right">0.9677060</td>
<td align="right">9073</td>
<td align="right">23</td>
<td align="right">35</td>
<td align="right">869</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.9948</td>
<td align="right">0.9788584</td>
<td align="right">0.9665971</td>
<td align="right">0.9726891</td>
<td align="right">9022</td>
<td align="right">32</td>
<td align="right">20</td>
<td align="right">926</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.9936</td>
<td align="right">0.9643545</td>
<td align="right">0.9737354</td>
<td align="right">0.9690223</td>
<td align="right">8935</td>
<td align="right">27</td>
<td align="right">37</td>
<td align="right">1001</td>
</tr>
<tr class="odd">
<td align="right">8</td>
<td align="right">0.9934</td>
<td align="right">0.9558233</td>
<td align="right">0.9774127</td>
<td align="right">0.9664975</td>
<td align="right">8982</td>
<td align="right">22</td>
<td align="right">44</td>
<td align="right">952</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="right">0.9925</td>
<td align="right">0.9670000</td>
<td align="right">0.9583746</td>
<td align="right">0.9626680</td>
<td align="right">8958</td>
<td align="right">42</td>
<td align="right">33</td>
<td align="right">967</td>
</tr>
</tbody>
</table>
<hr />
<p><img src="05-classification-nnets-figures/deep63-1.png" alt="plot of chunk deep63" style="width:80.0%" /></p>
</div>
</div>
<div id="preprocessing-of-data" class="section level2">
<h2><span class="header-section-number">5.5</span> Preprocessing of Data</h2>
<div id="introduction-9" class="section level3">
<h3><span class="header-section-number">5.5.1</span> Introduction</h3>
<hr />
<p>Do not underestimate the power of appropriate data preprocessing —
deep neural networks are not a universal replacement for a data engineer’s hard work!</p>
<p>On top of that, they are not interpretable – those are merely black-boxes.</p>
<p>Among the typical transformations of the input images we can find:</p>
<ul>
<li>normalisation of colours (setting brightness, stretching contrast, etc.),</li>
<li>repositioning of the image (centring),</li>
<li>deskewing (see below),</li>
<li>denoising (e.g., by blurring).</li>
</ul>
<p>Another frequently applied technique concerns an expansion of the training data
— we can add “artificially contaminated” images to the training
set (e.g., slightly rotated digits) so as to be more ready to whatever
will be provided in the test test.</p>
</div>
<div id="image-deskewing" class="section level3">
<h3><span class="header-section-number">5.5.2</span> Image Deskewing</h3>
<hr />
<p>Deskewing of images (“straightening” of the digits)
is amongst the most typical transformations
that can be applied on MNIST.</p>
<p>Unfortunately, we don’t have the necessary mathematical background to discuss this operation
in very detail.</p>
<p>Luckily, we can apply it on each image anyway.</p>
<p>See the GitHub repository at <a href="https://github.com/gagolews/Playground.R" class="uri">https://github.com/gagolews/Playground.R</a>
for an example notebook and the <code>deskew.R</code> script.</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb491-1" title="1"><span class="co"># See https://github.com/gagolews/Playground.R</span></a>
<a class="sourceLine" id="cb491-2" title="2"><span class="kw">source</span>(<span class="st">&quot;~/R/Playground.R/deskew.R&quot;</span>)</a>
<a class="sourceLine" id="cb491-3" title="3"><span class="co"># new_image &lt;- deskew(old_image)</span></a></code></pre></div>
<hr />
<p><img src="05-classification-nnets-figures/deskew2-1.png" alt="plot of chunk deskew2" style="width:80.0%" /></p>
<p>In each pair, the left image (black background) is the original one,
and the right image (palette inverted for purely dramatic effects)
is its deskewed version.</p>
<hr />
<p>Deskew everything:</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb492-1" title="1">Z_train &lt;-<span class="st"> </span>X_train</a>
<a class="sourceLine" id="cb492-2" title="2"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">dim</span>(Z_train)[<span class="dv">1</span>]) {</a>
<a class="sourceLine" id="cb492-3" title="3">    Z_train[i,,] &lt;-<span class="st"> </span><span class="kw">deskew</span>(Z_train[i,,])</a>
<a class="sourceLine" id="cb492-4" title="4">}</a>
<a class="sourceLine" id="cb492-5" title="5">Z_train2 &lt;-<span class="st"> </span><span class="kw">matrix</span>(Z_train, <span class="dt">ncol=</span><span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</a>
<a class="sourceLine" id="cb492-6" title="6"></a>
<a class="sourceLine" id="cb492-7" title="7"></a>
<a class="sourceLine" id="cb492-8" title="8">Z_test &lt;-<span class="st"> </span>X_test</a>
<a class="sourceLine" id="cb492-9" title="9"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">dim</span>(Z_test)[<span class="dv">1</span>]) {</a>
<a class="sourceLine" id="cb492-10" title="10">  Z_test[i,,] &lt;-<span class="st"> </span><span class="kw">deskew</span>(Z_test[i,,])</a>
<a class="sourceLine" id="cb492-11" title="11">}</a>
<a class="sourceLine" id="cb492-12" title="12">Z_test2 &lt;-<span class="st"> </span><span class="kw">matrix</span>(Z_test, <span class="dt">ncol=</span><span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</a></code></pre></div>
<hr />
<p>Multinomial logistic regression model (1-layer NN):</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb493-1" title="1">model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>()</a>
<a class="sourceLine" id="cb493-2" title="2"><span class="kw">layer_dense</span>(model, <span class="dt">units=</span><span class="dv">10</span>, <span class="dt">activation=</span><span class="st">&#39;softmax&#39;</span>)</a>
<a class="sourceLine" id="cb493-3" title="3"><span class="kw">compile</span>(model, <span class="dt">optimizer=</span><span class="st">&#39;sgd&#39;</span>,</a>
<a class="sourceLine" id="cb493-4" title="4">        <span class="dt">loss=</span><span class="st">&#39;categorical_crossentropy&#39;</span>)</a>
<a class="sourceLine" id="cb493-5" title="5"><span class="kw">fit</span>(model, Z_train2, Y_train2, <span class="dt">epochs=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb493-6" title="6"></a>
<a class="sourceLine" id="cb493-7" title="7">Y_pred2 &lt;-<span class="st"> </span><span class="kw">predict</span>(model, Z_test2)</a>
<a class="sourceLine" id="cb493-8" title="8">Y_pred &lt;-<span class="st"> </span><span class="kw">apply</span>(Y_pred2, <span class="dv">1</span>, which.max)<span class="op">-</span><span class="dv">1</span> <span class="co"># 1..10 -&gt; 0..9</span></a>
<a class="sourceLine" id="cb493-9" title="9"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_pred) <span class="co"># accuracy on the test set</span></a></code></pre></div>
<pre><code>## [1] 0.9449</code></pre>
<hr />
<p>Performance metrics for each digit separately:</p>
<table>
<thead>
<tr class="header">
<th align="right">i</th>
<th align="right">Acc</th>
<th align="right">Prec</th>
<th align="right">Rec</th>
<th align="right">F</th>
<th align="right">TN</th>
<th align="right">FN</th>
<th align="right">FP</th>
<th align="right">TP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">0.9936</td>
<td align="right">0.9525692</td>
<td align="right">0.9836735</td>
<td align="right">0.9678715</td>
<td align="right">8972</td>
<td align="right">16</td>
<td align="right">48</td>
<td align="right">964</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0.9951</td>
<td align="right">0.9754816</td>
<td align="right">0.9814978</td>
<td align="right">0.9784805</td>
<td align="right">8837</td>
<td align="right">21</td>
<td align="right">28</td>
<td align="right">1114</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">0.9866</td>
<td align="right">0.9508032</td>
<td align="right">0.9176357</td>
<td align="right">0.9339250</td>
<td align="right">8919</td>
<td align="right">85</td>
<td align="right">49</td>
<td align="right">947</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.9900</td>
<td align="right">0.9586694</td>
<td align="right">0.9415842</td>
<td align="right">0.9500500</td>
<td align="right">8949</td>
<td align="right">59</td>
<td align="right">41</td>
<td align="right">951</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.9882</td>
<td align="right">0.9354839</td>
<td align="right">0.9450102</td>
<td align="right">0.9402229</td>
<td align="right">8954</td>
<td align="right">54</td>
<td align="right">64</td>
<td align="right">928</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">0.9887</td>
<td align="right">0.9351955</td>
<td align="right">0.9383408</td>
<td align="right">0.9367655</td>
<td align="right">9050</td>
<td align="right">55</td>
<td align="right">58</td>
<td align="right">837</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.9904</td>
<td align="right">0.9517820</td>
<td align="right">0.9478079</td>
<td align="right">0.9497908</td>
<td align="right">8996</td>
<td align="right">50</td>
<td align="right">46</td>
<td align="right">908</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.9884</td>
<td align="right">0.9569138</td>
<td align="right">0.9289883</td>
<td align="right">0.9427443</td>
<td align="right">8929</td>
<td align="right">73</td>
<td align="right">43</td>
<td align="right">955</td>
</tr>
<tr class="odd">
<td align="right">8</td>
<td align="right">0.9846</td>
<td align="right">0.9051383</td>
<td align="right">0.9404517</td>
<td align="right">0.9224572</td>
<td align="right">8930</td>
<td align="right">58</td>
<td align="right">96</td>
<td align="right">916</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="right">0.9842</td>
<td align="right">0.9225422</td>
<td align="right">0.9207136</td>
<td align="right">0.9216270</td>
<td align="right">8913</td>
<td align="right">80</td>
<td align="right">78</td>
<td align="right">929</td>
</tr>
</tbody>
</table>
<hr />
<p><img src="05-classification-nnets-figures/deskew6-1.png" alt="plot of chunk deskew6" style="width:80.0%" /></p>
</div>
</div>
<div id="outro-4" class="section level2">
<h2><span class="header-section-number">5.6</span> Outro</h2>
<div id="remarks-4" class="section level3">
<h3><span class="header-section-number">5.6.1</span> Remarks</h3>
<hr />
<p>We have discussed a multinomial logistic regression model
as a generalisation of the binary one.</p>
<p>This in turn is a special case of feed-forward neural networks.</p>
<p>There’s a lot of hype (again…) for deep neural networks in many applications,
including vision, self-driving cars, natural language processing,
speech recognition etc.</p>
<hr />
<p>Many different architectures of neural networks and types of units
are being considered in theory and in practice, e.g.:</p>
<ul>
<li>convolutional neural networks apply a series of signal (e.g., image)
transformations in first layers, they might actually “discover”
deskewing automatically etc.;</li>
<li>recurrent neural networks can imitate long short-term memory
that can be used for speech synthesis and time series prediction.</li>
</ul>
<hr />
<p>Main drawbacks of deep neural networks:</p>
<ul>
<li>learning is very slow, especially with very deep architectures (days, weeks);</li>
<li>models are not explainable (black boxes) and hard to debug;</li>
<li>finding good architectures is more art than science
(maybe: more of a craftsmanship even);</li>
<li>sometimes using deep neural network is just an excuse for being too lazy
to do proper data cleansing and pre-processing.</li>
</ul>
<p>There are many issues and challenges that will be tackled in more advanced
AI/ML courses and books, such as <span class="citation">(Goodfellow et al. <a href="references.html#ref-deeplearn">2016</a>)</span>.</p>
</div>
<div id="beyond-mnist" class="section level3">
<h3><span class="header-section-number">5.6.2</span> Beyond MNIST</h3>
<hr />
<p><img src="05-classification-nnets-figures/globalsum-1.png" alt="plot of chunk globalsum" style="width:80.0%" /></p>
<hr />
<p><img src="05-classification-nnets-figures/globalsum2-1.png" alt="plot of chunk globalsum2" style="width:80.0%" /></p>
<hr />
<p>The MNIST dataset is a classic, although its use in research is discouraged nowadays
– the dataset is not considered challenging anymore – state of the art classifiers
can reach <span class="math inline">\(99.8\%\)</span> accuracy.</p>
<p>See Zalando’s Fashion-MNIST (by Kashif Rasul &amp; Han Xiao) at
<a href="https://github.com/zalandoresearch/fashion-mnist" class="uri">https://github.com/zalandoresearch/fashion-mnist</a> for a modern replacement.</p>
<p>Alternatively, take a look at CIFAR-10 and CIFAR-100 (<a href="https://www.cs.toronto.edu/~kriz/cifar.html" class="uri">https://www.cs.toronto.edu/~kriz/cifar.html</a>)
by A. Krizhevsky et al.
or at ImageNet (<a href="http://image-net.org/index" class="uri">http://image-net.org/index</a>) for an even greater challenge.</p>
<!--Common tricks:-->
<!-- https://github.com/afshinea/stanford-cs-230-deep-learning/blob/master/en/cheatsheet-deep-learning-tips-tricks.pdf -->
</div>
<div id="further-reading-4" class="section level3">
<h3><span class="header-section-number">5.6.3</span> Further Reading</h3>
<div id="section-4" class="section level4 allowframebreaks unnumbered">
<h4></h4>
<p>Recommended further reading:</p>
<ul>
<li><span class="citation">(James et al. <a href="references.html#ref-islr">2017</a>: Chapter 11)</span></li>
<li><span class="citation">(Goodfellow et al. <a href="references.html#ref-deeplearn">2016</a>)</span></li>
</ul>
<p>Other:</p>
<ul>
<li>keras package tutorials available at:
<a href="https://cran.r-project.org/web/packages/keras/index.html" class="uri">https://cran.r-project.org/web/packages/keras/index.html</a>
and <a href="https://keras.rstudio.com" class="uri">https://keras.rstudio.com</a></li>
</ul>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-with-trees-and-linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="optimisation-with-iterative-algorithms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
