<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Simple Linear Regression | Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Simple Linear Regression | Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Simple Linear Regression | Machine Learning Classics with R" />
  
  <meta name="twitter:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  

<meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap-trees.html"/>
<link rel="next" href="chap-regression-multiple.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

div.remark {
    border-left: 2px solid gray;
    padding-left: 1em;
}

div.definition {
    font-style: italic;
    border-left: 2px solid #550000;
    padding-left: 1em;
}

</style>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="chap-introduction.html"><a href="chap-introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="chap-introduction.html"><a href="chap-introduction.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a><ul>
<li class="chapter" data-level="1.1.1" data-path="chap-introduction.html"><a href="chap-introduction.html#data-sources"><i class="fa fa-check"></i><b>1.1.1</b> Data Sources</a></li>
<li class="chapter" data-level="1.1.2" data-path="chap-introduction.html"><a href="chap-introduction.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.2</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chap-introduction.html"><a href="chap-introduction.html#input-data-x"><i class="fa fa-check"></i><b>1.2</b> Input Data, <strong>X</strong></a><ul>
<li class="chapter" data-level="1.2.1" data-path="chap-introduction.html"><a href="chap-introduction.html#abstract-formalism"><i class="fa fa-check"></i><b>1.2.1</b> Abstract Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="chap-introduction.html"><a href="chap-introduction.html#concrete-example"><i class="fa fa-check"></i><b>1.2.2</b> Concrete Example</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="chap-introduction.html"><a href="chap-introduction.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.3</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="1.3.1" data-path="chap-introduction.html"><a href="chap-introduction.html#dimensionality-reduction"><i class="fa fa-check"></i><b>1.3.1</b> Dimensionality Reduction</a></li>
<li class="chapter" data-level="1.3.2" data-path="chap-introduction.html"><a href="chap-introduction.html#anomaly-detection"><i class="fa fa-check"></i><b>1.3.2</b> Anomaly Detection</a></li>
<li class="chapter" data-level="1.3.3" data-path="chap-introduction.html"><a href="chap-introduction.html#clustering"><i class="fa fa-check"></i><b>1.3.3</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="chap-introduction.html"><a href="chap-introduction.html#supervised-learning"><i class="fa fa-check"></i><b>1.4</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.4.1" data-path="chap-introduction.html"><a href="chap-introduction.html#desired-outputs-y"><i class="fa fa-check"></i><b>1.4.1</b> Desired Outputs, <strong>y</strong></a></li>
<li class="chapter" data-level="1.4.2" data-path="chap-introduction.html"><a href="chap-introduction.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.4.2</b> Types of Supervised Learning Problems</a></li>
<li class="chapter" data-level="1.4.3" data-path="chap-introduction.html"><a href="chap-introduction.html#one-dataset-many-problems"><i class="fa fa-check"></i><b>1.4.3</b> One Dataset – Many Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-hclust.html"><a href="chap-hclust.html"><i class="fa fa-check"></i><b>2</b> Agglomerative Hierarchical Clustering</a><ul>
<li class="chapter" data-level="2.1" data-path="chap-hclust.html"><a href="chap-hclust.html#dataset-partitions"><i class="fa fa-check"></i><b>2.1</b> Dataset Partitions</a><ul>
<li class="chapter" data-level="2.1.1" data-path="chap-hclust.html"><a href="chap-hclust.html#label-vectors"><i class="fa fa-check"></i><b>2.1.1</b> Label Vectors</a></li>
<li class="chapter" data-level="2.1.2" data-path="chap-hclust.html"><a href="chap-hclust.html#k-partitions"><i class="fa fa-check"></i><b>2.1.2</b> K-Partitions</a></li>
<li class="chapter" data-level="2.1.3" data-path="chap-hclust.html"><a href="chap-hclust.html#interesting-partitions"><i class="fa fa-check"></i><b>2.1.3</b> “Interesting” Partitions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="chap-hclust.html"><a href="chap-hclust.html#sec:euclidean"><i class="fa fa-check"></i><b>2.2</b> Euclidean Distance</a></li>
<li class="chapter" data-level="2.3" data-path="chap-hclust.html"><a href="chap-hclust.html#hierarchical-clustering-at-a-glance"><i class="fa fa-check"></i><b>2.3</b> Hierarchical Clustering at a Glance</a></li>
<li class="chapter" data-level="2.4" data-path="chap-hclust.html"><a href="chap-hclust.html#cluster-dendrograms"><i class="fa fa-check"></i><b>2.4</b> Cluster Dendrograms</a></li>
<li class="chapter" data-level="2.5" data-path="chap-hclust.html"><a href="chap-hclust.html#linkage-functions"><i class="fa fa-check"></i><b>2.5</b> Linkage Functions</a></li>
<li class="chapter" data-level="2.6" data-path="chap-hclust.html"><a href="chap-hclust.html#exercises-in-r"><i class="fa fa-check"></i><b>2.6</b> Exercises in R</a></li>
<li class="chapter" data-level="2.7" data-path="chap-hclust.html"><a href="chap-hclust.html#remarks"><i class="fa fa-check"></i><b>2.7</b> Remarks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-knn.html"><a href="chap-knn.html"><i class="fa fa-check"></i><b>3</b> Classification and Regression with K-Nearest Neighbours</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="chap-knn.html"><a href="chap-knn.html#classification-task"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="chap-knn.html"><a href="chap-knn.html#data"><i class="fa fa-check"></i><b>3.1.2</b> Data</a></li>
<li class="chapter" data-level="3.1.3" data-path="chap-knn.html"><a href="chap-knn.html#training-and-test-sets"><i class="fa fa-check"></i><b>3.1.3</b> Training and Test Sets</a></li>
<li class="chapter" data-level="3.1.4" data-path="chap-knn.html"><a href="chap-knn.html#discussed-methods"><i class="fa fa-check"></i><b>3.1.4</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="chap-knn.html"><a href="chap-knn.html#k-nearest-neighbour-classifier"><i class="fa fa-check"></i><b>3.2</b> K-nearest Neighbour Classifier</a><ul>
<li class="chapter" data-level="3.2.1" data-path="chap-knn.html"><a href="chap-knn.html#introduction-1"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="chap-knn.html"><a href="chap-knn.html#example-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="chap-knn.html"><a href="chap-knn.html#feature-engineering"><i class="fa fa-check"></i><b>3.2.3</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="chap-knn.html"><a href="chap-knn.html#model-assessment-and-selection"><i class="fa fa-check"></i><b>3.3</b> Model Assessment and Selection</a><ul>
<li class="chapter" data-level="3.3.1" data-path="chap-knn.html"><a href="chap-knn.html#performance-metrics"><i class="fa fa-check"></i><b>3.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="3.3.2" data-path="chap-knn.html"><a href="chap-knn.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>3.3.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="3.3.3" data-path="chap-knn.html"><a href="chap-knn.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>3.3.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="chap-knn.html"><a href="chap-knn.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>3.4</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="chap-knn.html"><a href="chap-knn.html#factor-data-type"><i class="fa fa-check"></i><b>3.4.1</b> Factor Data Type</a></li>
<li class="chapter" data-level="3.4.2" data-path="chap-knn.html"><a href="chap-knn.html#main-routine"><i class="fa fa-check"></i><b>3.4.2</b> Main Routine (*)</a></li>
<li class="chapter" data-level="3.4.3" data-path="chap-knn.html"><a href="chap-knn.html#mode"><i class="fa fa-check"></i><b>3.4.3</b> Mode</a></li>
<li class="chapter" data-level="3.4.4" data-path="chap-knn.html"><a href="chap-knn.html#nn-search-routines"><i class="fa fa-check"></i><b>3.4.4</b> NN Search Routines (*)</a></li>
<li class="chapter" data-level="3.4.5" data-path="chap-knn.html"><a href="chap-knn.html#different-metrics"><i class="fa fa-check"></i><b>3.4.5</b> Different Metrics (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="chap-knn.html"><a href="chap-knn.html#exercises"><i class="fa fa-check"></i><b>3.5</b> Exercises</a><ul>
<li class="chapter" data-level="3.5.1" data-path="chap-knn.html"><a href="chap-knn.html#wine-quality-best-k-nn-parameters-via-cross-validation"><i class="fa fa-check"></i><b>3.5.1</b> Wine Quality – Best K-NN Parameters via Cross-Validation (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="chap-knn.html"><a href="chap-knn.html#outro"><i class="fa fa-check"></i><b>3.6</b> Outro</a><ul>
<li class="chapter" data-level="3.6.1" data-path="chap-knn.html"><a href="chap-knn.html#remarks-1"><i class="fa fa-check"></i><b>3.6.1</b> Remarks</a></li>
<li class="chapter" data-level="3.6.2" data-path="chap-knn.html"><a href="chap-knn.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>3.6.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="3.6.3" data-path="chap-knn.html"><a href="chap-knn.html#further-reading"><i class="fa fa-check"></i><b>3.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-trees.html"><a href="chap-trees.html"><i class="fa fa-check"></i><b>4</b> Decision and Regression Trees</a><ul>
<li class="chapter" data-level="4.1" data-path="chap-trees.html"><a href="chap-trees.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="chap-trees.html"><a href="chap-trees.html#classification-task-1"><i class="fa fa-check"></i><b>4.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="4.1.2" data-path="chap-trees.html"><a href="chap-trees.html#data-1"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="chap-trees.html"><a href="chap-trees.html#decision-trees"><i class="fa fa-check"></i><b>4.2</b> Decision Trees</a><ul>
<li class="chapter" data-level="4.2.1" data-path="chap-trees.html"><a href="chap-trees.html#introduction-3"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="chap-trees.html"><a href="chap-trees.html#example-in-r-1"><i class="fa fa-check"></i><b>4.2.2</b> Example in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="chap-trees.html"><a href="chap-trees.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>4.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="chap-trees.html"><a href="chap-trees.html#exercises-1"><i class="fa fa-check"></i><b>4.3</b> Exercises</a><ul>
<li class="chapter" data-level="4.3.1" data-path="chap-trees.html"><a href="chap-trees.html#edstats-where-girls-are-better-at-maths-than-boys"><i class="fa fa-check"></i><b>4.3.1</b> EdStats – Where Girls Are Better at Maths Than Boys?</a></li>
<li class="chapter" data-level="4.3.2" data-path="chap-trees.html"><a href="chap-trees.html#edstats-and-world-factbook-joining-forces"><i class="fa fa-check"></i><b>4.3.2</b> EdStats and World Factbook – Joining Forces</a></li>
<li class="chapter" data-level="4.3.3" data-path="chap-trees.html"><a href="chap-trees.html#wine-quality-random-forest-and-xgboost"><i class="fa fa-check"></i><b>4.3.3</b> Wine Quality – Random Forest and XGBoost (*)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chap-trees.html"><a href="chap-trees.html#outro-1"><i class="fa fa-check"></i><b>4.4</b> Outro</a><ul>
<li class="chapter" data-level="4.4.1" data-path="chap-trees.html"><a href="chap-trees.html#remarks-2"><i class="fa fa-check"></i><b>4.4.1</b> Remarks</a></li>
<li class="chapter" data-level="4.4.2" data-path="chap-trees.html"><a href="chap-trees.html#further-reading-1"><i class="fa fa-check"></i><b>4.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html"><i class="fa fa-check"></i><b>5</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#simple-regression"><i class="fa fa-check"></i><b>5.1</b> Simple Regression</a><ul>
<li class="chapter" data-level="5.1.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#introduction-4"><i class="fa fa-check"></i><b>5.1.1</b> Introduction</a></li>
<li class="chapter" data-level="5.1.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#search-space-and-objective"><i class="fa fa-check"></i><b>5.1.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#introduction-5"><i class="fa fa-check"></i><b>5.2.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#solution-in-r"><i class="fa fa-check"></i><b>5.2.2</b> Solution in R</a></li>
<li class="chapter" data-level="5.2.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#analytic-solution"><i class="fa fa-check"></i><b>5.2.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="5.2.4" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>5.2.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#exercises-2"><i class="fa fa-check"></i><b>5.3</b> Exercises</a><ul>
<li class="chapter" data-level="5.3.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>5.3.1</b> The Anscombe Quartet</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#median-house-value-in-boston"><i class="fa fa-check"></i><b>5.3.2</b> Median House Value in Boston</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#outro-2"><i class="fa fa-check"></i><b>5.4</b> Outro</a><ul>
<li class="chapter" data-level="5.4.1" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#remarks-3"><i class="fa fa-check"></i><b>5.4.1</b> Remarks</a></li>
<li class="chapter" data-level="5.4.2" data-path="chap-regression-simple.html"><a href="chap-regression-simple.html#further-reading-2"><i class="fa fa-check"></i><b>5.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html"><i class="fa fa-check"></i><b>6</b> Multiple Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#introduction-6"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#formalism"><i class="fa fa-check"></i><b>6.1.1</b> Formalism</a></li>
<li class="chapter" data-level="6.1.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>6.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#multiple-linear-regression"><i class="fa fa-check"></i><b>6.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#problem-formulation"><i class="fa fa-check"></i><b>6.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="6.2.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>6.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#finding-the-best-model"><i class="fa fa-check"></i><b>6.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="6.3.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#model-diagnostics"><i class="fa fa-check"></i><b>6.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="6.3.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#variable-selection"><i class="fa fa-check"></i><b>6.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="6.3.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#variable-transformation"><i class="fa fa-check"></i><b>6.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="6.3.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>6.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#exercises-3"><i class="fa fa-check"></i><b>6.4</b> Exercises</a><ul>
<li class="chapter" data-level="6.4.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>6.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="6.4.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>6.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="6.4.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>6.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="6.4.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>6.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="6.4.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>6.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
<li class="chapter" data-level="6.4.6" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#median-house-value-in-boston-continued"><i class="fa fa-check"></i><b>6.4.6</b> Median House Value in Boston (Continued)</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#outro-3"><i class="fa fa-check"></i><b>6.5</b> Outro</a><ul>
<li class="chapter" data-level="6.5.1" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#remarks-4"><i class="fa fa-check"></i><b>6.5.1</b> Remarks</a></li>
<li class="chapter" data-level="6.5.2" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#other-methods-for-regression"><i class="fa fa-check"></i><b>6.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="6.5.3" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>6.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="6.5.4" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>6.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="6.5.5" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>6.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="6.5.6" data-path="chap-regression-multiple.html"><a href="chap-regression-multiple.html#further-reading-3"><i class="fa fa-check"></i><b>6.5.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-logistic.html"><a href="chap-logistic.html"><i class="fa fa-check"></i><b>7</b> Classification with Linear Models</a><ul>
<li class="chapter" data-level="7.1" data-path="chap-logistic.html"><a href="chap-logistic.html#introduction-7"><i class="fa fa-check"></i><b>7.1</b> Introduction</a><ul>
<li class="chapter" data-level="7.1.1" data-path="chap-logistic.html"><a href="chap-logistic.html#classification-task-2"><i class="fa fa-check"></i><b>7.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="7.1.2" data-path="chap-logistic.html"><a href="chap-logistic.html#data-2"><i class="fa fa-check"></i><b>7.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="chap-logistic.html"><a href="chap-logistic.html#binary-logistic-regression"><i class="fa fa-check"></i><b>7.2</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="chap-logistic.html"><a href="chap-logistic.html#motivation"><i class="fa fa-check"></i><b>7.2.1</b> Motivation</a></li>
<li class="chapter" data-level="7.2.2" data-path="chap-logistic.html"><a href="chap-logistic.html#logistic-model"><i class="fa fa-check"></i><b>7.2.2</b> Logistic Model</a></li>
<li class="chapter" data-level="7.2.3" data-path="chap-logistic.html"><a href="chap-logistic.html#example-in-r-2"><i class="fa fa-check"></i><b>7.2.3</b> Example in R</a></li>
<li class="chapter" data-level="7.2.4" data-path="chap-logistic.html"><a href="chap-logistic.html#loss-function-cross-entropy"><i class="fa fa-check"></i><b>7.2.4</b> Loss Function: Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chap-logistic.html"><a href="chap-logistic.html#exercises-4"><i class="fa fa-check"></i><b>7.3</b> Exercises</a><ul>
<li class="chapter" data-level="7.3.1" data-path="chap-logistic.html"><a href="chap-logistic.html#edstats-fitting-of-binary-logistic-regression-models"><i class="fa fa-check"></i><b>7.3.1</b> EdStats – Fitting of Binary Logistic Regression Models</a></li>
<li class="chapter" data-level="7.3.2" data-path="chap-logistic.html"><a href="chap-logistic.html#edstats-variable-selection-in-binary-logistic-regression"><i class="fa fa-check"></i><b>7.3.2</b> EdStats – Variable Selection in Binary Logistic Regression (*)</a></li>
<li class="chapter" data-level="7.3.3" data-path="chap-logistic.html"><a href="chap-logistic.html#currency-exchange-rates-growthfall"><i class="fa fa-check"></i><b>7.3.3</b> Currency Exchange Rates Growth/Fall</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="chap-logistic.html"><a href="chap-logistic.html#outro-4"><i class="fa fa-check"></i><b>7.4</b> Outro</a><ul>
<li class="chapter" data-level="7.4.1" data-path="chap-logistic.html"><a href="chap-logistic.html#remarks-5"><i class="fa fa-check"></i><b>7.4.1</b> Remarks</a></li>
<li class="chapter" data-level="7.4.2" data-path="chap-logistic.html"><a href="chap-logistic.html#further-reading-4"><i class="fa fa-check"></i><b>7.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html"><i class="fa fa-check"></i><b>8</b> Continuous Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#introduction-8"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#optimisation-problems"><i class="fa fa-check"></i><b>8.1.1</b> Optimisation Problems</a></li>
<li class="chapter" data-level="8.1.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>8.1.2</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="8.1.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>8.1.3</b> Example Objective over a 2D Domain</a></li>
<li class="chapter" data-level="8.1.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>8.1.4</b> Example Optimisation Problems in Machine Learning</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#iterative-methods"><i class="fa fa-check"></i><b>8.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="8.2.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#introduction-9"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-in-r-3"><i class="fa fa-check"></i><b>8.2.2</b> Example in R</a></li>
<li class="chapter" data-level="8.2.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>8.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="8.2.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#random-restarts"><i class="fa fa-check"></i><b>8.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#gradient-descent"><i class="fa fa-check"></i><b>8.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="8.3.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#function-gradient"><i class="fa fa-check"></i><b>8.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="8.3.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>8.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="8.3.3" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>8.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="8.3.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#example-mnist"><i class="fa fa-check"></i><b>8.3.4</b> Example: MNIST (*)</a></li>
<li class="chapter" data-level="8.3.5" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>8.3.5</b> Stochastic Gradient Descent (SGD) (*)</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>8.4</b> A Note on Convex Optimisation (*)</a></li>
<li class="chapter" data-level="8.5" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#outro-5"><i class="fa fa-check"></i><b>8.5</b> Outro</a><ul>
<li class="chapter" data-level="8.5.1" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#remarks-6"><i class="fa fa-check"></i><b>8.5.1</b> Remarks</a></li>
<li class="chapter" data-level="8.5.2" data-path="chap-optimisation-continuous.html"><a href="chap-optimisation-continuous.html#further-reading-5"><i class="fa fa-check"></i><b>8.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-kmeans.html"><a href="chap-kmeans.html"><i class="fa fa-check"></i><b>9</b> Clustering with K-Means</a><ul>
<li class="chapter" data-level="9.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#k-means-clustering"><i class="fa fa-check"></i><b>9.1</b> K-means Clustering</a><ul>
<li class="chapter" data-level="9.1.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#example-in-r-4"><i class="fa fa-check"></i><b>9.1.1</b> Example in R</a></li>
<li class="chapter" data-level="9.1.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#problem-statement"><i class="fa fa-check"></i><b>9.1.2</b> Problem Statement</a></li>
<li class="chapter" data-level="9.1.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>9.1.3</b> Algorithms for the K-means Problem</a></li>
<li class="chapter" data-level="9.1.4" data-path="chap-kmeans.html"><a href="chap-kmeans.html#k-means-revisited"><i class="fa fa-check"></i><b>9.1.4</b> K-means Revisited</a></li>
<li class="chapter" data-level="9.1.5" data-path="chap-kmeans.html"><a href="chap-kmeans.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>9.1.5</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#exercises-5"><i class="fa fa-check"></i><b>9.2</b> Exercises</a><ul>
<li class="chapter" data-level="9.2.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#clustering-of-the-world-factbook"><i class="fa fa-check"></i><b>9.2.1</b> Clustering of the World Factbook</a></li>
<li class="chapter" data-level="9.2.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#unbalance-dataset-k-means-needs-multiple-starts"><i class="fa fa-check"></i><b>9.2.2</b> Unbalance Dataset – K-Means Needs Multiple Starts</a></li>
<li class="chapter" data-level="9.2.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#clustering-of-typical-2d-benchmark-datasets"><i class="fa fa-check"></i><b>9.2.3</b> Clustering of Typical 2D Benchmark Datasets</a></li>
<li class="chapter" data-level="9.2.4" data-path="chap-kmeans.html"><a href="chap-kmeans.html#wine-quality-volatile.acidity-and-sulphates"><i class="fa fa-check"></i><b>9.2.4</b> Wine Quality – <code>volatile.acidity</code> and <code>sulphates</code></a></li>
<li class="chapter" data-level="9.2.5" data-path="chap-kmeans.html"><a href="chap-kmeans.html#wine-quality-chlorides-and-total.sulfur.dioxide"><i class="fa fa-check"></i><b>9.2.5</b> Wine Quality – <code>chlorides</code> and <code>total.sulfur.dioxide</code></a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="chap-kmeans.html"><a href="chap-kmeans.html#outro-6"><i class="fa fa-check"></i><b>9.3</b> Outro</a><ul>
<li class="chapter" data-level="9.3.1" data-path="chap-kmeans.html"><a href="chap-kmeans.html#remarks-7"><i class="fa fa-check"></i><b>9.3.1</b> Remarks</a></li>
<li class="chapter" data-level="9.3.2" data-path="chap-kmeans.html"><a href="chap-kmeans.html#further-reading-6"><i class="fa fa-check"></i><b>9.3.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html"><i class="fa fa-check"></i><b>10</b> Discrete Optimisation</a><ul>
<li class="chapter" data-level="10.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#introduction-10"><i class="fa fa-check"></i><b>10.1</b> Introduction</a><ul>
<li class="chapter" data-level="10.1.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#recap"><i class="fa fa-check"></i><b>10.1.1</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#outro-7"><i class="fa fa-check"></i><b>10.2</b> Outro</a><ul>
<li class="chapter" data-level="10.2.1" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#remarks-8"><i class="fa fa-check"></i><b>10.2.1</b> Remarks</a></li>
<li class="chapter" data-level="10.2.2" data-path="chap-optimisation-discrete.html"><a href="chap-optimisation-discrete.html#further-reading-7"><i class="fa fa-check"></i><b>10.2.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-images.html"><a href="chap-images.html"><i class="fa fa-check"></i><b>11</b> Shallow and Deep Neural Networks</a><ul>
<li class="chapter" data-level="11.1" data-path="chap-images.html"><a href="chap-images.html#introduction-11"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="chap-images.html"><a href="chap-images.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>11.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="11.1.2" data-path="chap-images.html"><a href="chap-images.html#data-3"><i class="fa fa-check"></i><b>11.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="chap-images.html"><a href="chap-images.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>11.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="chap-images.html"><a href="chap-images.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>11.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="11.2.2" data-path="chap-images.html"><a href="chap-images.html#extending-logistic-regression"><i class="fa fa-check"></i><b>11.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="11.2.3" data-path="chap-images.html"><a href="chap-images.html#softmax-function"><i class="fa fa-check"></i><b>11.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="11.2.4" data-path="chap-images.html"><a href="chap-images.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>11.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="11.2.5" data-path="chap-images.html"><a href="chap-images.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>11.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="11.2.6" data-path="chap-images.html"><a href="chap-images.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>11.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="chap-images.html"><a href="chap-images.html#artificial-neural-networks"><i class="fa fa-check"></i><b>11.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="11.3.1" data-path="chap-images.html"><a href="chap-images.html#artificial-neuron"><i class="fa fa-check"></i><b>11.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="11.3.2" data-path="chap-images.html"><a href="chap-images.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>11.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="11.3.3" data-path="chap-images.html"><a href="chap-images.html#example-in-r-5"><i class="fa fa-check"></i><b>11.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="chap-images.html"><a href="chap-images.html#deep-neural-networks"><i class="fa fa-check"></i><b>11.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="11.4.1" data-path="chap-images.html"><a href="chap-images.html#introduction-12"><i class="fa fa-check"></i><b>11.4.1</b> Introduction</a></li>
<li class="chapter" data-level="11.4.2" data-path="chap-images.html"><a href="chap-images.html#activation-functions"><i class="fa fa-check"></i><b>11.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="11.4.3" data-path="chap-images.html"><a href="chap-images.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>11.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="11.4.4" data-path="chap-images.html"><a href="chap-images.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>11.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="chap-images.html"><a href="chap-images.html#preprocessing-of-data"><i class="fa fa-check"></i><b>11.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="11.5.1" data-path="chap-images.html"><a href="chap-images.html#introduction-13"><i class="fa fa-check"></i><b>11.5.1</b> Introduction</a></li>
<li class="chapter" data-level="11.5.2" data-path="chap-images.html"><a href="chap-images.html#image-deskewing"><i class="fa fa-check"></i><b>11.5.2</b> Image Deskewing</a></li>
<li class="chapter" data-level="11.5.3" data-path="chap-images.html"><a href="chap-images.html#summary-of-all-the-models-considered"><i class="fa fa-check"></i><b>11.5.3</b> Summary of All the Models Considered</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="chap-images.html"><a href="chap-images.html#outro-8"><i class="fa fa-check"></i><b>11.6</b> Outro</a><ul>
<li class="chapter" data-level="11.6.1" data-path="chap-images.html"><a href="chap-images.html#remarks-9"><i class="fa fa-check"></i><b>11.6.1</b> Remarks</a></li>
<li class="chapter" data-level="11.6.2" data-path="chap-images.html"><a href="chap-images.html#beyond-mnist"><i class="fa fa-check"></i><b>11.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="11.6.3" data-path="chap-images.html"><a href="chap-images.html#further-reading-8"><i class="fa fa-check"></i><b>11.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-recommenders.html"><a href="chap-recommenders.html"><i class="fa fa-check"></i><b>12</b> Recommender Systems</a><ul>
<li class="chapter" data-level="12.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#introduction-14"><i class="fa fa-check"></i><b>12.1</b> Introduction</a><ul>
<li class="chapter" data-level="12.1.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#the-netflix-prize"><i class="fa fa-check"></i><b>12.1.1</b> The Netflix Prize</a></li>
<li class="chapter" data-level="12.1.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#main-approaches"><i class="fa fa-check"></i><b>12.1.2</b> Main Approaches</a></li>
<li class="chapter" data-level="12.1.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#formalism-1"><i class="fa fa-check"></i><b>12.1.3</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#collaborative-filtering"><i class="fa fa-check"></i><b>12.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="12.2.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#example"><i class="fa fa-check"></i><b>12.2.1</b> Example</a></li>
<li class="chapter" data-level="12.2.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#similarity-measures"><i class="fa fa-check"></i><b>12.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="12.2.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>12.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="12.2.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>12.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#exercise-the-movielens-dataset"><i class="fa fa-check"></i><b>12.3</b> Exercise: The MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="12.3.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#dataset"><i class="fa fa-check"></i><b>12.3.1</b> Dataset</a></li>
<li class="chapter" data-level="12.3.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#data-cleansing"><i class="fa fa-check"></i><b>12.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="12.3.3" data-path="chap-recommenders.html"><a href="chap-recommenders.html#item-item-similarities"><i class="fa fa-check"></i><b>12.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="12.3.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#example-recommendations"><i class="fa fa-check"></i><b>12.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="12.3.5" data-path="chap-recommenders.html"><a href="chap-recommenders.html#clustering-1"><i class="fa fa-check"></i><b>12.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="chap-recommenders.html"><a href="chap-recommenders.html#outro-9"><i class="fa fa-check"></i><b>12.4</b> Outro</a><ul>
<li class="chapter" data-level="12.4.1" data-path="chap-recommenders.html"><a href="chap-recommenders.html#remarks-10"><i class="fa fa-check"></i><b>12.4.1</b> Remarks</a></li>
<li class="chapter" data-level="12.4.2" data-path="chap-recommenders.html"><a href="chap-recommenders.html#further-reading-9"><i class="fa fa-check"></i><b>12.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="chap-text.html"><a href="chap-text.html"><i class="fa fa-check"></i><b>13</b> Natural Language Processing</a><ul>
<li class="chapter" data-level="13.1" data-path="chap-text.html"><a href="chap-text.html#to-do"><i class="fa fa-check"></i><b>13.1</b> TO DO</a></li>
<li class="chapter" data-level="13.2" data-path="chap-text.html"><a href="chap-text.html#further-reading-10"><i class="fa fa-check"></i><b>13.2</b> Further Reading</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-convention.html"><a href="appendix-convention.html"><i class="fa fa-check"></i><b>A</b> Notation Convention</a></li>
<li class="chapter" data-level="B" data-path="appendix-rintro.html"><a href="appendix-rintro.html"><i class="fa fa-check"></i><b>B</b> Setting Up the R Environment</a><ul>
<li class="chapter" data-level="B.1" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-r"><i class="fa fa-check"></i><b>B.1</b> Installing R</a></li>
<li class="chapter" data-level="B.2" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-an-ide"><i class="fa fa-check"></i><b>B.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="B.3" data-path="appendix-rintro.html"><a href="appendix-rintro.html#installing-recommended-packages"><i class="fa fa-check"></i><b>B.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="B.4" data-path="appendix-rintro.html"><a href="appendix-rintro.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>B.4</b> First R Script in RStudio</a></li>
<li class="chapter" data-level="B.5" data-path="appendix-rintro.html"><a href="appendix-rintro.html#exercises-6"><i class="fa fa-check"></i><b>B.5</b> Exercises</a><ul>
<li class="chapter" data-level="B.5.1" data-path="appendix-rintro.html"><a href="appendix-rintro.html#first-steps-with-vectors"><i class="fa fa-check"></i><b>B.5.1</b> First Steps with Vectors</a></li>
<li class="chapter" data-level="B.5.2" data-path="appendix-rintro.html"><a href="appendix-rintro.html#basic-plotting"><i class="fa fa-check"></i><b>B.5.2</b> Basic Plotting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appendix-rvector.html"><a href="appendix-rvector.html"><i class="fa fa-check"></i><b>C</b> Vector Algebra in R</a><ul>
<li class="chapter" data-level="C.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#motivation-1"><i class="fa fa-check"></i><b>C.1</b> Motivation</a></li>
<li class="chapter" data-level="C.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#numeric-vectors"><i class="fa fa-check"></i><b>C.2</b> Numeric Vectors</a><ul>
<li class="chapter" data-level="C.2.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>C.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="C.2.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-vector-operations"><i class="fa fa-check"></i><b>C.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#aggregation-functions"><i class="fa fa-check"></i><b>C.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="C.2.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#special-functions"><i class="fa fa-check"></i><b>C.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="C.2.6" data-path="appendix-rvector.html"><a href="appendix-rvector.html#norms-and-distances"><i class="fa fa-check"></i><b>C.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="C.2.7" data-path="appendix-rvector.html"><a href="appendix-rvector.html#dot-product"><i class="fa fa-check"></i><b>C.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="C.2.8" data-path="appendix-rvector.html"><a href="appendix-rvector.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>C.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#logical-vectors"><i class="fa fa-check"></i><b>C.3</b> Logical Vectors</a><ul>
<li class="chapter" data-level="C.3.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-logical-vectors"><i class="fa fa-check"></i><b>C.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="C.3.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#logical-operations"><i class="fa fa-check"></i><b>C.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="C.3.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#comparison-operations"><i class="fa fa-check"></i><b>C.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="C.3.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#aggregation-functions-1"><i class="fa fa-check"></i><b>C.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#character-vectors"><i class="fa fa-check"></i><b>C.4</b> Character Vectors</a><ul>
<li class="chapter" data-level="C.4.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-character-vectors"><i class="fa fa-check"></i><b>C.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="C.4.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>C.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="C.4.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>C.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#vector-subsetting"><i class="fa fa-check"></i><b>C.5</b> Vector Subsetting</a><ul>
<li class="chapter" data-level="C.5.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>C.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="C.5.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>C.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="C.5.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>C.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="C.5.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#replacing-elements"><i class="fa fa-check"></i><b>C.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="C.5.5" data-path="appendix-rvector.html"><a href="appendix-rvector.html#other-functions"><i class="fa fa-check"></i><b>C.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="appendix-rvector.html"><a href="appendix-rvector.html#named-vectors"><i class="fa fa-check"></i><b>C.6</b> Named Vectors</a><ul>
<li class="chapter" data-level="C.6.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-named-vectors"><i class="fa fa-check"></i><b>C.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="C.6.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>C.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="C.7" data-path="appendix-rvector.html"><a href="appendix-rvector.html#factors"><i class="fa fa-check"></i><b>C.7</b> Factors</a><ul>
<li class="chapter" data-level="C.7.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-factors"><i class="fa fa-check"></i><b>C.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="C.7.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#levels"><i class="fa fa-check"></i><b>C.7.2</b> Levels</a></li>
<li class="chapter" data-level="C.7.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#internal-representation"><i class="fa fa-check"></i><b>C.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.8" data-path="appendix-rvector.html"><a href="appendix-rvector.html#lists"><i class="fa fa-check"></i><b>C.8</b> Lists</a><ul>
<li class="chapter" data-level="C.8.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#creating-lists"><i class="fa fa-check"></i><b>C.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="C.8.2" data-path="appendix-rvector.html"><a href="appendix-rvector.html#named-lists"><i class="fa fa-check"></i><b>C.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="C.8.3" data-path="appendix-rvector.html"><a href="appendix-rvector.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>C.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="C.8.4" data-path="appendix-rvector.html"><a href="appendix-rvector.html#common-operations"><i class="fa fa-check"></i><b>C.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="appendix-rvector.html"><a href="appendix-rvector.html#exercises-7"><i class="fa fa-check"></i><b>C.9</b> Exercises</a><ul>
<li class="chapter" data-level="C.9.1" data-path="appendix-rvector.html"><a href="appendix-rvector.html#audeur-exchange-rates"><i class="fa fa-check"></i><b>C.9.1</b> AUD/EUR Exchange Rates</a></li>
</ul></li>
<li class="chapter" data-level="C.10" data-path="appendix-rvector.html"><a href="appendix-rvector.html#further-reading-11"><i class="fa fa-check"></i><b>C.10</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html"><i class="fa fa-check"></i><b>D</b> Matrix Algebra in R</a><ul>
<li class="chapter" data-level="D.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#creating-matrices"><i class="fa fa-check"></i><b>D.1</b> Creating Matrices</a><ul>
<li class="chapter" data-level="D.1.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix"><i class="fa fa-check"></i><b>D.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="D.1.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#stacking-vectors"><i class="fa fa-check"></i><b>D.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="D.1.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>D.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="D.1.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>D.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="D.1.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#other-methods"><i class="fa fa-check"></i><b>D.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="D.1.6" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#internal-representation-1"><i class="fa fa-check"></i><b>D.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#common-operations-1"><i class="fa fa-check"></i><b>D.2</b> Common Operations</a><ul>
<li class="chapter" data-level="D.2.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-transpose"><i class="fa fa-check"></i><b>D.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="D.2.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>D.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="D.2.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>D.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="D.2.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-multiplication"><i class="fa fa-check"></i><b>D.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="D.2.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>D.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="D.2.6" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#vectorised-special-functions"><i class="fa fa-check"></i><b>D.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="D.2.7" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-vector-operations"><i class="fa fa-check"></i><b>D.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#matrix-subsetting"><i class="fa fa-check"></i><b>D.3</b> Matrix Subsetting</a><ul>
<li class="chapter" data-level="D.3.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-individual-elements"><i class="fa fa-check"></i><b>D.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="D.3.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>D.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="D.3.3" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-submatrices"><i class="fa fa-check"></i><b>D.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="D.3.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>D.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="D.3.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>D.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#exercises-8"><i class="fa fa-check"></i><b>D.4</b> Exercises</a><ul>
<li class="chapter" data-level="D.4.1" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#currency-exchange-rates"><i class="fa fa-check"></i><b>D.4.1</b> Currency Exchange Rates</a></li>
<li class="chapter" data-level="D.4.2" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#currency-exchange-rates-relative-to-1999"><i class="fa fa-check"></i><b>D.4.2</b> Currency Exchange Rates Relative to 1999</a></li>
</ul></li>
<li class="chapter" data-level="D.5" data-path="appendix-rmatrix.html"><a href="appendix-rmatrix.html#further-reading-12"><i class="fa fa-check"></i><b>D.5</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="appendix-rdf.html"><a href="appendix-rdf.html"><i class="fa fa-check"></i><b>E</b> Data Frame Wrangling in R</a><ul>
<li class="chapter" data-level="E.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#creating-data-frames"><i class="fa fa-check"></i><b>E.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="E.2" data-path="appendix-rdf.html"><a href="appendix-rdf.html#importing-data-frames"><i class="fa fa-check"></i><b>E.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="E.3" data-path="appendix-rdf.html"><a href="appendix-rdf.html#data-frame-subsetting"><i class="fa fa-check"></i><b>E.3</b> Data Frame Subsetting</a><ul>
<li class="chapter" data-level="E.3.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>E.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="E.3.2" data-path="appendix-rdf.html"><a href="appendix-rdf.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>E.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="appendix-rdf.html"><a href="appendix-rdf.html#common-operations-2"><i class="fa fa-check"></i><b>E.4</b> Common Operations</a></li>
<li class="chapter" data-level="E.5" data-path="appendix-rdf.html"><a href="appendix-rdf.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>E.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="E.6" data-path="appendix-rdf.html"><a href="appendix-rdf.html#exercises-9"><i class="fa fa-check"></i><b>E.6</b> Exercises</a><ul>
<li class="chapter" data-level="E.6.1" data-path="appendix-rdf.html"><a href="appendix-rdf.html#urban-forest"><i class="fa fa-check"></i><b>E.6.1</b> Urban Forest</a></li>
</ul></li>
<li class="chapter" data-level="E.7" data-path="appendix-rdf.html"><a href="appendix-rdf.html#air-quality"><i class="fa fa-check"></i><b>E.7</b> Air Quality</a></li>
<li class="chapter" data-level="E.8" data-path="appendix-rdf.html"><a href="appendix-rdf.html#further-reading-13"><i class="fa fa-check"></i><b>E.8</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="appendix-datasets.html"><a href="appendix-datasets.html"><i class="fa fa-check"></i><b>F</b> Datasets</a><ul>
<li class="chapter" data-level="F.1" data-path="appendix-datasets.html"><a href="appendix-datasets.html#sustainable-society-indices"><i class="fa fa-check"></i><b>F.1</b> Sustainable Society Indices</a></li>
<li class="chapter" data-level="F.2" data-path="appendix-datasets.html"><a href="appendix-datasets.html#air-quality-1"><i class="fa fa-check"></i><b>F.2</b> Air Quality</a></li>
<li class="chapter" data-level="F.3" data-path="appendix-datasets.html"><a href="appendix-datasets.html#currency-exchange-rates-1"><i class="fa fa-check"></i><b>F.3</b> Currency Exchange Rates</a></li>
<li class="chapter" data-level="F.4" data-path="appendix-datasets.html"><a href="appendix-datasets.html#urban-forest-1"><i class="fa fa-check"></i><b>F.4</b> Urban Forest</a></li>
<li class="chapter" data-level="F.5" data-path="appendix-datasets.html"><a href="appendix-datasets.html#wine-quality"><i class="fa fa-check"></i><b>F.5</b> Wine Quality</a></li>
<li class="chapter" data-level="F.6" data-path="appendix-datasets.html"><a href="appendix-datasets.html#the-world-factbook-countries-of-the-world"><i class="fa fa-check"></i><b>F.6</b> The World Factbook (Countries of the World)</a></li>
<li class="chapter" data-level="F.7" data-path="appendix-datasets.html"><a href="appendix-datasets.html#edstats-country-level-education-statistics"><i class="fa fa-check"></i><b>F.7</b> EdStats (Country-Level Education Statistics)</a></li>
<li class="chapter" data-level="F.8" data-path="appendix-datasets.html"><a href="appendix-datasets.html#food-and-nutrient-database-for-dietary-studies-fndds"><i class="fa fa-check"></i><b>F.8</b> Food and Nutrient Database for Dietary Studies (FNDDS)</a></li>
<li class="chapter" data-level="F.9" data-path="appendix-datasets.html"><a href="appendix-datasets.html#clustering-benchmarks"><i class="fa fa-check"></i><b>F.9</b> Clustering Benchmarks</a></li>
<li class="chapter" data-level="F.10" data-path="appendix-datasets.html"><a href="appendix-datasets.html#movie-lens-todo"><i class="fa fa-check"></i><b>F.10</b> Movie Lens (TODO)</a></li>
<li class="chapter" data-level="F.11" data-path="appendix-datasets.html"><a href="appendix-datasets.html#other-todo"><i class="fa fa-check"></i><b>F.11</b> Other (TODO)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.3 2020-06-19 21:18 (d30643a)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap:regression-simple" class="section level1">
<h1><span class="header-section-number">5</span> Simple Linear Regression</h1>
<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->
<!-- TODO: citations


-->
<!-- TODO



move some quality metrics from Ch.2 to Ch.1

what is high correlation? 0.6 is not! time to stop tolerating weak
associations in the social sciences, you can do better, get
more data or don't do what you do... :/

Social sciences is built on  sand

If your data is crap or your models are weak, admit it and give up.




different coefficients of correlation (Spearman, Kendall) → Chapter 2



-->
<div id="simple-regression" class="section level2">
<h2><span class="header-section-number">5.1</span> Simple Regression</h2>
<div id="introduction-4" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Introduction</h3>
<p><strong>Simple regression</strong> is the easiest setting to start with – let’s assume
<span class="math inline">\(p=1\)</span>, i.e., all inputs are 1-dimensional.
Denote <span class="math inline">\(x_i=x_{i,1}\)</span>.</p>
<p>We will use it to build many intuitions, for example, it’ll be easy
to illustrate all the concepts graphically.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb242-1" title="1"><span class="kw">library</span>(<span class="st">&quot;ISLR&quot;</span>) <span class="co"># Credit dataset</span></a>
<a class="sourceLine" id="cb242-2" title="2"><span class="kw">plot</span>(Credit<span class="op">$</span>Balance, Credit<span class="op">$</span>Rating) <span class="co"># scatter plot</span></a></code></pre></div>
<div class="figure">
<img src="05-regression-simple-figures/credit_scatter-1.svg" alt="(#fig:credit_scatter) A scatter plot of Rating vs. Balance" />
<p class="caption">(#fig:credit_scatter) A scatter plot of Rating vs. Balance</p>
</div>
<p>In what follows we will be modelling the Credit Rating (<span class="math inline">\(Y\)</span>)
as a function of the average Credit Card Balance (<span class="math inline">\(X\)</span>) in USD
for customers <em>with positive Balance only</em>.
It is because it is evident from Figure @ref(fig:credit_scatter)
that some customers with zero balance obtained a credit rating
based on some external data source that we don’t have access to in
our very setting.</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb243-1" title="1">X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">as.numeric</span>(Credit<span class="op">$</span>Balance[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>]))</a>
<a class="sourceLine" id="cb243-2" title="2">Y &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">as.numeric</span>(Credit<span class="op">$</span>Rating[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>]))</a></code></pre></div>
<p>Figure @ref(fig:credit_XY_plot) gives the updated scatter plot
with the zero-balance clients “taken care of”.</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb244-1" title="1"><span class="kw">plot</span>(X, Y, <span class="dt">xlab=</span><span class="st">&quot;X (Balance)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Y (Rating)&quot;</span>)</a></code></pre></div>
<div class="figure">
<img src="05-regression-simple-figures/credit_XY_plot-1.svg" alt="(#fig:credit_XY_plot) A scatter plot of Rating vs. Balance with clients of Balance=0 removed" />
<p class="caption">(#fig:credit_XY_plot) A scatter plot of Rating vs. Balance with clients of Balance=0 removed</p>
</div>
<p>Our aim is to construct a function <span class="math inline">\(f\)</span> that
<strong>models</strong> Rating as a function of Balance,
<span class="math inline">\(f(X)=Y\)</span>.</p>
<p>We are equipped with <span class="math inline">\(n=310\)</span> reference (observed) Ratings
<span class="math inline">\(\mathbf{y}=[y_1\ \cdots\ y_n]^T\)</span>
for particular Balances <span class="math inline">\(\mathbf{x}=[x_1\ \cdots\ x_n]^T\)</span>.</p>
<p>Note the following naming conventions:</p>
<ul>
<li><p>Variable types:</p>
<ul>
<li><p><span class="math inline">\(X\)</span> – independent/explanatory/predictor variable</p></li>
<li><p><span class="math inline">\(Y\)</span> – dependent/response/predicted variable</p></li>
</ul></li>
<li><p>Also note that:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> – idealisation (any possible Rating)</p></li>
<li><p><span class="math inline">\(\mathbf{y}=[y_1\ \cdots\ y_n]^T\)</span> – values actually observed</p></li>
</ul></li>
</ul>
<p>The model will not be ideal, but it might be usable:</p>
<ul>
<li><p>We will be able to <strong>predict</strong> the rating of any new client.</p>
<p>What should be the rating of a client with Balance of $1500?</p>
<p>What should be the rating of a client with Balance of $2500?</p></li>
<li><p>We will be able to <strong>describe</strong> (understand) this reality using a single mathematical formula
so as to infer that there is an association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></p>
<p>Think of “data compression” and laws of physics, e.g., <span class="math inline">\(E=mc^2\)</span>.</p></li>
</ul>
<p>(*) Mathematically, we will assume that there is some “true” function that models the data
(true relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>),
but the observed outputs are subject to <strong>additive error</strong>:
<span class="math display">\[Y=f(X)+\varepsilon.\]</span></p>
<p><span class="math inline">\(\varepsilon\)</span> is a random term, classically we assume that
errors are independent of each other,
have expected value of <span class="math inline">\(0\)</span> (there is no systematic error = unbiased)
and that they follow a normal distribution.</p>
<p>(*) We denote this as <span class="math inline">\(\varepsilon\sim\mathcal{N}(0, \sigma)\)</span>
(read: random variable <span class="math inline">\(\varepsilon\)</span> follows a normal distribution
with expected value of <span class="math inline">\(0\)</span> and standard deviation of <span class="math inline">\(\sigma\)</span> for some <span class="math inline">\(\sigma\ge 0\)</span>).</p>
<p><span class="math inline">\(\sigma\)</span> controls the amount of noise (and hence, uncertainty).
Figure @ref(fig:normal_distribs) gives the plot of the probability
distribution function (PDFs, densities)
of <span class="math inline">\(\mathcal{N}(0, \sigma)\)</span> for different <span class="math inline">\(\sigma\)</span>s:</p>
<div class="figure">
<img src="05-regression-simple-figures/normal_distribs-1.svg" alt="(#fig:normal_distribs) Probability density functions of normal distributions with different standard deviations \sigma." />
<p class="caption">(#fig:normal_distribs) Probability density functions of normal distributions with different standard deviations <span class="math inline">\(\sigma\)</span>.</p>
</div>
</div>
<div id="search-space-and-objective" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Search Space and Objective</h3>
<p>There are many different functions that can be <strong>fitted</strong> into
the observed <span class="math inline">\((\mathbf{x},\mathbf{y})\)</span>,
compare Figure @ref(fig:credit_different_models).
Some of them are better than the other (with respect to
different aspects, such as fit quality, simplicity etc.).</p>
<div class="figure">
<img src="05-regression-simple-figures/credit_different_models-1.svg" alt="(#fig:credit_different_models) Different polynomial models fitted to data" />
<p class="caption">(#fig:credit_different_models) Different polynomial models fitted to data</p>
</div>
<p>Thus, we need a formal <strong>model selection criterion</strong>
that could enable as to tackle the model fitting
task on a computer.</p>
<p>Usually, we will be interested in a model
that minimises appropriately aggregated <strong>residuals</strong>
<span class="math inline">\(f(x_i)-y_i\)</span>, i.e.,
<strong>predicted outputs minus observed outputs</strong>,
often denoted with <span class="math inline">\(\hat{y}_i-y_i\)</span>,
for <span class="math inline">\(i=1,\dots,n\)</span>.</p>
<div class="figure">
<img src="05-regression-simple-figures/credit_residuals-1.svg" alt="(#fig:credit_residuals) Residuals are defined as the differences between the predicted and observed outputs \hat{y}_i-y_i" />
<p class="caption">(#fig:credit_residuals) Residuals are defined as the differences between the predicted and observed outputs <span class="math inline">\(\hat{y}_i-y_i\)</span></p>
</div>
<p>In Figure @ref(fig:credit_residuals), the residuals correspond to the
lengths of the dashed line segments – they measure the discrepancy between
the outputs generated by the model (what we get)
and the true outputs (what we want).</p>
<p>Top choice: sum of squared residuals:
<span class="math display">\[
\begin{array}{rl}
\mathrm{SSR}(f|\mathbf{x},\mathbf{y})
&amp; = \left( f(x_1)-y_1 \right)^2 + \dots + \left( f(x_n)-y_n \right)^2 \\
&amp; =
\displaystyle\sum_{i=1}^n \left( f(x_i)-y_i \right)^2
\end{array}
\]</span></p>
<dl>
<dt>Remark.</dt>
<dd><p>The notation <span class="math inline">\(\mathrm{SSR}(f|\mathbf{x},\mathbf{y})\)</span> means that
it is the error measure
corresponding to the model <span class="math inline">\((f)\)</span> <em>given</em> our data.<br />
We could’ve denoted it with <span class="math inline">\(\mathrm{SSR}_{\mathbf{x},\mathbf{y}}(f)\)</span>
or even <span class="math inline">\(\mathrm{SSR}(f)\)</span> to emphasise that <span class="math inline">\(\mathbf{x},\mathbf{y}\)</span>
are just fixed values and we are not interested
in changing them at all (they are “global variables”).</p>
</dd>
</dl>
<p>We enjoy SSR because (amongst others):</p>
<ul>
<li><p>larger errors are penalised much more than smaller ones</p>
<blockquote>
<p>(this can be considered a drawback as well)</p>
</blockquote></li>
<li><p>(**) statistically speaking, this has a clear underlying interpretation</p>
<blockquote>
<p>(assuming errors are normally distributed,
finding a model minimising the SSR is equivalent
to maximum likelihood estimation)</p>
</blockquote></li>
<li><p>the models minimising the SSR can often be found easily</p>
<blockquote>
<p>(corresponding optimisation tasks have an analytic solution –
studied already by Gauss in the late 18th century)</p>
</blockquote></li>
</ul>
<p>(**) Other choices:</p>
<ul>
<li>regularised SSR, e.g., lasso or ridge regression (in the case of multiple input variables)</li>
<li>sum or median of absolute values (robust regression)</li>
</ul>
<p>Fitting a model to data can be written as an optimisation problem:</p>
<p><span class="math display">\[
\min_{f\in\mathcal{F}} \mathrm{SSR}(f|\mathbf{x},\mathbf{y}),
\]</span></p>
<p>i.e., find <span class="math inline">\(f\)</span> minimising the SSR <strong>(seek “best” <span class="math inline">\(f\)</span>)</strong><br />
amongst the set of admissible models <span class="math inline">\(\mathcal{F}\)</span>.</p>
<p>Example <span class="math inline">\(\mathcal{F}\)</span>s:</p>
<ul>
<li><p><span class="math inline">\(\mathcal{F}=\{\text{All possible functions of one variable}\}\)</span> – if there are no repeated
<span class="math inline">\(x_i\)</span>’s, this corresponds to data <em>interpolation</em>; note that there
are many functions that give SSR of <span class="math inline">\(0\)</span>.</p></li>
<li><p><span class="math inline">\(\mathcal{F}=\{ x\mapsto x^2, x\mapsto \cos(x), x\mapsto \exp(2x+7)-9 \}\)</span> – obviously
an ad-hoc choice but you can easily choose the best amongst the 3 by computing 3 sums of squares.</p></li>
<li><p><span class="math inline">\(\mathcal{F}=\{ x\mapsto a+bx\}\)</span> – the space of linear functions of one variable</p></li>
<li><p>etc.</p></li>
</ul>
<p>(e.g., <span class="math inline">\(x\mapsto x^2\)</span> is read “<span class="math inline">\(x\)</span> maps to <span class="math inline">\(x^2\)</span>” and is
an elegant way to define an inline function <span class="math inline">\(f\)</span> such that <span class="math inline">\(f(x)=x^2\)</span>)</p>
</div>
</div>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">5.2</span> Simple Linear Regression</h2>
<div id="introduction-5" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Introduction</h3>
<p>If the family of admissible models <span class="math inline">\(\mathcal{F}\)</span> consists only of all linear functions of one variable,
we deal with a <strong>simple linear regression</strong>.</p>
<p>Our problem becomes:</p>
<p><span class="math display">\[
\min_{a,b\in\mathbb{R}} \sum_{i=1}^n \left(
a+bx_i-y_i
\right)^2
\]</span></p>
<p>In other words, we seek best fitting line in terms of the squared residuals.</p>
<p>This is the <strong>method of least squares</strong>.</p>
<p>This is particularly nice, because our search space
is just <span class="math inline">\(\mathbb{R}^2\)</span> – easy to handle both analytically and numerically.</p>
<div class="figure">
<img src="05-regression-simple-figures/credit_different_lines_ssr-1.svg" alt="(#fig:credit_different_lines_ssr) Three simple linear models together with the corresponding SSRs" />
<p class="caption">(#fig:credit_different_lines_ssr) Three simple linear models together with the corresponding SSRs</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Which of lines in Figure @ref(fig:credit_different_lines_ssr) is the least squares solution?</p>
</div>
<!-- TODO:
heatmap of SSR(a,b) for a = [0,1], b=[0,300]
-->
</div>
<div id="solution-in-r" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Solution in R</h3>
<p>Let’s fit the linear model minimising the SSR in R.
The <code>lm()</code> function (<code>l</code>inear <code>m</code>odels) has a convenient <em>formula</em>-based interface.</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb245-1" title="1">f &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>X)</a></code></pre></div>
<p>In R, the expression “<code>Y~X</code>” denotes a formula, which we read as:
variable <code>Y</code> is a function of <code>X</code>.
Note that the dependent variable is on the left side of the formula.
Here, <code>X</code> and <code>Y</code> are two R numeric vectors of identical lengths.</p>
<p>Let’s print the fitted model:</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb246-1" title="1"><span class="kw">print</span>(f)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Coefficients:
## (Intercept)            X  
##     226.471        0.266</code></pre>
<p>Hence, the fitted model is:
<span class="math display">\[
Y = f(X) = 226.47114+0.26615 X
\qquad (+ \varepsilon)
\]</span></p>
<p>Coefficient <span class="math inline">\(a\)</span> (intercept):</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb248-1" title="1">f<span class="op">$</span>coefficient[<span class="dv">1</span>]</a></code></pre></div>
<pre><code>## (Intercept) 
##      226.47</code></pre>
<p>Coefficient <span class="math inline">\(b\)</span> (slope):</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb250-1" title="1">f<span class="op">$</span>coefficient[<span class="dv">2</span>]</a></code></pre></div>
<pre><code>##       X 
## 0.26615</code></pre>
<p>Plotting, see Figure @ref(fig:credit_plot_lm):</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb252-1" title="1"><span class="kw">plot</span>(X, Y, <span class="dt">col=</span><span class="st">&quot;#000000aa&quot;</span>)</a>
<a class="sourceLine" id="cb252-2" title="2"><span class="kw">abline</span>(f, <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a></code></pre></div>
<div class="figure">
<img src="05-regression-simple-figures/credit_plot_lm-1.svg" alt="(#fig:credit_plot_lm) Fitted regression line" />
<p class="caption">(#fig:credit_plot_lm) Fitted regression line</p>
</div>
<p>SSR:</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb253-1" title="1"><span class="kw">sum</span>(f<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 2132108</code></pre>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb255-1" title="1"><span class="kw">sum</span>((f<span class="op">$</span>coefficient[<span class="dv">1</span>]<span class="op">+</span>f<span class="op">$</span>coefficient[<span class="dv">2</span>]<span class="op">*</span>X<span class="op">-</span>Y)<span class="op">^</span><span class="dv">2</span>) <span class="co"># equivalent</span></a></code></pre></div>
<pre><code>## [1] 2132108</code></pre>
<p>We can predict the model’s output for yet-unobserved inputs by
writing:</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb257-1" title="1">X_new &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1500</span>, <span class="dv">2000</span>, <span class="dv">2500</span>) <span class="co"># example inputs</span></a>
<a class="sourceLine" id="cb257-2" title="2">f<span class="op">$</span>coefficient[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>f<span class="op">$</span>coefficient[<span class="dv">2</span>]<span class="op">*</span>X_new</a></code></pre></div>
<pre><code>## [1] 625.69 758.76 891.84</code></pre>
<p>Note that linear models can also be fitted based on formulas that refer
to a data frame’s columns. For example, let us wrap
both <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> inside a data frame:</p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb259-1" title="1">XY &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Balance=</span>X, <span class="dt">Rating=</span>Y)</a>
<a class="sourceLine" id="cb259-2" title="2"><span class="kw">head</span>(XY, <span class="dv">3</span>)</a></code></pre></div>
<pre><code>##   Balance Rating
## 1     333    283
## 2     903    483
## 3     580    514</code></pre>
<p>By writing:</p>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb261-1" title="1">f &lt;-<span class="st"> </span><span class="kw">lm</span>(Rating<span class="op">~</span>Balance, <span class="dt">data=</span>XY)</a></code></pre></div>
<p>now <code>Balance</code> and <code>Rating</code> refer to column names in the <code>XY</code> data frame,
and not the objects in R’s “workspace”.</p>
<p>Based on the above, we can make a prediction
using the <code>predict()</code> function"</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb262-1" title="1">X_new &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Balance=</span><span class="kw">c</span>(<span class="dv">1500</span>, <span class="dv">2000</span>, <span class="dv">2500</span>))</a>
<a class="sourceLine" id="cb262-2" title="2"><span class="kw">predict</span>(f, X_new)</a></code></pre></div>
<pre><code>##      1      2      3 
## 625.69 758.76 891.84</code></pre>
<p>Interestingly:</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb264-1" title="1"><span class="kw">predict</span>(f, <span class="kw">data.frame</span>(<span class="dt">Balance=</span><span class="kw">c</span>(<span class="dv">5000</span>)))</a></code></pre></div>
<pre><code>##      1 
## 1557.2</code></pre>
<p>This is more than the highest possible rating – we have extrapolated
way beyond the observable data range.</p>
<p>Note that our <span class="math inline">\(Y=a+bX\)</span> model is <strong>interpretable</strong>
and <strong>well-behaving</strong>
(not all machine learning models will have this feature,
think: deep neural networks,
which we rather conceive as <em>black boxes</em>):</p>
<ul>
<li><p>we know that by increasing <span class="math inline">\(X\)</span> by a small amount,
<span class="math inline">\(Y\)</span> will also increase (positive correlation),</p></li>
<li><p>the model is continuous – small change in <span class="math inline">\(X\)</span>
doesn’t yield any drastic change in <span class="math inline">\(Y\)</span>,</p></li>
<li><p>we know what will happen if we increase or decrease <span class="math inline">\(X\)</span> by, say, <span class="math inline">\(100\)</span>,</p></li>
<li><p>the function is invertible – if we want Rating of <span class="math inline">\(500\)</span>,
we can compute the associated preferred Balance that should yield it
(provided that the model is valid).</p></li>
</ul>
</div>
<div id="analytic-solution" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Analytic Solution</h3>
<p>It may be shown (which we actually do below)
that the solution is:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rl}
b  = &amp; \dfrac{
n \displaystyle\sum_{i=1}^n x_i y_i - \displaystyle\sum_{i=1}^n  y_i \displaystyle\sum_{i=1}^n x_i
}{
n \displaystyle\sum_{i=1}^n x_i x_i -   \displaystyle\sum_{i=1}^n x_i\displaystyle\sum_{i=1}^n x_i
}\\
a = &amp; \dfrac{1}{n}\displaystyle\sum_{i=1}^n  y_i - b  \dfrac{1}{n} \displaystyle\sum_{i=1}^n x_i  \\
\end{array}
\right.
\]</span></p>
<p>Which can be implemented in R as follows:</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb266-1" title="1">n &lt;-<span class="st"> </span><span class="kw">length</span>(X)</a>
<a class="sourceLine" id="cb266-2" title="2">b &lt;-<span class="st"> </span>(n<span class="op">*</span><span class="kw">sum</span>(X<span class="op">*</span>Y)<span class="op">-</span><span class="kw">sum</span>(X)<span class="op">*</span><span class="kw">sum</span>(Y))<span class="op">/</span>(n<span class="op">*</span><span class="kw">sum</span>(X<span class="op">*</span>X)<span class="op">-</span><span class="kw">sum</span>(X)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb266-3" title="3">a &lt;-<span class="st"> </span><span class="kw">mean</span>(Y)<span class="op">-</span>b<span class="op">*</span><span class="kw">mean</span>(X)</a>
<a class="sourceLine" id="cb266-4" title="4"><span class="kw">c</span>(a, b) <span class="co"># the same as f$coefficients</span></a></code></pre></div>
<pre><code>## [1] 226.47114   0.26615</code></pre>
</div>
<div id="derivation-of-the-solution" class="section level3">
<h3><span class="header-section-number">5.2.4</span> Derivation of the Solution (**)</h3>
<dl>
<dt>Remark.</dt>
<dd><p>You can safely skip this part if you are yet to know
how to search for a minimum of a function of many variables
and what are partial derivatives.</p>
</dd>
</dl>
<p>Denote with:</p>
<p><span class="math display">\[
E(a,b)=\mathrm{SSR}(x\mapsto a+bx|\mathbf{x},\mathbf{y})
=\sum_{i=1}^n \left( a+bx_i - y_i \right) ^2.
\]</span></p>
<p>We seek the minimum of <span class="math inline">\(E\)</span> w.r.t. both <span class="math inline">\(a,b\)</span>.</p>
<dl>
<dt>Theorem.</dt>
<dd><p>If <span class="math inline">\(E\)</span> has a (local) minimum at <span class="math inline">\((a^*,b^*)\)</span>,
then its partial derivatives vanish therein,
i.e., <span class="math inline">\(\partial E/\partial a(a^*, b^*) = 0\)</span> and <span class="math inline">\(\partial E/\partial b(a^*, b^*)=0\)</span>.</p>
</dd>
</dl>
<p>We have:</p>
<p><span class="math display">\[
E(a,b)   = \displaystyle\sum_{i=1}^n \left( a+bx_i - y_i \right) ^2.
\]</span></p>
<p>We need to compute the partial derivatives <span class="math inline">\(\partial E/\partial a\)</span> (derivative of <span class="math inline">\(E\)</span>
w.r.t. variable <span class="math inline">\(a\)</span> – all other terms treated as constants)
and <span class="math inline">\(\partial E/\partial b\)</span> (w.r.t. <span class="math inline">\(b\)</span>).</p>
<p>Useful rules – derivatives w.r.t. <span class="math inline">\(a\)</span> (denote <span class="math inline">\(f&#39;(a)=(f(a))&#39;\)</span>):</p>
<ul>
<li><span class="math inline">\((f(a)+g(a))&#39;=f&#39;(a)+g&#39;(a)\)</span> (derivative of sum is sum of derivatives)</li>
<li><span class="math inline">\((f(a) g(a))&#39; = f&#39;(a)g(a) + f(a)g&#39;(a)\)</span> (derivative of product)</li>
<li><span class="math inline">\((f(g(a)))&#39; = f&#39;(g(a)) g&#39;(a)\)</span> (chain rule)</li>
<li><span class="math inline">\((c)&#39; = 0\)</span> for any constant <span class="math inline">\(c\)</span> (expression not involving <span class="math inline">\(a\)</span>)</li>
<li><span class="math inline">\((a^p)&#39; = pa^{p-1}\)</span> for any <span class="math inline">\(p\)</span></li>
<li>in particular: <span class="math inline">\((c a^2+d)&#39;=2ca\)</span>, <span class="math inline">\((ca)&#39;=c\)</span>, <span class="math inline">\(((ca+d)^2)&#39;=2(ca+d)c\)</span> (application of the above rules)</li>
</ul>
<!--\[
\begin{array}{rl}
E_a'(a,b)=&2\displaystyle\sum_{i=1}^n \left( a+bx_i - y_i \right) x_i \\
E_b'(a,b)=&2\displaystyle\sum_{i=1}^n \left( a+bx_i - y_i \right) \\
\end{array}
\]-->
<p>We seek <span class="math inline">\(a,b\)</span> such that <span class="math inline">\(\frac{\partial E}{\partial a}(a,b) = 0\)</span>
and <span class="math inline">\(\frac{\partial E}{\partial b}(a,b)=0\)</span>.</p>
<p><span class="math display">\[
\left\{
\begin{array}{rl}
\frac{\partial E}{\partial a}(a,b)=&amp;2\displaystyle\sum_{i=1}^n \left( a+bx_i - y_i \right) = 0\\
\frac{\partial E}{\partial b}(a,b)=&amp;2\displaystyle\sum_{i=1}^n \left( a+bx_i - y_i \right) x_i = 0 \\
\end{array}
\right.
\]</span></p>
<p>This is a system of 2 linear equations. Easy.</p>
<p>Rearranging like back in the school days:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rl}
b \displaystyle\sum_{i=1}^n x_i+ a n = &amp; \displaystyle\sum_{i=1}^n  y_i  \\
b \displaystyle\sum_{i=1}^n x_i x_i + a \displaystyle\sum_{i=1}^n x_i = &amp; \displaystyle\sum_{i=1}^n x_i y_i \\
\end{array}
\right.
\]</span></p>
<p>It is left as an exercise to show that the solution is:</p>
<p><span class="math display">\[
\left\{
\begin{array}{rl}
b^*  = &amp; \dfrac{
n \displaystyle\sum_{i=1}^n x_i y_i - \displaystyle\sum_{i=1}^n  y_i \displaystyle\sum_{i=1}^n x_i
}{
n \displaystyle\sum_{i=1}^n x_i x_i -   \displaystyle\sum_{i=1}^n x_i\displaystyle\sum_{i=1}^n x_i
}\\
a^* = &amp; \dfrac{1}{n}\displaystyle\sum_{i=1}^n  y_i - b^*  \dfrac{1}{n} \displaystyle\sum_{i=1}^n x_i  \\
\end{array}
\right.
\]</span></p>
<p>(we should additionally perform the second derivative test
to assure that this is the minimum of <span class="math inline">\(E\)</span> – which is exactly the case though)</p>
<!-- TODO rewrite nicely with means and correlation coefficients etc.

Pearson's r introduced in the next chapter though

(a <- cor(x,y)*sd(y)/sd(x))
(b <- mean(y)-a*mean(x))
-->
<p>(**) In the next chapter, we will introduce the notion of Pearson’s
linear coefficient, <span class="math inline">\(r\)</span> (see <code>cor()</code> in R). It might be shown that
<span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> can also be rewritten as:</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb268-1" title="1">(b &lt;-<span class="st"> </span><span class="kw">cor</span>(X,Y)<span class="op">*</span><span class="kw">sd</span>(Y)<span class="op">/</span><span class="kw">sd</span>(X))</a></code></pre></div>
<pre><code>##         [,1]
## [1,] 0.26615</code></pre>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb270-1" title="1">(a &lt;-<span class="st"> </span><span class="kw">mean</span>(Y)<span class="op">-</span>b<span class="op">*</span><span class="kw">mean</span>(X))</a></code></pre></div>
<pre><code>##        [,1]
## [1,] 226.47</code></pre>
<!-- TODO show optim() -->
</div>
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">5.3</span> Exercises</h2>
<div id="the-anscombe-quartet" class="section level3">
<h3><span class="header-section-number">5.3.1</span> The Anscombe Quartet</h3>
<p>Here is a famous illustrative example proposed by
the statistician Francis Anscombe in the early 1970s.</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb272-1" title="1"><span class="kw">print</span>(anscombe) <span class="co"># `anscombe` is a built-in object</span></a></code></pre></div>
<pre><code>##    x1 x2 x3 x4    y1   y2    y3    y4
## 1  10 10 10  8  8.04 9.14  7.46  6.58
## 2   8  8  8  8  6.95 8.14  6.77  5.76
## 3  13 13 13  8  7.58 8.74 12.74  7.71
## 4   9  9  9  8  8.81 8.77  7.11  8.84
## 5  11 11 11  8  8.33 9.26  7.81  8.47
## 6  14 14 14  8  9.96 8.10  8.84  7.04
## 7   6  6  6  8  7.24 6.13  6.08  5.25
## 8   4  4  4 19  4.26 3.10  5.39 12.50
## 9  12 12 12  8 10.84 9.13  8.15  5.56
## 10  7  7  7  8  4.82 7.26  6.42  7.91
## 11  5  5  5  8  5.68 4.74  5.73  6.89</code></pre>
<p>What we see above is a single data frame
that encodes four separate datasets:
<code>anscombe$x1</code> and <code>anscombe$y1</code> define the first pair of variables,
<code>anscombe$x2</code> and <code>anscombe$y2</code> define the second pair and so forth.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Split the above data (manually) into four data frames
<code>ans1</code>, …, <code>ans4</code> with columns <code>x</code> and <code>y</code>.</p>
<p>For example, <code>ans1</code> should look like:</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb274-1" title="1"><span class="kw">print</span>(ans1)</a></code></pre></div>
<pre><code>##     x     y
## 1  10  8.04
## 2   8  6.95
## 3  13  7.58
## 4   9  8.81
## 5  11  8.33
## 6  14  9.96
## 7   6  7.24
## 8   4  4.26
## 9  12 10.84
## 10  7  4.82
## 11  5  5.68</code></pre>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb276-1" title="1">ans1 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>anscombe<span class="op">$</span>x1, <span class="dt">y=</span>anscombe<span class="op">$</span>y1)</a>
<a class="sourceLine" id="cb276-2" title="2">ans2 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>anscombe<span class="op">$</span>x2, <span class="dt">y=</span>anscombe<span class="op">$</span>y2)</a>
<a class="sourceLine" id="cb276-3" title="3">ans3 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>anscombe<span class="op">$</span>x3, <span class="dt">y=</span>anscombe<span class="op">$</span>y3)</a>
<a class="sourceLine" id="cb276-4" title="4">ans4 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>anscombe<span class="op">$</span>x4, <span class="dt">y=</span>anscombe<span class="op">$</span>y4)</a>
<a class="sourceLine" id="cb276-5" title="5"><span class="kw">print</span>(ans1)</a></code></pre></div>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Compute the mean of each <code>x</code> and <code>y</code> variable.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb277-1" title="1"><span class="kw">mean</span>(ans1<span class="op">$</span>x) <span class="co"># individual column</span></a></code></pre></div>
<pre><code>## [1] 9</code></pre>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb279-1" title="1"><span class="kw">mean</span>(ans1<span class="op">$</span>y) <span class="co"># individual column</span></a></code></pre></div>
<pre><code>## [1] 7.5009</code></pre>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb281-1" title="1"><span class="kw">sapply</span>(ans2, mean) <span class="co"># all columns in ans2</span></a></code></pre></div>
<pre><code>##      x      y 
## 9.0000 7.5009</code></pre>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb283-1" title="1"><span class="kw">sapply</span>(anscombe, mean) <span class="co"># all columns in the full anscombe dataset</span></a></code></pre></div>
<pre><code>##     x1     x2     x3     x4     y1     y2     y3     y4 
## 9.0000 9.0000 9.0000 9.0000 7.5009 7.5009 7.5000 7.5009</code></pre>
<p><em>Comment: This is really interesting, all the means of <code>x</code> columns
as well as the means of <code>y</code>s are almost identical.</em></p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Compute the standard deviation of each <code>x</code> and <code>y</code> variable.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>The solution is similar to the previous one, just replace <code>mean</code> with <code>sd</code>.
Here, to learn something new, we will use the <code>knitr::kable()</code> function
that pretty-prints a given matrix or data frame:</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb285-1" title="1">results &lt;-<span class="st"> </span><span class="kw">sapply</span>(anscombe, sd)</a>
<a class="sourceLine" id="cb285-2" title="2">knitr<span class="op">::</span><span class="kw">kable</span>(results, <span class="dt">col.names=</span><span class="st">&quot;standard deviation&quot;</span>)</a></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">standard deviation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">x1</td>
<td align="right">3.3166</td>
</tr>
<tr class="even">
<td align="left">x2</td>
<td align="right">3.3166</td>
</tr>
<tr class="odd">
<td align="left">x3</td>
<td align="right">3.3166</td>
</tr>
<tr class="even">
<td align="left">x4</td>
<td align="right">3.3166</td>
</tr>
<tr class="odd">
<td align="left">y1</td>
<td align="right">2.0316</td>
</tr>
<tr class="even">
<td align="left">y2</td>
<td align="right">2.0317</td>
</tr>
<tr class="odd">
<td align="left">y3</td>
<td align="right">2.0304</td>
</tr>
<tr class="even">
<td align="left">y4</td>
<td align="right">2.0306</td>
</tr>
</tbody>
</table>
<p><em>Comment: This is even more interesting, because the numbers agree up to 2 decimal digits.</em></p>
</details>
<!--
TODO: cor() covered later
Compute the Pearson linear correlation coefficient
for each of the four pairs of `x` and `y`.

To recall, this can be done with the `cor()` function.
What is the Pearson $r$ coefficient is explained in Lecture 3.


```r
cor(ans1$x, ans1$y)
```

```
## [1] 0.81642
```

```r
cor(ans2$x, ans2$y)
```

```
## [1] 0.81624
```

```r
cor(ans3$x, ans3$y)
```

```
## [1] 0.81629
```

```r
cor(ans4$x, ans4$y)
```

```
## [1] 0.81652
```


*Comment: We get a feeling that we're being tricked by Anscombe...
All the variables are highly correlated, and the correlation
coefficients are more or less the same.*
-->
<div class="exercise"><strong>Exercise.</strong>
<p>Fit a simple linear regression model for each data set.
Draw the scatter plots again (<code>plot()</code>)
and add the regression lines (<code>lines()</code> or <code>abline()</code>).</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>To recall, this can be done with the <code>lm()</code> function
explained in Lecture 2.</p>
<p>At this point we should already have become lazy – the tasks are very
repetitious. Let’s automate them by writing a single function
that does all the above for any data set:</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb286-1" title="1">fit_models &lt;-<span class="st"> </span><span class="cf">function</span>(ans) {</a>
<a class="sourceLine" id="cb286-2" title="2">    <span class="co"># ans is a data frame with columns x and y</span></a>
<a class="sourceLine" id="cb286-3" title="3">    f &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>x, <span class="dt">data=</span>ans) <span class="co"># fit linear model</span></a>
<a class="sourceLine" id="cb286-4" title="4">    <span class="kw">print</span>(f<span class="op">$</span>coefficients) <span class="co"># estimated coefficients</span></a>
<a class="sourceLine" id="cb286-5" title="5">    <span class="kw">plot</span>(ans<span class="op">$</span>x, ans<span class="op">$</span>y) <span class="co"># scatter plot</span></a>
<a class="sourceLine" id="cb286-6" title="6">    <span class="kw">abline</span>(f, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>) <span class="co"># regression line</span></a>
<a class="sourceLine" id="cb286-7" title="7">    <span class="kw">return</span>(f)</a>
<a class="sourceLine" id="cb286-8" title="8">}</a></code></pre></div>
<p>Now we can apply it on the four particular examples.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb287-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="co"># four plots on 1 figure (2x2 grid)</span></a>
<a class="sourceLine" id="cb287-2" title="2">f1 &lt;-<span class="st"> </span><span class="kw">fit_models</span>(ans1)</a></code></pre></div>
<pre><code>## (Intercept)           x 
##     3.00009     0.50009</code></pre>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb289-1" title="1">f2 &lt;-<span class="st"> </span><span class="kw">fit_models</span>(ans2)</a></code></pre></div>
<pre><code>## (Intercept)           x 
##      3.0009      0.5000</code></pre>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb291-1" title="1">f3 &lt;-<span class="st"> </span><span class="kw">fit_models</span>(ans3)</a></code></pre></div>
<pre><code>## (Intercept)           x 
##     3.00245     0.49973</code></pre>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb293-1" title="1">f4 &lt;-<span class="st"> </span><span class="kw">fit_models</span>(ans4)</a></code></pre></div>
<pre><code>## (Intercept)           x 
##     3.00173     0.49991</code></pre>
<div class="figure">
<img src="05-regression-simple-figures/anscombe_fit_apply-1.svg" alt="(#fig:anscombe_fit_apply) Fitted regression lines for the Anscombe quartet" />
<p class="caption">(#fig:anscombe_fit_apply) Fitted regression lines for the Anscombe quartet</p>
</div>
<p><em>Comment: All the estimated models are virtually the same,
the regression lines are
<span class="math inline">\(y=0.5x+3\)</span>, compare Figure @ref(fig:anscombe_fit_apply).</em></p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Create scatter plots of the residuals (predicted <span class="math inline">\(\hat{y}_i\)</span> minus
true <span class="math inline">\(y_i\)</span>) as a function of the predicted <span class="math inline">\(\hat{y}_i=f(x_i)\)</span> for every
<span class="math inline">\(i=1,\dots,11\)</span>.</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>To recall, the model predictions can be generated by (amongst others)
calling the <code>predict()</code> function.</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb295-1" title="1">y_pred1 &lt;-<span class="st"> </span>f1<span class="op">$</span>fitted.values <span class="co"># predict(f1, ans1)</span></a>
<a class="sourceLine" id="cb295-2" title="2">y_pred2 &lt;-<span class="st"> </span>f2<span class="op">$</span>fitted.values <span class="co"># predict(f2, ans2)</span></a>
<a class="sourceLine" id="cb295-3" title="3">y_pred3 &lt;-<span class="st"> </span>f3<span class="op">$</span>fitted.values <span class="co"># predict(f3, ans3)</span></a>
<a class="sourceLine" id="cb295-4" title="4">y_pred4 &lt;-<span class="st"> </span>f4<span class="op">$</span>fitted.values <span class="co"># predict(f4, ans4)</span></a></code></pre></div>
<p>Plots of residuals as a function of the predicted (fitted) values
are given in Figure @ref(fig:anscombe_resid_plot).</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb296-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="co"># four plots on 1 figure (2x2 grid)</span></a>
<a class="sourceLine" id="cb296-2" title="2"><span class="kw">plot</span>(y_pred1, y_pred1<span class="op">-</span>ans1<span class="op">$</span>y)</a>
<a class="sourceLine" id="cb296-3" title="3"><span class="kw">plot</span>(y_pred2, y_pred2<span class="op">-</span>ans2<span class="op">$</span>y)</a>
<a class="sourceLine" id="cb296-4" title="4"><span class="kw">plot</span>(y_pred3, y_pred3<span class="op">-</span>ans3<span class="op">$</span>y)</a>
<a class="sourceLine" id="cb296-5" title="5"><span class="kw">plot</span>(y_pred4, y_pred4<span class="op">-</span>ans4<span class="op">$</span>y)</a></code></pre></div>
<div class="figure">
<img src="05-regression-simple-figures/anscombe_resid_plot-1.svg" alt="(#fig:anscombe_resid_plot) Residuals vs. fitted values for the regression lines fitted to the Anscombe quartet" />
<p class="caption">(#fig:anscombe_resid_plot) Residuals vs. fitted values for the regression lines fitted to the Anscombe quartet</p>
</div>
<p><em>Comment: Ideally, the residuals shouldn’t be correlated
with the predicted values – they should “oscillate” randomly
around 0. This is only the case of the first dataset.
All the other cases are “alarming” in the sense that
they suggest that the obtained models are “suspicious”
(perhaps data cleansing is needed or a linear model is not at all appropriate).</em></p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Draw conclusions (in your own words).</p>
</div>
<details class="solution"><summary style="color: blue">Click here for a solution.</summary>
<p>We’re being taught a lesson here: don’t perform data analysis tasks
automatically, don’t look at bare numbers only, visualise your data first!</p>
</details>
<div class="exercise"><strong>Exercise.</strong>
<p>Read more about Anscombe’s quartet at <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet" class="uri">https://en.wikipedia.org/wiki/Anscombe%27s_quartet</a></p>
</div>
</div>
<div id="median-house-value-in-boston" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Median House Value in Boston</h3>
<p>The famous <code>Boston</code> dataset from the <code>MASS</code> package
records the historical (in the 1970s) median house value
(<code>medv</code> column, in 1000s USD) for 506 suburbs around
Boston, MA, USA.</p>
<p>You can access the dataset by calling:</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb297-1" title="1"><span class="co"># call install.packages(&quot;MASS&quot;) first (only once)</span></a>
<a class="sourceLine" id="cb297-2" title="2"><span class="kw">library</span>(<span class="st">&quot;MASS&quot;</span>)</a>
<a class="sourceLine" id="cb297-3" title="3"><span class="kw">head</span>(Boston, <span class="dv">3</span>)</a></code></pre></div>
<pre><code>##      crim zn indus chas   nox    rm  age    dis rad tax ptratio  black
## 1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90
## 2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90
## 3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83
##   lstat medv
## 1  4.98 24.0
## 2  9.14 21.6
## 3  4.03 34.7</code></pre>
<p>Read the description of each of the 14 columns in the dataset’s manual,
see <code>?Boston</code>.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Fit a simple linear model of <span class="math inline">\(Y=\)</span> <code>medv</code> as a function of <span class="math inline">\(X=\)</span> <code>lstat</code>.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Fit a quadratic polynomial model <span class="math inline">\((aX^2+bX+c)\)</span> for the same pair of variables.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Fit a 5-degree polynomial model <span class="math inline">\((aX^5+bX^4+cX^3+dX^2+eX+f)\)</span> for the same pair of variables.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Draw the scatter plot of the two variables and add the fitted regression
curves (all three on a single plot, use different colours).</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Compute RMSE, MAE and <span class="math inline">\(R^2\)</span> of each model. Provide the interpretations of the
obtained values.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>For each model, draw the plot of the residuals (<span class="math inline">\(\hat{y}_i-y_i\)</span>)
as a function of the predicted outputs (<span class="math inline">\(\hat{y}_i\)</span>).
Describe these plots in your own words.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Predict the <code>medv</code> values for <code>lstat</code> of 0, 25, 50 and 75
using all the models. Compare and discuss the results.
Which of the predictions seem trustworthy?</p>
</div>
</div>
</div>
<div id="outro-2" class="section level2">
<h2><span class="header-section-number">5.4</span> Outro</h2>
<div id="remarks-3" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Remarks</h3>
<p>In supervised learning, with each input point,
there’s an associated reference output value.</p>
<p>Learning a model = constructing a function that approximates
(minimising some error measure) the given data.</p>
<p>Regression = the output variable <span class="math inline">\(Y\)</span> is continuous.</p>
<p>We studied linear models with a single independent variable based on
the least squares (SSR) fit.</p>
<p>In the next part we will extend this setting to the case
of many variables, i.e., <span class="math inline">\(p&gt;1\)</span>, called multiple regression.</p>
</div>
<div id="further-reading-2" class="section level3">
<h3><span class="header-section-number">5.4.2</span> Further Reading</h3>
<p>Recommended further reading: <span class="citation">(James et al. <a href="references.html#ref-islr">2017</a>: Chapters 1, 2 and 3)</span></p>
<p>Other: <span class="citation">(Hastie et al. <a href="references.html#ref-esl">2017</a>: Chapter 1, Sections 3.2 and 3.3)</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-regression-multiple.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
