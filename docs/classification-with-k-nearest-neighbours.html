<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Classification with K-Nearest Neighbours | Lightweight Machine Learning Classics with R</title>
  <meta name="description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Classification with K-Nearest Neighbours | Lightweight Machine Learning Classics with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://lmlcr.gagolewski.com" />
  
  <meta property="og:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  <meta name="github-repo" content="gagolews/lmlcr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Classification with K-Nearest Neighbours | Lightweight Machine Learning Classics with R" />
  
  <meta name="twitter:description" content="Explore some of the most fundamental algorithms which have stood the test of time and provide the basis for innovative solutions in data-driven AI. Learn how to use the R language for implementing various stages of data processing and modelling activities. Appreciate mathematics as the universal language for formalising data-intense problems and communicating their solutions. The book is for you if you’re yet to be fluent with university-level linear algebra, calculus and probability theory or you’ve forgotten all the maths you’ve ever learned, and are seeking a gentle, yet thorough, introduction to the topic." />
  

<meta name="author" content="Marek Gagolewski" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="decision-and-regression-trees.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<style type="text/css">
div.exercise {
    font-style: italic;
    border-left: 2px solid gray;
    padding-left: 1em;
}

details.solution {
    border-left: 2px solid #ff8888;
    padding-left: 1em;
    margin-bottom: 2em;
}

</style>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li style='font-style: italic' ><a href="./">LMLCR</a></li>
<li style='font-size: smaller' ><a href="https://www.gagolewski.com">Marek Gagolewski</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a><ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#main-types-of-machine-learning-problems"><i class="fa fa-check"></i><b>1.1.1</b> Main Types of Machine Learning Problems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#supervised-learning"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#formalism"><i class="fa fa-check"></i><b>1.2.1</b> Formalism</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#desired-outputs"><i class="fa fa-check"></i><b>1.2.2</b> Desired Outputs</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#types-of-supervised-learning-problems"><i class="fa fa-check"></i><b>1.2.3</b> Types of Supervised Learning Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html"><i class="fa fa-check"></i><b>2</b> Classification with K-Nearest Neighbours</a><ul>
<li class="chapter" data-level="2.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#classification-task"><i class="fa fa-check"></i><b>2.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="2.1.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#data"><i class="fa fa-check"></i><b>2.1.2</b> Data</a></li>
<li class="chapter" data-level="2.1.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-and-test-sets"><i class="fa fa-check"></i><b>2.1.3</b> Training and Test Sets</a></li>
<li class="chapter" data-level="2.1.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#discussed-methods"><i class="fa fa-check"></i><b>2.1.4</b> Discussed Methods</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#k-nearest-neighbour-classifier"><i class="fa fa-check"></i><b>2.2</b> K-nearest Neighbour Classifier</a><ul>
<li class="chapter" data-level="2.2.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#introduction-2"><i class="fa fa-check"></i><b>2.2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#example-in-r"><i class="fa fa-check"></i><b>2.2.2</b> Example in R</a></li>
<li class="chapter" data-level="2.2.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#feature-engineering"><i class="fa fa-check"></i><b>2.2.3</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#model-assessment-and-selection"><i class="fa fa-check"></i><b>2.3</b> Model Assessment and Selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#performance-metrics"><i class="fa fa-check"></i><b>2.3.1</b> Performance Metrics</a></li>
<li class="chapter" data-level="2.3.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#how-to-choose-k-for-k-nn-classification"><i class="fa fa-check"></i><b>2.3.2</b> How to Choose K for K-NN Classification?</a></li>
<li class="chapter" data-level="2.3.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#training-validation-and-test-sets"><i class="fa fa-check"></i><b>2.3.3</b> Training, Validation and Test sets</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#implementing-a-k-nn-classifier"><i class="fa fa-check"></i><b>2.4</b> Implementing a K-NN Classifier (*)</a><ul>
<li class="chapter" data-level="2.4.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#factor-data-type"><i class="fa fa-check"></i><b>2.4.1</b> Factor Data Type</a></li>
<li class="chapter" data-level="2.4.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#main-routine"><i class="fa fa-check"></i><b>2.4.2</b> Main Routine (*)</a></li>
<li class="chapter" data-level="2.4.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#mode"><i class="fa fa-check"></i><b>2.4.3</b> Mode</a></li>
<li class="chapter" data-level="2.4.4" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#nn-search-routines"><i class="fa fa-check"></i><b>2.4.4</b> NN Search Routines (*)</a></li>
<li class="chapter" data-level="2.4.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#different-metrics"><i class="fa fa-check"></i><b>2.4.5</b> Different Metrics (*)</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a><ul>
<li class="chapter" data-level="2.5.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#wine-quality-best-k-nn-parameters-via-cross-validation"><i class="fa fa-check"></i><b>2.5.1</b> Wine Quality – Best K-NN Parameters via Cross-Validation (*)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#outro"><i class="fa fa-check"></i><b>2.6</b> Outro</a><ul>
<li class="chapter" data-level="2.6.1" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#remarks"><i class="fa fa-check"></i><b>2.6.1</b> Remarks</a></li>
<li class="chapter" data-level="2.6.2" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#side-note-k-nn-regression"><i class="fa fa-check"></i><b>2.6.2</b> Side Note: K-NN Regression</a></li>
<li class="chapter" data-level="2.6.3" data-path="classification-with-k-nearest-neighbours.html"><a href="classification-with-k-nearest-neighbours.html#further-reading"><i class="fa fa-check"></i><b>2.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html"><i class="fa fa-check"></i><b>3</b> Decision and Regression Trees</a><ul>
<li class="chapter" data-level="3.1" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#classification-task-1"><i class="fa fa-check"></i><b>3.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="3.1.2" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#data-1"><i class="fa fa-check"></i><b>3.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#decision-trees"><i class="fa fa-check"></i><b>3.2</b> Decision Trees</a><ul>
<li class="chapter" data-level="3.2.1" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#introduction-4"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#example-in-r-1"><i class="fa fa-check"></i><b>3.2.2</b> Example in R</a></li>
<li class="chapter" data-level="3.2.3" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#a-note-on-decision-tree-learning"><i class="fa fa-check"></i><b>3.2.3</b> A Note on Decision Tree Learning</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#exercises-1"><i class="fa fa-check"></i><b>3.3</b> Exercises</a><ul>
<li class="chapter" data-level="3.3.1" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#edstats-where-girls-are-better-at-maths-than-boys"><i class="fa fa-check"></i><b>3.3.1</b> EdStats – Where Girls Are Better at Maths Than Boys?</a></li>
<li class="chapter" data-level="3.3.2" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#edstats-and-world-factbook-joining-forces"><i class="fa fa-check"></i><b>3.3.2</b> EdStats and World Factbook – Joining Forces</a></li>
<li class="chapter" data-level="3.3.3" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#wine-quality-random-forest-and-xgboost"><i class="fa fa-check"></i><b>3.3.3</b> Wine Quality – Random Forest and XGBoost (*)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#outro-1"><i class="fa fa-check"></i><b>3.4</b> Outro</a><ul>
<li class="chapter" data-level="3.4.1" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#remarks-1"><i class="fa fa-check"></i><b>3.4.1</b> Remarks</a></li>
<li class="chapter" data-level="3.4.2" data-path="decision-and-regression-trees.html"><a href="decision-and-regression-trees.html#further-reading-1"><i class="fa fa-check"></i><b>3.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-regression"><i class="fa fa-check"></i><b>4.1</b> Simple Regression</a><ul>
<li class="chapter" data-level="4.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction-5"><i class="fa fa-check"></i><b>4.1.1</b> Introduction</a></li>
<li class="chapter" data-level="4.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#search-space-and-objective"><i class="fa fa-check"></i><b>4.1.2</b> Search Space and Objective</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-1"><i class="fa fa-check"></i><b>4.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="4.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction-6"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#solution-in-r"><i class="fa fa-check"></i><b>4.2.2</b> Solution in R</a></li>
<li class="chapter" data-level="4.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analytic-solution"><i class="fa fa-check"></i><b>4.2.3</b> Analytic Solution</a></li>
<li class="chapter" data-level="4.2.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#derivation-of-the-solution"><i class="fa fa-check"></i><b>4.2.4</b> Derivation of the Solution (**)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-2"><i class="fa fa-check"></i><b>4.3</b> Exercises</a><ul>
<li class="chapter" data-level="4.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-anscombe-quartet"><i class="fa fa-check"></i><b>4.3.1</b> The Anscombe Quartet</a></li>
<li class="chapter" data-level="4.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#median-house-value-in-boston"><i class="fa fa-check"></i><b>4.3.2</b> Median House Value in Boston</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#outro-2"><i class="fa fa-check"></i><b>4.4</b> Outro</a><ul>
<li class="chapter" data-level="4.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#remarks-2"><i class="fa fa-check"></i><b>4.4.1</b> Remarks</a></li>
<li class="chapter" data-level="4.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#further-reading-2"><i class="fa fa-check"></i><b>4.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introduction-7"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#formalism-1"><i class="fa fa-check"></i><b>5.1.1</b> Formalism</a></li>
<li class="chapter" data-level="5.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#simple-linear-regression---recap"><i class="fa fa-check"></i><b>5.1.2</b> Simple Linear Regression - Recap</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>5.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#problem-formulation"><i class="fa fa-check"></i><b>5.2.1</b> Problem Formulation</a></li>
<li class="chapter" data-level="5.2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#fitting-a-linear-model-in-r"><i class="fa fa-check"></i><b>5.2.2</b> Fitting a Linear Model in R</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#finding-the-best-model"><i class="fa fa-check"></i><b>5.3</b> Finding the Best Model</a><ul>
<li class="chapter" data-level="5.3.1" data-path="multiple-regression.html"><a href="multiple-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>5.3.1</b> Model Diagnostics</a></li>
<li class="chapter" data-level="5.3.2" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-selection"><i class="fa fa-check"></i><b>5.3.2</b> Variable Selection</a></li>
<li class="chapter" data-level="5.3.3" data-path="multiple-regression.html"><a href="multiple-regression.html#variable-transformation"><i class="fa fa-check"></i><b>5.3.3</b> Variable Transformation</a></li>
<li class="chapter" data-level="5.3.4" data-path="multiple-regression.html"><a href="multiple-regression.html#predictive-vs.-descriptive-power"><i class="fa fa-check"></i><b>5.3.4</b> Predictive vs. Descriptive Power</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#exercises-3"><i class="fa fa-check"></i><b>5.4</b> Exercises</a><ul>
<li class="chapter" data-level="5.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#anscombes-quartet-revisited"><i class="fa fa-check"></i><b>5.4.1</b> Anscombe’s Quartet Revisited</a></li>
<li class="chapter" data-level="5.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-simple-models-involving-the-gdp-per-capita"><i class="fa fa-check"></i><b>5.4.2</b> Countries of the World – Simple models involving the GDP per capita</a></li>
<li class="chapter" data-level="5.4.3" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-most-correlated-variables"><i class="fa fa-check"></i><b>5.4.3</b> Countries of the World – Most correlated variables (*)</a></li>
<li class="chapter" data-level="5.4.4" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-non-linear-model-based-on-the-gdp-per-capita"><i class="fa fa-check"></i><b>5.4.4</b> Countries of the World – A non-linear model based on the GDP per capita</a></li>
<li class="chapter" data-level="5.4.5" data-path="multiple-regression.html"><a href="multiple-regression.html#countries-of-the-world-a-multiple-regression-model-for-the-per-capita-gdp"><i class="fa fa-check"></i><b>5.4.5</b> Countries of the World – A multiple regression model for the per capita GDP</a></li>
<li class="chapter" data-level="5.4.6" data-path="multiple-regression.html"><a href="multiple-regression.html#median-house-value-in-boston-continued"><i class="fa fa-check"></i><b>5.4.6</b> Median House Value in Boston (Continued)</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#outro-3"><i class="fa fa-check"></i><b>5.5</b> Outro</a><ul>
<li class="chapter" data-level="5.5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#remarks-3"><i class="fa fa-check"></i><b>5.5.1</b> Remarks</a></li>
<li class="chapter" data-level="5.5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#other-methods-for-regression"><i class="fa fa-check"></i><b>5.5.2</b> Other Methods for Regression</a></li>
<li class="chapter" data-level="5.5.3" data-path="multiple-regression.html"><a href="multiple-regression.html#derivation-of-the-solution-1"><i class="fa fa-check"></i><b>5.5.3</b> Derivation of the Solution (**)</a></li>
<li class="chapter" data-level="5.5.4" data-path="multiple-regression.html"><a href="multiple-regression.html#solution-in-matrix-form"><i class="fa fa-check"></i><b>5.5.4</b> Solution in Matrix Form (***)</a></li>
<li class="chapter" data-level="5.5.5" data-path="multiple-regression.html"><a href="multiple-regression.html#pearsons-r-in-matrix-form"><i class="fa fa-check"></i><b>5.5.5</b> Pearson’s r in Matrix Form (**)</a></li>
<li class="chapter" data-level="5.5.6" data-path="multiple-regression.html"><a href="multiple-regression.html#further-reading-3"><i class="fa fa-check"></i><b>5.5.6</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html"><i class="fa fa-check"></i><b>6</b> Classification with Linear Models</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#introduction-8"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#classification-task-2"><i class="fa fa-check"></i><b>6.1.1</b> Classification Task</a></li>
<li class="chapter" data-level="6.1.2" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#data-2"><i class="fa fa-check"></i><b>6.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#binary-logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Binary Logistic Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#motivation"><i class="fa fa-check"></i><b>6.2.1</b> Motivation</a></li>
<li class="chapter" data-level="6.2.2" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#logistic-model"><i class="fa fa-check"></i><b>6.2.2</b> Logistic Model</a></li>
<li class="chapter" data-level="6.2.3" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#example-in-r-2"><i class="fa fa-check"></i><b>6.2.3</b> Example in R</a></li>
<li class="chapter" data-level="6.2.4" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#loss-function-cross-entropy"><i class="fa fa-check"></i><b>6.2.4</b> Loss Function: Cross-entropy</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#exercises-4"><i class="fa fa-check"></i><b>6.3</b> Exercises</a><ul>
<li class="chapter" data-level="6.3.1" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#edstats-fitting-of-binary-logistic-regression-models"><i class="fa fa-check"></i><b>6.3.1</b> EdStats – Fitting of Binary Logistic Regression Models</a></li>
<li class="chapter" data-level="6.3.2" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#edstats-variable-selection-in-binary-logistic-regression"><i class="fa fa-check"></i><b>6.3.2</b> EdStats – Variable Selection in Binary Logistic Regression (*)</a></li>
<li class="chapter" data-level="6.3.3" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#currency-exchange-rates-growthfall"><i class="fa fa-check"></i><b>6.3.3</b> Currency Exchange Rates Growth/Fall</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#outro-4"><i class="fa fa-check"></i><b>6.4</b> Outro</a><ul>
<li class="chapter" data-level="6.4.1" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#remarks-4"><i class="fa fa-check"></i><b>6.4.1</b> Remarks</a></li>
<li class="chapter" data-level="6.4.2" data-path="classification-with-linear-models.html"><a href="classification-with-linear-models.html#further-reading-4"><i class="fa fa-check"></i><b>6.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html"><i class="fa fa-check"></i><b>7</b> Continuous Optimisation with Iterative Algorithms</a><ul>
<li class="chapter" data-level="7.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-9"><i class="fa fa-check"></i><b>7.1</b> Introduction</a><ul>
<li class="chapter" data-level="7.1.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#optimisation-problems"><i class="fa fa-check"></i><b>7.1.1</b> Optimisation Problems</a></li>
<li class="chapter" data-level="7.1.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#types-of-minima-and-maxima"><i class="fa fa-check"></i><b>7.1.2</b> Types of Minima and Maxima</a></li>
<li class="chapter" data-level="7.1.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-objective-over-a-2d-domain"><i class="fa fa-check"></i><b>7.1.3</b> Example Objective over a 2D Domain</a></li>
<li class="chapter" data-level="7.1.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-optimisation-problems-in-machine-learning"><i class="fa fa-check"></i><b>7.1.4</b> Example Optimisation Problems in Machine Learning</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#iterative-methods"><i class="fa fa-check"></i><b>7.2</b> Iterative Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#introduction-10"><i class="fa fa-check"></i><b>7.2.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-in-r-3"><i class="fa fa-check"></i><b>7.2.2</b> Example in R</a></li>
<li class="chapter" data-level="7.2.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#convergence-to-local-optima"><i class="fa fa-check"></i><b>7.2.3</b> Convergence to Local Optima</a></li>
<li class="chapter" data-level="7.2.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#random-restarts"><i class="fa fa-check"></i><b>7.2.4</b> Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>7.3</b> Gradient Descent</a><ul>
<li class="chapter" data-level="7.3.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#function-gradient"><i class="fa fa-check"></i><b>7.3.1</b> Function Gradient (*)</a></li>
<li class="chapter" data-level="7.3.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#three-facts-on-the-gradient"><i class="fa fa-check"></i><b>7.3.2</b> Three Facts on the Gradient</a></li>
<li class="chapter" data-level="7.3.3" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#gradient-descent-algorithm-gd"><i class="fa fa-check"></i><b>7.3.3</b> Gradient Descent Algorithm (GD)</a></li>
<li class="chapter" data-level="7.3.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#example-mnist"><i class="fa fa-check"></i><b>7.3.4</b> Example: MNIST (*)</a></li>
<li class="chapter" data-level="7.3.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>7.3.5</b> Stochastic Gradient Descent (SGD) (*)</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#a-note-on-convex-optimisation"><i class="fa fa-check"></i><b>7.4</b> A Note on Convex Optimisation (*)</a></li>
<li class="chapter" data-level="7.5" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#outro-5"><i class="fa fa-check"></i><b>7.5</b> Outro</a><ul>
<li class="chapter" data-level="7.5.1" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#remarks-5"><i class="fa fa-check"></i><b>7.5.1</b> Remarks</a></li>
<li class="chapter" data-level="7.5.2" data-path="continuous-optimisation-with-iterative-algorithms.html"><a href="continuous-optimisation-with-iterative-algorithms.html#further-reading-5"><i class="fa fa-check"></i><b>7.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="discrete-optimisation.html"><a href="discrete-optimisation.html"><i class="fa fa-check"></i><b>8</b> Discrete Optimisation</a><ul>
<li class="chapter" data-level="8.1" data-path="discrete-optimisation.html"><a href="discrete-optimisation.html#introduction-11"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="discrete-optimisation.html"><a href="discrete-optimisation.html#recap"><i class="fa fa-check"></i><b>8.1.1</b> Recap</a></li>
<li class="chapter" data-level="8.1.2" data-path="discrete-optimisation.html"><a href="discrete-optimisation.html#k-means-revisited"><i class="fa fa-check"></i><b>8.1.2</b> K-means Revisited</a></li>
<li class="chapter" data-level="8.1.3" data-path="discrete-optimisation.html"><a href="discrete-optimisation.html#optim-vs.-kmeans"><i class="fa fa-check"></i><b>8.1.3</b> optim() vs. kmeans()</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="discrete-optimisation.html"><a href="discrete-optimisation.html#outro-6"><i class="fa fa-check"></i><b>8.2</b> Outro</a><ul>
<li class="chapter" data-level="8.2.1" data-path="discrete-optimisation.html"><a href="discrete-optimisation.html#remarks-6"><i class="fa fa-check"></i><b>8.2.1</b> Remarks</a></li>
<li class="chapter" data-level="8.2.2" data-path="discrete-optimisation.html"><a href="discrete-optimisation.html#further-reading-6"><i class="fa fa-check"></i><b>8.2.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>9</b> Clustering</a><ul>
<li class="chapter" data-level="9.1" data-path="clustering.html"><a href="clustering.html#unsupervised-learning"><i class="fa fa-check"></i><b>9.1</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="9.1.1" data-path="clustering.html"><a href="clustering.html#introduction-12"><i class="fa fa-check"></i><b>9.1.1</b> Introduction</a></li>
<li class="chapter" data-level="9.1.2" data-path="clustering.html"><a href="clustering.html#main-types-of-unsupervised-learning-problems"><i class="fa fa-check"></i><b>9.1.2</b> Main Types of Unsupervised Learning Problems</a></li>
<li class="chapter" data-level="9.1.3" data-path="clustering.html"><a href="clustering.html#definitions"><i class="fa fa-check"></i><b>9.1.3</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="clustering.html"><a href="clustering.html#k-means-clustering"><i class="fa fa-check"></i><b>9.2</b> K-means Clustering</a><ul>
<li class="chapter" data-level="9.2.1" data-path="clustering.html"><a href="clustering.html#example-in-r-4"><i class="fa fa-check"></i><b>9.2.1</b> Example in R</a></li>
<li class="chapter" data-level="9.2.2" data-path="clustering.html"><a href="clustering.html#problem-statement"><i class="fa fa-check"></i><b>9.2.2</b> Problem Statement</a></li>
<li class="chapter" data-level="9.2.3" data-path="clustering.html"><a href="clustering.html#algorithms-for-the-k-means-problem"><i class="fa fa-check"></i><b>9.2.3</b> Algorithms for the K-means Problem</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="clustering.html"><a href="clustering.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>9.3</b> Agglomerative Hierarchical Clustering</a><ul>
<li class="chapter" data-level="9.3.1" data-path="clustering.html"><a href="clustering.html#introduction-13"><i class="fa fa-check"></i><b>9.3.1</b> Introduction</a></li>
<li class="chapter" data-level="9.3.2" data-path="clustering.html"><a href="clustering.html#example-in-r-5"><i class="fa fa-check"></i><b>9.3.2</b> Example in R</a></li>
<li class="chapter" data-level="9.3.3" data-path="clustering.html"><a href="clustering.html#linkage-functions"><i class="fa fa-check"></i><b>9.3.3</b> Linkage Functions</a></li>
<li class="chapter" data-level="9.3.4" data-path="clustering.html"><a href="clustering.html#cluster-dendrograms"><i class="fa fa-check"></i><b>9.3.4</b> Cluster Dendrograms</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="clustering.html"><a href="clustering.html#exercises-5"><i class="fa fa-check"></i><b>9.4</b> Exercises</a><ul>
<li class="chapter" data-level="9.4.1" data-path="clustering.html"><a href="clustering.html#clustering-of-the-world-factbook"><i class="fa fa-check"></i><b>9.4.1</b> Clustering of the World Factbook</a></li>
<li class="chapter" data-level="9.4.2" data-path="clustering.html"><a href="clustering.html#unbalance-dataset-k-means-needs-multiple-starts"><i class="fa fa-check"></i><b>9.4.2</b> Unbalance Dataset – K-Means Needs Multiple Starts</a></li>
<li class="chapter" data-level="9.4.3" data-path="clustering.html"><a href="clustering.html#clustering-of-typical-2d-benchmark-datasets"><i class="fa fa-check"></i><b>9.4.3</b> Clustering of Typical 2D Benchmark Datasets</a></li>
<li class="chapter" data-level="9.4.4" data-path="clustering.html"><a href="clustering.html#wine-quality-volatile.acidity-and-sulphates"><i class="fa fa-check"></i><b>9.4.4</b> Wine Quality – <code>volatile.acidity</code> and <code>sulphates</code></a></li>
<li class="chapter" data-level="9.4.5" data-path="clustering.html"><a href="clustering.html#wine-quality-chlorides-and-total.sulfur.dioxide"><i class="fa fa-check"></i><b>9.4.5</b> Wine Quality – <code>chlorides</code> and <code>total.sulfur.dioxide</code></a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="clustering.html"><a href="clustering.html#outro-7"><i class="fa fa-check"></i><b>9.5</b> Outro</a><ul>
<li class="chapter" data-level="9.5.1" data-path="clustering.html"><a href="clustering.html#remarks-7"><i class="fa fa-check"></i><b>9.5.1</b> Remarks</a></li>
<li class="chapter" data-level="9.5.2" data-path="clustering.html"><a href="clustering.html#further-reading-7"><i class="fa fa-check"></i><b>9.5.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html"><i class="fa fa-check"></i><b>10</b> Shallow and Deep Neural Networks</a><ul>
<li class="chapter" data-level="10.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-14"><i class="fa fa-check"></i><b>10.1</b> Introduction</a><ul>
<li class="chapter" data-level="10.1.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#binary-logistic-regression-recap"><i class="fa fa-check"></i><b>10.1.1</b> Binary Logistic Regression: Recap</a></li>
<li class="chapter" data-level="10.1.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#data-3"><i class="fa fa-check"></i><b>10.1.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>10.2</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="10.2.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#a-note-on-data-representation"><i class="fa fa-check"></i><b>10.2.1</b> A Note on Data Representation</a></li>
<li class="chapter" data-level="10.2.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#extending-logistic-regression"><i class="fa fa-check"></i><b>10.2.2</b> Extending Logistic Regression</a></li>
<li class="chapter" data-level="10.2.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#softmax-function"><i class="fa fa-check"></i><b>10.2.3</b> Softmax Function</a></li>
<li class="chapter" data-level="10.2.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#one-hot-encoding-and-decoding"><i class="fa fa-check"></i><b>10.2.4</b> One-Hot Encoding and Decoding</a></li>
<li class="chapter" data-level="10.2.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#cross-entropy-revisited"><i class="fa fa-check"></i><b>10.2.5</b> Cross-entropy Revisited</a></li>
<li class="chapter" data-level="10.2.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#problem-formulation-in-matrix-form"><i class="fa fa-check"></i><b>10.2.6</b> Problem Formulation in Matrix Form (**)</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neural-networks"><i class="fa fa-check"></i><b>10.3</b> Artificial Neural Networks</a><ul>
<li class="chapter" data-level="10.3.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#artificial-neuron"><i class="fa fa-check"></i><b>10.3.1</b> Artificial Neuron</a></li>
<li class="chapter" data-level="10.3.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#logistic-regression-as-a-neural-network"><i class="fa fa-check"></i><b>10.3.2</b> Logistic Regression as a Neural Network</a></li>
<li class="chapter" data-level="10.3.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r-6"><i class="fa fa-check"></i><b>10.3.3</b> Example in R</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#deep-neural-networks"><i class="fa fa-check"></i><b>10.4</b> Deep Neural Networks</a><ul>
<li class="chapter" data-level="10.4.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-15"><i class="fa fa-check"></i><b>10.4.1</b> Introduction</a></li>
<li class="chapter" data-level="10.4.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#activation-functions"><i class="fa fa-check"></i><b>10.4.2</b> Activation Functions</a></li>
<li class="chapter" data-level="10.4.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---2-layers"><i class="fa fa-check"></i><b>10.4.3</b> Example in R - 2 Layers</a></li>
<li class="chapter" data-level="10.4.4" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#example-in-r---6-layers"><i class="fa fa-check"></i><b>10.4.4</b> Example in R - 6 Layers</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#preprocessing-of-data"><i class="fa fa-check"></i><b>10.5</b> Preprocessing of Data</a><ul>
<li class="chapter" data-level="10.5.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#introduction-16"><i class="fa fa-check"></i><b>10.5.1</b> Introduction</a></li>
<li class="chapter" data-level="10.5.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#image-deskewing"><i class="fa fa-check"></i><b>10.5.2</b> Image Deskewing</a></li>
<li class="chapter" data-level="10.5.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#summary-of-all-the-models-considered"><i class="fa fa-check"></i><b>10.5.3</b> Summary of All the Models Considered</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#outro-8"><i class="fa fa-check"></i><b>10.6</b> Outro</a><ul>
<li class="chapter" data-level="10.6.1" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#remarks-8"><i class="fa fa-check"></i><b>10.6.1</b> Remarks</a></li>
<li class="chapter" data-level="10.6.2" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#beyond-mnist"><i class="fa fa-check"></i><b>10.6.2</b> Beyond MNIST</a></li>
<li class="chapter" data-level="10.6.3" data-path="shallow-and-deep-neural-networks.html"><a href="shallow-and-deep-neural-networks.html#further-reading-8"><i class="fa fa-check"></i><b>10.6.3</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="recommender-systems.html"><a href="recommender-systems.html"><i class="fa fa-check"></i><b>11</b> Recommender Systems</a><ul>
<li class="chapter" data-level="11.1" data-path="recommender-systems.html"><a href="recommender-systems.html#introduction-17"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="recommender-systems.html"><a href="recommender-systems.html#the-netflix-prize"><i class="fa fa-check"></i><b>11.1.1</b> The Netflix Prize</a></li>
<li class="chapter" data-level="11.1.2" data-path="recommender-systems.html"><a href="recommender-systems.html#main-approaches"><i class="fa fa-check"></i><b>11.1.2</b> Main Approaches</a></li>
<li class="chapter" data-level="11.1.3" data-path="recommender-systems.html"><a href="recommender-systems.html#formalism-2"><i class="fa fa-check"></i><b>11.1.3</b> Formalism</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="recommender-systems.html"><a href="recommender-systems.html#collaborative-filtering"><i class="fa fa-check"></i><b>11.2</b> Collaborative Filtering</a><ul>
<li class="chapter" data-level="11.2.1" data-path="recommender-systems.html"><a href="recommender-systems.html#example"><i class="fa fa-check"></i><b>11.2.1</b> Example</a></li>
<li class="chapter" data-level="11.2.2" data-path="recommender-systems.html"><a href="recommender-systems.html#similarity-measures"><i class="fa fa-check"></i><b>11.2.2</b> Similarity Measures</a></li>
<li class="chapter" data-level="11.2.3" data-path="recommender-systems.html"><a href="recommender-systems.html#user-based-collaborative-filtering"><i class="fa fa-check"></i><b>11.2.3</b> User-Based Collaborative Filtering</a></li>
<li class="chapter" data-level="11.2.4" data-path="recommender-systems.html"><a href="recommender-systems.html#item-based-collaborative-filtering"><i class="fa fa-check"></i><b>11.2.4</b> Item-Based Collaborative Filtering</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="recommender-systems.html"><a href="recommender-systems.html#exercise-the-movielens-dataset"><i class="fa fa-check"></i><b>11.3</b> Exercise: The MovieLens Dataset (*)</a><ul>
<li class="chapter" data-level="11.3.1" data-path="recommender-systems.html"><a href="recommender-systems.html#dataset"><i class="fa fa-check"></i><b>11.3.1</b> Dataset</a></li>
<li class="chapter" data-level="11.3.2" data-path="recommender-systems.html"><a href="recommender-systems.html#data-cleansing"><i class="fa fa-check"></i><b>11.3.2</b> Data Cleansing</a></li>
<li class="chapter" data-level="11.3.3" data-path="recommender-systems.html"><a href="recommender-systems.html#item-item-similarities"><i class="fa fa-check"></i><b>11.3.3</b> Item-Item Similarities</a></li>
<li class="chapter" data-level="11.3.4" data-path="recommender-systems.html"><a href="recommender-systems.html#example-recommendations"><i class="fa fa-check"></i><b>11.3.4</b> Example Recommendations</a></li>
<li class="chapter" data-level="11.3.5" data-path="recommender-systems.html"><a href="recommender-systems.html#clustering-1"><i class="fa fa-check"></i><b>11.3.5</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="recommender-systems.html"><a href="recommender-systems.html#outro-9"><i class="fa fa-check"></i><b>11.4</b> Outro</a><ul>
<li class="chapter" data-level="11.4.1" data-path="recommender-systems.html"><a href="recommender-systems.html#remarks-9"><i class="fa fa-check"></i><b>11.4.1</b> Remarks</a></li>
<li class="chapter" data-level="11.4.2" data-path="recommender-systems.html"><a href="recommender-systems.html#further-reading-9"><i class="fa fa-check"></i><b>11.4.2</b> Further Reading</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="natural-language-processing.html"><a href="natural-language-processing.html"><i class="fa fa-check"></i><b>12</b> Natural Language Processing</a><ul>
<li class="chapter" data-level="12.1" data-path="natural-language-processing.html"><a href="natural-language-processing.html#to-do"><i class="fa fa-check"></i><b>12.1</b> TO DO</a></li>
<li class="chapter" data-level="12.2" data-path="natural-language-processing.html"><a href="natural-language-processing.html#further-reading-10"><i class="fa fa-check"></i><b>12.2</b> Further Reading</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="notation-convention.html"><a href="notation-convention.html"><i class="fa fa-check"></i><b>A</b> Notation Convention</a></li>
<li class="chapter" data-level="B" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html"><i class="fa fa-check"></i><b>B</b> Setting Up the R Environment</a><ul>
<li class="chapter" data-level="B.1" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-r"><i class="fa fa-check"></i><b>B.1</b> Installing R</a></li>
<li class="chapter" data-level="B.2" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-an-ide"><i class="fa fa-check"></i><b>B.2</b> Installing an IDE</a></li>
<li class="chapter" data-level="B.3" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#installing-recommended-packages"><i class="fa fa-check"></i><b>B.3</b> Installing Recommended Packages</a></li>
<li class="chapter" data-level="B.4" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#first-r-script-in-rstudio"><i class="fa fa-check"></i><b>B.4</b> First R Script in RStudio</a></li>
<li class="chapter" data-level="B.5" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#exercises-6"><i class="fa fa-check"></i><b>B.5</b> Exercises</a><ul>
<li class="chapter" data-level="B.5.1" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#first-steps-with-vectors"><i class="fa fa-check"></i><b>B.5.1</b> First Steps with Vectors</a></li>
<li class="chapter" data-level="B.5.2" data-path="setting-up-the-r-environment.html"><a href="setting-up-the-r-environment.html#basic-plotting"><i class="fa fa-check"></i><b>B.5.2</b> Basic Plotting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html"><i class="fa fa-check"></i><b>C</b> Vector Algebra in R</a><ul>
<li class="chapter" data-level="C.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#motivation-1"><i class="fa fa-check"></i><b>C.1</b> Motivation</a></li>
<li class="chapter" data-level="C.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#numeric-vectors"><i class="fa fa-check"></i><b>C.2</b> Numeric Vectors</a><ul>
<li class="chapter" data-level="C.2.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-numeric-vectors"><i class="fa fa-check"></i><b>C.2.1</b> Creating Numeric Vectors</a></li>
<li class="chapter" data-level="C.2.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-scalar-operations"><i class="fa fa-check"></i><b>C.2.2</b> Vector-Scalar Operations</a></li>
<li class="chapter" data-level="C.2.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-vector-operations"><i class="fa fa-check"></i><b>C.2.3</b> Vector-Vector Operations</a></li>
<li class="chapter" data-level="C.2.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions"><i class="fa fa-check"></i><b>C.2.4</b> Aggregation Functions</a></li>
<li class="chapter" data-level="C.2.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#special-functions"><i class="fa fa-check"></i><b>C.2.5</b> Special Functions</a></li>
<li class="chapter" data-level="C.2.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#norms-and-distances"><i class="fa fa-check"></i><b>C.2.6</b> Norms and Distances</a></li>
<li class="chapter" data-level="C.2.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#dot-product"><i class="fa fa-check"></i><b>C.2.7</b> Dot Product (*)</a></li>
<li class="chapter" data-level="C.2.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#missing-and-other-special-values"><i class="fa fa-check"></i><b>C.2.8</b> Missing and Other Special Values</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-vectors"><i class="fa fa-check"></i><b>C.3</b> Logical Vectors</a><ul>
<li class="chapter" data-level="C.3.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-logical-vectors"><i class="fa fa-check"></i><b>C.3.1</b> Creating Logical Vectors</a></li>
<li class="chapter" data-level="C.3.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#logical-operations"><i class="fa fa-check"></i><b>C.3.2</b> Logical Operations</a></li>
<li class="chapter" data-level="C.3.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#comparison-operations"><i class="fa fa-check"></i><b>C.3.3</b> Comparison Operations</a></li>
<li class="chapter" data-level="C.3.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#aggregation-functions-1"><i class="fa fa-check"></i><b>C.3.4</b> Aggregation Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#character-vectors"><i class="fa fa-check"></i><b>C.4</b> Character Vectors</a><ul>
<li class="chapter" data-level="C.4.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-character-vectors"><i class="fa fa-check"></i><b>C.4.1</b> Creating Character Vectors</a></li>
<li class="chapter" data-level="C.4.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#concatenating-character-vectors"><i class="fa fa-check"></i><b>C.4.2</b> Concatenating Character Vectors</a></li>
<li class="chapter" data-level="C.4.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#collapsing-character-vectors"><i class="fa fa-check"></i><b>C.4.3</b> Collapsing Character Vectors</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#vector-subsetting"><i class="fa fa-check"></i><b>C.5</b> Vector Subsetting</a><ul>
<li class="chapter" data-level="C.5.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-positive-indices"><i class="fa fa-check"></i><b>C.5.1</b> Subsetting with Positive Indices</a></li>
<li class="chapter" data-level="C.5.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-negative-indices"><i class="fa fa-check"></i><b>C.5.2</b> Subsetting with Negative Indices</a></li>
<li class="chapter" data-level="C.5.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-with-logical-vectors"><i class="fa fa-check"></i><b>C.5.3</b> Subsetting with Logical Vectors</a></li>
<li class="chapter" data-level="C.5.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#replacing-elements"><i class="fa fa-check"></i><b>C.5.4</b> Replacing Elements</a></li>
<li class="chapter" data-level="C.5.5" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#other-functions"><i class="fa fa-check"></i><b>C.5.5</b> Other Functions</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-vectors"><i class="fa fa-check"></i><b>C.6</b> Named Vectors</a><ul>
<li class="chapter" data-level="C.6.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-named-vectors"><i class="fa fa-check"></i><b>C.6.1</b> Creating Named Vectors</a></li>
<li class="chapter" data-level="C.6.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-named-vectors-with-character-string-indices"><i class="fa fa-check"></i><b>C.6.2</b> Subsetting Named Vectors with Character String Indices</a></li>
</ul></li>
<li class="chapter" data-level="C.7" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#factors"><i class="fa fa-check"></i><b>C.7</b> Factors</a><ul>
<li class="chapter" data-level="C.7.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-factors"><i class="fa fa-check"></i><b>C.7.1</b> Creating Factors</a></li>
<li class="chapter" data-level="C.7.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#levels"><i class="fa fa-check"></i><b>C.7.2</b> Levels</a></li>
<li class="chapter" data-level="C.7.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#internal-representation"><i class="fa fa-check"></i><b>C.7.3</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="C.8" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#lists"><i class="fa fa-check"></i><b>C.8</b> Lists</a><ul>
<li class="chapter" data-level="C.8.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#creating-lists"><i class="fa fa-check"></i><b>C.8.1</b> Creating Lists</a></li>
<li class="chapter" data-level="C.8.2" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#named-lists"><i class="fa fa-check"></i><b>C.8.2</b> Named Lists</a></li>
<li class="chapter" data-level="C.8.3" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#subsetting-and-extracting-from-lists"><i class="fa fa-check"></i><b>C.8.3</b> Subsetting and Extracting From Lists</a></li>
<li class="chapter" data-level="C.8.4" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#common-operations"><i class="fa fa-check"></i><b>C.8.4</b> Common Operations</a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#exercises-7"><i class="fa fa-check"></i><b>C.9</b> Exercises</a><ul>
<li class="chapter" data-level="C.9.1" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#audeur-exchange-rates"><i class="fa fa-check"></i><b>C.9.1</b> AUD/EUR Exchange Rates</a></li>
</ul></li>
<li class="chapter" data-level="C.10" data-path="vector-algebra-in-r.html"><a href="vector-algebra-in-r.html#further-reading-11"><i class="fa fa-check"></i><b>C.10</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html"><i class="fa fa-check"></i><b>D</b> Matrix Algebra in R</a><ul>
<li class="chapter" data-level="D.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#creating-matrices"><i class="fa fa-check"></i><b>D.1</b> Creating Matrices</a><ul>
<li class="chapter" data-level="D.1.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix"><i class="fa fa-check"></i><b>D.1.1</b> <code>matrix()</code></a></li>
<li class="chapter" data-level="D.1.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#stacking-vectors"><i class="fa fa-check"></i><b>D.1.2</b> Stacking Vectors</a></li>
<li class="chapter" data-level="D.1.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#beyond-numeric-matrices"><i class="fa fa-check"></i><b>D.1.3</b> Beyond Numeric Matrices</a></li>
<li class="chapter" data-level="D.1.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#naming-rows-and-columns"><i class="fa fa-check"></i><b>D.1.4</b> Naming Rows and Columns</a></li>
<li class="chapter" data-level="D.1.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#other-methods"><i class="fa fa-check"></i><b>D.1.5</b> Other Methods</a></li>
<li class="chapter" data-level="D.1.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#internal-representation-1"><i class="fa fa-check"></i><b>D.1.6</b> Internal Representation (*)</a></li>
</ul></li>
<li class="chapter" data-level="D.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#common-operations-1"><i class="fa fa-check"></i><b>D.2</b> Common Operations</a><ul>
<li class="chapter" data-level="D.2.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-transpose"><i class="fa fa-check"></i><b>D.2.1</b> Matrix Transpose</a></li>
<li class="chapter" data-level="D.2.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-scalar-operations"><i class="fa fa-check"></i><b>D.2.2</b> Matrix-Scalar Operations</a></li>
<li class="chapter" data-level="D.2.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-matrix-operations"><i class="fa fa-check"></i><b>D.2.3</b> Matrix-Matrix Operations</a></li>
<li class="chapter" data-level="D.2.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-multiplication"><i class="fa fa-check"></i><b>D.2.4</b> Matrix Multiplication (*)</a></li>
<li class="chapter" data-level="D.2.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#aggregation-of-rows-and-columns"><i class="fa fa-check"></i><b>D.2.5</b> Aggregation of Rows and Columns</a></li>
<li class="chapter" data-level="D.2.6" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#vectorised-special-functions"><i class="fa fa-check"></i><b>D.2.6</b> Vectorised Special Functions</a></li>
<li class="chapter" data-level="D.2.7" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-vector-operations"><i class="fa fa-check"></i><b>D.2.7</b> Matrix-Vector Operations</a></li>
</ul></li>
<li class="chapter" data-level="D.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#matrix-subsetting"><i class="fa fa-check"></i><b>D.3</b> Matrix Subsetting</a><ul>
<li class="chapter" data-level="D.3.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-individual-elements"><i class="fa fa-check"></i><b>D.3.1</b> Selecting Individual Elements</a></li>
<li class="chapter" data-level="D.3.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>D.3.2</b> Selecting Rows and Columns</a></li>
<li class="chapter" data-level="D.3.3" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-submatrices"><i class="fa fa-check"></i><b>D.3.3</b> Selecting Submatrices</a></li>
<li class="chapter" data-level="D.3.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-logical-vectors-and-matrices"><i class="fa fa-check"></i><b>D.3.4</b> Selecting Based on Logical Vectors and Matrices</a></li>
<li class="chapter" data-level="D.3.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#selecting-based-on-two-column-matrices"><i class="fa fa-check"></i><b>D.3.5</b> Selecting Based on Two-Column Matrices</a></li>
</ul></li>
<li class="chapter" data-level="D.4" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#exercises-8"><i class="fa fa-check"></i><b>D.4</b> Exercises</a><ul>
<li class="chapter" data-level="D.4.1" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#currency-exchange-rates"><i class="fa fa-check"></i><b>D.4.1</b> Currency Exchange Rates</a></li>
<li class="chapter" data-level="D.4.2" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#currency-exchange-rates-relative-to-1999"><i class="fa fa-check"></i><b>D.4.2</b> Currency Exchange Rates Relative to 1999</a></li>
</ul></li>
<li class="chapter" data-level="D.5" data-path="matrix-algebra-in-r.html"><a href="matrix-algebra-in-r.html#further-reading-12"><i class="fa fa-check"></i><b>D.5</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html"><i class="fa fa-check"></i><b>E</b> Data Frame Wrangling in R</a><ul>
<li class="chapter" data-level="E.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#creating-data-frames"><i class="fa fa-check"></i><b>E.1</b> Creating Data Frames</a></li>
<li class="chapter" data-level="E.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#importing-data-frames"><i class="fa fa-check"></i><b>E.2</b> Importing Data Frames</a></li>
<li class="chapter" data-level="E.3" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#data-frame-subsetting"><i class="fa fa-check"></i><b>E.3</b> Data Frame Subsetting</a><ul>
<li class="chapter" data-level="E.3.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-a-list"><i class="fa fa-check"></i><b>E.3.1</b> Each Data Frame is a List</a></li>
<li class="chapter" data-level="E.3.2" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#each-data-frame-is-matrix-like"><i class="fa fa-check"></i><b>E.3.2</b> Each Data Frame is Matrix-like</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#common-operations-2"><i class="fa fa-check"></i><b>E.4</b> Common Operations</a></li>
<li class="chapter" data-level="E.5" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#metaprogramming-and-formulas"><i class="fa fa-check"></i><b>E.5</b> Metaprogramming and Formulas (*)</a></li>
<li class="chapter" data-level="E.6" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#exercises-9"><i class="fa fa-check"></i><b>E.6</b> Exercises</a><ul>
<li class="chapter" data-level="E.6.1" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#urban-forest"><i class="fa fa-check"></i><b>E.6.1</b> Urban Forest</a></li>
</ul></li>
<li class="chapter" data-level="E.7" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#air-quality"><i class="fa fa-check"></i><b>E.7</b> Air Quality</a></li>
<li class="chapter" data-level="E.8" data-path="data-frame-wrangling-in-r.html"><a href="data-frame-wrangling-in-r.html#further-reading-13"><i class="fa fa-check"></i><b>E.8</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="datasets.html"><a href="datasets.html"><i class="fa fa-check"></i><b>F</b> Datasets</a><ul>
<li class="chapter" data-level="F.1" data-path="datasets.html"><a href="datasets.html#air-quality-1"><i class="fa fa-check"></i><b>F.1</b> Air Quality</a></li>
<li class="chapter" data-level="F.2" data-path="datasets.html"><a href="datasets.html#currency-exchange-rates-1"><i class="fa fa-check"></i><b>F.2</b> Currency Exchange Rates</a></li>
<li class="chapter" data-level="F.3" data-path="datasets.html"><a href="datasets.html#urban-forest-1"><i class="fa fa-check"></i><b>F.3</b> Urban Forest</a></li>
<li class="chapter" data-level="F.4" data-path="datasets.html"><a href="datasets.html#wine-quality"><i class="fa fa-check"></i><b>F.4</b> Wine Quality</a></li>
<li class="chapter" data-level="F.5" data-path="datasets.html"><a href="datasets.html#the-world-factbook-countries-of-the-world"><i class="fa fa-check"></i><b>F.5</b> The World Factbook (Countries of the World)</a></li>
<li class="chapter" data-level="F.6" data-path="datasets.html"><a href="datasets.html#edstats-country-level-education-statistics"><i class="fa fa-check"></i><b>F.6</b> EdStats (Country-Level Education Statistics)</a></li>
<li class="chapter" data-level="F.7" data-path="datasets.html"><a href="datasets.html#food-and-nutrient-database-for-dietary-studies-fndds"><i class="fa fa-check"></i><b>F.7</b> Food and Nutrient Database for Dietary Studies (FNDDS)</a></li>
<li class="chapter" data-level="F.8" data-path="datasets.html"><a href="datasets.html#clustering-benchmarks"><i class="fa fa-check"></i><b>F.8</b> Clustering Benchmarks</a></li>
<li class="chapter" data-level="F.9" data-path="datasets.html"><a href="datasets.html#movie-lens-todo"><i class="fa fa-check"></i><b>F.9</b> Movie Lens (TODO)</a></li>
<li class="chapter" data-level="F.10" data-path="datasets.html"><a href="datasets.html#other-todo"><i class="fa fa-check"></i><b>F.10</b> Other (TODO)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li style='font-size: smaller' ><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Licensed under CC BY-NC-ND 4.0</a></li>
<li style='font-size: smaller' ><a href="./">DRAFT v0.3 2020-06-08 21:11 (2ec3f97)</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lightweight Machine Learning Classics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-with-k-nearest-neighbours" class="section level1">
<h1><span class="header-section-number">2</span> Classification with K-Nearest Neighbours</h1>
<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->
<!--

Add K-NN Regression

Add K-nearest centroids

-->
<!-- TODO: citations




 10.1109/TIT.1967.1053964
 10.1109/TIT.1968.1054155


 curse of dimensionality
 https://minds.wisconsin.edu/bitstream/handle/1793/60174/TR1377.pdf?sequence=1
 etc.

 vp-trees, gnats, kd-trees, FANN
-->
<!-- TODO:


2D illustration of NEAREST NEIGHBOURS!!!

exercise -- density, not NN-based classification or
regression - consider the epsilon-neighbourhood????

-->
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<div id="classification-task" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Classification Task</h3>
<p>Let <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span> be an input matrix
that consists of <span class="math inline">\(n\)</span> points in a <span class="math inline">\(p\)</span>-dimensional space (each of the <span class="math inline">\(n\)</span> objects
is described by means of <span class="math inline">\(p\)</span> numerical features).</p>
<p>Recall that in supervised learning, with each
<span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> we associate the desired output <span class="math inline">\(y_i\)</span>.</p>
<p><span class="math display">\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p} \\
x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p} \\
\end{array}
\right]
\qquad
\mathbf{y} = \left[
\begin{array}{c}
{y}_{1} \\
{y}_{2} \\
\vdots\\
{y}_{n} \\
\end{array}
\right].
\]</span></p>
<div style="margin-top: 1em">

</div>
<p>In this chapter we are interested in <strong>classification</strong> tasks;
we assume that each <span class="math inline">\(y_i\)</span> is a <em>label</em> (e.g., a character string) –
it is of quantitative/categorical type.</p>
<p>Most commonly, we are faced with <strong>binary classification</strong> tasks
where there are only two possible distinct labels.</p>
<p>We traditionally denote them with <span class="math inline">\(0\)</span>s and <span class="math inline">\(1\)</span>s.</p>
<p>For example:</p>
<table>
<thead>
<tr class="header">
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>no</td>
<td>yes</td>
</tr>
<tr class="even">
<td>false</td>
<td>true</td>
</tr>
<tr class="odd">
<td>failure</td>
<td>success</td>
</tr>
<tr class="even">
<td>healthy</td>
<td>ill</td>
</tr>
</tbody>
</table>
<p>On the other hand, in <strong>multiclass classification</strong>,
we assume that each <span class="math inline">\(y_i\)</span> takes more than two possible values.</p>
<p>Example plot of a synthetic dataset
with the reference binary <span class="math inline">\(y\)</span>s is given in Figure <a href="classification-with-k-nearest-neighbours.html#fig:classify_intro">1</a>.
The “true” decision boundary is at <span class="math inline">\(X_1=0\)</span> but the classes
slightly overlap (the dataset is a bit noisy).</p>
<div class="figure">
<img src="02-neighbours-figures/classify_intro-1.svg" alt="Figure 1: A synthetic 2D dataset with the true decision boundary at X_1=0" id="fig:classify_intro" />
<p class="caption">Figure 1: A synthetic 2D dataset with the true decision boundary at <span class="math inline">\(X_1=0\)</span></p>
</div>
</div>
<div id="data" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Data</h3>
<p>For illustration, let’s consider the Wine Quality dataset <span class="citation">(Cortez et al. <a href="references.html#ref-wines">2009</a>)</span>
that can be downloaded from the UCI Machine Learning Repository
(<a href="https://archive.ics.uci.edu/ml/datasets/Wine+Quality" class="uri">https://archive.ics.uci.edu/ml/datasets/Wine+Quality</a>) –
white wines only.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" title="1">wine_quality &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/wine_quality_all.csv&quot;</span>,</a>
<a class="sourceLine" id="cb32-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a>
<a class="sourceLine" id="cb32-3" title="3">white_wines &lt;-<span class="st"> </span>wine_quality[wine_quality<span class="op">$</span>color <span class="op">==</span><span class="st"> &quot;white&quot;</span>,]</a>
<a class="sourceLine" id="cb32-4" title="4">(n &lt;-<span class="st"> </span><span class="kw">nrow</span>(white_wines)) <span class="co"># number of samples</span></a></code></pre></div>
<pre><code>## [1] 4898</code></pre>
<p>These are Vinho Verde wine samples from the north of Portugal,
see <a href="https://www.vinhoverde.pt/en/homepage" class="uri">https://www.vinhoverde.pt/en/homepage</a>.</p>
<p>There are 11 physicochemical features reported.
Moreover, there is a wine rating (which we won’t consider here)
on the scale 0 (bad) to 10 (excellent)
given by wine experts.</p>
<p>The input matrix <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span>
consists of the first 10 numeric variables:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" title="1">X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(white_wines[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])</a>
<a class="sourceLine" id="cb34-2" title="2"><span class="kw">dim</span>(X)</a></code></pre></div>
<pre><code>## [1] 4898   10</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" title="1"><span class="kw">head</span>(X, <span class="dv">2</span>) <span class="co"># first two rows</span></a></code></pre></div>
<pre><code>##      fixed.acidity volatile.acidity citric.acid residual.sugar
## 1600           7.0             0.27        0.36           20.7
## 1601           6.3             0.30        0.34            1.6
##      chlorides free.sulfur.dioxide total.sulfur.dioxide density  pH
## 1600     0.045                  45                  170   1.001 3.0
## 1601     0.049                  14                  132   0.994 3.3
##      sulphates
## 1600      0.45
## 1601      0.49</code></pre>
<p>The 11th variable measures the amount of alcohol (in %).</p>
<p>We will convert this dependent variable to a binary one:</p>
<ul>
<li>0 == (<code>alcohol  &lt; 12</code>) == lower-alcohol wines,</li>
<li>1 == (<code>alcohol &gt;= 12</code>) == higher-alcohol wines</li>
</ul>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" title="1"><span class="co"># recall that TRUE == 1</span></a>
<a class="sourceLine" id="cb38-2" title="2">Y &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(<span class="kw">as.numeric</span>(white_wines<span class="op">$</span>alcohol <span class="op">&gt;=</span><span class="st"> </span><span class="dv">12</span>)))</a>
<a class="sourceLine" id="cb38-3" title="3"><span class="kw">table</span>(Y)</a></code></pre></div>
<pre><code>## Y
##    0    1 
## 4085  813</code></pre>
<p>Now <span class="math inline">\((\mathbf{X},\mathbf{y})\)</span> is a basis for an interesting (yet challenging)
binary classification task.</p>
</div>
<div id="training-and-test-sets" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Training and Test Sets</h3>
<p>Recall that we are genuinely interested in the construction of supervised learning models for the two following purposes:</p>
<ul>
<li><strong>description</strong> – to explain a given dataset in simpler terms,</li>
<li><strong>prediction</strong> – to forecast the values of the dependent variable
for inputs that are yet to be observed.</li>
</ul>
<p>In the latter case:</p>
<ul>
<li>we don’t want our models to <em>overfit</em> to current data,</li>
<li>we want our models to <em>generalise</em> well
to new data.</li>
</ul>
<div style="margin-top: 1em">

</div>
<p>One way to assess if a model has sufficient predictive power is based
on a random <strong>train-test split</strong> of the original dataset:</p>
<ul>
<li><em>training sample</em> (usually 60-80% of the observations) – used to construct a model,</li>
<li><em>test sample</em> (remaining 40-20%) – used to assess the goodness of fit.</li>
</ul>
<dl>
<dt>Remark.</dt>
<dd><p><strong>Test sample must not be used in the training phase!</strong> (No cheating!)</p>
</dd>
</dl>
<p>60/40% train-test split in R:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" title="1"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># reproducibility matters</span></a>
<a class="sourceLine" id="cb40-2" title="2">random_indices &lt;-<span class="st"> </span><span class="kw">sample</span>(n)</a>
<a class="sourceLine" id="cb40-3" title="3"><span class="kw">head</span>(random_indices) <span class="co"># preview</span></a></code></pre></div>
<pre><code>## [1] 2463 2511 2227  526 4291 2986</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" title="1"><span class="co"># first 60% of the indices (they are arranged randomly)</span></a>
<a class="sourceLine" id="cb42-2" title="2"><span class="co"># will constitute the train sample:</span></a>
<a class="sourceLine" id="cb42-3" title="3">train_indices &lt;-<span class="st"> </span>random_indices[<span class="dv">1</span><span class="op">:</span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.6</span>)]</a>
<a class="sourceLine" id="cb42-4" title="4">X_train &lt;-<span class="st"> </span>X[train_indices,]</a>
<a class="sourceLine" id="cb42-5" title="5">Y_train &lt;-<span class="st"> </span>Y[train_indices]</a>
<a class="sourceLine" id="cb42-6" title="6"><span class="co"># the remaining indices (40%) go to the test sample:</span></a>
<a class="sourceLine" id="cb42-7" title="7">X_test  &lt;-<span class="st"> </span>X[<span class="op">-</span>train_indices,]</a>
<a class="sourceLine" id="cb42-8" title="8">Y_test  &lt;-<span class="st"> </span>Y[<span class="op">-</span>train_indices]</a></code></pre></div>
</div>
<div id="discussed-methods" class="section level3">
<h3><span class="header-section-number">2.1.4</span> Discussed Methods</h3>
<p>Our aim is to build a classifier that takes 10 wine physicochemical
features and determines whether it’s a “strong” wine.</p>
<p>We will discuss 3 simple and educational (yet practically useful)
classification algorithms:</p>
<ul>
<li><em>K-nearest neighbour scheme</em> – this chapter,</li>
<li><em>Decision trees</em> – the next chapter,</li>
<li><em>Logistic regression</em> – the next chapter.</li>
</ul>
</div>
</div>
<div id="k-nearest-neighbour-classifier" class="section level2">
<h2><span class="header-section-number">2.2</span> K-nearest Neighbour Classifier</h2>
<div id="introduction-2" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Introduction</h3>
<dl>
<dt>Rule.</dt>
<dd><p>“If you don’t know what to do in a situation, just act like the people around you”</p>
</dd>
</dl>
<p>For some integer <span class="math inline">\(K\ge 1\)</span>, the <strong>K-Nearest Neighbour (<em>K-NN</em>) Classifier</strong>
proceeds as follows.</p>
<p>To classify a new point <span class="math inline">\(\mathbf{x}&#39;\)</span>:</p>
<ol style="list-style-type: decimal">
<li>find the <span class="math inline">\(K\)</span> nearest neighbours of a given point <span class="math inline">\(\mathbf{x}&#39;\)</span> amongst the points in the train set,
denoted <span class="math inline">\(\mathbf{x}_{i_1,\cdot}, \dots, \mathbf{x}_{i_K,\cdot}\)</span>:
<ol style="list-style-type: lower-alpha">
<li>compute the Euclidean distances between <span class="math inline">\(\mathbf{x}&#39;\)</span> and each <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span> from the train set,
<span class="math display">\[d_i = \|\mathbf{x}&#39;-\mathbf{x}_{i,\cdot}\|\]</span></li>
<li>order <span class="math inline">\(d_i\)</span>s in increasing order,
<span class="math inline">\(d_{i_1} \le d_{i_2} \le \dots \le d_{i_K}\)</span></li>
<li>pick first <span class="math inline">\(K\)</span> indices (these are the <em>nearest</em> neighbours)</li>
</ol></li>
<li>fetch the corresponding reference labels <span class="math inline">\(y_{i_1}, \dots, y_{i_K}\)</span></li>
<li>return their <em>mode</em> as a result, i.e., the most frequently occurring label (a.k.a. <em>majority vote</em>)</li>
</ol>
<!-- > If a mode is not unique, return a randomly chosen mode (ties are broken at random). -->
<p>Here is how <span class="math inline">\(K\)</span>-NN classifier works on a synthetic 2D dataset.
Firstly let’s consider <span class="math inline">\(K=1\)</span>, see Figure <a href="classification-with-k-nearest-neighbours.html#fig:fig_plot_knn1">2</a>.
Gray and pink regions depict how new points would be classified.
In particular 1-NN is “greedy” in the sense that we just
locate the nearest point.</p>
<div class="figure">
<img src="02-neighbours-figures/fig_plot_knn1-1.svg" alt="Figure 2: 1-NN class bounds for our 2D synthetic dataset" id="fig:fig_plot_knn1" />
<p class="caption">Figure 2: 1-NN class bounds for our 2D synthetic dataset</p>
</div>
<dl>
<dt>Remark.</dt>
<dd><p>(*) 1-NN classification is essentially based
on a dataset’s so-called Voronoi diagram.</p>
</dd>
</dl>
<p>Increasing <span class="math inline">\(K\)</span> somehow smoothens the decision boundary (this makes it
less “local” and more “global”).
Figure <a href="classification-with-k-nearest-neighbours.html#fig:fig_plot_knn3">3</a> depicts the <span class="math inline">\(K=3\)</span> case.</p>
<div class="figure">
<img src="02-neighbours-figures/fig_plot_knn3-1.svg" alt="Figure 3: 3-NN class bounds for our 2D synthetic dataset" id="fig:fig_plot_knn3" />
<p class="caption">Figure 3: 3-NN class bounds for our 2D synthetic dataset</p>
</div>
<div class="figure">
<img src="02-neighbours-figures/fig_plot_knn25-1.svg" alt="Figure 4: 25-NN class bounds for our 2D synthetic dataset" id="fig:fig_plot_knn25" />
<p class="caption">Figure 4: 25-NN class bounds for our 2D synthetic dataset</p>
</div>
<p>Recall that the “true” decision boundary for this synthetic dataset
is at <span class="math inline">\(X_1=0\)</span>. The 25-NN classifier did quite a good job, see Figure <a href="classification-with-k-nearest-neighbours.html#fig:fig_plot_knn25">4</a>.</p>
</div>
<div id="example-in-r" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Example in R</h3>
<p>We shall be calling the <code>knn()</code> function from package <code>FNN</code>
to classify the points from the test sample
extracted from the <code>white_wines</code> dataset:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" title="1"><span class="kw">library</span>(<span class="st">&quot;FNN&quot;</span>)</a></code></pre></div>
<p>Let’s make prediction using the 5-nn classifier:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" title="1">Y_knn5 &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb44-2" title="2"><span class="kw">head</span>(Y_test, <span class="dv">28</span>) <span class="co"># True Ys</span></a></code></pre></div>
<pre><code>##  [1] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" title="1"><span class="kw">head</span>(Y_knn5, <span class="dv">28</span>) <span class="co"># Predicted Ys</span></a></code></pre></div>
<pre><code>##  [1] 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" title="1"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn5) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.81735</code></pre>
<p>9-nn classifier:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb50-1" title="1">Y_knn9 &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">9</span>)</a>
<a class="sourceLine" id="cb50-2" title="2"><span class="kw">head</span>(Y_test, <span class="dv">28</span>) <span class="co"># True Ys</span></a></code></pre></div>
<pre><code>##  [1] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb52-1" title="1"><span class="kw">head</span>(Y_knn9, <span class="dv">28</span>) <span class="co"># Predicted Ys</span></a></code></pre></div>
<pre><code>##  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## Levels: 0 1</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb54-1" title="1"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn9) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.81939</code></pre>
</div>
<div id="feature-engineering" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Feature Engineering</h3>
<p>Note that the Euclidean distance that we used above
implicitly assumes that every feature (independent variable)
is on the same scale.</p>
<p>However, when dealing with, e.g., physical quantities,
we often perform conversions of units of measurement (kg → g, feet → m etc.).</p>
<p>Transforming a single feature may drastically change the metric
structure of the dataset
and therefore highly affect the obtained predictions.</p>
<p>To “bring data to the same scale”, we often apply a trick called <strong>standardisation</strong>.</p>
<p>Computing the so-called <strong>Z-scores</strong> of the <span class="math inline">\(j\)</span>-th feature, <span class="math inline">\(\mathbf{x}_{\cdot,j}\)</span>,
is done by subtracting from each observation the sample mean and dividing the result by the sample
standard deviation:</p>
<p><span class="math display">\[z_{i,j} = \frac{x_{i,j}-\bar{x}_{\cdot,j}}{s_{x_{\cdot,j}}}\]</span></p>
<p>This a new feature <span class="math inline">\(\mathbf{z}_{\cdot,j}\)</span> that always has mean 0 and standard deviation of 1.</p>
<p>Moreover, it is <em>unit-less</em> (e.g., we divide a value in kgs by a value in kgs,
the units are cancelled out).
This, amongst others, prevents one of the features from dominating
the other ones.</p>
<p>Z-scores are easy to interpret, e.g., 0.5 denotes an observation
that is 0.5 standard deviations above the mean
and -3 informs us that a value is 3 standard deviations below the mean.</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) If data are normally distributed (bell-shaped histogram),
with very high probability, most (expected value is 99.74%) observations
should have Z-scores between -3 and 3. Those that don’t, are
“suspicious”, maybe they are outliers? We should inspect them manually.</p>
</dd>
</dl>
<p>Let’s compute <code>Z_train</code> and <code>Z_test</code>,
being the standardised versions of <code>X_train</code>
and <code>X_test</code>, respectively.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" title="1">means &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">2</span>, mean) <span class="co"># column means</span></a>
<a class="sourceLine" id="cb56-2" title="2">sds   &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">2</span>, sd)   <span class="co"># column standard deviations</span></a>
<a class="sourceLine" id="cb56-3" title="3">Z_train &lt;-<span class="st"> </span>X_train <span class="co"># copy</span></a>
<a class="sourceLine" id="cb56-4" title="4">Z_test  &lt;-<span class="st"> </span>X_test  <span class="co"># copy</span></a>
<a class="sourceLine" id="cb56-5" title="5"><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(X)) {</a>
<a class="sourceLine" id="cb56-6" title="6">    Z_train[,j] &lt;-<span class="st"> </span>(Z_train[,j]<span class="op">-</span>means[j])<span class="op">/</span>sds[j]</a>
<a class="sourceLine" id="cb56-7" title="7">    Z_test[,j]  &lt;-<span class="st"> </span>(Z_test[,j] <span class="op">-</span>means[j])<span class="op">/</span>sds[j]</a>
<a class="sourceLine" id="cb56-8" title="8">}</a></code></pre></div>
<p>Note that we have transformed the training and test sample in the very same
way. Computing means and standard deviations separately for these two datasets
is a common error – it is the training set that we use in the course of the
learning process.
The above can be re-written as:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" title="1">Z_train &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(X_train, <span class="dv">1</span>, <span class="cf">function</span>(r) (r<span class="op">-</span>means)<span class="op">/</span>sds))</a>
<a class="sourceLine" id="cb57-2" title="2">Z_test  &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(X_test,  <span class="dv">1</span>, <span class="cf">function</span>(r) (r<span class="op">-</span>means)<span class="op">/</span>sds))</a></code></pre></div>
<p>See Figure <a href="classification-with-k-nearest-neighbours.html#fig:standardise_depict_hist">5</a> for an illustration.
Note that the righthand figures (histograms of standardised variables)
are on the same scale now.</p>
<div class="figure">
<img src="02-neighbours-figures/standardise_depict_hist-1.svg" alt="Figure 5: Empirical distribution of two variables (pH on the top, fixed.acidity on the bottom) before (left) and after (right) standardising" id="fig:standardise_depict_hist" />
<p class="caption">Figure 5: Empirical distribution of two variables (pH on the top, fixed.acidity on the bottom) before (left) and after (right) standardising</p>
</div>
<dl>
<dt>Remark.</dt>
<dd><p>Of course, standardisation is only about shifting and scaling, it
preserves the shape of the distribution. If the original variable
is right skewed or bimodal, its standardised version will remain as such.</p>
</dd>
</dl>
<p>Let’s compute the accuracy of K-NN classifiers acting on standardised data.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb58-1" title="1">Y_knn5s &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train, Z_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb58-2" title="2"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn5s) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.91429</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb60-1" title="1">Y_knn9s &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train, Z_test, Y_train, <span class="dt">k=</span><span class="dv">9</span>)</a>
<a class="sourceLine" id="cb60-2" title="2"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn9s) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.91378</code></pre>
<p>The accuracy is much better.</p>
<p>Standardisation is an example of <em>feature engineering</em>.</p>
<p>Good models rarely work well “straight out of the box” – if that was the case,
we wouldn’t need data scientists and machine learning engineers!</p>
<p>To increase models’ accuracy, we often spend a lot of time:</p>
<ul>
<li>cleansing data (e.g., removing outliers)</li>
<li>extracting new features</li>
<li>transforming existing features</li>
<li>trying to find a set of features that are relevant</li>
</ul>
<p>This is the “more art than science” part of data science (sic!), and
hence most textbooks are not really eager for discussing such topics
(including this one).</p>
<p>Sorry, this is sad but true. The solutions that work well in the case of dataset
A may fail in the B case and vice versa. However, the more exercises you solve,
the greater the arsenal of ideas/possible approaches you will have at hand
when dealing with real-world problems.</p>
<p>Feature selection – example (manually selected columns):</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" title="1">features &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;density&quot;</span>, <span class="st">&quot;residual.sugar&quot;</span>)</a>
<a class="sourceLine" id="cb62-2" title="2">Y_knn5s &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train[,features], Z_test[,features],</a>
<a class="sourceLine" id="cb62-3" title="3">    Y_train, <span class="dt">k=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb62-4" title="4"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn5s) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.91633</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" title="1">Y_knn9s &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train[,features], Z_test[,features],</a>
<a class="sourceLine" id="cb64-2" title="2">    Y_train, <span class="dt">k=</span><span class="dv">9</span>)</a>
<a class="sourceLine" id="cb64-3" title="3"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_knn9s) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.925</code></pre>
<div class="exercise"><strong>Exercise.</strong>
<p>Try to find a combination of 2-4 features (by guessing or applying magic tricks)
that increases the accuracy of a <span class="math inline">\(K\)</span>-NN classifier on this dataset.</p>
</div>
</div>
</div>
<div id="model-assessment-and-selection" class="section level2">
<h2><span class="header-section-number">2.3</span> Model Assessment and Selection</h2>
<div id="performance-metrics" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Performance Metrics</h3>
<!--

More metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics

-->
<p>Recall that <span class="math inline">\(y_i\)</span> denotes the true label associated with the <span class="math inline">\(i\)</span>-th observation.</p>
<p>Let <span class="math inline">\(\hat{y}_i\)</span> denote the classifier’s output for a given <span class="math inline">\(\mathbf{x}_{i,\cdot}\)</span>.</p>
<p>Ideally, we’d wish that <span class="math inline">\(\hat{y}_i=y_i\)</span>.</p>
<p>Sadly, in practice we will make errors.</p>
<p>Here are the 4 possible situations (true vs. predicted label):</p>
<table>
<thead>
<tr class="header">
<th>.</th>
<th><span class="math inline">\(y_i=0\)</span></th>
<th><span class="math inline">\(y_i=1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\hat{y}_i=0\)</span></td>
<td><strong>True Negative</strong></td>
<td>False Negative (Type II error)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{y}_i=1\)</span></td>
<td>False Positive (Type I error)</td>
<td><strong>True Positive</strong></td>
</tr>
</tbody>
</table>
<p>Note that the terms <strong>positive</strong> and <strong>negative</strong> refer to
the classifier’s output, i.e., occur when <span class="math inline">\(\hat{y}_i\)</span> is equal to <span class="math inline">\(1\)</span> and <span class="math inline">\(0\)</span>, respectively.</p>
<p>A <strong>confusion matrix</strong> is used to summarise
the correctness of predictions for the whole sample:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" title="1">Y_pred &lt;-<span class="st"> </span><span class="kw">knn</span>(Z_train, Z_test, Y_train, <span class="dt">k=</span><span class="dv">9</span>)</a>
<a class="sourceLine" id="cb66-2" title="2">(C &lt;-<span class="st"> </span><span class="kw">table</span>(Y_pred, Y_test))</a></code></pre></div>
<pre><code>##       Y_test
## Y_pred    0    1
##      0 1607  133
##      1   36  184</code></pre>
<p>For example,</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" title="1">C[<span class="dv">1</span>,<span class="dv">1</span>] <span class="co"># number of TNs</span></a></code></pre></div>
<pre><code>## [1] 1607</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" title="1">C[<span class="dv">2</span>,<span class="dv">1</span>] <span class="co"># number of FPs</span></a></code></pre></div>
<pre><code>## [1] 36</code></pre>
<p><strong>Accuracy</strong> is the ratio of the correctly classified instances
to all the instances.</p>
<p>In other words, it is the probability of making a correct prediction.</p>
<p><span class="math display">\[
\text{Accuracy} = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
= \frac{1}{n} \sum_{i=1}^n \mathbb{I}\left(
y_i = \hat{y}_i
\right)
\]</span>
where <span class="math inline">\(\mathbb{I}\)</span> is the indicator function,
<span class="math inline">\(\mathbb{I}(l)=1\)</span> if logical condition <span class="math inline">\(l\)</span> is true and <span class="math inline">\(0\)</span> otherwise.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" title="1"><span class="kw">mean</span>(Y_test <span class="op">==</span><span class="st"> </span>Y_pred) <span class="co"># accuracy</span></a></code></pre></div>
<pre><code>## [1] 0.91378</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" title="1">(C[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span><span class="kw">sum</span>(C) <span class="co"># equivalently</span></a></code></pre></div>
<pre><code>## [1] 0.91378</code></pre>
<p>In many applications we are dealing with <strong>unbalanced problems</strong>, where
the case <span class="math inline">\(y_i=1\)</span> is relatively rare,
yet predicting it correctly is much more important than being
accurate with respect to class <span class="math inline">\(0\)</span>.</p>
<dl>
<dt>Remark.</dt>
<dd><p>Think of medical applications, e.g., HIV testing
or tumour diagnosis.</p>
</dd>
</dl>
<p>In such a case, <em>accuracy</em> as a metric fails to quantify what we are aiming for.</p>
<dl>
<dt>Remark.</dt>
<dd><p>If only 1% of the cases have true <span class="math inline">\(y_i=1\)</span>,
then a dummy classifier that always
outputs <span class="math inline">\(\hat{y}_i=0\)</span> has 99% accuracy.</p>
</dd>
</dl>
<p>Metrics such as precision and recall (and their aggregated version, F-measure)
aim to address this problem.</p>
<p><strong>Precision</strong></p>
<p><span class="math display">\[
\text{Precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}
\]</span></p>
<p>If the classifier outputs <span class="math inline">\(1\)</span>,
what is the probability that this is indeed true?</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" title="1">C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">1</span>]) <span class="co"># Precision</span></a></code></pre></div>
<pre><code>## [1] 0.83636</code></pre>
<p><strong>Recall</strong> (a.k.a. sensitivity, hit rate or true positive rate)</p>
<p><span class="math display">\[
\text{Recall} = \frac{\text{TP}}{\text{TP}+\text{FN}}
\]</span></p>
<p>If the true class is <span class="math inline">\(1\)</span>, what is the probability that the classifier
will detect it?</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" title="1">C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">1</span>,<span class="dv">2</span>]) <span class="co"># Recall</span></a></code></pre></div>
<pre><code>## [1] 0.58044</code></pre>
<dl>
<dt>Remark.</dt>
<dd><p>Precision or recall? It depends on an application.
Think of medical diagnosis, medical screening, plagiarism detection,
etc. — which measure is more important in each of the settings listed?</p>
</dd>
</dl>
<p>As a compromise, we can use the <strong>F-measure</strong>
(a.k.a. <span class="math inline">\(F_1\)</span>-measure),
which is the harmonic mean of precision
and recall:</p>
<p><span class="math display">\[
\text{F} = \frac{1}{
    \frac{
        \frac{1}{\text{Precision}}+\frac{1}{\text{Recall}}
    }{2}
}
=
\left(
\frac{1}{2}
\left(
\text{Precision}^{-1}+\text{Recall}^{-1}
\right)
\right)^{-1}
=
\frac{\text{TP}}{\text{TP} + \frac{\text{FP} + \text{FN}}{2}}
\]</span></p>
<div class="exercise"><strong>Exercise.</strong>
<p>Show that the above equality holds.</p>
</div>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" title="1">C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">2</span>,<span class="dv">1</span>]) <span class="co"># F</span></a></code></pre></div>
<pre><code>## [1] 0.68529</code></pre>
<p>The following function can come in handy in the future:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" title="1">get_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(Y_pred, Y_test)</a>
<a class="sourceLine" id="cb82-2" title="2">{</a>
<a class="sourceLine" id="cb82-3" title="3">    C &lt;-<span class="st"> </span><span class="kw">table</span>(Y_pred, Y_test) <span class="co"># confusion matrix</span></a>
<a class="sourceLine" id="cb82-4" title="4">    <span class="kw">stopifnot</span>(<span class="kw">dim</span>(C) <span class="op">==</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb82-5" title="5">    <span class="kw">c</span>(<span class="dt">Acc=</span>(C[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span><span class="kw">sum</span>(C), <span class="co"># accuracy</span></a>
<a class="sourceLine" id="cb82-6" title="6">      <span class="dt">Prec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># precision</span></a>
<a class="sourceLine" id="cb82-7" title="7">      <span class="dt">Rec=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span>C[<span class="dv">1</span>,<span class="dv">2</span>]), <span class="co"># recall</span></a>
<a class="sourceLine" id="cb82-8" title="8">      <span class="dt">F=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(C[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span><span class="fl">0.5</span><span class="op">*</span>C[<span class="dv">2</span>,<span class="dv">1</span>]), <span class="co"># F-measure</span></a>
<a class="sourceLine" id="cb82-9" title="9">      <span class="co"># Confusion matrix items:</span></a>
<a class="sourceLine" id="cb82-10" title="10">      <span class="dt">TN=</span>C[<span class="dv">1</span>,<span class="dv">1</span>], <span class="dt">FN=</span>C[<span class="dv">1</span>,<span class="dv">2</span>],</a>
<a class="sourceLine" id="cb82-11" title="11">      <span class="dt">FP=</span>C[<span class="dv">2</span>,<span class="dv">1</span>], <span class="dt">TP=</span>C[<span class="dv">2</span>,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb82-12" title="12">    ) <span class="co"># return a named vector</span></a>
<a class="sourceLine" id="cb82-13" title="13">}</a></code></pre></div>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" title="1"><span class="kw">get_metrics</span>(Y_pred, Y_test)</a></code></pre></div>
<pre><code>##        Acc       Prec        Rec          F         TN         FN 
##    0.91378    0.83636    0.58044    0.68529 1607.00000  133.00000 
##         FP         TP 
##   36.00000  184.00000</code></pre>
</div>
<div id="how-to-choose-k-for-k-nn-classification" class="section level3">
<h3><span class="header-section-number">2.3.2</span> How to Choose K for K-NN Classification?</h3>
<p>We haven’t yet considered the question which <span class="math inline">\(K\)</span> yields <em>the best</em>
classifier.</p>
<p>Best == one that has the highest <em>predictive power</em>.</p>
<p>Best == with respect to some chosen metric (accuracy, recall, precision, F-measure, …)</p>
<p>Let’s study how the metrics on the test set change as functions of the number of nearest neighbours considered, <span class="math inline">\(K\)</span>.</p>
<p>Auxiliary function:</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" title="1">knn_metrics &lt;-<span class="st"> </span><span class="cf">function</span>(k, X_train, X_test, Y_train, Y_test)</a>
<a class="sourceLine" id="cb85-2" title="2">{</a>
<a class="sourceLine" id="cb85-3" title="3">    Y_pred &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span>k) <span class="co"># classify</span></a>
<a class="sourceLine" id="cb85-4" title="4">    <span class="kw">get_metrics</span>(Y_pred, Y_test)</a>
<a class="sourceLine" id="cb85-5" title="5">}</a></code></pre></div>
<p>For example:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" title="1"><span class="kw">knn_metrics</span>(<span class="dv">5</span>, Z_train, Z_test, Y_train, Y_test)</a></code></pre></div>
<pre><code>##        Acc       Prec        Rec          F         TN         FN 
##    0.91429    0.82251    0.59937    0.69343 1602.00000  127.00000 
##         FP         TP 
##   41.00000  190.00000</code></pre>
<p>Example call to evaluate metrics as a function of different <span class="math inline">\(K\)</span>s:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" title="1">Ks &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">19</span>, <span class="dt">by=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb88-2" title="2">Ps &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">t</span>(</a>
<a class="sourceLine" id="cb88-3" title="3">    <span class="kw">sapply</span>(Ks, <span class="co"># on each element in this vector</span></a>
<a class="sourceLine" id="cb88-4" title="4">        knn_metrics,     <span class="co"># apply this function</span></a>
<a class="sourceLine" id="cb88-5" title="5">        Z_train, Z_test, Y_train, Y_test <span class="co"># aux args</span></a>
<a class="sourceLine" id="cb88-6" title="6">    )))</a></code></pre></div>
<dl>
<dt>Remark.</dt>
<dd><p>Note that <code>sapply(X, f, arg1, arg2, ...)</code>
outputs a list <code>Y</code> such that
<code>Y[[i]] = f(X[i], arg1, arg2, ...)</code>
which is then simplified to a matrix.</p>
</dd>
<dt>Remark.</dt>
<dd><p>We transpose this result, <code>t()</code>, in order to get each metric
corresponding to different columns in the result.
As usual, if you keep wondering, e.g., why <code>t()</code>, play with
the code yourself – it’s fun fun fun.</p>
</dd>
</dl>
<p>Example results:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" title="1"><span class="kw">round</span>(<span class="kw">cbind</span>(<span class="dt">K=</span>Ks, Ps), <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##     K  Acc Prec  Rec    F   TN  FN FP  TP
## 1   1 0.92 0.77 0.72 0.74 1574  90 69 227
## 2   3 0.92 0.79 0.66 0.72 1587 108 56 209
## 3   5 0.91 0.82 0.60 0.69 1602 127 41 190
## 4   7 0.91 0.82 0.56 0.67 1604 138 39 179
## 5   9 0.91 0.84 0.58 0.69 1607 133 36 184
## 6  11 0.91 0.85 0.56 0.68 1611 138 32 179
## 7  13 0.91 0.83 0.57 0.68 1606 136 37 181
## 8  15 0.91 0.83 0.55 0.66 1607 144 36 173
## 9  17 0.91 0.82 0.53 0.64 1607 149 36 168
## 10 19 0.90 0.81 0.52 0.63 1603 151 40 166</code></pre>
<p>Figure <a href="classification-with-k-nearest-neighbours.html#fig:whichK5">6</a> is worth a thousand tables though (see <code>?matplot</code> in R). The reader is kindly asked to draw conclusions themself.</p>
<div class="figure">
<img src="02-neighbours-figures/whichK5-1.svg" alt="Figure 6: Performance of K-nn classifiers as a function of K for standardised and raw data" id="fig:whichK5" />
<p class="caption">Figure 6: Performance of <span class="math inline">\(K\)</span>-nn classifiers as a function of <span class="math inline">\(K\)</span> for standardised and raw data</p>
</div>
<!--

(\*) **ROC** (Receiver Operating Characteristic) curve:


```r
TPR <- Ps$TP/(Ps$TP+Ps$FN) # True Positive Rate (recall)
FPR <- Ps$FP/(Ps$FP+Ps$TN) # False Positive Rate
plot(FPR, TPR, asp=1, xlim=c(0,1), ylim=c(0,1))
abline(a=0, b=1, lty=3)
```

![plot of chunk unnamed-chunk-1](02-neighbours-figures/unnamed-chunk-1-1){#fig:unnamed-chunk-1}

-->
</div>
<div id="training-validation-and-test-sets" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Training, Validation and Test sets</h3>
<p>In the <span class="math inline">\(K\)</span>-NN classification task, there are many hyperparameters to tune up:</p>
<ul>
<li><p>Which <span class="math inline">\(K\)</span> should we choose?</p></li>
<li><p>Should we standardise the dataset?</p></li>
<li><p>Which variables should be taken into account when computing the Euclidean distance?</p></li>
</ul>
<!--
- Which metric should be used?
-->
<dl>
<dt>Remark.</dt>
<dd><p><strong>If we select the best hyperparameter set based on test
sample error, we will run into the trap of overfitting again</strong>.
This time we’ll be overfitting to the test set — the model that is optimal
for a given test sample doesn’t have to generalise well to other test samples (!).</p>
</dd>
</dl>
<p>In order to overcome this problem,
we can perform a random <strong>train-validation-test split</strong> of the original dataset:</p>
<ul>
<li><em>training sample</em> (e.g., 60%) – used to construct the models</li>
<li><em>validation sample</em> (e.g., 20%) – used to tune the hyperparameters of the classifier</li>
<li><em>test sample</em> (e.g., 20%) – used to assess the goodness of fit</li>
</ul>
<!--
By the way, this is how most data mining competitions are assessed --
you will never have access to the final test sample used
to determine the winner. The best you can do is to "guess".
-->
<p>An example way to perform a 60/20/20% train-validation-test split:</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" title="1"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># reproducibility matters</span></a>
<a class="sourceLine" id="cb91-2" title="2">random_indices &lt;-<span class="st"> </span><span class="kw">sample</span>(n)</a>
<a class="sourceLine" id="cb91-3" title="3">n1 &lt;-<span class="st"> </span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.6</span>)</a>
<a class="sourceLine" id="cb91-4" title="4">n2 &lt;-<span class="st"> </span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb91-5" title="5">X2_train &lt;-<span class="st"> </span>X[random_indices[<span class="dv">1</span>     <span class="op">:</span>n1], ]</a>
<a class="sourceLine" id="cb91-6" title="6">Y2_train &lt;-<span class="st"> </span>Y[random_indices[<span class="dv">1</span>     <span class="op">:</span>n1]  ]</a>
<a class="sourceLine" id="cb91-7" title="7">X2_valid &lt;-<span class="st"> </span>X[random_indices[(n1<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n2], ]</a>
<a class="sourceLine" id="cb91-8" title="8">Y2_valid &lt;-<span class="st"> </span>Y[random_indices[(n1<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n2]  ]</a>
<a class="sourceLine" id="cb91-9" title="9">X2_test  &lt;-<span class="st"> </span>X[random_indices[(n2<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n ], ]</a>
<a class="sourceLine" id="cb91-10" title="10">Y2_test  &lt;-<span class="st"> </span>Y[random_indices[(n2<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n ]  ]</a>
<a class="sourceLine" id="cb91-11" title="11"><span class="kw">stopifnot</span>(<span class="kw">nrow</span>(X2_train)<span class="op">+</span><span class="kw">nrow</span>(X2_valid)<span class="op">+</span><span class="kw">nrow</span>(X2_test)</a>
<a class="sourceLine" id="cb91-12" title="12">    <span class="op">==</span><span class="st"> </span><span class="kw">nrow</span>(X))</a></code></pre></div>
<div class="exercise"><strong>Exercise.</strong>
<p>Find the best <span class="math inline">\(K\)</span> on the validation set and compute the error metrics
on the test set.</p>
</div>
<dl>
<dt>Remark.</dt>
<dd><p>(*) If our dataset is too small,
we can use various <em>cross-validation</em> techniques
instead of a train-validate-test split.
<!-- TODO: see exercise.... --></p>
</dd>
</dl>
</div>
</div>
<div id="implementing-a-k-nn-classifier" class="section level2">
<h2><span class="header-section-number">2.4</span> Implementing a K-NN Classifier (*)</h2>
<div id="factor-data-type" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Factor Data Type</h3>
<p>Recall that (see Appendix B for more details)
<code>factor</code> type in R is a very convenient means to encode categorical data
(such as <span class="math inline">\(\mathbf{y}\)</span>):</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" title="1">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>)</a>
<a class="sourceLine" id="cb92-2" title="2">f &lt;-<span class="st"> </span><span class="kw">factor</span>(x, <span class="dt">levels=</span><span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>))</a>
<a class="sourceLine" id="cb92-3" title="3">f</a></code></pre></div>
<pre><code>## [1] yes no  no  yes no 
## Levels: no yes</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" title="1"><span class="kw">table</span>(f) <span class="co"># counts</span></a></code></pre></div>
<pre><code>## f
##  no yes 
##   3   2</code></pre>
<p>Internally, objects of type <code>factor</code> are represented as integer vectors
with elements in <span class="math inline">\(\{1,\dots,M\}\)</span>, where <span class="math inline">\(M\)</span> is the number of possible levels.</p>
<p>Labels, used to “decipher” the numeric codes, are stored separately.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" title="1"><span class="kw">as.numeric</span>(f) <span class="co"># 2nd label, 1st label, 1st label etc.</span></a></code></pre></div>
<pre><code>## [1] 2 1 1 2 1</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" title="1"><span class="kw">levels</span>(f)</a></code></pre></div>
<pre><code>## [1] &quot;no&quot;  &quot;yes&quot;</code></pre>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" title="1"><span class="kw">levels</span>(f) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;failure&quot;</span>, <span class="st">&quot;success&quot;</span>) <span class="co"># re-encode</span></a>
<a class="sourceLine" id="cb100-2" title="2">f</a></code></pre></div>
<pre><code>## [1] success failure failure success failure
## Levels: failure success</code></pre>
</div>
<div id="main-routine" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Main Routine (*)</h3>
<p>Let’s implement a K-NN classifier ourselves
by using a top-bottom approach.</p>
<p>We will start with a general description of the admissible inputs
and the expected output.</p>
<p>Then we will arrange the processing of data into
conveniently manageable chunks.</p>
<p>The function’s declaration will look like:</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" title="1">our_knn &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb102-2" title="2">    <span class="co"># k=1 denotes a parameter with a default value</span></a>
<a class="sourceLine" id="cb102-3" title="3">    <span class="co"># ...</span></a>
<a class="sourceLine" id="cb102-4" title="4">}</a></code></pre></div>
<p>Load an example dataset on which we will test our algorithm:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" title="1">wine_quality &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/wine_quality_all.csv&quot;</span>,</a>
<a class="sourceLine" id="cb103-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a>
<a class="sourceLine" id="cb103-3" title="3">white_wines &lt;-<span class="st"> </span>wine_quality[wine_quality<span class="op">$</span>color <span class="op">==</span><span class="st"> &quot;white&quot;</span>,]</a>
<a class="sourceLine" id="cb103-4" title="4">X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(white_wines[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])</a>
<a class="sourceLine" id="cb103-5" title="5">Y &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">as.character</span>(<span class="kw">as.numeric</span>(white_wines<span class="op">$</span>alcohol <span class="op">&gt;=</span><span class="st"> </span><span class="dv">12</span>)))</a></code></pre></div>
<p>Note that <code>Y</code> is now a factor object.</p>
<p>Train-test split:</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb104-1" title="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb104-2" title="2">random_indices &lt;-<span class="st"> </span><span class="kw">sample</span>(n)</a>
<a class="sourceLine" id="cb104-3" title="3">train_indices &lt;-<span class="st"> </span>random_indices[<span class="dv">1</span><span class="op">:</span><span class="kw">floor</span>(n<span class="op">*</span><span class="fl">0.6</span>)]</a>
<a class="sourceLine" id="cb104-4" title="4">X_train &lt;-<span class="st"> </span>X[train_indices,]</a>
<a class="sourceLine" id="cb104-5" title="5">Y_train &lt;-<span class="st"> </span>Y[train_indices]</a>
<a class="sourceLine" id="cb104-6" title="6">X_test  &lt;-<span class="st"> </span>X[<span class="op">-</span>train_indices,]</a>
<a class="sourceLine" id="cb104-7" title="7">Y_test  &lt;-<span class="st"> </span>Y[<span class="op">-</span>train_indices]</a></code></pre></div>
<p>First, we should specify the type and form of the arguments
we’re expecting:</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" title="1"><span class="co"># this is the body of our_knn() - part 1</span></a>
<a class="sourceLine" id="cb105-2" title="2"><span class="kw">stopifnot</span>(<span class="kw">is.numeric</span>(X_train), <span class="kw">is.matrix</span>(X_train))</a>
<a class="sourceLine" id="cb105-3" title="3"><span class="kw">stopifnot</span>(<span class="kw">is.numeric</span>(X_test), <span class="kw">is.matrix</span>(X_test))</a>
<a class="sourceLine" id="cb105-4" title="4"><span class="kw">stopifnot</span>(<span class="kw">is.factor</span>(Y_train))</a>
<a class="sourceLine" id="cb105-5" title="5"><span class="kw">stopifnot</span>(<span class="kw">ncol</span>(X_train) <span class="op">==</span><span class="st"> </span><span class="kw">ncol</span>(X_test))</a>
<a class="sourceLine" id="cb105-6" title="6"><span class="kw">stopifnot</span>(<span class="kw">nrow</span>(X_train) <span class="op">==</span><span class="st"> </span><span class="kw">length</span>(Y_train))</a>
<a class="sourceLine" id="cb105-7" title="7"><span class="kw">stopifnot</span>(k <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb105-8" title="8">n_train &lt;-<span class="st"> </span><span class="kw">nrow</span>(X_train)</a>
<a class="sourceLine" id="cb105-9" title="9">n_test  &lt;-<span class="st"> </span><span class="kw">nrow</span>(X_test)</a>
<a class="sourceLine" id="cb105-10" title="10">p &lt;-<span class="st"> </span><span class="kw">ncol</span>(X_train)</a>
<a class="sourceLine" id="cb105-11" title="11">M &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">levels</span>(Y_train))</a></code></pre></div>
<p>Therefore,</p>
<p><span class="math inline">\(\mathtt{X\_train}\in\mathbb{R}^{\mathtt{n\_train}\times \mathtt{p}}\)</span>,
<span class="math inline">\(\mathtt{X\_test}\in\mathbb{R}^{\mathtt{n\_test}\times \mathtt{p}}\)</span> and
<span class="math inline">\(\mathtt{Y\_train}\in\{1,\dots,M\}^{\mathtt{n\_train}}\)</span></p>
<dl>
<dt>Remark.</dt>
<dd><p>Recall that R <code>factor</code> objects are internally encoded as integer vectors.</p>
</dd>
</dl>
<p>Next, we will call the (to-be-done) function <code>our_get_knnx()</code>,
which seeks nearest neighbours of all the points:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" title="1"><span class="co"># our_get_knnx returns a matrix nn_indices of size n_test*k,</span></a>
<a class="sourceLine" id="cb106-2" title="2"><span class="co"># where nn_indices[i,j] denotes the index of</span></a>
<a class="sourceLine" id="cb106-3" title="3"><span class="co"># X_test[i,]&#39;s j-th nearest neighbour in X_train.</span></a>
<a class="sourceLine" id="cb106-4" title="4"><span class="co"># (It is the point X_train[nn_indices[i,j],]).</span></a>
<a class="sourceLine" id="cb106-5" title="5">nn_indices &lt;-<span class="st"> </span><span class="kw">our_get_knnx</span>(X_train, X_test, k)</a></code></pre></div>
<p>Then, for each point in <code>X_test</code>,
we fetch the labels corresponding to its nearest neighbours
and compute their mode:</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb107-1" title="1">Y_pred &lt;-<span class="st"> </span><span class="kw">numeric</span>(n_test) <span class="co"># vector of length n_test</span></a>
<a class="sourceLine" id="cb107-2" title="2"><span class="co"># For now we will operate on the integer labels in {1,...,M}</span></a>
<a class="sourceLine" id="cb107-3" title="3">Y_train_int &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(Y_train)</a>
<a class="sourceLine" id="cb107-4" title="4"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n_test) {</a>
<a class="sourceLine" id="cb107-5" title="5">    <span class="co"># Get the labels of the NNs of the i-th point:</span></a>
<a class="sourceLine" id="cb107-6" title="6">    nn_labels_i &lt;-<span class="st"> </span>Y_train_int[nn_indices[i,]]</a>
<a class="sourceLine" id="cb107-7" title="7">    <span class="co"># Compute the mode (majority vote):</span></a>
<a class="sourceLine" id="cb107-8" title="8">    Y_pred[i] &lt;-<span class="st"> </span><span class="kw">our_mode</span>(nn_labels_i) <span class="co"># in {1,...,M}</span></a>
<a class="sourceLine" id="cb107-9" title="9">}</a></code></pre></div>
<p>Finally, we should convert the resulting integer vector
to an object of type <code>factor</code>:</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" title="1"><span class="co"># Convert Y_pred to factor:</span></a>
<a class="sourceLine" id="cb108-2" title="2"><span class="kw">return</span>(<span class="kw">factor</span>(Y_pred, <span class="dt">labels=</span><span class="kw">levels</span>(Y_train)))</a></code></pre></div>
<!--
**Test-driven development** -- before writing


```r
test_our_knn <- function() {
    # ...
}
```



```r
test_our_mode <- function() {
    stopifnot(our_mode(c(1, 1, 1, 1)) == 1)
    stopifnot(our_mode(c(2, 2, 2, 2)) == 2)
    stopifnot(our_mode(c(3, 1, 3, 3)) == 3)
    stopifnot(our_mode(c(1, 1, 3, 3, 2)) %in% c(1, 3))
}
```


-->
</div>
<div id="mode" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Mode</h3>
<p>To implement the mode, we can use the <code>tabulate()</code> function.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Read the function’s man page, see <code>?tabulate</code>.</p>
</div>
<p>For example:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb109-1" title="1"><span class="kw">tabulate</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 4 2 0 0 1</code></pre>
<p>There might be multiple modes – in such a case, we should pick one at random.</p>
<p>For that, we can use the <code>sample()</code> function.</p>
<div class="exercise"><strong>Exercise.</strong>
<p>Read the function’s man page, see <code>?sample</code>.
Note that its behaviour is different when it’s first argument is a vector of length 1.</p>
</div>
<p>An example implementation:</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" title="1">our_mode &lt;-<span class="st"> </span><span class="cf">function</span>(Y) {</a>
<a class="sourceLine" id="cb111-2" title="2">    <span class="co"># tabulate() will take care of</span></a>
<a class="sourceLine" id="cb111-3" title="3">    <span class="co"># checking the correctness of Y</span></a>
<a class="sourceLine" id="cb111-4" title="4">    t &lt;-<span class="st"> </span><span class="kw">tabulate</span>(Y)</a>
<a class="sourceLine" id="cb111-5" title="5">    mode_candidates &lt;-<span class="st"> </span><span class="kw">which</span>(t <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(t))</a>
<a class="sourceLine" id="cb111-6" title="6">    <span class="cf">if</span> (<span class="kw">length</span>(mode_candidates) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) <span class="kw">return</span>(mode_candidates)</a>
<a class="sourceLine" id="cb111-7" title="7">    <span class="cf">else</span> <span class="kw">return</span>(<span class="kw">sample</span>(mode_candidates, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb111-8" title="8">}</a></code></pre></div>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb112-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>))</a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 2</code></pre>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>))</a></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" title="1"><span class="kw">our_mode</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 1</code></pre>
</div>
<div id="nn-search-routines" class="section level3">
<h3><span class="header-section-number">2.4.4</span> NN Search Routines (*)</h3>
<p>Last but not least, we should implement the <code>our_get_knnx()</code> function.</p>
<p>It is the function responsible for seeking the indices of nearest neighbours.</p>
<p>It turns out this function will actually constitute the K-NN classifier’s performance
bottleneck in case of big data samples.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" title="1"><span class="co"># our_get_knnx returns a matrix nn_indices of size n_test*k,</span></a>
<a class="sourceLine" id="cb122-2" title="2"><span class="co"># where nn_indices[i,j] denotes the index of</span></a>
<a class="sourceLine" id="cb122-3" title="3"><span class="co"># X_test[i,]&#39;s j-th nearest neighbour in X_train.</span></a>
<a class="sourceLine" id="cb122-4" title="4"><span class="co"># (It is the point X_train[nn_indices[i,j],]).</span></a>
<a class="sourceLine" id="cb122-5" title="5">our_get_knnx &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, k) {</a>
<a class="sourceLine" id="cb122-6" title="6">    <span class="co"># ...</span></a>
<a class="sourceLine" id="cb122-7" title="7">}</a></code></pre></div>
<p>A naive approach to <code>our_get_knnx()</code> relies on computing all pairwise distances,
and sorting them.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb123-1" title="1">our_get_knnx &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, k) {</a>
<a class="sourceLine" id="cb123-2" title="2">    n_test &lt;-<span class="st"> </span><span class="kw">nrow</span>(X_test)</a>
<a class="sourceLine" id="cb123-3" title="3">    nn_indices &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA_real_</span>, <span class="dt">nrow=</span>n_test, <span class="dt">ncol=</span>k)</a>
<a class="sourceLine" id="cb123-4" title="4">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n_test) {</a>
<a class="sourceLine" id="cb123-5" title="5">        d &lt;-<span class="st"> </span><span class="kw">apply</span>(X_train, <span class="dv">1</span>, <span class="cf">function</span>(x)</a>
<a class="sourceLine" id="cb123-6" title="6">            <span class="kw">sqrt</span>(<span class="kw">sum</span>((x<span class="op">-</span>X_test[i,])<span class="op">^</span><span class="dv">2</span>)))</a>
<a class="sourceLine" id="cb123-7" title="7">        <span class="co"># now d[j] is the distance</span></a>
<a class="sourceLine" id="cb123-8" title="8">        <span class="co"># between X_train[j,] and X_test[i,]</span></a>
<a class="sourceLine" id="cb123-9" title="9">        nn_indices[i,] &lt;-<span class="st"> </span><span class="kw">order</span>(d)[<span class="dv">1</span><span class="op">:</span>k]</a>
<a class="sourceLine" id="cb123-10" title="10">    }</a>
<a class="sourceLine" id="cb123-11" title="11">    nn_indices</a>
<a class="sourceLine" id="cb123-12" title="12">}</a></code></pre></div>
<p>A comparison with <code>FNN:knn()</code>:</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" title="1"><span class="kw">system.time</span>(Ya &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>))</a></code></pre></div>
<pre><code>##    user  system elapsed 
##   0.163   0.000   0.162</code></pre>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" title="1"><span class="kw">system.time</span>(Yb &lt;-<span class="st"> </span><span class="kw">our_knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>))</a></code></pre></div>
<pre><code>##    user  system elapsed 
##  18.705   0.000  18.703</code></pre>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" title="1"><span class="kw">mean</span>(Ya <span class="op">==</span><span class="st"> </span>Yb) <span class="co"># 1.0 on perfect match</span></a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>Both functions return identical results but our implementation is “slightly” slower.</p>
<p><code>FNN:knn()</code> is efficiently written in C++, which is a compiled programming language.</p>
<p>R, on the other hand (just like Python and Matlab) is interpreted, therefore
as a rule of thumb we should consider it an order of magnitude slower (see, however, the Julia language).</p>
<p>Let’s substitute our naive implementation with the equivalent one,
but written in C++ (available in the <code>FNN</code> package).</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Note that we can write a C++ implementation ourselves,
see the Rcpp package for seamless R and C++ integration.</p>
</dd>
</dl>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb130-1" title="1">our_get_knnx &lt;-<span class="st"> </span><span class="cf">function</span>(X_train, X_test, k) {</a>
<a class="sourceLine" id="cb130-2" title="2">    <span class="co"># this is used by our_knn()</span></a>
<a class="sourceLine" id="cb130-3" title="3">    FNN<span class="op">::</span><span class="kw">get.knnx</span>(X_train, X_test, k, <span class="dt">algorithm=</span><span class="st">&quot;brute&quot;</span>)<span class="op">$</span>nn.index</a>
<a class="sourceLine" id="cb130-4" title="4">}</a>
<a class="sourceLine" id="cb130-5" title="5"><span class="kw">system.time</span>(Ya &lt;-<span class="st"> </span><span class="kw">knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>))</a></code></pre></div>
<pre><code>##    user  system elapsed 
##   0.163   0.000   0.162</code></pre>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb132-1" title="1"><span class="kw">system.time</span>(Yb &lt;-<span class="st"> </span><span class="kw">our_knn</span>(X_train, X_test, Y_train, <span class="dt">k=</span><span class="dv">5</span>))</a></code></pre></div>
<pre><code>##    user  system elapsed 
##   0.054   0.000   0.054</code></pre>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" title="1"><span class="kw">mean</span>(Ya <span class="op">==</span><span class="st"> </span>Yb) <span class="co"># 1.0 on perfect match</span></a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>Note that our solution requires <span class="math inline">\(c\cdot n_\text{test}\cdot n_\text{train}\cdot p\)</span>
arithmetic operations for some <span class="math inline">\(c&gt;1\)</span>.
The overall cost of sorting is at least <span class="math inline">\(d\cdot n_\text{test}\cdot n_\text{train}\cdot\log n_\text{train}\)</span>
for some <span class="math inline">\(d&gt;1\)</span>.</p>
<p>This does not scale well with both <span class="math inline">\(n_\text{test}\)</span> and <span class="math inline">\(n_\text{train}\)</span>
(think – big data).</p>
<div style="margin-top: 1em">

</div>
<p>It turns out that there are special <strong>spatial data structures</strong>
– such as <em>metric trees</em> – that aim to speed up searching for nearest
neighbours in <em>low-dimensional spaces</em> (for small <span class="math inline">\(p\)</span>).</p>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Searching in high-dimensional spaces is hard due to the so-called
curse of dimensionality.</p>
</dd>
</dl>
<p>For example, <code>FNN::get.knnx()</code> also implements the so-called
kd-trees.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb136-1" title="1"><span class="kw">library</span>(<span class="st">&quot;microbenchmark&quot;</span>)</a>
<a class="sourceLine" id="cb136-2" title="2">test_speed &lt;-<span class="st"> </span><span class="cf">function</span>(n, p, k) {</a>
<a class="sourceLine" id="cb136-3" title="3">    A &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n<span class="op">*</span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p)</a>
<a class="sourceLine" id="cb136-4" title="4">    s &lt;-<span class="st"> </span><span class="kw">summary</span>(microbenchmark<span class="op">::</span><span class="kw">microbenchmark</span>(</a>
<a class="sourceLine" id="cb136-5" title="5">        <span class="dt">brute=</span>FNN<span class="op">::</span><span class="kw">get.knnx</span>(A, A, k, <span class="dt">algorithm=</span><span class="st">&quot;brute&quot;</span>),</a>
<a class="sourceLine" id="cb136-6" title="6">        <span class="dt">kd_tree=</span>FNN<span class="op">::</span><span class="kw">get.knnx</span>(A, A, k, <span class="dt">algorithm=</span><span class="st">&quot;kd_tree&quot;</span>),</a>
<a class="sourceLine" id="cb136-7" title="7">        <span class="dt">times=</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb136-8" title="8">    ), <span class="dt">unit=</span><span class="st">&quot;s&quot;</span>)</a>
<a class="sourceLine" id="cb136-9" title="9">    <span class="co"># minima of 3 time measurements:</span></a>
<a class="sourceLine" id="cb136-10" title="10">    <span class="kw">structure</span>(s<span class="op">$</span>min, <span class="dt">names=</span><span class="kw">as.character</span>(s<span class="op">$</span>expr))</a>
<a class="sourceLine" id="cb136-11" title="11">}</a></code></pre></div>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb137-1" title="1"><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">2</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>##    brute  kd_tree 
## 0.342544 0.014548</code></pre>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb139-1" title="1"><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">5</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>##    brute  kd_tree 
## 0.502122 0.073742</code></pre>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb141-1" title="1"><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">10</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>##   brute kd_tree 
## 0.76959 0.79579</code></pre>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb143-1" title="1"><span class="kw">test_speed</span>(<span class="dv">10000</span>, <span class="dv">20</span>, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>##   brute kd_tree 
##  1.5047  6.2339</code></pre>
</div>
<div id="different-metrics" class="section level3">
<h3><span class="header-section-number">2.4.5</span> Different Metrics (*)</h3>
<p>The Euclidean distance is just one particular example
of many possible <strong>metrics</strong> (metric == a mathematical term,
above we have used this term in a more relaxed fashion, when referring
to accuracy etc.).</p>
<p>Mathematically, we say that <span class="math inline">\(d\)</span> is a metric on a set <span class="math inline">\(X\)</span>
(e.g., <span class="math inline">\(\mathbb{R}^p\)</span>), whenever
it is a function <span class="math inline">\(d:X\times X\to [0,\infty]\)</span> such that for all <span class="math inline">\(x,x&#39;,x&#39;&#39;\in X\)</span>:</p>
<ul>
<li><span class="math inline">\(d(x, x&#39;) = 0\)</span> if and only if <span class="math inline">\(x=x&#39;\)</span>,</li>
<li><span class="math inline">\(d(x, x&#39;) = d(x&#39;, x)\)</span> (it is symmetric)</li>
<li><span class="math inline">\(d(x, x&#39;&#39;) \le d(x, x&#39;) + d(x&#39;, x&#39;&#39;)\)</span> (it fulfils the triangle inequality)</li>
</ul>
<dl>
<dt>Remark.</dt>
<dd><p>(*) Not all the properties are required in all the applications;
sometimes we might need a few additional ones.</p>
</dd>
</dl>
<p>We can easily generalise the way we introduced the K-NN method
to have a classifier that is based on a point’s neighbourhood
with respect to any metric.</p>
<p>Example metrics on <span class="math inline">\(\mathbb{R}^p\)</span>:</p>
<ul>
<li><strong>Euclidean</strong>
<span class="math display">\[
d_2(\mathbf{x}, \mathbf{x}&#39;) = \| \mathbf{x}-\mathbf{x}&#39; \| = \| \mathbf{x}-\mathbf{x}&#39; \|_2 = \sqrt{ \sum_{i=1}^p (x_i-x_i&#39;)^2 }
\]</span></li>
<li><strong>Manhattan</strong> (taxicab)
<span class="math display">\[
d_1(\mathbf{x}, \mathbf{x}&#39;) = \| \mathbf{x}-\mathbf{x}&#39; \|_1 = { \sum_{i=1}^p |x_i-x_i&#39;| }
\]</span></li>
<li><strong>Chebyshev</strong> (maximum)
<span class="math display">\[
d_\infty(\mathbf{x}, \mathbf{x}&#39;) = \| \mathbf{x}-\mathbf{x}&#39; \|_\infty = \max_{i=1,\dots,p} |x_i-x_i&#39;|
\]</span></li>
</ul>
<!--
These are all examples of $L_p$ metrics, $p\ge 1$:
\[
d_p(\mathbf{x}, \mathbf{x}') = \| \mathbf{x}-\mathbf{x}' \|_p = \left( \sum_{i=1}^p |x_i-x_i'|^p \right)^{1/p}
\]
-->
<p>We can define metrics on different spaces too.</p>
<p>For example, the <strong>Levenshtein distance</strong> is a popular choice
for comparing character strings (also DNA sequences etc.)</p>
<p>It is an <em>edit distance</em> – it measures the minimal number of
single-character insertions, deletions or substitutions to change
one string into another.</p>
<p>For instance:</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" title="1"><span class="kw">adist</span>(<span class="st">&quot;happy&quot;</span>, <span class="st">&quot;nap&quot;</span>)</a></code></pre></div>
<pre><code>##      [,1]
## [1,]    3</code></pre>
<p>This is because we need 1 substitution and 2 deletions,</p>
<p>happy → nappy → napp → nap.</p>
<p>See also:</p>
<ul>
<li>the Hamming distance for categorical vectors (or strings of equal lengths),</li>
<li>the Jaccard distance for sets,</li>
<li>the Kendall tau rank distance for rankings.</li>
</ul>
<p>Moreover, R package <code>stringdist</code> includes implementations
of numerous string metrics.</p>
<!-- Mahalanobis distance -->
</div>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">2.5</span> Exercises</h2>
<div id="wine-quality-best-k-nn-parameters-via-cross-validation" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Wine Quality – Best K-NN Parameters via Cross-Validation (*)</h3>
<p>Consider the Wine Quality dataset (see Appendix F<!-- TODO --> for details):</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb147-1" title="1">wine_quality &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;datasets/wine_quality_all.csv&quot;</span>,</a>
<a class="sourceLine" id="cb147-2" title="2">    <span class="dt">comment.char=</span><span class="st">&quot;#&quot;</span>)</a>
<a class="sourceLine" id="cb147-3" title="3"><span class="kw">head</span>(wine_quality, <span class="dv">3</span>)</a></code></pre></div>
<pre><code>##   fixed.acidity volatile.acidity citric.acid residual.sugar chlorides
## 1           7.4             0.70        0.00            1.9     0.076
## 2           7.8             0.88        0.00            2.6     0.098
## 3           7.8             0.76        0.04            2.3     0.092
##   free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates
## 1                  11                   34  0.9978 3.51      0.56
## 2                  25                   67  0.9968 3.20      0.68
## 3                  15                   54  0.9970 3.26      0.65
##   alcohol response color
## 1     9.4        5   red
## 2     9.8        5   red
## 3     9.8        5   red</code></pre>
<div class="exercise"><strong>Exercise.</strong>
<p>Add a new column named <code>quality</code>. A wine should get a <code>quality</code> of <code>1</code>
if its rating is greater than or equal to 7 (a good wine)
and a quality of <code>0</code> otherwise.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Perform a random train-test split of size 60-40%:
create the matrices <code>X_train</code> and <code>X_test</code> containing
the 11 physicochemical wine features and
the corresponding label vectors <code>Y_train</code> and <code>Y_test</code>
that inform on the wines’ <code>quality</code>.</p>
</div>
<div class="exercise"><strong>Exercise.</strong>
<p>Determine the <em>best</em> parameter setting
for the K-nearest neighbour classification of the <code>quality</code> variable
based on the 11 physicochemical features. Perform the so-called
grid (exhaustive) search over all the possible combinations of the following
parameters:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(K\)</span>: 1, 3, 5, 7 or 9,</li>
<li>preprocessing: none (raw input data),
standardised variables or robustly standardised variables,</li>
<li>metric: <span class="math inline">\(L_2\)</span> (Euclidean) or <span class="math inline">\(L_1\)</span> (Manhattan).</li>
</ol>
<p>In other words, there are <span class="math inline">\(5\cdot 3\cdot 2=30\)</span> combinations of parameters in total,
and hence – <span class="math inline">\(30\)</span> different scenarios to consider.
By <em>the best classifier</em> we mean the one that maximises the <span class="math inline">\(F\)</span>-measure
obtained by the so-called 5-fold cross-validation (see below).</p>
</div>
<dl>
<dt>Robust standardisation.</dt>
<dd><p>To perform a <em>robust standardisation</em>, for each column individually,
subtract its median and then divide it by its median absolute deviation
(MAD, i.e., <code>median(abs(x-median(x)))</code>). This data preprocessing scheme
is less sensitive to outliers than the classic standardisation.</p>
</dd>
<dt>Remark.</dt>
<dd><p>Note that the <span class="math inline">\(L_1\)</span> metric-based K-nearest neighbour method
is not available in the <code>FNN</code> package. You need to implement
it yourself.</p>
</dd>
<dt>Cross-validation.</dt>
<dd><p>We have discussed that it would not be fair to use the test
set for choosing of the optimal parameters (we would be overfitting to the
test set). We know that one possible
way to assure the transparent evaluation of a classifier
is to perform a train-validate-test split and use the validation set
for parameter tuning.</p>
<p>Here we will use a different technique – one
that estimates the methods’ “true” predictive
performance more accurately, yet at the cost of significantly increased
run-time. Namely, in <em>5-fold cross-validation</em>, we split the original
train set randomly into 5 disjoint parts: A, B, C, D, E
(more or less of the same number of observations). We use each
combination of 4 chunks as training sets and the remaining part
as the validation set, on which we compute the F-measure:</p>
<table>
<thead>
<tr class="header">
<th>train set</th>
<th>validation set</th>
<th>F-measure</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>B, C, D, E</td>
<td>A</td>
<td><span class="math inline">\(F_A\)</span></td>
</tr>
<tr class="even">
<td>A, C, D, E</td>
<td>B</td>
<td><span class="math inline">\(F_B\)</span></td>
</tr>
<tr class="odd">
<td>A, B, D, E</td>
<td>C</td>
<td><span class="math inline">\(F_C\)</span></td>
</tr>
<tr class="even">
<td>A, B, C, E</td>
<td>D</td>
<td><span class="math inline">\(F_D\)</span></td>
</tr>
<tr class="odd">
<td>A, B, C, D</td>
<td>E</td>
<td><span class="math inline">\(F_E\)</span></td>
</tr>
</tbody>
</table>
<p>At the end we report the average <span class="math inline">\(F\)</span>-measure, <span class="math inline">\((F_A+F_B+F_C+F_D+F_E)/5\)</span>.</p>
</dd>
</dl>
</div>
</div>
<div id="outro" class="section level2">
<h2><span class="header-section-number">2.6</span> Outro</h2>
<div id="remarks" class="section level3">
<h3><span class="header-section-number">2.6.1</span> Remarks</h3>
<p>Note that K-NN is suitable for any kind of multiclass classification.</p>
<p>However, in practice it’s pretty slow for larger datasets – to
classify a single point we have to query the whole training set (which
should be available at all times).</p>
<p>In the next part we will discuss some other well-known classifiers:</p>
<ul>
<li><em>Decision trees</em></li>
<li><em>Logistic regression</em></li>
</ul>
<!--
They are restricted to binary (0/1) outputs.
They will have to be extended
somehow to allow for more classes.
-->
</div>
<div id="side-note-k-nn-regression" class="section level3">
<h3><span class="header-section-number">2.6.2</span> Side Note: K-NN Regression</h3>
<p>The K-Nearest Neighbour scheme is intuitively pleasing.</p>
<p>No wonder it has inspired a similar approach for solving a regression task.</p>
<p>In order to make a prediction for a new point <span class="math inline">\(\mathbf{x}&#39;\)</span>:</p>
<ol style="list-style-type: decimal">
<li>find the K-nearest neighbours of <span class="math inline">\(\mathbf{x}&#39;\)</span> amongst the points in the train set,
denoted <span class="math inline">\(\mathbf{x}_{i_1,\cdot}, \dots, \mathbf{x}_{i_K,\cdot}\)</span>,</li>
<li>fetch the corresponding reference outputs <span class="math inline">\(y_{i_1}, \dots, y_{i_K}\)</span>,</li>
<li>return their arithmetic mean as a result,
<span class="math display">\[\hat{y}=\frac{1}{K} \sum_{j=1}^K y_{i_j}.\]</span></li>
</ol>
<p>Recall our modelling of the Credit Rating (<span class="math inline">\(Y\)</span>)
as a function of the average Credit Card Balance (<span class="math inline">\(X\)</span>)
based on the <code>ISLR::Credit</code> dataset.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb149-1" title="1"><span class="kw">library</span>(<span class="st">&quot;ISLR&quot;</span>) <span class="co"># Credit dataset</span></a>
<a class="sourceLine" id="cb149-2" title="2">Xc &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">as.numeric</span>(Credit<span class="op">$</span>Balance[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>]))</a>
<a class="sourceLine" id="cb149-3" title="3">Yc &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">as.numeric</span>(Credit<span class="op">$</span>Rating[Credit<span class="op">$</span>Balance<span class="op">&gt;</span><span class="dv">0</span>]))</a></code></pre></div>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb150-1" title="1"><span class="kw">library</span>(<span class="st">&quot;FNN&quot;</span>) <span class="co"># knn.reg function</span></a>
<a class="sourceLine" id="cb150-2" title="2">x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">seq</span>(<span class="kw">min</span>(Xc), <span class="kw">max</span>(Xc), <span class="dt">length.out=</span><span class="dv">101</span>))</a>
<a class="sourceLine" id="cb150-3" title="3">y1  &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(Xc, x, Yc, <span class="dt">k=</span><span class="dv">1</span>)<span class="op">$</span>pred</a>
<a class="sourceLine" id="cb150-4" title="4">y5  &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(Xc, x, Yc, <span class="dt">k=</span><span class="dv">5</span>)<span class="op">$</span>pred</a>
<a class="sourceLine" id="cb150-5" title="5">y25 &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(Xc, x, Yc, <span class="dt">k=</span><span class="dv">25</span>)<span class="op">$</span>pred</a></code></pre></div>
<p>The three models are depicted in Figure <a href="classification-with-k-nearest-neighbours.html#fig:knnreg3">7</a>.
Again, the higher the <span class="math inline">\(K\)</span>, the smoother the curve. On the other hand, for
small <span class="math inline">\(K\)</span> we adapt better to what’s in a point’s neighbourhood.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb151-1" title="1"><span class="kw">plot</span>(Xc, Yc, <span class="dt">col=</span><span class="st">&quot;#666666c0&quot;</span>,</a>
<a class="sourceLine" id="cb151-2" title="2">    <span class="dt">xlab=</span><span class="st">&quot;Balance&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Rating&quot;</span>)</a>
<a class="sourceLine" id="cb151-3" title="3"><span class="kw">lines</span>(x, y1,  <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb151-4" title="4"><span class="kw">lines</span>(x, y5,  <span class="dt">col=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb151-5" title="5"><span class="kw">lines</span>(x, y25, <span class="dt">col=</span><span class="dv">4</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb151-6" title="6"><span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;K=1&quot;</span>, <span class="st">&quot;K=5&quot;</span>, <span class="st">&quot;K=25&quot;</span>),</a>
<a class="sourceLine" id="cb151-7" title="7">    <span class="dt">col=</span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>), <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">bg=</span><span class="st">&quot;white&quot;</span>)</a></code></pre></div>
<div class="figure">
<img src="02-neighbours-figures/knnreg3-1.svg" alt="Figure 7: K-NN regression example" id="fig:knnreg3" />
<p class="caption">Figure 7: K-NN regression example</p>
</div>
<!--

TODO: exercise -- density, not NN-based classification
regression - consider the epsilon-neighbourhood.

-->
</div>
<div id="further-reading" class="section level3">
<h3><span class="header-section-number">2.6.3</span> Further Reading</h3>
<p>Recommended further reading: <span class="citation">(Hastie et al. <a href="references.html#ref-esl">2017</a>: Section 13.3)</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="decision-and-regression-trees.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": "lmlcr.pdf",
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
