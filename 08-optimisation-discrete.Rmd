# Discrete Optimisation

<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->


<!--

TODO : actually do some combinatorial optimisation

variable selection for lm/glm????

tabu search???

-->



## Introduction

### Recap





Recall that an **optimisation task** deals with finding an element $\mathbf{x}$
in a **search space** $\mathbb{D}$,
that minimises or maximises an **objective function** $f:\mathbb{D}\to\mathbb{R}$:
\[
\min_{\mathbf{x}\in \mathbb{D}} f(\mathbf{x}) \quad\text{or}\quad\max_{\mathbf{x}\in \mathbb{D}} f(\mathbf{x}),
\]


In one of the previous chapters, we were dealing with
**unconstrained continuous optimisation**,
i.e., we assumed the search space is $\mathbb{D}=\mathbb{R}^p$ for some $p$.

\ \

Example problems of this kind: minimising mean squared error
in linear regression
or minimising cross-entropy in logistic regression.





The class of general-purpose iterative algorithms we've previously studied
fit into the following scheme:


1. $\mathbf{x}^{(0)}$ -- initial guess (e.g., generated at random)

2. for $i=1,...,M$:
    a. $\mathbf{x}^{(i)} = \mathbf{x}^{(i-1)}+\text{[guessed direction, e.g.,}-\eta\nabla f(\mathbf{x})\text{]}$
    b. if $|f(\mathbf{x}^{(i)})-f(\mathbf{x}^{(i-1)})| < \varepsilon$ break

3. return $\mathbf{x}^{(i)}$ as result


where:

* $M$ = maximum number of iterations
* $\varepsilon$ = tolerance, e.g, $10^{-8}$
* $\eta>0$ = learning rate




The algorithms such as gradient descent and BFGS (see `optim()`)
give satisfactory results in the case of **smooth and well-behaving objective functions**.

However, if an objective has, e.g., many plateaus (regions where it is almost constant),
those methods might easily get stuck in local minima.

The K-means clustering's objective function is a not particularly pleasant
one -- it involves a nested search for the closest cluster, with the use of the $\min$ operator.



### K-means Revisited





In **K-means clustering** we are minimising the squared Euclidean distance
to each point's cluster centre:
\[
\min_{\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{k=1,\dots,K}
\sum_{j=1}^p \left(x_{i,j}-\mu_{k,j}\right)^2
\right).
\]


This is an (NP-)hard problem!
There is no efficient exact algorithm.

We need approximations. In the last chapter, we have
discussed the iterative Lloyd's algorithm (1957),
which is amongst a few procedures implemented in the `kmeans()` function.







To recall, Lloyd's algorithm (1957) is sometimes referred to as "the" K-means algorithm:

1. Start with random cluster centres $\boldsymbol\mu_{1,\cdot}, \dots, \boldsymbol\mu_{K,\cdot}$.

2. For each point $\mathbf{x}_{i,\cdot}$, determine its closest centre $C(i)\in\{1,\dots,K\}$.

3. For each cluster $k\in\{1,\dots,K\}$, compute the new cluster centre $\boldsymbol\mu_{k,\cdot}$ as the componentwise arithmetic mean
of the coordinates of all the point indices $i$ such that $C(i)=k$.

4. If the cluster centres changed since last iteration, go to step 2, otherwise stop and return the result.

\ \

As the procedure might get stuck in a local minimum,
a few restarts are recommended (as usual).

Hence, we are used to calling:

```{r,eval=FALSE}
kmeans(X, centers=k, nstart=10)
```


### optim() vs. kmeans()




Let us compare how a general-purpose optimiser such as the BFGS algorithm
implemented in `optim()` compares with a customised, problem-specific solver.

We will need some benchmark data.

```{r,gendata,cache=TRUE}
gen_cluster <- function(n, p, m, s) {
    vectors <- matrix(rnorm(n*p), nrow=n, ncol=p)
    unit_vectors <- vectors/sqrt(rowSums(vectors^2))
    unit_vectors*rnorm(n, 0, s)+rep(m, each=n)
}
```

The above function generates $n$ points in $\mathbb{R}^p$
from a distribution centred at $\mathbf{m}\in\mathbb{R}^p$,
spread randomly in every possible direction with scale factor $s$.




Two example clusters in $\mathbb{R}^2$:

```{r,dependson='gendata',gendata_example,cache=TRUE,echo=-1}
set.seed(1243)
# plot the "black" cluster
plot(gen_cluster(500, 2, c(0, 0), 1), col="#00000022", pch=16,
    xlim=c(-3, 4), ylim=c(-3, 4), asp=1, ann=FALSE)
# plot the "red" cluster
points(gen_cluster(250, 2, c(1.5, 1), 0.5), col="#ff000022", pch=16)
```




Let's generate the benchmark dataset $\mathbf{X}$
that consists of three clusters in a high-dimensional space.

```{r,dependson='gendata',gendata2,cache=TRUE}
set.seed(123)
p  <- 32
Ns <- c(50, 100, 20)
Ms <- c(0, 1, 2)
s  <- 1.5*p
K  <- length(Ns)

X <- lapply(1:K, function(k)
    gen_cluster(Ns[k], p, rep(Ms[k], p), s))
X <- do.call(rbind, X) # rbind(X[[1]], X[[2]], X[[3]])
```



<!--
X <- as.matrix(read.csv("datasets/sipu_unbalance.csv",
    header=FALSE, sep=" ", comment.char="#"))
X <- X/10000-30 # a more user-friendly scale
K <- 8
p <- 2

library("FNN")
get_fitness <- function(mu, X) {
    # For each point in X,
    # get the index of the closest point in mu:
    memb <- FNN::get.knnx(mu, X, 1)$nn.index

    # compute the sum of squared distances
    # between each point and its closes cluster centre:
    sum((X-mu[memb,])^2)
}

Storn R, Price K (1997). “Differential Evolution – A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces.”Journal of Global Optimization,11(4), 341–359.

Mullen K, Ardia D, Gil D, Windover D, Cline J (2011). “DEoptim:  An R Package for GlobalOptimization by Differential Evolution.”Journal of Statistical Software,40(6), 1–26. URLhttp://www.jstatsoft.org/v40/i06/

library("DEoptim")
obj <- function(mu) {
            get_fitness(matrix(mu, nrow=K), X)
        }
res <- DEoptim(fn=obj,
        lower=rep(apply(X, 2, min), each=K),
        upper=rep(apply(X, 2, max), each=K)
        #control=list(itermax=1000)
        )

mu_res <- matrix(res$optim$bestmem, nrow=K)
plot(X)
points(mu_res, col=2,cex=3)
get_fitness(mu_res, X)
km <- kmeans(X, mu_res)
get_fitness(km$centers, X)




res <- optim(X[sample(nrow(X), K),],
    fn=obj,
        #lower=rep(apply(X, 2, min), each=K),
        #upper=rep(apply(X, 2, max), each=K),
        method="SANN",
        control = list(maxit = 20000)
        )

mu_res <- matrix(res$par, nrow=K)
plot(X)
points(mu_res, col=2,cex=3)
get_fitness(mu_res, X)
km <- kmeans(X, mu_res)
get_fitness(km$centers, X)

Zambrano-Bigiarini, M., Rojas, R. (2013). “A model-independent Particle Swarm Optimisation software for model calibration.” Environmental Modelling & Software, 43, 5-25. doi: 10.1016/j.envsoft.2013.01.004, http://dx.doi.org/10.1016/j.envsoft.2013.01.004.

library("hydroPSO")
res <- hydroPSO(fn=obj,
        lower=rep(apply(X, 2, min), each=K),
        upper=rep(apply(X, 2, max), each=K)
        #control=list(itermax=1000)
        )

mu_res <- matrix(res$par, nrow=K)
plot(X)
points(mu_res, col=2,cex=3)
get_fitness(mu_res, X)
km <- kmeans(X, mu_res)
get_fitness(km$centers, X)




cntr <- matrix(ncol=2, byrow=TRUE, c( # initial guess
   -15,   5,
   -12,   10,
   -10,   5,
    15,   0,
    15,   10,
    20,   5,
    25,   0,
    25,   10))
km <- kmeans(X, cntr)
get_fitness(km$centers, X)

km <- kmeans(X, K, nstart=10)
get_fitness(km$centers, X)

-->





The objective function for the K-means clustering problem:

```{r,dependson='gendata2',gendata3,cache=TRUE}
library("FNN")
get_fitness <- function(mu, X) {
    # For each point in X,
    # get the index of the closest point in mu:
    memb <- FNN::get.knnx(mu, X, 1)$nn.index

    # compute the sum of squared distances
    # between each point and its closes cluster centre:
    sum((X-mu[memb,])^2)
}
```








Setting up the solvers:

```{r,dependson='gendata3',gendata3b,cache=TRUE}
min_HartiganWong <- function(mu0, X)
    get_fitness(
        # algorithm="Hartigan-Wong"
        kmeans(X, mu0, iter.max=100)$centers,
    X)
min_Lloyd <- function(mu0, X)
    get_fitness(
        kmeans(X, mu0, iter.max=100, algorithm="Lloyd")$centers,
    X)
min_optim <- function(mu0, X)
    optim(mu0,
        function(mu, X) {
            get_fitness(matrix(mu, nrow=nrow(mu0)), X)
        }, X=X, method="BFGS", control=list(reltol=1e-16)
    )$val
```




Running the simulation:

```{r,dependson='gendata3b',gendata4,cache=TRUE,warning=FALSE}
nstart <- 100
set.seed(123)
res <- replicate(nstart, {
  mu0 <- X[sample(nrow(X), K),]
    c(
        HartiganWong=min_HartiganWong(mu0, X),
        Lloyd=min_Lloyd(mu0, X),
        optim=min_optim(mu0, X)
    )
})
```






Notice a considerable variability of the
objective function at the local minima found:

```{r gendata5,dependson='gendata4',cache=TRUE}
boxplot(as.data.frame(t(res)), horizontal=TRUE, col="white")
```






```{r,dependson='gendata5',gendata6,cache=TRUE}
print(apply(res, 1, function(x)
    c(summary(x), sd=sd(x))
))
```

Of course, we are interested in the smallest value of the objective,
because we're trying to pinpoint the global minimum.


```{r,dependson='gendata5',gendata6c,cache=TRUE}
print(apply(res, 1, min))
```





The Hartigan-Wong algorithm (the default one in `kmeans()`)
is the most reliable one of the three:

* it gives the best solution (low bias)
* the solutions have the lowest degree of variability (low variance)
* it is the fastest:





```{r,dependson='gendata3b',gendata4b,cache=TRUE}
library("microbenchmark")
set.seed(123)
mu0 <- X[sample(nrow(X), K),]
summary(microbenchmark(
    HartiganWong=min_HartiganWong(mu0, X),
    Lloyd=min_Lloyd(mu0, X),
    optim=min_optim(mu0, X),
    times=10
), unit="relative")
```





```{r,dependson='gendata5',gendata6b,cache=TRUE}
print(min(res))
```


Is it the global minimum?

> We don't know, we just didn't happen to find anything better (yet).

Did we put enough effort to find it?

> Well, maybe. We can try more random restarts:

```{r,dependson='gendata6',gendata7,cache=TRUE}
res_tried_very_hard <- kmeans(X, K, nstart=100000, iter.max=10000)$centers
print(get_fitness(res_tried_very_hard, X))
```

Is it good enough?

> It depends what we'd like to do with this. Does it make your boss happy?
Does it generate revenue? Does it help solve any other problem?
Is it useful anyhow?
Are you really looking for the global minimum?





## Outro

### Remarks





For any $p\ge 1$, the search space type determines the problem class:

- $\mathbb{D}\subseteq\mathbb{R}^p$ -- **continuous optimisation**

    In particular:

    - $\mathbb{D}=\mathbb{R}^p$ -- continuous unconstrained
    - $\mathbb{D}=[a_1,b_1]\times\dots\times[a_n,b_n]$ -- continuous with box constraints
    - constrained with $k$ linear inequality constraints

        \[
        \left\{
        \begin{array}{lll}
        a_{1,1} x_1 + \dots + a_{1,p} x_p &\le& b_1 \\
        &\vdots&\\
        a_{k,1} x_1 + \dots + a_{k,p} x_p &\le& b_k \\
        \end{array}
        \right.
        \]




However, there are other possibilities as well:

- $\mathbb{D}\subseteq\mathbb{Z}^p$ ($\mathbb{Z}$ -- the set of integers) -- **discrete optimisation**

    In particular:
    - $\mathbb{D}=\{0,1\}^p$ -- 0--1 optimisation (hard!)


- $\mathbb{D}$ is finite (but perhaps large, its objects can be enumerated) -- **combination optimisation**

    For example:
    - $\mathbb{D}=$ all possible routes between two points on a map.

> These  optimisation tasks tend to be much harder than the continuous ones.

Genetic algorithms might come in handy in such cases.






Specialised methods, customised to solve a specific problem (like Lloyd's algorithm)
will often outperform generic ones (like SGD, genetic algorithms)
in terms of speed and reliability.


All in all, we prefer a suboptimal solution obtained by means of heuristics
to no solution at all.




Problems that you could try solving with GAs include variable selection
in multiple regression -- finding the subset of features optimising the AIC
(this is a hard problem to and forward selection was just a simple greed heuristic).



### Further Reading

...
