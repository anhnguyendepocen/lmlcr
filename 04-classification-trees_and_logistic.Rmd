# Classification with Trees and Linear Models

<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->


## Introduction

### Classification Task

---

Let $\mathbf{X}\in\mathbb{R}^{n\times p}$ be an input matrix
that consists of $n$ points in a $p$-dimensional space (each of the $n$ objects
is described by means of $p$ numerical features)

Recall that in supervised learning, with each
$\mathbf{x}_{i,\cdot}$ we associate the desired output $y_i$.

Hence, our dataset is $[\mathbf{X}\ \mathbf{y}]$ --
where each object is represented as a row vector
$[\mathbf{x}_{i,\cdot}\ y_i]$, $i=1,\dots,n$:

\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} & y_1\\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} & y_2\\
\vdots & \vdots & \ddots & \vdots    & \vdots\\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} & y_n\\
\end{array}
\right].
\]


. . .



In this chapter we are still  interested in  **classification** tasks;
we assume that each $y_i$ is a descriptive label.


---

Let's assume that we are faced with **binary classification** tasks.

Hence, there are only two possible labels that we traditionally denote with $0$s and $1$s.

For example:

0       | 1
--------|------------
no      | yes
false   | true
failure | success
healthy | ill




---

The synthetic 2D dataset once again (true decision boundary is at $X_1=0$):


```{r,echo=FALSE,fig.height=6}
par(mar=c(4,4,0.5,0.5))
set.seed(123)
n0 <- 50 # n0 points in class 0
n1 <- 50 # n0 points in class 0
Xs <- rbind(
    cbind(rnorm(n0, -1, 1), rnorm(n0, 0, 1)), # N( (-1, 0), (1, 1) )
    cbind(rnorm(n1, +1, 1), rnorm(n1, 0, 1))  # N( (+1, 0), (1, 1) )
)
Ys <- factor(rep(c("0", "1"), c(n0, n1)))

plot(Xs[,1], Xs[,2], col=Ys, pch=c(1,2)[Ys], las=1, xlab="X1", ylab="X2", asp=1)
legend("topleft", col=c(1,2), pch=c(1,2), legend=c("Class 0", "Class 1"))
abline(v=0, lty=3, col="blue")
```








### Data

---

For illustration, we'll be considering the Wine Quality dataset
(white wines only):


```{r load1,cache=TRUE}
wines <- read.csv("datasets/winequality-all.csv", comment="#")
wines <- wines[wines$color == "white",]
(n <- nrow(wines)) # number of samples
```

---

The input matrix $\mathbf{X}\in\mathbb{R}^{n\times p}$
consists of the first 10 numeric variables:

```{r load2,dependson='load1',cache=TRUE}
X <- as.matrix(wines[,1:10])
dim(X)
head(X, 2) # first two rows
```




---

The 11th variable measures the amount of alcohol (in %).

We will convert this dependent variable to a binary one:

- 0 == (`alcohol  < 12`) == lower-alcohol wines
- 1 == (`alcohol >= 12`) == higher-alcohol wines

```{r load3,dependson='load2',cache=TRUE}
# recall that TRUE == 1
Y <- factor(as.character(as.numeric(wines$alcohol >= 12)))
table(Y)
```



---

60/40% train-test split:

```{r load4,dependson='load3',cache=TRUE}
set.seed(123) # reproducibility matters
random_indices <- sample(n)
head(random_indices) # preview
# first 60% of the indices (they are arranged randomly)
# will constitute the train sample:
train_indices <- random_indices[1:floor(n*0.6)]
X_train <- X[train_indices,]
Y_train <- Y[train_indices]
# the remaining indices (40%) go to the test sample:
X_test  <- X[-train_indices,]
Y_test  <- Y[-train_indices]
```



---

Let's also compute `Z_train` and `Z_test`, being the standardised versions of `X_train`
and `X_test`, respectively.

```{r load5,dependson='load4',cache=TRUE}
means <- apply(X, 2, mean) # column means
sds   <- apply(X, 2, sd)   # column standard deviations
Z_train <- t(apply(X_train, 1, function(c) (c-means)/sds))
Z_test  <- t(apply(X_test,  1, function(c) (c-means)/sds))
```



---


```{r loaded,dependson='load5',cache=TRUE}
get_metrics <- function(Y_pred, Y_test)
{
    C <- table(Y_pred, Y_test) # confusion matrix
    stopifnot(dim(C) == c(2, 2))
    c(Acc=(C[1,1]+C[2,2])/sum(C), # accuracy
      Prec=C[2,2]/(C[2,2]+C[2,1]), # precision
      Rec=C[2,2]/(C[2,2]+C[1,2]), # recall
      F=C[2,2]/(C[2,2]+0.5*C[1,2]+0.5*C[2,1]), # F-measure
      # Confusion matrix items:
      TN=C[1,1], FN=C[1,2],
      FP=C[2,1], TP=C[2,2]
    ) # return a named vector
}
```

---



Let's go back to the K-NN algorithm.

```{r}
library("FNN")
Y_knn5   <- knn(X_train, X_test, Y_train, k=5)
Y_knn9   <- knn(X_train, X_test, Y_train, k=9)
Y_knn5s  <- knn(Z_train, Z_test, Y_train, k=5)
Y_knn9s  <- knn(Z_train, Z_test, Y_train, k=9)
```

---

```{r}
cbind(
    Knn5=get_metrics(Y_knn5, Y_test),
    Knn9=get_metrics(Y_knn9, Y_test),
    Knn5s=get_metrics(Y_knn5s, Y_test),
    Knn9s=get_metrics(Y_knn9s, Y_test)
)
```

---

In this chapter we discuss the following simple and educational (yet practically useful)
classification algorithms:

- *decision trees*,
- *logistic regression*.


## Decision Trees

### Introduction



---

Note that a K-NN classifier discussed in the previous chapter
is **model-free**.
The whole training set must be stored and referred to at all times.

Therefore, it doesn't *explain* the data we have -- we may use it solely
for the purpose of *prediction*.


Perhaps one of the most interpretable (and hence human-friendly) models
consist of decision rules of the form:

**IF $x_{i,j_1}\le v_1$ AND ... AND $x_{i,j_r}\le v_r$ THEN $\hat{y}_i=1$.**

These can be organised into a **hierarchy** for greater readability.

This idea inspired the notion of **decision trees** [@cart].


---


```{r,echo=FALSE,out.width='100%',fig.width=10,fig.height=5}
library("rpart")
library("rpart.plot")
set.seed(123)



plot_rpart <- function(ts, XYs) {
  xx1 <- seq(-4, 4, length.out=250)
  xx2 <- seq(-4, 4, length.out=250)
  xx <- expand.grid(xx1, xx2)

  dimnames(xx)[[2]] <- names(XYs)[1:2]

  yy <- predict(ts, data.frame(xx, names=c("V1", "V2")), type="class")
  par(mar=c(4,4,2.5,0.5))
  image(xx1, xx2, matrix(as.numeric(yy)-1, nrow=length(xx1), ncol=length(xx2)),
        col=c("#00000044", "#ff000044"), las=1, xlab="X1", ylab="X2", asp=1,
        main=sprintf("Decision Tree Class Bounds, cp=%g", cp))


  points(Xs[,1], Xs[,2], col=Ys, pch=c(1,2)[Ys])
  legend("topleft", col=c(1,2), pch=c(1,2), legend=c("Class 0", "Class 1"), bg="white")
  abline(v=0, lty=3, col="blue")
  par(mar=c(0,0,0,0))
}


XYs <- as.data.frame(cbind(Xs, Y=as.numeric(as.character(Ys))))
names(XYs) <- c("X1", "X2", "Y")
cp <- 0.5
ts <- rpart(Y~., data=XYs, method="class", cp=cp)
par(mfrow=c(1,2))
plot_rpart(ts, XYs)
rpart.plot(ts)
```


Each tree node reports 3 pieces of information:

- dominating class (0 or 1)
- (relative) proportion of 1s  represented in a node
- (absolute) proportion of all observations in a node



---


```{r,echo=FALSE,out.width='100%',fig.width=10,fig.height=5}
cp <- 0.01
ts <- rpart(Y~., data=XYs, method="class", cp=cp)
par(mfrow=c(1,2))
plot_rpart(ts, XYs)
rpart.plot(ts)
```


---


```{r,echo=FALSE,out.width='100%',fig.width=10,fig.height=5}
cp <- 0.001
control <- list(minsplit=1, minbucket=5)
ts <- rpart(Y~., data=XYs, method="class", cp=cp, control=control)
par(mfrow=c(1,2))
plot_rpart(ts, XYs)
rpart.plot(ts, tweak=1.2)
```






### Example in R

---



We will use the `rpart()` function from the `rpart` package
to build a classification tree.

```{r}
library("rpart")
library("rpart.plot")
set.seed(123)
```

`rpart()` uses a formula (`~`) interface, hence it will be easier
to feed it with data in a data.frame form.

```{r}
XY_train <- cbind(as.data.frame(X_train), Y=Y_train)
XY_test <- cbind(as.data.frame(X_test), Y=Y_test)
```

---


Fit and plot a decision tree:

```{r,echo=-1,out.width="100%",fig.width=8,fig.height=5}
par(mar=c(0,0,0,0))
t1 <- rpart(Y~., data=XY_train, method="class")
rpart.plot(t1, tweak=1.1, fallen.leaves=FALSE, digits=3)
```



---

<!--
The fitted model is rather... simple.

Only the `alcohol` variable is taken into account.


Well note how these two distributions are shifted:

```{r,echo=-1}
par(mar=c(4,4,0.5,0.5))
#vioplot::vioplot(alcohol~Y, data=XY_train,
#    horizontal=TRUE, las=1)
```

---


-->

Make predictions:

```{r}
Y_pred <- predict(t1, XY_test, type="class")
get_metrics(Y_pred, Y_test)
```

---

> (\*) Interestingly, `rpart()` also provides us with information
about the importance degrees of each independent variable.

```{r}
t1$variable.importance/sum(t1$variable.importance)
```

---

We can build a less complex tree by playing
with the `cp` parameter.

```{r,out.width='100%',fig.width=10,fig.height=5}
# cp = complexity parameter, smaller → more complex tree
t2 <- rpart(Y~., data=XY_train, method="class", cp=0.1)
rpart.plot(t2, tweak=1.1, fallen.leaves=FALSE, digits=3)
```


---

```{r}
Y_pred <- predict(t2, XY_test, type="class")
get_metrics(Y_pred, Y_test)
```



---

A more complex tree:

```{r,out.width='100%',fig.width=10,fig.height=5}
# cp = complexity parameter, smaller → more complex tree
t3 <- rpart(Y~., data=XY_train, method="class", cp=0.00001)
rpart.plot(t3, tweak=1.1, fallen.leaves=FALSE, digits=3)
```


---

```{r}
Y_pred <- predict(t3, XY_test, type="class")
get_metrics(Y_pred, Y_test)
```


### A Note on Decision Tree Learning

---

Learning an optimal decision tree is a computationally hard problem
-- we need some heuristics.

Examples:

* ID3 (Iterative Dichotomiser 3) [@id3]
* C4.5 algorithm [@c45]
* CART by Leo Breiman et al., [@cart]

(\*\*) Decision trees are most often constructed by a *greedy*, *top-down*
*recursive partitioning*, see., e.g., [@rpart].






<!--

## TODO Random Forests and XGBoost

## ...

---


just show how to use it

boosting

bagging

```{r}
#library("randomForest")
#rf <- randomForest(X_train, Y_train)
#Y_pred <- predict(rf, X_test)
#get_metrics(Y_pred, Y_test)
```


---


```{r}
#library("xgboost")
#xg <- xgboost(X_train, Y_train)
#Y_pred <- predict(xg, X_test)
#get_metrics(Y_pred, Y_test)
```

-->






## Binary Logistic Regression

### Motivation

---


Recall that for a regression task, we fitted a very simple family of models
-- the linear ones -- by minimising the sum of squared residuals.

This approach was pretty effective.

Theoretically, we could treat the class labels as numeric $0$s and $1$s
and apply regression models in a binary classification task.



```{r lm1,dependson='loaded',echo=-1,cache=TRUE}
set.seed(123)
XY_train_r <- cbind(as.data.frame(X_train),
    Y=as.numeric(Y_train)-1 # 0.0 or 1.0
)
XY_test_r <- cbind(as.data.frame(X_test),
    Y=as.numeric(Y_test)-1 # 0.0 or 1.0
)
f_r <- lm(Y~density+residual.sugar+pH, data=XY_train_r)
```

---


```{r lm2,dependson='lm1',cache=TRUE}
Y_pred_r <- predict(f_r, XY_test_r)
summary(Y_pred_r)
```

The predicted outputs, $\hat{Y}$, are arbitrary real numbers,
but we can convert them to binary ones by checking if, e.g., $\hat{Y}>0.5$.

```{r lm3,dependson='lm2',cache=TRUE}
Y_pred <- as.numeric(Y_pred_r>0.5)
round(get_metrics(Y_pred, XY_test_r$Y), 3)
```

---

The threshold $T=0.5$  could even be treated as a free parameter
we optimise for (w.r.t. different metrics over the validation sample).

```{r,echo=FALSE,cache=TRUE,fig.height=5, lm4,dependson='lm3',cache=TRUE}
set.seed(123)
par(mar=c(4,4,0.5,0.5))
f_r <- lm(Y~density+residual.sugar+pH, data=XY_train_r)
Y_pred_r <- predict(f_r, XY_test_r)
Ts <- seq(0.1, 0.85, by=0.05)
Ps <- as.data.frame(t(sapply(Ts, function(t) {
  Y_pred <- as.numeric(Y_pred_r>t)
  get_metrics(factor(Y_pred, levels=c("0", "1")), factor(XY_test_r$Y, levels=c("0", "1")))
})))

matplot(Ts, Ps[,1:4], las=1, xlab="T", ylab="Metric", type="l", ylim=c(0,1))
legend("top", legend=names(Ps[,1:4]), col=1:4, lty=1:4, ncol=4)
```


Despite we can, we shouldn't use linear regression for classification.
There are better models.




### Logistic Model

---

Inspired by this idea, we could try modelling
the ***probability* that a given point belongs to class $1$**.

This could also provide us with the *confidence* in our prediction.

Probability is a number in $[0,1]$, but the outputs of a linear model are arbitrary real numbers.

However, we could transform those real-valued outputs by means
of some function $\phi:\mathbb{R}\to[0,1]$ (preferably S-shaped == sigmoid),
so as to get:

$\Pr(Y=1|\mathbf{X},\boldsymbol\beta)=\phi(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)$

> The above reads as "Probability that $Y$ is from class 1 given $\mathbf{X}$
and $\boldsymbol\beta$".

---

A popular choice is the **logistic sigmoid function**,

\[
\phi(y) = \frac{1}{1+e^{-y}} = \frac{e^y}{1+e^y}
\]

```{r,echo=FALSE,cache=TRUE,fig.height=5,sigmoid}
par(mar=c(4,4,0.5,0.5))
xxx <- seq(-3, 3, length.out=101)
yyy <- 1/(1+exp(-xxx))
plot(xxx, yyy, type='l', xlab='y', ylab=expression(phi(y)), las=1, ylim=c(0,1))
abline(v=0, lty=3)
abline(h=c(0, 0.5, 1), lty=3)
```


---

Hence our model becomes:

\[
Y=\frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p)}}
\]

We call it a **generalised linear model** (glm).

(There are many other possible generalisations)


### Example in R

---

Let us first fit
a simple  (i.e., $p=1$) logistic regression model
using the `density` variable.

> "logit" below denotes the inverse of the logistic sigmoid function.


```{r}
f <- glm(Y~density, data=XY_train, family=binomial("logit"))
summary(f)
```

---


```{r,echo=FALSE,cache=TRUE,fig.height=5,        eval=TRUE}
par(mar=c(4,4,0.5,0.5))
xxx <- seq(min(XY_train$density), max(XY_train$density), length.out=101)
yyy <- 1/(1+exp(-(f$coef[1]+f$coef[2]*xxx)))
plot(xxx, yyy, type='l', xlab='density', ylab="P(Y=1|density)", las=1, ylim=c(0,1))
points(XY_train$density, jitter(as.numeric(XY_train$Y)-1, 0.1), col=
    c("#00000011", "#ff000011")[as.numeric(XY_train$Y)], pch=16)
abline(h=c(0, 0.5, 1), lty=3)
legend(8, 0.95, legend=sprintf("1/(1+exp(-(%+g%+g*density))", f$coef[1], f$coef[2]), lty=1)
```

---

Some predicted probabilities:


```{r}
round(head(predict(f, XY_test, type="response"), 12), 2)
```

We classify $Y$ as 1 if the corresponding membership probability
is greater than $0.5$.

```{r}
Y_pred <- as.numeric(predict(f, XY_test, type="response")>0.5)
get_metrics(Y_pred, Y_test)
```

---

And now a fit based on some other input variables:

```{r}
f <- glm(Y~density+residual.sugar+total.sulfur.dioxide,
    data=XY_train, family=binomial("logit"))
Y_pred <- as.numeric(predict(f, XY_test, type="response")>0.5)
get_metrics(Y_pred, Y_test)
```


> Exercise: try fitting the models based on different combinations
of features.






### Loss Function

---

The fitting of the model can be written as an optimisation task:

\[
\min_{\beta_0, \beta_1,\dots, \beta_p\in\mathbb{R}}
\frac{1}{n} \sum_{i=1}^n
e\left(\frac{1}{1+e^{-(\beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p})}}, y_i \right)
\]

where $e(\hat{y}, y)$ denotes the penalty that measures the
"difference" between the true $y$ and its predicted version $\hat{y}$.

In the ordinary regression, we use the squared residual $e(\hat{y}, y) = (\hat{y}-y)^2$.

In **logistic regression** (the kind of a classifier we are interested in right now), we use
the **cross-entropy** (a.k.a. **log-loss**),

\[
e(\hat{y},y) = - \left(y\log \hat{y} + (1-y)\log(1-\hat{y})\right)
\]

The corresponding loss function has not only nice statistical properties (\*\*)
but also an intuitive interpretation.

---

Note that the predicted $\hat{y}$ is in $(0,1)$ and the true $y$ equals to either 0 or 1.

Recall also that $\log t\in(-\infty, 0)$ for $t\in (0,1)$.


\[
e(\hat{y},y) = - \left(y\log \hat{y} + (1-y)\log(1-\hat{y})\right)
\]


- if true $y=1$, then the penalty becomes $e(\hat{y}, 1) = -\log(\hat{y})$

    - $\hat{y}$ is the probability that the classified input is indeed from class $1$
    - we'd be happy if the classifier outputted $\hat{y}\simeq 1$ in this case;
    this is not  penalised as $-\log(t)\to 0$ as $t\to 1$
    - however, if the classifier is totally wrong, i.e., it thinks that
    $\hat{y}\simeq 0$, then the penalty will be very high, as $-\log(t)\to+\infty$
    as $t\to 0$


- if true $y=0$, then the penalty becomes $e(\hat{y}, 0) = -\log(1-\hat{y})$

    - $1-\hat{y}$ is the predicted probability that the input is from class $0$
    - we penalise heavily the case where $1-\hat{y}$ is small (we'd be happy
    if the classifier was sure that $1-\hat{y}\simeq 1$, because this is the ground-truth)

---

> (\*) Interestingly, there is no analytical formula
for the optimal set of parameters ($\beta_0,\beta_1,\dots,\beta_p$)
minimising the log-loss.

> In the chapter on optimisation, we shall see that
the solution to the logistic regression can be solved numerically
by means of quite simple iterative algorithms.

## Outro

### Remarks

---

Other prominent classification algorithms:

- Naive Bayes and other probabilistic approaches,
- Support Vector Machines (SVMs) and other kernel methods,
- (Artificial) (Deep) Neural Networks.


---

Interestingly, in the next chapter we will  note that the logistic regression model
is a special case of a *feed-forward single layer neural network*.

We will also generalise the binary logistic regression to the case of
a multiclass classification.


The state-of-the art classifiers called
*Random Forests* and *XGBoost* (see also: *AdaBoost*) are based on decision trees.
They tend to be more accurate but -- at the same time -- they fail to
exhibit the decision trees' important feature: interpretability.

Trees can also be used for regression tasks, see R package `rpart`.



### Further Reading

#### {.allowframebreaks .unnumbered}

Recommended further reading:

- [@islr: Chapters 4 and 8]

Other:

- [@esl: Chapters 4 and 7 as well as (\*) Chapters 9,  10,  13, 15]
