# Agglomerative Hierarchical Clustering {#chap:hclust}

<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->

<!--

Exercise: dimensionality reduction with auto-encoders?

Exercise: apply genieclust

Exercise: implement MDS with optim?

-->



The aim of clustering (a.k.a. data segmentation or quantisation)
is to split the input dataset into *interesting*
subgroups in an unsupervised manner. Example applications of clustering include:

* taxonomisation, e.g.,
partition consumers to more "uniform"
groups to better understand who they are and what do they need,

* aggregation, e.g., to reduce the number of observations
by substituting them by "group representatives" or "prototypes",

* object detection in images, e.g.,
tumour tissues on PET/CT scans,

* complex networks analysis,
e.g., detecting communities in friendship,
retweets and other networks,

* spatial data analysis, e.g., identifying densely populated areas
or traffic jams on maps,

* text analysis, e.g., for grouping documents into automatically
detected topics,

* fine-tuning of supervised learning algorithms,
e.g., recommender systems indicating content
that was rated highly by users from the same group
or learning multiple manifolds in a dimension reduction task.





## Dataset Partitions

### Label Vectors

Let's assume that the input data set $\mathbf{X}\in\mathbb{R}^{n\times p}$
consists of $n$ observations and that we wish to identify
$K$ clusters, for some $K\ge 2$ which gives the number of subgroups.
Such a clustering of $\mathbf{X}$ will be represented
by a vector $\boldsymbol{y}\in\{1, 2, \dots, K\}^n$
such that $y_i$ gives the *cluster ID* (cluster identifier or number,
an integer between
1 and K) of the $i$-th observation, $i=1,2,\dots,n$.
Thus, we can think of clustering as of the process of labelling
or assigning colours to each data point (for example, 1 = black, 2 = red,
3 = green, 4 = blue etc.).
The only restriction we impose on each such *label* vector is that
*each of the $K$ labels must occur at least once*.
This will guarantee that we have indeed a division of $\mathbf{X}$
into $K$ subgroups, and not less.


{ BEGIN remark }
Given an integer $K$
and a vector $\boldsymbol{y}$ with elements $\{1, 2, \dots, K\}$,
for instance:

```{r label_vector_check1}
y <- c(3, 1, 1, 2, 2, 1, 2, 3, 1, 1, 1)
K <- 3 # by the way, it's max(y)
```

we can check whether it represents a proper label vector by
computing the number of occurrences of each label:

```{r label_vector_check2}
tabulate(y, K) # how many 1s, ..., Ks are there in y, respectively
all(tabulate(y, K) > 0) # do we have at least 1 occurrence of each label?
```

or, for example, determining the set of all unique
values in $\boldsymbol{y}$:

```{r label_vector_check3}
unique(y)
length(unique(y)) == K # do we have K unique values in y?
```
{ END remark }


Each such label vector  yields a $K$-partition
of the input data set, that is, its grouping into
$K$ disjoint subsets -- every element in $\mathbf{X}$
belongs to one (and only one) cluster.




### K-Partitions (\*)

As it is beneficial for us to be *gently* exposed to as many easy
mathematical formalisms as possible, let us use some set-theoretic notation
to come up with a rigorous definition of a dataset grouping.
Formally, *clustering* aims to find a *special kind*
of a *$K$-partition* of the input dataset,
which -- without loss of generality -- we can identify
with the set $\{1, 2, \dots, n\}$ of indexes of observations (rows)
in $\mathbf{X}$.




{ BEGIN definition }
We say that $\mathcal{C}=\{C_1,C_2,\dots,C_K\}$ forms
a *$K$-partition* of the set $\{1, 2, \dots, n\}$,
whenever:

* $\bigcup_{k=1}^K C_k=C_1\cup C_2\cup\dots\cup C_K=\{1, 2, \dots, n\}$
(the union of all clusters
is the whole set; every point is assigned to some cluster;
no point is neglected),

* $C_k\cap C_l=\emptyset$ for all $k\neq l$ (clusters are pairwise disjoint;
their intersection is empty; they share no common elements),

* $C_k\neq\emptyset$ for all $k$ (each cluster is nonempty).

<!-- * $C_k\subseteq\{1, 2, \dots, n\}$ for all $k$, -->
{ END definition }


<!--(\*) There is a one-to-one correspondence between partitions
and equivalence relations-->

In the label vector representation,
we assume that $y_i = k$ iff $i\in C_k$, i.e.,
the $i$-th point is assigned the $k$-th label if and only if
it belongs to the $k$-th cluster.
It is easily seen that each label vector yields
a grouping of $\{1, 2, \dots, n\}$ that obviously fulfil
the first two conditions in the above condition
and that the third property is fulfilled by assuming
that $(\forall k\in\{1,2,\dots,K\})$ $(\exists i\in\{1,2,\dots,n\})$
$y_i = k$ (read: *for every $k$ there exists $i$ such that...*),
i.e., each label occurs at least once in $\boldsymbol{y}$.





{ BEGIN remark }
The benefits of using mathematical notation is that we are super-precise,
efficient and universal. Natural tongue is vague and leaves much room
for interpretation. We used so many *words* to explain the above concepts.
Once we build greater skill of *speaking* the language of mathematics
(note that we've started with basic phrases such as *Good Afternoon*,
*I am Sorry* and *Thank You*), it'll be more natural for
us to just write:

"Let $\mathcal{C}=\{ C_1,\dots, C_K \}$ s.t.
$\textstyle\bigcup_{k=1}^K C_k = \{1,\dots, n\}$,
$(\forall k\neq l)$ $C_k\cap C_l = \emptyset$, $C_k\neq \emptyset$"

without all the talking. This is what more advanced book related
to machine learning and statistics do. We'll get to that.
By the way, thanks to this it's relatively easy to read maths
papers (especially those from the 1950s) written in other languages
(such as Russian, German or French).
{ END remark }



In theory, the number of possible $K$-partitions of a set
with $n$ elements is given by
*the Stirling number of the second kind*:
\[
\left\{{n \atop K}\right\}=\sum_{{j=0}}^{{K}} \frac{(-1)^{{j}}
(K-j)^{n}}{j! (K-j)!} = \frac{K^n}{K!}
- \frac{(K-1)^n}{(K-1)!}
+ \frac{(K-2)^n}{2 (K-2)!}
- \frac{(K-3)^n}{6 (K-3)!}
+ \dots,
\]
where $n!=1\cdot 2 \cdot 3\cdot \dots\cdot n$ with $0!=1$ ($n$-factorial).
In particular, already $\left\{{n \atop 2}\right\}=2^{n-1}-1$
and $\left\{{n \atop K}\right\}\simeq K^n/K!$ for large $n$ -- that is a lot.



$n$ | $\left\{{n \atop 2}\right\}$ | $\left\{{n \atop 3}\right\}$ | $\left\{{n \atop 4}\right\}$ | $\left\{{n \atop 5}\right\}$ | $\left\{{n \atop 6}\right\}$ | $\left\{{n \atop 7}\right\}$ | $\left\{{n \atop 8}\right\}$ | $\left\{{n \atop 9}\right\}$ | $\left\{{n \atop 10}\right\}$
---:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|
2   | 1                            |                              |                              |                              |                              |                              |                              |                              |                              |
3   | 3                            |                              | 1                            |                              |                              |                              |                              |                              |                              |
4   | 7                            |                              | 6                            | 1                            |                              |                              |                              |                              |                              |
5   | 15                           | 25                           | 10                           | 1                            |                              |                              |                              |                              |                              |
6   | 31                           | 90                           | 65                           | 15                           | 1                            |                              |                              |                              |                              |
7   | 63                           | 301                          | 350                          | 140                          | 21                           | 1                            |                              |                              |                              |
8   | 127                          | 966                          | 1701                         | 1050                         | 266                          | 28                           | 1                            |                              |                              |
9   | 255                          | 3025                         | 7770                         | 6951                         | 2646                         | 462                          | 36                           | 1                            |                              |
10  | 511                          | 9330                         | 34105                        | 42525                        | 22827                        | 5880                         | 750                          | 45                           | 1                            |

Table: The number of possible partitions of a dataset with $n$ elements to $K$ clusters, denoted $\left\{{n \atop K}\right\}$, grows rapidly as $n$ increases {#tbl:stirling}




### "Interesting" Partitions




We are not just interested in "any" partition, because
there are simply way too plentiful (compare Table {@tbl:stirling}); some of them
will certainly be more meaningful or valuable than others.
However, even one of the most famous ML textbooks provides us with only
a vague hint of what we might be looking for:



{ BEGIN definition }
(or, rather, a "definition").
Clustering concerns "segmenting a collection of objects into subsets
so that those within each cluster are more **closely related**
to one another than objects assigned to different clusters" [@esl].
{ END definition }


It is not uncommon (see, e.g., [@many_clustering_algorithms])
to equate the general definition of data clustering problems with... the
particular outputs generated by specific clustering algorithms. It some sense,
that sounds fair -- clustering is both art and science
(compare [@clustering_science_art]).
From this perspective, we might be interested in
identifying the two main types of clustering algorithms:


* *parametric* (model-based) --
find clusters of specific shapes or following specific multidimensional
probability distributions,
e.g., K-means,
expectation-maximisation for Gaussian mixtures (EM),
average linkage agglomerative clustering;

* *nonparametric* (model-free) -- identify high-density or
well-separable regions, perhaps in the presence of noise points,
e.g., single linkage agglomerative clustering, Genie, (H)DBSCAN, BIRCH.




In this chapter we'll take a look at
*(agglomerative) hierarchical clustering* algorithms which build
a cluster structure
*from the bottom*, i.e., by merging small points groups to form larger ones.
In Chapter \@ref(chap:kmeans), we'll discuss the K-means algorithm,
which seeks "good" cluster prototypes.





## Euclidean Distance


In order to build our first clustering algorithm,
we need to refer to the notion of a *distance*,
which
quantifies the extent to which two observations
in a dataset (two rows in $\mathbf{X}$) are different from
or similar to each other.

In this chapter we will be dealing with the most natural,
"school" straight-line distance, called the Euclidean metric.
It works as if we were measuring how two points
are far away from each other with a ruler.

Given two $p$-tuples $\boldsymbol{u}=(u_1,\dots,u_p)$
and $\boldsymbol{v}=(p_1,\dots,v_p)$, the *Euclidean metric*
is a function $d$ such that:
\[
d(\boldsymbol{u}, \boldsymbol{v}) = \sqrt{
(u_1-v_1)^2 + (u_2-v_2)^2 + \dots + (u_p-v_p)^2
}
=
\sqrt{
\sum_{i=1}^p (u_i-v_i)^2
}.
\]
The Euclidean metric will often also be denoted with
$\|\boldsymbol{u}-\boldsymbol{v}\|$.



{ BEGIN remark }
$\sum$ (Greek capital letter Sigma) denotes summation.
Read "$\sum_{i=1}^n z_i$" as "the sum of $z_i$ for $i$ from $1$ to $n$";
this is just a convenient shorthand for $z_1+z_2+\dots+z_n$.
{ END remark }



{ BEGIN exercise }
Consider the following $\mathbf{X}\in\mathbb{R}^{3\times 2}$:
\[
\mathbf{X} = \left[\begin{array}{cc}
0 & 0 \\
1 & 0 \\
\frac{1}{4} & 1 \\
\end{array}\right]
\]
Calculate (by hand)
$d(\mathbf{x}_{1,\cdot}, \mathbf{x}_{2,\cdot})$,
$d(\mathbf{x}_{1,\cdot}, \mathbf{x}_{3,\cdot})$,
$d(\mathbf{x}_{2,\cdot}, \mathbf{x}_{3,\cdot})$,
$d(\mathbf{x}_{1,\cdot}, \mathbf{x}_{1,\cdot})$ and
$d(\mathbf{x}_{2,\cdot}, \mathbf{x}_{1,\cdot})$.
{ END exercise }



In order to compute all pairwise distances between the rows in a
matrix with R, we can call the `dist()` function
(see also Figure {@fig:euclidean_distance_fig} for a graphical illustration):

```{r euclidean_distance}
X <- rbind(
    c(0,    0),
    c(1,    0),
    c(0.25, 1)
)
print(X)
dist(X)
````




```{r euclidean_distance_fig,echo=FALSE,fig.cap="Example points and Euclidean distances between them"}
plot(X[,1], X[,2], asp=1)
text(X[1,1], X[1,2], sprintf("(%g,%g)", X[1,1], X[1,2]), pos=2)
text(X[2,1], X[2,2], sprintf("(%g,%g)", X[2,1], X[2,2]), pos=4)
text(X[3,1], X[3,2], sprintf("(%g,%g)", X[3,1], X[3,2]), pos=4)
polygon(X[,1], X[,2])
text(mean(X[c(1,2),1]),mean(X[c(1,2),2]), round(dist(X[c(1,2),]),4), pos=3)
text(mean(X[c(1,3),1]),mean(X[c(1,3),2]), round(dist(X[c(1,3),]),4), pos=4)
text(mean(X[c(3,2),1]),mean(X[c(3,2),2]), round(dist(X[c(3,2),]),4), pos=2)
```


Note that only 3 values were reported. This is due to the fact
that:

* the distance between a point and itself is always 0,
i.e., $d(\boldsymbol{u},\boldsymbol{u})=0$;

* the order of the points (from-to or to-from) doesn't matter,
we have $d(\boldsymbol{u},\boldsymbol{v})=d(\boldsymbol{v},\boldsymbol{u})$
(this is called *symmetry*).


If we want the full distance matrix, we can call:

```{r euclidean_distance_matrix}
as.matrix(dist(X))
```

Note the zeros on the main diagonal and that the matrix is symmetric
around it.


{ BEGIN remark }
Overall, there are $n(n-1)/2$ "interesting" pairwise distances
for a dataset of $n$ points. In R, these are stored using `numeric` type,
each being an 8-byte floating point number. Therefore, by calling `dist()`
on matrices with too many rows we can easily run out of memory;
already for $n=100{,}000$ we need ca. 40 GB of RAM.
{ END remark }

{ BEGIN remark }
Later <!-- TODO -->
we will mention that there are many possible distances,
allowing to measure the similarity of points not only in $\mathbb{R}^p$,
but also character strings, protein sequences, film ratings, etc.;
there is even an encyclopedia of distances (see [@encyclopedia_distances])!.
{ END remark }






## Hierarchical Clustering at a Glance


Hierarchical methods generate a whole hierarchy
of *nested* partitions. A $K$-partition for any $K$
can be extracted later at any time.
In the case of *agglomerative* hierarchical algorithms:

* at the lowest level of the hierarchy, each point belongs to its own
cluster (there are $n$ singletons);

* at the highest level of the hierarchy,
there is one cluster that embraces all the points;

* moving from the $i$-th to the $(i+1)$-th level,
we select (somehow; see below) a pair of clusters to be merged.


Let's consider the  Sustainable Society Indices Dataset
(see Appendix \@ref(appendix:datasets))
that measures the Human,
Environmental  and Economic Wellbeing
in each country on the scale $[0,10]$.

```{r ssi_load}
ssi <- read.csv("datasets/ssi_2016_dimensions.csv",
    comment.char="#")
head(ssi)
```

In other to better understand how the algorithms of interest work,
we'll restrict ourselves a smaller sample of countries, say,
to 37 members of the OECD:

```{r ssi_oecd}
oecd <- c("Australia", "Austria", "Belgium", "Canada", "Chile", "Colombia",
"Czech Republic", "Denmark", "Estonia", "Finland", "France", "Germany",
"Greece", "Hungary", "Iceland", "Ireland", "Israel", "Italy", "Japan",
"Korea, South", "Latvia", "Lithuania", "Luxembourg", "Mexico",
"Netherlands", "New Zealand", "Norway", "Poland", "Portugal",
"Slovak Republic", "Slovenia", "Spain", "Sweden", "Switzerland",
"Turkey", "United Kingdom", "United States")
ssi <- ssi[ssi[,"Country"] %in% oecd, ]
X <- as.matrix(ssi[,-1]) # everything except the Country column
dimnames(X)[[1]] <- ssi[,1] # set row names
head(X)
dim(X) # n and p
```

Hence, we have $\mathbf{X}\in\mathbb{R}^{`r nrow(X)` \times `r ncol(X)`}$.
Note that the matrix in R has row and column names set
for greater readability.



. . .

The most basic implementation of a few
agglomerative hierarchical clustering algorithms
is provided by the `hclust()` function, which works on a pairwise
distance matrix.
By default, the methods computes the so-called *complete* linkage.
We'll explain what that means later, let's just simply use it in
a "black-box" fashion now, i.e., without going into details:

```{r dist_hclust_complete}
# Euclidean distances between all pairs of points:
D <- dist(X)
h <- hclust(D) # method="complete"
print(h)
```

The obtained hierarchy can be *cut* at an arbitrary level
by applying the `cutree()` function. Let's use $K=3$:

```{r cutree}
K <- 3
y <- cutree(h, K) # extract the 3-partition
```

A picture is worth a thousand words, therefore
let's generate a map with the `rworldmap` package (see Figure {@fig:ssi_map}):

```{r ssi_map,echo=-c(1,2),results="hide",cache=TRUE,message=FALSE,fig.cap="OECD countries grouped w.r.t. the SSI dimensions (complete linkage, $K=3$)"}
par(ann=FALSE) # no axes
par(mar=c(0, 0, 0, 0)) # no figure margins
library("rworldmap") # see the package's manual for details
mapdata <- data.frame(Country=dimnames(X)[[1]], Cluster=y)
mapdata <- joinCountryData2Map(mapdata, joinCode="NAME",
  nameJoinColumn="Country")
mapCountryData(mapdata, nameColumnToPlot="Cluster",
    catMethod="categorical", missingCountryCol="white",
    colourPalette=palette.colors(K, "Okabe-Ito"),
    mapTitle="", addLegend=TRUE)
```

We have so many questions now! First of all, why the countries were grouped
in such a way? Well, this is because the algorithm is programmed
to perform certain computational steps that lead to this particular solution.
We'll discuss them in very detail soon.

A better question is: what characterises the countries in each cluster?
To explore possible answers to this question, we can compute, e.g.,
the average indicators in each group:

```{r ssi_colmeans}
aggregate(as.data.frame(X), list(Cluster=y), mean)
```

Therefore, on average, the countries in the 1st cluster
have higher Economic Wellbeing scores than the countries in the 2nd cluster,
and these outperform those from the 3rd cluster. On the other hand,
the 3rd cluster countries score much more highly on the Environmental
Wellbeing dimension. This summary, however, is valid only if
the countries in each cluster are score similarly on each scale
(are homogeneous).

Luckily, we have only 3 dimensions, therefore we can visualise
different two-dimensional projections of this dataset.
Figure {@fig:ssi_complete_pairs}
depicts a slightly "tuned up" (we've added the ISO 3-letter country codes)
version of a scatter plot matrix
that we would normally obtain by calling `pairs(X, col=y, pch=y)`.



```{r ssi_complete_pairs,echo=FALSE,fig.height=6,fig.cap="Scatterplot matrix for the SSI dimensions with the 3-partition generated by complete linkage"}
#pairs(X, col=y, pch=y)
par(mfcol=c(2,2))

country_iso3 <- as.character(mapdata@data$ISO3[match(dimnames(X)[[1]], mapdata@data$Country)])

plot(X[,3], X[,2], col=y, pch=y,
    xlab=dimnames(X)[[2]][3], ylab=dimnames(X)[[2]][2])
text(X[,3], X[,2], col=y, country_iso3, pos=1, xpd=TRUE)

plot.new()

plot(X[,1], X[,2], col=y, pch=y,
    xlab=dimnames(X)[[2]][1], ylab=dimnames(X)[[2]][2])
text(X[,1], X[,2], col=y, country_iso3, pos=1, xpd=TRUE)

plot(X[,1], X[,3], col=y, pch=y,
    xlab=dimnames(X)[[2]][1], ylab=dimnames(X)[[2]][3])
text(X[,1], X[,3], col=y, country_iso3, pos=1, xpd=TRUE)


```

In seems that Economic Wellbeing is the deciding factor for distinguishing
between these countries. We see on the Economic vs. Environmental Wellbeing plot
that the clusters are nicely separated. Human Wellbeing, on the other hand,
is pretty "mixed up" across the clusters.





## Cluster Dendrograms


A *dendrogram*  (which we can plot by calling `plot(h)`, where `h` is
the result returned by `hclust()`) depicts the distances
(as defined by the linkage function) between the clusters merged at every stage
of the agglomerative procedure.
This can provide us with some insight into the underlying data structure
as well as with hits about at which level the tree could be cut.

Figure {@fig:dendrograms} depicts the three dendrograms
that correspond to the clusterings  obtained by applying different linkages.
Each tree has 150 leaves (at the bottom) that represent the 150 points
in our example dataset. Each "edge" (joint) represents a group of points
being merged. For instance, the very top joint in the middle subfigure
is located at height of $\simeq 6$, which is exactly the
maximal pairwise distance (complete linkage) between the points
in the last two last clusters.


```{r dendrogram,fig.cap="Cluster dendrogram generated by complete linkage",echo=-1}
par(mar=c(0.5, 2, 0.5, 0.5))
plot(h, xlab=NA, main=NA); box()
```



```{r dendrograms,fig.height=6,fig.cap="Cluster dendrograms for the single, complete and average linkages",echo=FALSE}
par(mfrow=c(3,1))
h <- hclust(D, method="single")
plot(h, labels=FALSE, ann=FALSE); box()
legend("topright", legend="single", bg="white")
h <- hclust(D, method="complete")
plot(h, labels=FALSE, ann=FALSE); box()
legend("topright", legend="complete", bg="white")
h <- hclust(D, method="average")
plot(h, labels=FALSE, ann=FALSE); box()
legend("topright", legend="average", bg="white")
```





## Linkage Functions

Let's formalise the clustering process.
Initially,  $\mathcal{C}^{(0)}=\{\{\mathbf{x}_{1,\cdot}\},\dots,\{\mathbf{x}_{n,\cdot}\}\}$,
i.e., each point is a member of its own cluster.

While an agglomerative  hierarchical clustering algorithm is being computed,
there are $n-k$ clusters at the $k$-th step of the procedure,
$\mathcal{C}^{(k)}=\{C_1^{(k)},\dots,C_{n-k}^{(k)}\}$.

When proceeding from step $k$ to $k+1$,
we determine the two groups $C_u^{(k)}$ and $C_v^{(k)}$, $u<v$,
to be *merged* together so that the clustering at the higher level
is of the form:
\[
\mathcal{C}^{(k+1)} = \left\{
C_1^{(k)},\dots,C_{u-1}^{(k)},
C_u^{(k)}{\cup C_v^{(k)}},
C_{u+1}^{(k)},\dots,C_{v-1}^{(k)},
C_{v+1}^{(k)},\dots,C_{n-k}^{(k)}
\right\}.
\]


Thus, $(\mathcal{C}^{(0)}, \mathcal{C}^{(1)}, \dots, \mathcal{C}^{(n-1)})$
form a sequence of *nested* partitions of
the input dataset
with the last level being just one big cluster,
$\mathcal{C}^{(n-1)}=\left\{ \{\mathbf{x}_{1,\cdot},\mathbf{x}_{2,\cdot},\dots,\mathbf{x}_{n,\cdot}\} \right\}$.


. . .

There is one component missing -- how to determine the pair
of clusters $C_u^{(k)}$ and $C_v^{(k)}$ to be merged with each other
at the $k$-th iteration?
Of course this will be expressed as some optimisation problem (although this time,
a simple one)!  The decision will be based on:
\[
\mathrm{arg}\min_{u < v} d^*(C_u^{(k)}, C_v^{(k)}),
\]
where $d^*(C_u^{(k)}, C_v^{(k)})$ is the *distance* between two clusters
$C_u^{(k)}$ and $C_v^{(k)}$.

Note that we usually only consider the distances between *individual points*,
not sets of points. Hence, $d^*$ must be a suitable extension
of a pointwise distance $d$ (usually the Euclidean metric)
to whole sets.

We will assume that $d^*(\{\mathbf{x}_{i,\cdot}\},\{\mathbf{x}_{j,\cdot}\})=
d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot})$, i.e., the distance between
singleton clusters is the same as the distance between the points themselves.
As far as more populous point groups are concerned, there are many popular
choices of $d^*$ (which in the context of hierarchical clustering we call
*linkage functions*):

- single linkage:

    \[
    d_\text{S}^*(C_u^{(k)}, C_v^{(k)}) =
    \min_{\mathbf{x}_{i,\cdot}\in C_u^{(k)}, \mathbf{x}_{j,\cdot}\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}),
    \]

- complete linkage:

    \[
    d_\text{C}^*(C_u^{(k)}, C_v^{(k)}) =
    \max_{\mathbf{x}_{i,\cdot}\in C_u^{(k)}, \mathbf{x}_{j,\cdot}\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}),
    \]

- average linkage:

    \[
    d_\text{A}^*(C_u^{(k)}, C_v^{(k)}) =
    \frac{1}{|C_u^{(k)}| |C_v^{(k)}|} \sum_{\mathbf{x}_{i,\cdot}\in C_u^{(k)}}\sum_{\mathbf{x}_{j,\cdot}\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}).
    \]


An illustration of the way different linkages are computed
is given in Figure {@fig:linkages}.

```{r linkages,echo=FALSE,fig.cap="In single linkage, we find the closest pair of points; in complete linkage, we seek the pair furthest away from each other; in average linkage, we determine the arithmetic mean of all pairwise distances"}
set.seed(123)
par(mar=rep(0.5,4))
XXX <- jitter(t(as.matrix(iris[,2:3])))
y <- as.numeric(iris[,5])
XXX[,y==3] <- c(2, 1)*XXX[,y==3]+c(-0.5,-0.5)
y[y==3] <- 4

d <- as.matrix(dist(t(XXX)))
d12 <- d[1:50, 51:100]
d23 <- d[51:100, 101:150]
d13 <- d[1:50, 101:150]

kol1 <- rgb(0,0.2,0,0.2)
kol2 <- rgb(0,0.5,0.0,0.5)
kol3 <- rgb(0,0.9,0.0,0.9)

par(mfrow=c(1,3))
# single
plot(XXX[1,], XXX[2,], col=y, pch='.', asp=1, axes=FALSE, xlab=NA, ylab=NA)
box()
# stopifnot(min(d23) < min(d13) && min(d23) < min(d12))
IDXXX_single <- which(min(d23) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol3, lwd=3)
IDXXX_single <- which(min(d13) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol1, lwd=3)
IDXXX_single <- which(min(d12) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol2, lwd=3)
legend("bottomright", legend="single linkage", bg="white")
points(XXX[1,], XXX[2,], col=y, pch=y)

# complete
plot(XXX[1,], XXX[2,], col=y, pch='.', asp=1, axes=FALSE, xlab=NA, ylab=NA)
box()
IDXXX_single <- which(max(d23) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol2, lwd=3)
IDXXX_single <- which(max(d13) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol1, lwd=3)
IDXXX_single <- which(max(d12) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol3, lwd=3)
legend("bottomright", legend="complete linkage", bg="white")
points(XXX[1,], XXX[2,], col=y, pch=y)

# average
plot(XXX[1,], XXX[2,], col=y, pch='.', asp=1, axes=FALSE, xlab=NA, ylab=NA)
box()
#c(mean(d12), mean(d13), mean(d23))
for (i in sample(1:50)) {
    segments(XXX[1,0+i], XXX[2,0+i], XXX[1,51:100], XXX[2, 51:100], col=kol1)
    segments(XXX[1,0+i], XXX[2,0+i], XXX[1,101:150], XXX[2, 101:150], col=kol2)
    segments(XXX[1,50+i], XXX[2, 50+i], XXX[1,101:150], XXX[2, 101:150], col=kol3)
}
legend("bottomright", legend="average linkage", bg="white")
points(XXX[1,], XXX[2,], col=y, pch=y)
```

. . .

Assuming $d_\text{S}^*$, $d_\text{C}^*$ or $d_\text{A}^*$
in the aforementioned procedure leads to single, complete or average
linkage-based agglomerative hierarchical clustering algorithms,
respectively
(referred to as single linkage etc. for brevity).



```{r linkages_hier527658932}
hs <- hclust(D, method="single")
hc <- hclust(D, method="complete")
ha <- hclust(D, method="average")
```

Figure {@fig:linkages_hier52} compares the 5-, 4- and 3-partitions
obtained by applying the 3 above linkages. Note that it's in very nature of
the single linkage algorithm that it's highly sensitive to outliers.

```{r linkages_hier52,fig.height=6,fig.cap="3 cuts of 3 different hierarchies",echo=FALSE}
#par(c(1,1,0.5,0.5))
par(mfrow=c(3,3))
for (k in 5:3) {
    ccc <- cutree(hs, k=k)
    plot(X, col=ccc, pch=ccc, ann=FALSE)
    legend("top", legend=sprintf("k=%d, single", k), bg="white")
    ccc <- cutree(hc, k=k)
    plot(X, col=ccc, pch=ccc, ann=FALSE)
    legend("top", legend=sprintf("k=%d, complete", k), bg="white")
    ccc <- cutree(ha, k=k)
    plot(X, col=ccc, pch=ccc, ann=FALSE)
    legend("top", legend=sprintf("k=%d, average", k), bg="white")
}
```




## Exercises


TODO: reuse from the K-means chapter



## Outro

### Remarks

....

### Further Reading

...


Note that we have studied "crisp" (disjoint) partitions only.

[@jainsurvey]
[@wierzchon_klopotek]

As we've said, there are $n(n-1)/2$ unique pairwise distances between $n$ points.
Don't try calling `dist(hclust(X))` on large data matrices;
packages `fastcluster` and `genieclust`, among others,
aim to solve this problem.


