# Agglomerative Hierarchical Clustering {#chap:hclust}

<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->

<!--

Exercise: dimensionality reduction with auto-encoders?

Exercise: apply genieclust

Exercise: implement MDS with optim?

-->



The aim of clustering (a.k.a. data segmentation or quantisation)
is to split the input dataset into *interesting*
subgroups in an unsupervised manner. Example applications of clustering include:

* taxonomisation, e.g.,
partition consumers to more "uniform"
groups to better understand who they are and what do they need,

* aggregation, e.g., to reduce the number of observations
by substituting them by "group representatives" or "prototypes",

* object detection in images, e.g.,
tumour tissues on PET/CT scans,

* complex networks analysis,
e.g., detecting communities in friendship,
retweets and other networks,

* spatial data analysis, e.g., identifying densely populated areas
or traffic jams on maps,

* text analysis, e.g., for grouping documents into automatically
detected topics,

* fine-tuning of supervised learning algorithms,
e.g., recommender systems indicating content
that was rated highly by users from the same group
or learning multiple manifolds in a dimension reduction task.





## Dataset Partitions

### Label Vectors

Let's assume that the input data set $\mathbf{X}\in\mathbb{R}^{n\times p}$
consists of $n$ observations and that we wish to identify
$K$ clusters, for some $K\ge 2$ which gives the number of subgroups.
Such a clustering of $\mathbf{X}$ will be represented
by a vector $\boldsymbol{y}\in\{1, 2, \dots, K\}^n$
such that $y_i$ gives the *cluster ID* (cluster identifier or number,
an integer between
1 and K) of the $i$-th observation, $i=1,2,\dots,n$.
Thus, we can think of clustering as of the process of labelling
or assigning colours to each data point (for example, 1 = black, 2 = red,
3 = green, 4 = blue etc.).
The only restriction we impose on each such *label* vector is that
*each of the $K$ labels must occur at least once*.
This will guarantee that we have indeed a division of $\mathbf{X}$
into $K$ subgroups, and not less.


{ BEGIN remark }
Given an integer $K$
and a vector $\boldsymbol{y}$ with elements $\{1, 2, \dots, K\}$,
for instance:

```{r label_vector_check1}
y <- c(3, 1, 1, 2, 2, 1, 2, 3, 1, 1, 1)
K <- 3 # by the way, it's max(y)
```

we can check whether it represents a proper label vector by
computing the number of occurrences of each label:

```{r label_vector_check2}
tabulate(y, K) # how many 1s, ..., Ks are there in y, respectively
all(tabulate(y, K) > 0) # do we have at least 1 occurrence of each label?
```

or, for example, determining the set of all unique
values in $\boldsymbol{y}$:

```{r label_vector_check3}
unique(y)
length(unique(y)) == K # do we have K unique values in y?
```
{ END remark }


Each such label vector  yields a $K$-partition
of the input data set, that is, its grouping into
$K$ disjoint subsets -- every element in $\mathbf{X}$
belongs to one (and only one) cluster.




### K-Partitions (\*)

As it is beneficial for us to be *gently* exposed to as many
mathematical formalisms as possible, let us use some set-theoretic notation
to come up with a rigorous definition of a dataset grouping.
Formally, *clustering* aims to find a *special kind*
of a *$K$-partition* of the input dataset,
which -- without loss of generality -- we can identify
with the set $\{1, 2, \dots, n\}$ of indexes of observations (rows)
in $\mathbf{X}$.




{ BEGIN definition }
We say that $\mathcal{C}=\{C_1,C_2,\dots,C_K\}$ forms
a *$K$-partition* of the set $\{1, 2, \dots, n\}$,
whenever:

* $\bigcup_{k=1}^K C_k=C_1\cup C_2\cup\dots\cup C_K=\{1, 2, \dots, n\}$
(the union of all clusters
is the whole set; every point is assigned to some cluster;
no point is neglected),

* $C_k\cap C_l=\emptyset$ for all $k\neq l$ (clusters are pairwise disjoint;
their intersection is empty; they share no common elements),

* $C_k\neq\emptyset$ for all $k$ (each cluster is nonempty).

<!-- * $C_k\subseteq\{1, 2, \dots, n\}$ for all $k$, -->
{ END definition }


<!--(\*) There is a one-to-one correspondence between partitions
and equivalence relations-->

In the label vector representation,
we assume that $y_i = k$ iff $i\in C_k$, i.e.,
the $i$-th point is assigned the $k$-th label if and only if
it belongs to the $k$-th cluster.
It is easily seen that each label vector yields
a grouping of $\{1, 2, \dots, n\}$ that obviously fulfil
the first two conditions in the above condition
and that the third property is fulfilled by assuming
that $(\forall k\in\{1,2,\dots,K\})$ $(\exists i\in\{1,2,\dots,n\})$
$y_i = k$ (read: *for every $k$ there exists $i$ such that...*),
i.e., each label occurs at least once in $\boldsymbol{y}$.





{ BEGIN remark }
The benefits of using mathematical notation is that we are super-precise,
efficient and universal. Natural tongue is vague and leaves much room
for interpretation. We used so many *words* to explain the above concepts.
Once we build greater skill of *speaking* the language of mathematics
(note that we've started with basic phrases such as *Good Afternoon*,
*I am Sorry* and *Thank You*), it'll be more natural for
us to just write:

"Let $\mathcal{C}=\{ C_1,\dots, C_K \}$ s.t.
$\textstyle\bigcup_{k=1}^K C_k = \{1,\dots, n\}$,
$(\forall k\neq l)$ $C_k\cap C_l = \emptyset$, $C_k\neq \emptyset$"

without all the talking. This is what more advanced book related
to machine learning and statistics do. We'll get to that.
By the way, thanks to this it's relatively easy to read maths
papers (especially those from the 1950s) written in other languages
(such as Russian, German or French).
{ END remark }



In theory, the number of possible $K$-partitions of a set
with $n$ elements is given by
*the Stirling number of the second kind*:
\[
\left\{{n \atop K}\right\}=\sum_{{j=0}}^{{K}} \frac{(-1)^{{j}}
(K-j)^{n}}{j! (K-j)!} = \frac{K^n}{K!}
- \frac{(K-1)^n}{(K-1)!}
+ \frac{(K-2)^n}{2 (K-2)!}
- \frac{(K-3)^n}{6 (K-3)!}
+ \dots,
\]
where $n!=1\cdot 2 \cdot 3\cdot \dots\cdot n$ with $0!=1$ ($n$-factorial).
In particular, already $\left\{{n \atop 2}\right\}=2^{n-1}-1$
and $\left\{{n \atop K}\right\}\simeq K^n/K!$ for large $n$ -- that is a lot.



$n$ | $\left\{{n \atop 2}\right\}$ | $\left\{{n \atop 3}\right\}$ | $\left\{{n \atop 4}\right\}$ | $\left\{{n \atop 5}\right\}$ | $\left\{{n \atop 6}\right\}$ | $\left\{{n \atop 7}\right\}$ | $\left\{{n \atop 8}\right\}$ | $\left\{{n \atop 9}\right\}$ | $\left\{{n \atop 10}\right\}$
---:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|
2   | 1                            |                              |                              |                              |                              |                              |                              |                              |                              |
3   | 3                            |                              | 1                            |                              |                              |                              |                              |                              |                              |
4   | 7                            |                              | 6                            | 1                            |                              |                              |                              |                              |                              |
5   | 15                           | 25                           | 10                           | 1                            |                              |                              |                              |                              |                              |
6   | 31                           | 90                           | 65                           | 15                           | 1                            |                              |                              |                              |                              |
7   | 63                           | 301                          | 350                          | 140                          | 21                           | 1                            |                              |                              |                              |
8   | 127                          | 966                          | 1701                         | 1050                         | 266                          | 28                           | 1                            |                              |                              |
9   | 255                          | 3025                         | 7770                         | 6951                         | 2646                         | 462                          | 36                           | 1                            |                              |
10  | 511                          | 9330                         | 34105                        | 42525                        | 22827                        | 5880                         | 750                          | 45                           | 1                            |

Table: The number of possible partitions of a dataset with $n$ elements to $K$ clusters, denoted $\left\{{n \atop K}\right\}$, grows rapidly as $n$ increases {#tbl:stirling}




### "Interesting" Partitions




We are not just interested in "any" partition, because
there are simply way too plentiful (compare Table {@tbl:stirling}); some of them
will certainly be more meaningful or valuable than others.
However, even one of the most famous ML textbooks provides us with only
a vague hint of what we might be looking for:



{ BEGIN definition }
(or, rather, a "definition").
Clustering concerns "segmenting a collection of objects into subsets
so that those within each cluster are more **closely related**
to one another than objects assigned to different clusters" [@esl].
{ END definition }


It is not uncommon <!-- TODO: cite -->
to equate the general definition of data clustering problems with... the
particular outputs generated by specific clustering algorithms. It some sense,
that sounds fair. From this perspective, we might be interested in
identifying the two main types of clustering algorithms:


* *parametric* (model-based) --
find clusters of specific shapes or following specific multidimensional
probability distributions,
e.g., $K$-means (see Chapter \@ref(chap:kmeans)),
expectation-maximisation for Gaussian mixtures (EM),
average linkage agglomerative clustering;

* *nonparametric* (model-free) -- identify high-density or
well-separable regions, perhaps in the presence of noise points,
e.g., single linkage agglomerative clustering, Genie, (H)DBSCAN, BIRCH.




In this chapter we'll take a look at two classical approaches to clustering:

- *K-means clustering* that looks for a specific number of clusters,
- *(agglomerative) hierarchical clustering* that outputs a whole hierarchy
of nested data partitions.








### Introduction


In K-means, we need to specify the number of clusters, $K$, in advance.
What if we don't have any idea how to choose this parameter (which is often
the case)?

Also, the problem with K-means is that there is no guarantee that a
$K$-partition is any "similar" to the $K'$-one for $K\neq K'$,
see Figure {@fig:kmeans_different_K}.



```{r kmeans1,echo=-1}
set.seed(123)
X <- as.matrix(iris[,c(3,2)])
# never forget to set nstart>>1!
km <- kmeans(X, centers=3, nstart=10)
km$cluster # labels assigned to each of 150 points:
```



```{r kmeans_different_K,fig.cap="3-means (colours) vs. 4-means (symbols) on example data; the \"circle\" cluster cannot decide if it likes the green or the black one more"}
km1 <- kmeans(X, 3, nstart=10)
km2 <- kmeans(X, 4, nstart=10)
plot(X, col=km1$cluster, pch=km2$cluster, asp=1)
```



Hierarchical methods, on the other hand, output a whole hierarchy
of mutually *nested* partitions, which increase the interpretability
of the results.
A $K$-partition for any $K$ can be extracted later at any time.


In this book we will be interested in *agglomerative* hierarchical algorithms:

* at the lowest level of the hierarchy, each point belongs to its own
cluster (there are $n$ singletons);

* at the highest level of the hierarchy,
there is one cluster that embraces all the points;

* moving from the $i$-th to the $(i+1)$-th level,
we select (somehow; see below) a pair of clusters to be merged.



### Example in R


The most basic implementation of a few
agglomerative hierarchical clustering algorithms
is provided by the `hclust()` function, which works on a pairwise
distance matrix.


```{r dist_hclust_complete}
# Euclidean distances between all pairs of points:
D <- dist(X)
# Apply Complete Linkage (the default, details below):
h <- hclust(D) # method="complete"
print(h)
```


{ BEGIN remark }
There are $n(n-1)/2$ unique pairwise distances between $n$ points.
Don't try calling `dist()` on large data matrices.
Already $n=100{,}000$ points consumes 40 GB of available memory
(assuming that each distance is stored as an 8-byte double-precision floating
point number); packages `fastcluster` and `genie`, among other,
aim to solve this problem.
{ END remark }

<!--
#n <- 100000
#8*n*(n-1)/2/1e9
-->


The obtained hierarchy (*tree*) can be *cut* at an arbitrary level
by applying the `cutree()` function.

```{r cutree}
cutree(h, k=3) # extract the 3-partition
```


The cuts of the hierarchy at different levels
are depicted in Figure {@fig:complete_linkage_hier5_intro}.
The obtained 3-partition also matches the true Iris species quite well.
However, now it makes total sense to "zoom" our partitioning in or out
and investigate how are the subgroups decomposed or aggregated when we change
$K$.



```{r complete_linkage_hier5_intro,fig.height=6,fig.cap="Complete linkage -- 4 different cuts"}
par(mfrow=c(2,2))
plot(X, col=cutree(h, k=5), ann=FALSE)
legend("top", legend="k=5", bg="white")
plot(X, col=cutree(h, k=4), ann=FALSE)
legend("top", legend="k=4", bg="white")
plot(X, col=cutree(h, k=3), ann=FALSE)
legend("top", legend="k=3", bg="white")
plot(X, col=cutree(h, k=2), ann=FALSE)
legend("top", legend="k=2", bg="white")
```




### Linkage Functions

Let's formalise the clustering process.
Initially,  $\mathcal{C}^{(0)}=\{\{\mathbf{x}_{1,\cdot}\},\dots,\{\mathbf{x}_{n,\cdot}\}\}$,
i.e., each point is a member of its own cluster.

While an agglomerative  hierarchical clustering algorithm is being computed,
there are $n-k$ clusters at the $k$-th step of the procedure,
$\mathcal{C}^{(k)}=\{C_1^{(k)},\dots,C_{n-k}^{(k)}\}$.

When proceeding from step $k$ to $k+1$,
we determine the two groups $C_u^{(k)}$ and $C_v^{(k)}$, $u<v$,
to be *merged* together so that the clustering at the higher level
is of the form:
\[
\mathcal{C}^{(k+1)} = \left\{
C_1^{(k)},\dots,C_{u-1}^{(k)},
C_u^{(k)}{\cup C_v^{(k)}},
C_{u+1}^{(k)},\dots,C_{v-1}^{(k)},
C_{v+1}^{(k)},\dots,C_{n-k}^{(k)}
\right\}.
\]


Thus, $(\mathcal{C}^{(0)}, \mathcal{C}^{(1)}, \dots, \mathcal{C}^{(n-1)})$
form a sequence of *nested* partitions of
the input dataset
with the last level being just one big cluster,
$\mathcal{C}^{(n-1)}=\left\{ \{\mathbf{x}_{1,\cdot},\mathbf{x}_{2,\cdot},\dots,\mathbf{x}_{n,\cdot}\} \right\}$.


. . .

There is one component missing -- how to determine the pair
of clusters $C_u^{(k)}$ and $C_v^{(k)}$ to be merged with each other
at the $k$-th iteration?
Of course this will be expressed as some optimisation problem (although this time,
a simple one)!  The decision will be based on:
\[
\mathrm{arg}\min_{u < v} d^*(C_u^{(k)}, C_v^{(k)}),
\]
where $d^*(C_u^{(k)}, C_v^{(k)})$ is the *distance* between two clusters
$C_u^{(k)}$ and $C_v^{(k)}$.

Note that we usually only consider the distances between *individual points*,
not sets of points. Hence, $d^*$ must be a suitable extension
of a pointwise distance $d$ (usually the Euclidean metric)
to whole sets.

We will assume that $d^*(\{\mathbf{x}_{i,\cdot}\},\{\mathbf{x}_{j,\cdot}\})=
d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot})$, i.e., the distance between
singleton clusters is the same as the distance between the points themselves.
As far as more populous point groups are concerned, there are many popular
choices of $d^*$ (which in the context of hierarchical clustering we call
*linkage functions*):

- single linkage:

    \[
    d_\text{S}^*(C_u^{(k)}, C_v^{(k)}) =
    \min_{\mathbf{x}_{i,\cdot}\in C_u^{(k)}, \mathbf{x}_{j,\cdot}\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}),
    \]

- complete linkage:

    \[
    d_\text{C}^*(C_u^{(k)}, C_v^{(k)}) =
    \max_{\mathbf{x}_{i,\cdot}\in C_u^{(k)}, \mathbf{x}_{j,\cdot}\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}),
    \]

- average linkage:

    \[
    d_\text{A}^*(C_u^{(k)}, C_v^{(k)}) =
    \frac{1}{|C_u^{(k)}| |C_v^{(k)}|} \sum_{\mathbf{x}_{i,\cdot}\in C_u^{(k)}}\sum_{\mathbf{x}_{j,\cdot}\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}).
    \]


An illustration of the way different linkages are computed
is given in Figure {@fig:linkages}.

```{r linkages,echo=FALSE,fig.cap="In single linkage, we find the closest pair of points; in complete linkage, we seek the pair furthest away from each other; in average linkage, we determine the arithmetic mean of all pairwise distances"}
set.seed(123)
par(mar=rep(0.5,4))
XXX <- jitter(t(as.matrix(iris[,2:3])))
y <- as.numeric(iris[,5])
XXX[,y==3] <- c(2, 1)*XXX[,y==3]+c(-0.5,-0.5)
y[y==3] <- 4

d <- as.matrix(dist(t(XXX)))
d12 <- d[1:50, 51:100]
d23 <- d[51:100, 101:150]
d13 <- d[1:50, 101:150]

kol1 <- rgb(0,0.2,0,0.2)
kol2 <- rgb(0,0.5,0.0,0.5)
kol3 <- rgb(0,0.9,0.0,0.9)

par(mfrow=c(1,3))
# single
plot(XXX[1,], XXX[2,], col=y, pch='.', asp=1, axes=FALSE, xlab=NA, ylab=NA)
box()
# stopifnot(min(d23) < min(d13) && min(d23) < min(d12))
IDXXX_single <- which(min(d23) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol3, lwd=3)
IDXXX_single <- which(min(d13) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol1, lwd=3)
IDXXX_single <- which(min(d12) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol2, lwd=3)
legend("bottomright", legend="single linkage", bg="white")
points(XXX[1,], XXX[2,], col=y, pch=y)

# complete
plot(XXX[1,], XXX[2,], col=y, pch='.', asp=1, axes=FALSE, xlab=NA, ylab=NA)
box()
IDXXX_single <- which(max(d23) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol2, lwd=3)
IDXXX_single <- which(max(d13) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol1, lwd=3)
IDXXX_single <- which(max(d12) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol3, lwd=3)
legend("bottomright", legend="complete linkage", bg="white")
points(XXX[1,], XXX[2,], col=y, pch=y)

# average
plot(XXX[1,], XXX[2,], col=y, pch='.', asp=1, axes=FALSE, xlab=NA, ylab=NA)
box()
#c(mean(d12), mean(d13), mean(d23))
for (i in sample(1:50)) {
    segments(XXX[1,0+i], XXX[2,0+i], XXX[1,51:100], XXX[2, 51:100], col=kol1)
    segments(XXX[1,0+i], XXX[2,0+i], XXX[1,101:150], XXX[2, 101:150], col=kol2)
    segments(XXX[1,50+i], XXX[2, 50+i], XXX[1,101:150], XXX[2, 101:150], col=kol3)
}
legend("bottomright", legend="average linkage", bg="white")
points(XXX[1,], XXX[2,], col=y, pch=y)
```

. . .

Assuming $d_\text{S}^*$, $d_\text{C}^*$ or $d_\text{A}^*$
in the aforementioned procedure leads to single, complete or average
linkage-based agglomerative hierarchical clustering algorithms,
respectively
(referred to as single linkage etc. for brevity).



```{r linkages_hier527658932}
hs <- hclust(D, method="single")
hc <- hclust(D, method="complete")
ha <- hclust(D, method="average")
```

Figure {@fig:linkages_hier52} compares the 5-, 4- and 3-partitions
obtained by applying the 3 above linkages. Note that it's in very nature of
the single linkage algorithm that it's highly sensitive to outliers.

```{r linkages_hier52,fig.height=6,fig.cap="3 cuts of 3 different hierarchies",echo=FALSE}
#par(c(1,1,0.5,0.5))
par(mfrow=c(3,3))
for (k in 5:3) {
    ccc <- cutree(hs, k=k)
    plot(X, col=ccc, pch=ccc, ann=FALSE)
    legend("top", legend=sprintf("k=%d, single", k), bg="white")
    ccc <- cutree(hc, k=k)
    plot(X, col=ccc, pch=ccc, ann=FALSE)
    legend("top", legend=sprintf("k=%d, complete", k), bg="white")
    ccc <- cutree(ha, k=k)
    plot(X, col=ccc, pch=ccc, ann=FALSE)
    legend("top", legend=sprintf("k=%d, average", k), bg="white")
}
```



### Cluster Dendrograms


A *dendrogram*  (which we can plot by calling `plot(h)`, where `h` is
the result returned by `hclust()`) depicts the distances
(as defined by the linkage function) between the clusters merged at every stage
of the agglomerative procedure.
This can provide us with some insight into the underlying data structure
as well as with hits about at which level the tree could be cut.

Figure {@fig:dendrograms} depicts the three dendrograms
that correspond to the clusterings  obtained by applying different linkages.
Each tree has 150 leaves (at the bottom) that represent the 150 points
in our example dataset. Each "edge" (joint) represents a group of points
being merged. For instance, the very top joint in the middle subfigure
is located at height of $\simeq 6$, which is exactly the
maximal pairwise distance (complete linkage) between the points
in the last two last clusters.


```{r dendrograms,fig.height=6,fig.cap="Cluster dendrograms for the single, complete and average linkages",echo=FALSE}
par(mfrow=c(3,1))
h <- hclust(D, method="single")
plot(h, labels=FALSE, ann=FALSE); box()
legend("topright", legend="single", bg="white")
h <- hclust(D, method="complete")
plot(h, labels=FALSE, ann=FALSE); box()
legend("topright", legend="complete", bg="white")
h <- hclust(D, method="average")
plot(h, labels=FALSE, ann=FALSE); box()
legend("topright", legend="average", bg="white")
```



## Exercises


TODO: reuse from the K-means chapter



## Outro

### Remarks

....

### Further Reading

...


Note that we have studied "crisp" (disjoint) partitions only.

[@jainsurvey]
[@wierzchon_klopotek]
