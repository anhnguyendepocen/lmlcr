# Classification with Nearest Neighbours {#chap:knn}

<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->


{ LATEX \color{gray} }

**TODO** In this chapter, we will:

* solve a prediction task with k-nearest neighbour classifier

* discuss performance metrics for binary classification: accuracy,
precision, recall, and F-measure

* introduce best practices of optimal classifier  selection -- in particular, explain the difference between training, validation,
and test sets


<!-- (alternative: cross-validation, see exercise TODO) -->

<!-- ROC curve? -->

{ LATEX \normalcolor }





<!--

Add K-NN Regression



add CHAMELEON and (H)DBSCAN(*) ???
-- beyond supervised learning

Karypis, G., EH. Han, V. Kumar (1999): CHAMELEON: A Hierarchical Clustering Algorithm Using Dynamic Modeling, IEEE Computer, 32(8): 68â€“75.

-->



<!-- TODO:


2D illustration of NEAREST NEIGHBOURS!!!

exercise -- density, not NN-based classification or
regression - consider the epsilon-neighbourhood????

-->


## Introduction

In the previous chapter, we were only given an input matrix
$\mathbf{X}\in\mathbb{R}^{n\times p}$ representing
$n$ objects described by means of $p$ numerical features.

In this chapter we are going to be interested in  *supervised learning* tasks;
we assume that with each $\mathbf{x}_{i,\cdot}$ ($i=1,\dots,n$)
we associate the desired output $y_i$:

\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} \\
\end{array}
\right],
\qquad
\mathbf{y} = \left[
\begin{array}{c}
{y}_{1} \\
{y}_{2} \\
\vdots\\
{y}_{n} \\
\end{array}
\right].
\]


More precisely, we'd now like to focus on *classification* -- we assume that
each $y_i$ is of qualitative type (e.g., a label, a category, a colour).


The classes are usually *encoded* with consecutive integers,
say, $1, 2, \dots, L$, where $L$ is the total number of unique cases.
Mathematically, we'll write that $y_i\in\{1,\dots,L\}$.


{ BEGIN remark }
`factor` datatype in R (see Section \@ref(sec:factor) for more details)
gives a very convenient means to encode categorical data
(such as $\mathbf{y}$):

```{r factor1}
y <- c("black", "green", "black", "red", "green", "green", "blue", "red")
(y <- factor(y, levels=c("black", "red", "green", "blue")))
```

<!-- table(y) # every label and the number of occurrences -->


Internally, objects of type `factor` are represented as integer vectors
with elements in $\{1,\dots,L\}$, where $L$ is the number of possible levels.
Labels, used to "decipher" the numeric codes, are stored separately
and can always be replaced with other ones.

```{r factor2}
as.numeric(y) # 1st label, 3rd label, 1st label, and so forth
length(levels(y)) # L = number of levels
levels(y) # 1=black, 2=red, 3=green, 4=red
levels(y) <- c("k", "r", "g", "b") # re-encode
y
```
{ END remark }


In this chapter we'll assume $L=2$, i.e., that each $y_i$ can only take one of
two possible values. Such a case is very common in practice;
it has been even given a special name: *binary classification*.
The two classes are traditionally denoted as $0$ and $1$,
see Table \@ref(tab:binary-classification) for some examples.

Table: (\#tab:binary-classification) Some examples of output labels in binary classification tasks

0       | 1
--------|------------
no      | yes
false   | true
failure | success
reject  | accept
healthy | ill



{ BEGIN remark }
$0$s and $1$s are mathematically convenient, because
instead of stating "if $y_i=1$, then let $z=a$ and otherwise take $z=b$",
we can simply write "$z=y_i\, a + (1-y_i)\, b$"
(see, e.g., the definition of cross-entropy in \@ref(chap:logistic)).
Ah, those neat math tricks!
{ END remark }


Figure \@ref(fig:classify-intro) gives a scatter plot of an example synthetic
two-dimensional dataset (i.e., $p=2$)
with the reference binary $y$s depicted using different plotting symbols.
We see that the two classes  overlap slightly -- they cannot be
separated by means of any simple boundary (e.g., a line).
<!-- The "true" decision boundary is at $X_1=0$ but the classes -->
<!-- slightly overlap (the dataset is a bit noisy). -->



```{r classify-intro,echo=FALSE,fig.cap="A synthetic 2D dataset where each point is assigned one of two distinct labels",cache=TRUE}
set.seed(123)
n0 <- 50 # n0 points in class 0
n1 <- 50 # n1 points in class 1
Xs <- rbind(
    cbind(rnorm(n0, -2, 2), rnorm(n0, 0, 1.5)), # N( (-2, 0), (2, 1.5) )
    cbind(rnorm(n1, +2, 2), rnorm(n1, 0, 1.5))  # N( (+2, 0), (2, 1.5) )
)
Ys <- factor(rep(c("0", "1"), c(n0, n1)))

plot(Xs[,1], Xs[,2], col=Ys, pch=c(1,2)[Ys], xlab="X1", ylab="X2", asp=1,
        xlim=c(-8,8), ylim=c(-4,4))
legend("topleft", col=c(1,2), pch=c(1,2),
    legend=c("Class 0", "Class 1"), bg="white")
# abline(v=0, lty=3, col="blue")
```





## K-Nearest Neighbours Classifier

Our main aim will be to "build" an algorithm
that takes a previously unobserved input sequence
$\boldsymbol{x}'$ and which  -- based what we
know already, i.e., on $\mathbf{X}$ and $\mathbf{y}$
generates a corresponding prediction, $\hat{y}'$
(hopefully being equal to the true $y'$, whatever it is).


For instance, we may have a set of medical records, where we
store patient data regarding their measurable health-relate parameters
like blood pressure, severity of different symptoms etc.
Moreover, each patient is labelled -- we know if they have been diagnosed
a disease or not. But wait, there comes a new patient! We measure their
blood pressure, as some questions, input the data into our algorithm
and -- based on the result -- advise them to stay in bed.


. . .

In this chapter we will consider a very simple classifier,
based on an idea which dates back to at least 1950s (see [@knn1], [@knn2]).

Rule.

: "If you don't know what to do in a situation, just act like the people around you"



For some integer $K\ge 1$, the *K-Nearest Neighbour  (K-NN) Classifier*
proceeds as follows. To classify a new point $\boldsymbol{x}'$:

1. Find the $K$ nearest neighbours of a given point $\boldsymbol{x}'$
amongst the points in $\mathbf{X}$, i.e.,
points $\mathbf{x}_{i_1,\cdot}, \dots, \mathbf{x}_{i_K,\cdot}$
that are the closest to $\boldsymbol{x}'$ w.r.t. the Euclidean distance:
    a. compute the Euclidean distances between each $\mathbf{x}_{i,\cdot}$ from the input set and $\boldsymbol{x}'$:
    \[ d_i = d(\mathbf{x}_{i,\cdot}, \boldsymbol{x}'),\]
    b. order $d_i$s in increasing order,
    and fetch the indices $i_1,\dots,i_K$
    which yield:
    \[ d_{i_1} \le d_{i_2} \le \dots \le d_{i_K}.\]
<!--     c. pick first $K$ indices (these are the *nearest* neighbours). -->

2. Fetch the reference labels $y_{i_1}, \dots, y_{i_K}$
corresponding to the $K$ nearest neighbours.

3. Return their *mode* as a result, i.e., the most frequently occurring label (also called *majority vote*).

<!-- > If a mode is not unique, return a randomly chosen mode (ties are broken at random). -->








. . .

Let's illustrate how a $K$-NN classifier works on the above 2D synthetic dataset.
First we consider $K=1$, see Figure \@ref(fig:fig-plot-knn1).
Dark and light regions depict how new points would be classified (class 0 and
1, respectively).
For instance, the point $(0, 2)$ lays in the "dark zone", therefore
we'd assign it label 0.


```{r plot-knn,dependson="classify-intro",echo=FALSE,cache=TRUE}
plot_knn <- function(K) {
    xx1 <- seq(-8.2, 8.2, length.out=500)
    xx2 <- seq(-4.2, 4.2, length.out=500)
    xx <- expand.grid(xx1, xx2)
    yy <- FNN::knn(Xs, xx, Ys, k=K)
    image(xx1, xx2, matrix(as.numeric(yy)-1,
        nrow=length(xx1), ncol=length(xx2)),
        col=c("#00000022", "#ffe0e044"),
        xlab="X1", ylab="X2", asp=1,
        xlim=c(-8,8), ylim=c(-4,4)
        )
    points(Xs[,1], Xs[,2], col=Ys, pch=c(1,2)[Ys])
    legend("topleft", col=c(1,2), pch=c(1,2),
        legend=c("Class 0", "Class 1"), bg="white")
#     abline(v=0, lty=3, col="blue")
}
```



```{r fig-plot-knn1,echo=FALSE,cache=TRUE,dependson='plot-knn',fig.cap="1-NN class bounds for the 2D synthetic dataset"}
plot_knn(1)
```



We see that 1-NN is "greedy" or "lazy" in the sense that we just
locate the nearest point.
Increasing $K$ somehow smoothens the decision boundary (this makes it
less "local" and more "global").
Figure \@ref(fig:fig-plot-knn3) depicts the $K=3$ case.

```{r fig-plot-knn3,echo=FALSE,cache=TRUE,dependson='plot-knn',fig.cap="3-NN class bounds the our 2D synthetic dataset"}
plot_knn(3)
```



```{r fig-plot-knn15,echo=FALSE,cache=TRUE,dependson='plot-knn',fig.cap="15-NN class bounds the our 2D synthetic dataset"}
plot_knn(15)
```

The 15-NN classifier, see  Figure \@ref(fig:fig-plot-knn15),
does a quite good job with identifying
a  boundary between the two classes -- the whole $\mathbb{R}^2$ space
seems to be (more or less) split into two disconnected subregions;
there are no "lakes" or "islands" or significant sizes.






## Example in R


As a real-world illustration, let's consider a subset of the Wine Quality dataset
(see Appendix \@ref(sec:wine-quality) for more details)
that features a sample of white wines and includes only 3 of their
physicochemical characteristics:


```{r input1,cache=TRUE}
wine_train <- read.csv("datasets/wine_train.csv")
head(wine_train)
dim(wine_train) # number of rows, number of columns
```

The last column determines whether experts claim that a given wine
is of low quality (class `1`, we don't want that if we make a party)
or not (class `0`). We will use it as a
response variable in a wine classification task.


{ BEGIN remark }
Note that above $\mathbf{X}$ and $\mathbf{y}$ are stored together
as a single object, $[\mathbf{X}\ \mathbf{y}]$:


\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} & y_1\\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} & y_2\\
\vdots & \vdots & \ddots & \vdots    & \vdots\\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} & y_n\\
\end{array}
\right],
\]

In R, matrices (see Appendix \@ref(appendix:rmatrix))
are used to store data of the same type.
As $\mathbf{X}$ consists of real numbers,
we might sometimes run into a "type mismatch" error
if we want to extend it with $\mathbf{y}$ that is not numeric.
This is why such "combined" objects are usually
stored as R `data.frame`s (see Appendix \@ref(appendix:rdf));
this class allows columns of mixed types.
{ END remark }



Let's extract the input matrix
$\mathbf{X}\in\mathbb{R}^{`r nrow(wine_train)`\times `r ncol(wine_train)-1`}$
by taking the first 3 columns from the `wine_train` data frame
and the reference outputs $\mathbf{y}\in\{0,1\}^{`r nrow(wine_train)`}$
given by the last column.

```{r input2,dependson='input1',cache=TRUE}
X_train <- as.matrix(wine_train[,-4]) # all except the last column
y_train <- factor(wine_train[,4])
```



Now $[\mathbf{X}\ \mathbf{y}]$ is a basis for an interesting (and challenging)
binary classification task. It constitutes our *training sample* -- one
from which we actually *learn* what levels of chemical features make a
gathering with some vino a disappointing experience.
Yet, we don't get a certificate in wine tasting only
to hang some fancy diploma in a golden frame on the wall.
We want to *apply* the knowledge we've gained for the bottles whose corks
(or caps) are yet to be popped off.


Here are some bottles for which we can show off our skills:


```{r input3,dependson='input2',cache=TRUE}
X_test <- as.matrix(read.csv("datasets/wine_test_X.csv"))
head(X_test)
dim(X_test)
```

We will denote is as
$\mathbf{X}'\in\mathbb{R}^{`r nrow(X_test)`\times`r ncol(X_test)`}$
and call it a *test sample*.
Let's invoke the `knn()` function from package `FNN`
to classify the points $\mathbf{X}'$ with the 5-nearest neighbour rule
so as to obtain the corresponding predicted $\hat{\mathbf{y}}'$.

```{r libFNN,message=FALSE}
library("FNN") # load the package
```

```{r y-knn5}
y_pred <- knn(X_train, X_test, y_train, k=5)
head(y_pred, 32) # predicted outputs
```


Great, we can now enjoy the bottles no.
`r paste(which(0==head(y_pred, 15)), collapse=", ")`, ...
because the classifier claims they're not bad.



## Classifier Assessment

<!--

More metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics

-->


But wait, are the consumers *really* going to enjoy the aforementioned wines?
The 5-NN classifier might as well be deceiving us or be generating random
predictions. It wouldn't be responsible to put it in production without making
sure it's doing its job reasonably well.

Luckily, we have gathered information on the *true* labels for the
observations in the test set, $\mathbf{y}'$:

```{r input4,dependson='input3',cache=TRUE}
y_test <- factor(read.csv("datasets/wine_test_y.csv")[,1])
head(y_test, 32) # true outputs
```

Comparing the displayed fragments of `y_test` and `y_pred` vectors
(32 items), we see that most of the predictions are valid, but there are
`r sum(head(y_pred, 32)!=head(y_test, 32))` mismatches.
As there are `r nrow(X_test)` observations
in the test set, it'll be better if we come up with some
synthetic metrics that could summarise the *overall* performance of
our classifier.

*Accuracy* is perhaps the most straightforward descriptor, being
the ratio of the correctly classified instances to all the instances.


```{r accuracy1}
mean(y_test == y_pred) # accuracy
```

Could be worse. In `r round(mean(y_test == y_pred)*100)`% cases
we make a correct prediction -- we classify a wine as "not bad" when
it's actually decent and label it as "bad" when it's indeed horrible.


{ BEGIN remark }
Recall from Appendix \@ref(appendix:rvector) that the `==` operator
`in R works
in an elementwise manner. It outputs a logical vector whose $i$-th
element is `TRUE` whenever `y_test[i]` is equal to `y_pred[i]`.
Calling `mean()` on a logical vector converts it to a numeric one:
each `TRUE` is replaced with `1` and `FALSE` becomes `0`.
Arithmetic mean is nothing else that the sum of elements divided
by their count. The sum of `1`s and `0`s is actually the number of `1s`,
i.e., for how many $i$s it holds that `y_test[i]` is equal to `y_pred[i]`.
{ END remark }




Recall that $y_i'$ is the true label associated with the $i$-th observation
in the test set. Let $\hat{y}_i'$ denote the classifier's output for
a given $\mathbf{x}_{i,\cdot}'$.
In our case, the  outputs are binary, i.e., $\hat{y}_i', y_i'\in\{0,1\}$.
Table \@ref(tab:true-vs-predicted) lists the 4 possible scenarios --
all the distinct pairs of $(\hat{y}_i', y_i')$.


Table: (\#tab:true-vs-predicted) True vs. predicted labels in a binary classification task; ideally, the number of false positives and false negatives should be kept to a minimum

.              | $y_i'=0$                       | $y_i'=1$
---------------|--------------------------------|-------------
$\hat{y}_i'=0$ | **True Negative**              | False Negative (Type II error)
$\hat{y}_i'=1$ | False Positive (Type I error)  | **True Positive**



We'd be happy with as many true positives and true negatives as possible.
Note that the terms **positive** and **negative** refer to
the classifier's output, i.e., they indicate whether
$\hat{y}_i$ is equal to $1$ and $0$, respectively.


Let's take a deeper look at why the classifier's accuracy is "only"
`r round(mean(y_test == y_pred)*100)`%.
To summarise the correctness of predictions for the whole sample,
we can compute the *confusion matrix*:


```{r confusion1}
(C <- table(y_pred, y_test))
```


Actually, the classifier labels `r C[1,2]` bad wines as not-bad
and `r C[2,1]` not-bad wines as bad.

In many applications we deal with *unbalanced problems*, where
the case $y_i=1$ is relatively rare(r),
yet predicting it  correctly is much more important than being
accurate with respect to class $0$ -- think of medical applications,
e.g., HIV testing or tumour diagnosis.


```{r wine-y-tables}
table(y_train)
table(y_test)
```

In our case, the "not-bad" class is much more populous than the "bad" one.
Moreover, most will agree that it's better to be surprised with a vino
labelled as bad, than disappointed with a highly recommended product.
Therefore, *accuracy* as a metric may fail to quantify what we are aiming for.


{ BEGIN remark }
If only 1% of the cases have true $y_i'=1$,
then a dummy classifier that always
outputs $\hat{y}_i'=0$ has 99% accuracy.
{ END remark }





{ BEGIN remark }
Accuracy can be computed from  a confusion matrix based on the formula:

\[
\text{Accuracy} = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}.
\]

<!--
= \frac{1}{n} \sum_{i=1}^n \mathbb{I}\left(
y_i = \hat{y}_i
\right)
where $\mathbb{I}$ is the indicator function,
$\mathbb{I}(l)=1$ if logical condition $l$ is true and $0$ otherwise.
-->

```{r accuracy3}
(C[1,1]+C[2,2])/sum(C) # equivalent to the above
```
{ END remark }


Metrics such as precision and recall (and their aggregated version, F-measure)
aim to address the above problem problem:

* *Precision* answers the question: If the classifier outputs $1$,
what is the probability that this is indeed true?

    \[
    \text{Precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}.
    \]



    ```{r precision}
    C[2,2]/(C[2,2]+C[2,1]) # Precision
    ```

* *Recall* (a.k.a. sensitivity, hit rate or true positive rate) --
    If the true class is $1$, what is the probability that the classifier
    will detect it?

    \[
    \text{Recall} = \frac{\text{TP}}{\text{TP}+\text{FN}}.
    \]


    ```{r recall}
    C[2,2]/(C[2,2]+C[1,2]) # Recall
    ```






{ BEGIN exercise }
Precision or recall? It depends on an application.
In each of the following settings, which measure is more important?

* medical diagnosis,
* medical screening,
* suggestions of potential matches in a dating app,
* plagiarism detection,
* wine recommendation.
{ END exercise }


As too many metrics may be daunting for some, as a compromise,
we can use the *F-measure* (a.k.a. $F_1$-measure),
which is the harmonic mean of precision
and recall:

\[
\text{F} = \frac{1}{
    \frac{
        \frac{1}{\text{Precision}}+\frac{1}{\text{Recall}}
    }{2}
}
=
\left(
\frac{1}{2}
\left(
\text{Precision}^{-1}+\text{Recall}^{-1}
\right)
\right)^{-1}
=
\frac{\text{TP}}{\text{TP} + \frac{\text{FP} + \text{FN}}{2}}.
\]



```{r fmeasure}
C[2,2]/(C[2,2]+0.5*C[1,2]+0.5*C[2,1]) # F
```




{ BEGIN exercise }
Show that the above equality indeed holds.
{ END exercise }





{ BEGIN remark }
The following function will come in handy in the future:

```{r get_metrics}
get_metrics <- function(y_pred, y_test)
{
    # first, let's make sure that both inputs are encoded
    # as factors with the same levels:
    all_levels <- unique(c(as.character(y_pred), as.character(y_test)))
    y_pred <- factor(y_pred, levels=all_levels)
    y_test <- factor(y_test, levels=all_levels)
    # compute the confusion matrix:
    C <- table(y_pred, y_test)
    stopifnot(dim(C) == c(2, 2))
    # fetch the metrics:
    c(Acc=(C[1,1]+C[2,2])/sum(C), # accuracy
      Prec=C[2,2]/(C[2,2]+C[2,1]), # precision
      Rec=C[2,2]/(C[2,2]+C[1,2]), # recall
      F=C[2,2]/(C[2,2]+0.5*C[1,2]+0.5*C[2,1]), # F-measure
      # Confusion matrix items:
      TN=C[1,1], FN=C[1,2],
      FP=C[2,1], TP=C[2,2]
    ) # return a named vector
}
```


For example:

```{r get_metrics2}
get_metrics(y_pred, y_test)
```
{ END remark }





## Classifier Selection


Note that the nearest neighbour scheme implies in fact a whole
family of classifiers. For each $K$, the corresponding K-NN method
can be thought of as a different algorithm.
Therefore, it is wise to pose the question:
how to choose the best $K$ for K-NN classification?

Here, by the *best* we mean one that has the highest *predictive power*,
which we quantify by means of some chosen metric (such as accuracy, recall, precision, F-measure, etc.).

Let's study how the performance metrics change when we vary
the number of nearest neighbours, $K$.
Then, we'll choose the parameter that corresponds to, say, the greatest
F-measure. However, we should not compute this metric on the test set!
There is always a risk that we can *overfit* to current data --
construct a classifier
that performs extremely well on the samples we have but
does not *generalise* well to the ones to come. The bottles we inspected
is just a drop in the wine ocean that our machine learning solution
will be filtering.

We are lucky though as we have one more wine sample available!


```{r input5,cache=TRUE}
X_validate <- as.matrix(read.csv("datasets/wine_validate_X.csv"))
y_validate <- factor(read.csv("datasets/wine_validate_y.csv")[,1])
dim(X_validate)
```


We will call it a *validation* (or development) set and use it
for determining the optimal $K$. Then the *test* set will be recalled
so as to perform the final evaluation (it's going to mimic the wines
to come).




. . .

The following function computes the performance metrics
for the $K$-NN classifier as a function of $K$:

```{r whichK1,cache=TRUE}
knn_metrics <- function(K, X_train, X_validate, y_train, y_validate)
{
    y_pred <- knn(X_train, X_validate, y_train, k=K) # classify
    get_metrics(y_pred, y_validate)
}
```

For example:

```{r whichK2,cache=TRUE}
knn_metrics(5, X_train, X_validate, y_train, y_validate)
```

Let's evaluate the performance metrics as a function of different odd
(compare Section \@(sec:mode)) $K$s:

```{r whichK3,cache=TRUE}
Ks <- seq(1, 19, by=2) # 1, 3, 5, ...
Ps <- sapply(Ks, # on each element in this vector
        knn_metrics,     # apply this function
        X_train, X_validate, y_train, y_validate # aux args
    )
# convert to "vertical" form - each K in separate row:
Ps <- as.data.frame(t(Ps))
```



{ BEGIN remark }
Note that `sapply(X, f, arg1, arg2, ...)`
outputs a matrix `Z` such that it's $i$-th column is
`f(X[i], arg1, arg2, ...)`. We transpose it by calling `t()` to
get a "vertical" (long) representation.
{ END remark }


<!--

We transpose this result, `t()`, in order to get each metric
corresponding to different columns in the result.
As usual, if you keep wondering, e.g.,  why `t()`, play with
the code yourself -- it's fun fun fun.

-->





Example results:

```{r whichK4,cache=TRUE}
round(cbind(K=Ks, Ps), 2)
```

Figure \@ref(fig:whichK5) is worth a thousand tables though
(see `?matplot` in R).
It seems that precision tends to slightly increase, whereas recall -- decrease
as $K$ increases (on this dataset, it's not a general rule).






```{r whichK5,echo=TRUE,cache=TRUE,fig.cap="Performance of $K$-NN classifiers on the validation set as a function of $K$"}
matplot(Ks, Ps[,1:4], xlab="K", ylab="Metric",
    col=1:4, lty=1:4, pch=1:4, type="b", ylim=c(0,1))
legend("top", legend=names(Ps[,1:4]),
    col=1:4, lty=1:4, pch=1:4, ncol=4, bg="white")
```

<!--

(\*) **ROC** (Receiver Operating Characteristic) curve:

```{r }
TPR <- Ps$TP/(Ps$TP+Ps$FN) # True Positive Rate (recall)
FPR <- Ps$FP/(Ps$FP+Ps$TN) # False Positive Rate
plot(FPR, TPR, asp=1, xlim=c(0,1), ylim=c(0,1))
abline(a=0, b=1, lty=3)
```

-->


It's very interesting that precision and recall (and hence F-measure)
are much lower on the validation set than on the test set.
This might be due to the small dataset sizes. As both of them have been
generated independently and at random, and they constitute a representative
sample of the population of all wines, the truth perhaps lies somewhere
"in-between" the reported metrics.

Anyway, we get the best F-measure for $K=`r Ks[which.max(Ps[,"F"])]`$.
Let's see how well it is likely to perform "in production",
i.e., on the test set:


```{r whichK6,cache=TRUE}
knn_metrics(11, X_train, X_test, y_train, y_test)
```


This classifier is actually worse than the 5-NN one on the test set,
but there is nothing we can do about it.
It is an example of best practice to stick to the parameter we have
identified as the optimal one on the validation sample --
we have promised to treat the test set as an independent
judge in our case (unless new data and more evidence come and we will
be in a position to reevaluate/reconstruct our algorithm).
No worries, we will encounter dilemma and disappointments of this
kind in our everyday data science activities (see the next Chapter for more).


<!--
```{r whichK7,cache=TRUE}
#knn_metrics(5, X_train, X_test, y_train, y_test)
```
-->



## Implementing a K-NN Classifier (\*)



### Main Routine


To show that machine learning methods are not magical creatures who
we should be terrified of, but rather the fruit of work of programmers
just like us (have you ever thought of volunteering in an open source
programming project for the good of the whole community?),
let's implement a K-NN classifier ourselves, from scratch.

We'll use a top-bottom approach,
starting with a general description of the admissible inputs
and the expected output. Then we'll arrange the workflow for processing
of data into conveniently manageable chunks.

The function's "declaration" will look like:

```{r our_knn1,eval=FALSE}
our_knn <- function(X_train, X_test, y_train, k=1) {
    # k=1 denotes a parameter with a default value
    # ... (see below) ...
}
```




It's advisable to specify the type and form of the arguments
we're expecting on input. The `stopifnot()` function verifies if a given
logical condition is true. If not, an error is raised.

```{r our_knn_paramchecks,eval=FALSE}
# this is the body of our_knn() - part 1
stopifnot(is.numeric(X_train), is.matrix(X_train))
stopifnot(is.numeric(X_test), is.matrix(X_test))
stopifnot(is.factor(y_train))
stopifnot(ncol(X_train) == ncol(X_test))
stopifnot(nrow(X_train) == length(y_train))
stopifnot(k >= 1, k <= length(y_train))
n_train <- nrow(X_train)
n_test  <- nrow(X_test)
p <- ncol(X_train)
L <- length(levels(y_train))
```

Therefore, we may assume from now on that
$\mathtt{X\_train}\in\mathbb{R}^{\mathtt{n\_train}\times \mathtt{p}}$,
$\mathtt{X\_test}\in\mathbb{R}^{\mathtt{n\_test}\times \mathtt{p}}$ and
$\mathtt{y\_train}\in\{1,\dots,L\}^{\mathtt{n\_train}}$.
Recall that R `factor` objects are internally encoded as integer vectors.





Next, we'll call the (to-be-done) function `our_get_knnx()`,
which seeks nearest neighbours of all the points:

```{r our_knn_flow1,eval=FALSE}
# our_get_knnx returns a matrix nn_indices of size n_test*k,
# where nn_indices[i,j] denotes the index of
# X_test[i,]'s j-th nearest neighbour in X_train.
# (It is the point X_train[nn_indices[i,j],]).
nn_indices <- our_get_knnx(X_train, X_test, k)
```


Then, for each point in `X_test`,
we fetch the labels corresponding to its nearest neighbours
and compute their mode (`our_mode()` function, wait for it):

```{r our_knn_flow2,eval=FALSE}
y_pred <- numeric(n_test) # vector of length n_test
# For now we will operate on the integer labels in {1,...,M}
y_train_int <- as.numeric(y_train)
for (i in 1:n_test) {
    # Get the labels of the NNs of the i-th point:
    nn_labels_i <- y_train_int[nn_indices[i,]]
    # Compute the mode (majority vote):
    y_pred[i] <- our_mode(nn_labels_i) # in {1,...,M}
}
```

Finally, we should convert the resulting integer vector
to an object of type `factor`:

```{r our_knn_flow3,eval=FALSE}
# Convert y_pred to factor:
return(factor(y_pred, labels=levels(y_train)))
```



```{r our_knn_final,echo=FALSE}
our_knn <- function(X_train, X_test, y_train, k=1) {

    <<our_knn_paramchecks>>
    <<our_knn_flow1>>
    <<our_knn_flow2>>
    <<our_knn_flow3>>
}
```



<!--
**Test-driven development** -- before writing

```{r}
test_our_knn <- function() {
    # ...
}
```


```{r}
test_our_mode <- function() {
    stopifnot(our_mode(c(1, 1, 1, 1)) == 1)
    stopifnot(our_mode(c(2, 2, 2, 2)) == 2)
    stopifnot(our_mode(c(3, 1, 3, 3)) == 3)
    stopifnot(our_mode(c(1, 1, 3, 3, 2)) %in% c(1, 3))
}
```


-->

### Mode {#sec:mode}




To implement the mode, we can use the `tabulate()` function.


{ BEGIN exercise }
Read the function's documentation, see `?tabulate`.
{ END exercise }

For example:

```{r tabulate}
tabulate(c(1, 2, 1, 1, 1, 5, 2))
```


We should keep in mind that there might be multiple modes --
in such a case, it's best to pick one at random (to avoid any bias).
For that, we can use the `sample()` function.


{ BEGIN exercise }
Read the function's man page, see `?sample`.
Note that, dangerously, its behaviour is different when its first argument
is a vector of length 1.
{ END exercise }




An example implementation:

```{r our_mode}
our_mode <- function(Y) {
    # tabulate() will take care of
    # checking the correctness of Y
    t <- tabulate(Y) # factors == integer vectors here
    # get indices corresponding to the maximal counts
    mode_candidates <- which(t == max(t))
    if (length(mode_candidates) == 1) return(mode_candidates)
    else return(sample(mode_candidates, 1))
}
```

Some tests:

```{r our_mode2,echo=-1}
set.seed(7)
our_mode(c(1, 1, 1, 1))
our_mode(c(2, 2, 2, 2))
our_mode(c(3, 1, 3, 3))
our_mode(c(1, 1, 3, 3, 2))
our_mode(c(1, 1, 3, 3, 2))
```

<!-- TODO: test-driven development, unit tests -- mention -->




###  NN Search Methods


Last but not least, we should implement the `our_get_knnx()` function.
It's the function responsible for seeking the indices of nearest neighbours.


```{r our_get_knnx}
# our_get_knnx returns a matrix nn_indices of size n_test*k,
# where nn_indices[i,j] denotes the index of
# X_test[i,]'s j-th nearest neighbour in X_train.
# (It is the point X_train[nn_indices[i,j],]).
our_get_knnx <- function(X_train, X_test, k) {
    # ...
}
```


A naive approach to `our_get_knnx()` relies on computing
all pairwise distances,
and sorting them.

```{r our_get_knnx2}
our_get_knnx <- function(X_train, X_test, k) {
    n_test <- nrow(X_test)
    nn_indices <- matrix(NA_real_, nrow=n_test, ncol=k)
    for (i in 1:n_test) {
        d <- apply(X_train, 1, function(x)
            sqrt(sum((x-X_test[i,])^2)))
        # now d[j] is the distance
        # between X_train[j,] and X_test[i,]
        nn_indices[i,] <- order(d)[1:k]
    }
    return(nn_indices)
}
```





A comparison with `FNN::knn()` with regards to time needed to run
the algorithms:

```{r fnn_knn,cache=TRUE}
library("microbenchmark")
summary(microbenchmark(
    our={A<-our_knn(X_train, X_test, y_train, k=5)},
    fnn={B<-FNN::knn(X_train, X_test, y_train, k=5)},
    unit="ms" #  # milliseconds
))
mean(A == B) # 1.0 on perfect match
```

Both functions return identical results but our implementation is 10x slower.
It turns out that `our_get_knnx()` is the part that actually constitutes
the K-NN classifier's performance bottleneck in case on big data samples.
`FNN::knn()` is efficiently written in C++, which is a compiled programming language.

R, on the other hand (just like Python and Matlab) is interpreted, therefore --
as a rule of thumb -- we should consider it an order of magnitude slower
(see, however, the Julia language).

Let's substitute our naive implementation of `our_get_knnx()`
with the equivalent one,
but written in C++ (available in the `FNN` package).


{ BEGIN remark }
Note that we can write a C++ implementation ourselves,
see the `Rcpp` package [@rcpp] for convenient R and C++ integration.
{ END remark }



```{r our_get_knnx_time,cache=TRUE}
our_get_knnx <- function(X_train, X_test, k) {
    # this is used by our_knn()
    FNN::get.knnx(X_train, X_test, k, algorithm="brute")$nn.index
}

summary(microbenchmark(
    our ={A<-our_knn(X_train, X_test, y_train, k=5)},
    fnn1={B<-FNN::knn(X_train, X_test, y_train, k=5)}, # kd_tree, see below
    fnn2={C<-FNN::knn(X_train, X_test, y_train, k=5, algorithm="brute")},
    unit="ms" # milliseconds
))
mean(A == B) # 1.0 on perfect match
```

The timings are really interesting, taking into account
that `FNN::knn()` uses `FNN::get.knnx()` as well
(the R package ecosystem is comprised of open source software,
we have the freedom to read the function's source code
by calling `print(FNN::knn)`). Of course, before
drawing conclusions about the quality of our implementation, we should
test the aforementioned procedures on samples of different sizes and dimensionalities. This is left to the reader as an:

{ BEGIN exercise }
Test the run-times of the aforementioned procedures on samples
of different sizes and dimensionalities, for example, on
datasets generated randomly.
{ END exercise }


. . .

Before we conclude, let us note that there are special
*spatial search data structures*
-- such as metric trees -- that aim to speed up searching for nearest
neighbours in *low-dimensional spaces* (for small $p$).
For example, `FNN::get.knnx()` also implements the so-called
kd-trees.

<!--
 vp-trees, gnats, kd-trees, FANN
-->


Here is a function that generates `n` random points in a `p`
dimensional hypercube. The function fill report time taken
to look up `k` nearest neighbours based on a brute-force algorithm
(all pairs of points considered) vs. on the kd-tree.

```{r test_speed,cache=TRUE}
test_speed <- function(n, p, k) {
    A <- matrix(runif(n*p), nrow=n, ncol=p)
    s <- summary(microbenchmark::microbenchmark(
        brute_force=FNN::get.knnx(A, A, k, algorithm="brute"),
        kd_tree=FNN::get.knnx(A, A, k, algorithm="kd_tree"),
        times=3
    ), unit="s")
    # report minimum of each 3 time measurements:
    return(structure(s$min, names=as.character(s$expr)))
}
```

Example timings:

```{r test_speed2cache=TRUE,dependson='test_speed'}
test_speed(10000, 2, 5)
test_speed(10000, 5, 5)
test_speed(10000, 10, 5)
test_speed(10000, 20, 5)
```

In spaces of higher dimensionality, the brute force algorithm
is actually faster.
It turns out that searching in high-dimensional spaces is hard due
to the various phenomena collectively referred to as the
*curse of dimensionality* (see, e.g., [@foundds]).
Yet, is low-dimensional data boring? Well, our physical
world is perceived as 3-dimensional, spatial data (on maps) is 2-dimensional,
therefore, there are interesting use cases of kd-trees anyway.



<!--

## Exercises

TODO

another pre-fabricated train/test split?


-->



## Remarks






{ LATEX \color{gray} }

**TODO** .....

Note that the K-nearest neighbour method is suitable
for any multiclass classification, i.e., for any number of levels $L$
of $\mathbf{y}$. However, precision and recall can only be computed
if $L=2$.

In practice searching for nearest neighbours is time-consuming
for larger datasets -- to
classify a single point we have to query the whole training set
(unless it's a space of low dimensionality).


Note that our implementation
requires $c\cdot n_\text{test}\cdot n_\text{train}\cdot p$
arithmetic operations for some $c>1$.
The overall cost of sorting is at least $d\cdot n_\text{test}\cdot n_\text{train}\cdot\log n_\text{train}$
for some $d>1$.
This does not scale well with both $n_\text{test}$ and $n_\text{train}$
(think -- big data).


Moreover, the training set should be available at all times.
Other algorithms discussed in this book will try to come up with
a synthetic/compressed representation (a model) of the training set.


However, the advantage of K-NN is that it naturally adapts to new training
points -- we don't have to recompute any models to take them into account
(hence, we can say, that K-NN can also work in an *online mode).


K-NN is an influential concept -- see Chapter \@ref(chap:recommenders)
for how it is naturally employed in recommender systems.

<!--TODO: exercise -- density, not NN-based classification
regression - consider the epsilon-neighbourhood.-->

Instead of nearest neighbours, we could be also considering so-called
$\epsilon$-neighbourhoods -- all the points from $\mathbf{X}$
whose distance to a given $\boldsymbol{x}'$ is not greater than
$\epsilon$ (for example, DBSCAN [@pre_dbscan] [@dbscan]
is a clustering method based on this concept).
However, in classification task we should be aware of the fact that
some test points might have empty $\epsilon$-neighbourhoods and that
this issue should be resolved somehow.



. . .


(\*) 1-NN classification is essentially based
on a dataset's so-called Voronoi diagram.
Interestingly, in single linkage clustering, we also seek 1-nearest neighbours
(between clusters).

{ LATEX \normalcolor }




Recommended further reading: [@esl: Section 13.3]

Next Chapter....


Further we will discuss some other well-known classifiers:

- *Decision trees*
- *Logistic regression*




