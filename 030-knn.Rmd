# Classification with Nearest Neighbours {#chap:knn}

<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->


{ LATEX \color{gray} }

**TODO** In this chapter, we will:

* ...

prediction

k-nearest neighbour scheme


performance metrics for binary classification

best model selection

train/test/validate (alternative: cross-validation, see exercise TODO)

overfitting, generalisation

* ...


{ LATEX \normalcolor }





<!--

Add K-NN Regression



add CHAMELEON and (H)DBSCAN(*) ???
-- beyond supervised learning

Karypis, G., EH. Han, V. Kumar (1999): CHAMELEON: A Hierarchical Clustering Algorithm Using Dynamic Modeling, IEEE Computer, 32(8): 68â€“75.

-->



<!-- TODO:


2D illustration of NEAREST NEIGHBOURS!!!

exercise -- density, not NN-based classification or
regression - consider the epsilon-neighbourhood????

-->


## Introduction

In the previous chapter, we were only given an input matrix
$\mathbf{X}\in\mathbb{R}^{n\times p}$ representing
$n$ objects described by means of $p$ numerical features.

In this chapter we are going to be interested in  *supervised learning* tasks;
we assume that with each $\mathbf{x}_{i,\cdot}$ ($i=1,\dots,n$)
we associate the desired output $y_i$:

\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} \\
\end{array}
\right],
\qquad
\mathbf{y} = \left[
\begin{array}{c}
{y}_{1} \\
{y}_{2} \\
\vdots\\
{y}_{n} \\
\end{array}
\right].
\]


More precisely, we'd now like to focus on *classification* -- we assume that
each $y_i$ is of qualitative type (e.g., a label, a category, a colour).


The classes are usually *encoded* with consecutive integers,
say, $1, 2, \dots, K$, where $K$ is the total number of classes.
Mathematically, we'll write that $y_i\in\{1,\dots,K\}$.


{ BEGIN remark }
`factor` datatype in R (see Section \@ref(sec:factor) for more details)
gives a very convenient means to encode categorical data
(such as $\mathbf{y}$):

```{r factor1}
y <- c("black", "green", "black", "red", "green", "green", "blue", "red")
(y <- factor(y, levels=c("black", "red", "green", "blue")))
```

<!-- table(y) # every label and the number of occurrences -->


Internally, objects of type `factor` are represented as integer vectors
with elements in $\{1,\dots,K\}$, where $K$ is the number of possible levels.
Labels, used to "decipher" the numeric codes, are stored separately
and can always be replaced with other ones.

```{r factor2}
as.numeric(y) # 1st label, 3rd label, 1st label, and so forth
length(levels(y)) # K = number of levels
levels(y) # 1=black, 2=red, 3=green, 4=red
levels(y) <- c("k", "r", "g", "b") # re-encode
y
```
{ END remark }


In this chapter we'll assume $K=2$, i.e., that each $y_i$ can only take one of
two possible values. Such a case is very common in practice;
it has been even given a special name: *binary classification*.
The two classes are traditionally denoted as $0$ and $1$,
see Table \@ref(tab:binary-classification) for some examples.

Table: (\#tab:binary-classification) Some examples of output labels in binary classification tasks

0       | 1
--------|------------
no      | yes
false   | true
failure | success
reject  | accept
healthy | ill



{ BEGIN remark }
$0$s and $1$s are mathematically convenient, because
instead of stating "if $y_i=1$, then let $z=a$ and otherwise take $z=b$",
we can simply write "$z=y_i\, a + (1-y_i)\, b$"
(see, e.g., the definition of cross-entropy in \@ref(chap:logistic)).
Ah, those neat math tricks!
{ END remark }


Figure \@ref(fig:classify-intro) gives a scatter plot of an example synthetic
two-dimensional dataset (i.e., $p=2$)
with the reference binary $y$s depicted using different plotting symbols.
We see that the two classes  overlap slightly -- they cannot be
separated by means of any simple boundary (e.g., a line).
<!-- The "true" decision boundary is at $X_1=0$ but the classes -->
<!-- slightly overlap (the dataset is a bit noisy). -->



```{r classify-intro,echo=FALSE,fig.cap="A synthetic 2D dataset where each point is assigned one of two distinct labels",cache=TRUE}
set.seed(123)
n0 <- 50 # n0 points in class 0
n1 <- 50 # n1 points in class 1
Xs <- rbind(
    cbind(rnorm(n0, -2, 2), rnorm(n0, 0, 1.5)), # N( (-2, 0), (2, 1.5) )
    cbind(rnorm(n1, +2, 2), rnorm(n1, 0, 1.5))  # N( (+2, 0), (2, 1.5) )
)
Ys <- factor(rep(c("0", "1"), c(n0, n1)))

plot(Xs[,1], Xs[,2], col=Ys, pch=c(1,2)[Ys], xlab="X1", ylab="X2", asp=1,
        xlim=c(-8,8), ylim=c(-4,4))
legend("topleft", col=c(1,2), pch=c(1,2),
    legend=c("Class 0", "Class 1"), bg="white")
# abline(v=0, lty=3, col="blue")
```





## K-Nearest Neighbours Classifier

Our main aim will be to "build" an algorithm
that takes a previously unobserved input sequence
$\boldsymbol{x}'$ and which  -- based what we
know already, i.e., on $\mathbf{X}$ and $\mathbf{y}$
generates a corresponding prediction, $\hat{y}'$
(hopefully being equal to the true $y'$, whatever it is).


For instance, we may have a set of medical records, where we
store patient data regarding their measurable health-relate parameters
like blood pressure, severity of different symptoms etc.
Moreover, each patient is labelled -- we know if they have been diagnosed
a disease or not. But wait, there comes a new patient! We measure their
blood pressure, as some questions, input the data into our algorithm
and -- based on the result -- advise them to stay in bed.


. . .

In this chapter we will consider a very simple classifier,
based on an idea which dates back to at least 1950s (see [@knn1], [@knn2]).

Rule.

: "If you don't know what to do in a situation, just act like the people around you"



For some integer $K\ge 1$, the *K-Nearest Neighbour  (K-NN) Classifier*
proceeds as follows. To classify a new point $\boldsymbol{x}'$:

1. Find the $K$ nearest neighbours of a given point $\boldsymbol{x}'$
amongst the points in $\mathbf{X}$, i.e.,
points $\mathbf{x}_{i_1,\cdot}, \dots, \mathbf{x}_{i_K,\cdot}$
that are the closest to $\boldsymbol{x}'$ w.r.t. the Euclidean distance:
    a. compute the Euclidean distances between each $\mathbf{x}_{i,\cdot}$ from the input set and $\boldsymbol{x}'$:
    \[ d_i = d(\mathbf{x}_{i,\cdot}, \boldsymbol{x}'),\]
    b. order $d_i$s in increasing order,
    and fetch the indices $i_1,\dots,i_K$
    which yield:
    \[ d_{i_1} \le d_{i_2} \le \dots \le d_{i_K}.\]
<!--     c. pick first $K$ indices (these are the *nearest* neighbours). -->

2. Fetch the reference labels $y_{i_1}, \dots, y_{i_K}$
corresponding to the $K$ nearest neighbours.

3. Return their *mode* as a result, i.e., the most frequently occurring label (also called *majority vote*).

<!-- > If a mode is not unique, return a randomly chosen mode (ties are broken at random). -->








. . .

Let's illustrate how a $K$-NN classifier works on the above 2D synthetic dataset.
First we consider $K=1$, see Figure \@ref(fig:fig-plot-knn1).
Dark and light regions depict how new points would be classified (class 0 and
1, respectively).
For instance, the point $(0, 2)$ lays in the "dark zone", therefore
we'd assign it label 0.


```{r plot-knn,dependson="classify-intro",echo=FALSE,cache=TRUE}
plot_knn <- function(K) {
    xx1 <- seq(-8.2, 8.2, length.out=500)
    xx2 <- seq(-4.2, 4.2, length.out=500)
    xx <- expand.grid(xx1, xx2)
    yy <- FNN::knn(Xs, xx, Ys, k=K)
    image(xx1, xx2, matrix(as.numeric(yy)-1,
        nrow=length(xx1), ncol=length(xx2)),
        col=c("#00000022", "#ffe0e044"),
        xlab="X1", ylab="X2", asp=1,
        xlim=c(-8,8), ylim=c(-4,4)
        )
    points(Xs[,1], Xs[,2], col=Ys, pch=c(1,2)[Ys])
    legend("topleft", col=c(1,2), pch=c(1,2),
        legend=c("Class 0", "Class 1"), bg="white")
#     abline(v=0, lty=3, col="blue")
}
```



```{r fig-plot-knn1,echo=FALSE,cache=TRUE,dependson='plot-knn',fig.cap="1-NN class bounds for the 2D synthetic dataset"}
plot_knn(1)
```



We see that 1-NN is "greedy" or "lazy" in the sense that we just
locate the nearest point.
Increasing $K$ somehow smoothens the decision boundary (this makes it
less "local" and more "global").
Figure \@ref(fig:fig-plot-knn3) depicts the $K=3$ case.

```{r fig-plot-knn3,echo=FALSE,cache=TRUE,dependson='plot-knn',fig.cap="3-NN class bounds the our 2D synthetic dataset"}
plot_knn(3)
```



```{r fig-plot-knn15,echo=FALSE,cache=TRUE,dependson='plot-knn',fig.cap="15-NN class bounds the our 2D synthetic dataset"}
plot_knn(15)
```

The 15-NN classifier, see  Figure \@ref(fig:fig-plot-knn15),
does a quite good job with identifying
a  boundary between the two classes -- the whole $\mathbb{R}^2$ space
seems to be (more or less) split into two disconnected subregions;
there are no "lakes" or "islands" or significant sizes.






## Example in R


For a real-world illustration, let's consider a subset of the Wine Quality dataset
(see Appendix \@ref(sec:wine-quality) for more details)
that features a sample of white wines and includes only 3 of their
physicochemical characteristics:


```{r input1,cache=TRUE}
wine_train <- read.csv("datasets/wine_train.csv")
head(wine_train)
dim(wine_train) # number of rows, number of columns
```

The last column determines whether experts claim that a given wine
is of low quality (class `1`, we don't want that if we make a party)
or not (class `0`). We will use it as a
response variable in a wine classification task.


{ BEGIN remark }
Note that above $\mathbf{X}$ and $\mathbf{y}$ are stored together
as a single object, $[\mathbf{X}\ \mathbf{y}]$:


\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} & y_1\\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} & y_2\\
\vdots & \vdots & \ddots & \vdots    & \vdots\\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} & y_n\\
\end{array}
\right],
\]

In R, matrices (see Appendix \@ref(appendix:rmatrix))
are used to store data of the same type.
As $\mathbf{X}$ consists of real numbers,
we might sometimes run into a "type mismatch" error
if we want to extend it with $\mathbf{y}$ that is not numeric.
This is why such "combined" objects are usually
stored as R `data.frame`s (see Appendix \@ref(appendix:rdf));
this class allows columns of mixed types.
{ END remark }



Let's extract the input matrix
$\mathbf{X}\in\mathbb{R}^{`r nrow(wine_train)`\times `r ncol(wine_train)-1`}$
by taking the first 3 columns from the `wine_train` data frame
and the reference outputs $\mathbf{y}\in\{0,1\}^{`r nrow(wine_train)`}$
given by the last column.

```{r input2,dependson='input1',cache=TRUE}
X_train <- as.matrix(wine_train[,-4]) # all except the last column
y_train <- factor(wine_train[,4])
```



Now $[\mathbf{X}\ \mathbf{y}]$ is a basis for an interesting (and challenging)
binary classification task. It constitutes our *training sample* -- one
from which we actually *learn* what levels of chemical features make a
gathering with some vino a disappointing experience.
Yet, we don't get a certificate in wine tasting only
to hang some fancy diploma in a golden frame on the wall.
We want to *apply* the knowledge we've gained for the bottles whose corks
(or caps) are yet to be popped off.


Here are some bottles for which we can show off our skills:


```{r input3,dependson='input2',cache=TRUE}
X_test <- as.matrix(read.csv("datasets/wine_test_X.csv"))
head(X_test)
dim(X_test)
```

We will denote is as
$\mathbf{X}'\in\mathbb{R}^{`r nrow(X_test)`\times`r ncol(X_test)`}$
and call it a *test sample*.
Let's invoke the `knn()` function from package `FNN`
to classify the points $\mathbf{X}'$ with the 5-nearest neighbour rule
so as to obtain the corresponding predicted $\hat{\mathbf{y}}'$.

```{r libFNN,message=FALSE}
library("FNN") # load package
```

```{r y-knn5}
y_pred <- knn(X_train, X_test, y_train, k=5)
head(y_pred, 28) # predicted outputs
```


Great, we can now enjoy the bottles no.
`r paste(which(0==head(y_pred, 15)), collapse=", ")`, ...
because the classifier claims they're not bad.



## Model Assessment

<!--

More metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics

-->


But wait, are we *really* going to enjoy the aforementioned wines?
The classifier might as well be deceiving us or be generating random
predictions. It'd be responsible to put it in production without making
sure it's doing its job reasonably well.



```{r input4,dependson='input3',cache=TRUE}
y_test <- factor(read.csv("datasets/wine_test_y.csv")[,1])
head(y_test, 28) # true outputs
```


accuracy


```{r accuracy1}
mean(y_test == y_pred) # accuracy
```




 Performance Metrics

Recall that $y_i'$ denotes the true label associated with the $i$-th observation
in the test set.

Let $\hat{y}_i'$ denote the classifier's output for
a given $\mathbf{x}_{i,\cdot}'$.

Ideally, we'd wish that $\hat{y}_i'=y_i'$.

Sadly, in practice we will make errors.
Table \@ref(tab:true-vs-predicted) lists the 4 possible situations.

Table: (\#tab:true-vs-predicted) True vs. predicted labels in a binary classification task; ideally, the number of false positives and false negatives should be kept to a minimum

.              | $y_i'=0$                       | $y_i'=1$
---------------|--------------------------------|-------------
$\hat{y}_i'=0$ | **True Negative**              | False Negative (Type II error)
$\hat{y}_i'=1$ | False Positive (Type I error)  | **True Positive**



Note that the terms **positive** and **negative** refer to
the classifier's output, i.e., occur when $\hat{y}_i$ is equal to $1$ and $0$, respectively.





A **confusion matrix** is used to summarise
the correctness of predictions for the whole sample:


```{r confusion1}
(C <- table(y_pred, y_test))
```

For example,

```{r confusion2}
C[1,1] # number of TNs
C[2,1] # number of FPs
```






**Accuracy** is the ratio of the correctly classified instances
to all the instances.

In other words, it is the probability of making a correct prediction.

\[
\text{Accuracy} = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
= \frac{1}{n} \sum_{i=1}^n \mathbb{I}\left(
y_i = \hat{y}_i
\right)
\]
where $\mathbb{I}$ is the indicator function,
$\mathbb{I}(l)=1$ if logical condition $l$ is true and $0$ otherwise.

```{r accuracy}
mean(y_test == y_pred) # accuracy
(C[1,1]+C[2,2])/sum(C) # equivalently
```




In many applications we are dealing with **unbalanced problems**, where
the case $y_i=1$ is relatively rare,
yet predicting it  correctly is much more important than being
accurate with respect to class $0$.
Think of medical applications, e.g., HIV testing
or tumour diagnosis.



table(y_train)


In such a case, *accuracy* as a metric fails to quantify what we are aiming for.


{ BEGIN remark }
If only 1% of the cases have true $y_i'=1$,
then a dummy classifier that always
outputs $\hat{y}_i'=0$ has 99% accuracy.
{ END remark }

Metrics such as precision and recall (and their aggregated version, F-measure)
aim to address this problem.








**Precision**



\[
\text{Precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}
\]


If the classifier outputs $1$,
what is the probability that this is indeed true?

```{r precision}
C[2,2]/(C[2,2]+C[2,1]) # Precision
```




**Recall** (a.k.a. sensitivity, hit rate or true positive rate)

\[
\text{Recall} = \frac{\text{TP}}{\text{TP}+\text{FN}}
\]

If the true class is $1$, what is the probability that the classifier
will detect it?


```{r recall}
C[2,2]/(C[2,2]+C[1,2]) # Recall
```






{ BEGIN exercise }
Precision or recall? It depends on an application.
Think of medical diagnosis, medical screening, plagiarism detection,
etc. --- which measure is more important in each of the settings listed?
{ END exercise }



As a compromise, we can use the **F-measure**
(a.k.a. $F_1$-measure),
which is the harmonic mean of precision
and recall:

\[
\text{F} = \frac{1}{
    \frac{
        \frac{1}{\text{Precision}}+\frac{1}{\text{Recall}}
    }{2}
}
=
\left(
\frac{1}{2}
\left(
\text{Precision}^{-1}+\text{Recall}^{-1}
\right)
\right)^{-1}
=
\frac{\text{TP}}{\text{TP} + \frac{\text{FP} + \text{FN}}{2}}
\]


{ BEGIN exercise }
Show that the above equality holds.
{ END exercise }




```{r fmeasure}
C[2,2]/(C[2,2]+0.5*C[1,2]+0.5*C[2,1]) # F
```





The following function can come in handy in the future:

```{r get_metrics}
get_metrics <- function(y_pred, y_test)
{
    all_levels <- unique(c(y_pred, y_test)) # distinct labels
    y_pred <- factor(y_pred, levels=all_levels)
    y_test <- factor(y_test, levels=all_levels)
    C <- table(y_pred, y_test) # confusion matrix
    stopifnot(dim(C) == c(2, 2))
    c(Acc=(C[1,1]+C[2,2])/sum(C), # accuracy
      Prec=C[2,2]/(C[2,2]+C[2,1]), # precision
      Rec=C[2,2]/(C[2,2]+C[1,2]), # recall
      F=C[2,2]/(C[2,2]+0.5*C[1,2]+0.5*C[2,1]), # F-measure
      # Confusion matrix items:
      TN=C[1,1], FN=C[1,2],
      FP=C[2,1], TP=C[2,2]
    ) # return a named vector
}
```






```{r get_metrics2}
get_metrics(y_pred, y_test)
```


## Model Selection


How to Choose K for K-NN Classification?


- we don't want our models to  *overfit* to current data,
- we want our models to *generalise* well
to new data.

We haven't yet considered the question which $K$ yields *the best*
classifier.

Best == one that has the highest *predictive power*.

Best == with respect to some chosen metric (accuracy, recall, precision, F-measure, ...)

Let's study how the metrics on the test set change as functions of the number of nearest neighbours considered, $K$.





Auxiliary function:

```{r whichK1,cache=TRUE}
knn_metrics <- function(k, X_train, X_test, y_train, y_test)
{
    y_pred <- knn(X_train, X_test, y_train, k=k) # classify
    get_metrics(y_pred, y_test)
}
```

For example:

```{r whichK2,cache=TRUE}
knn_metrics(5, X_train, X_test, y_train, y_test)
```






Example call to evaluate metrics as a function of different $K$s:

```{r whichK3,cache=TRUE}
Ks <- seq(1, 19, by=2)
Ps <- as.data.frame(t(
    sapply(Ks, # on each element in this vector
        knn_metrics,     # apply this function
        X_train, X_test, y_train, y_test # aux args
    )))
```



{ BEGIN remark }
Note that `sapply(X, f, arg1, arg2, ...)`
outputs a list `Y` such that
`Y[[i]] = f(X[i], arg1, arg2, ...)`
which is then simplified to a matrix.
{ END remark }



{ BEGIN remark }
We transpose this result, `t()`, in order to get each metric
corresponding to different columns in the result.
As usual, if you keep wondering, e.g.,  why `t()`, play with
the code yourself -- it's fun fun fun.
{ END remark }





Example results:

```{r whichK4,cache=TRUE}
round(cbind(K=Ks, Ps), 2)
```

Figure \@ref(fig:whichK5) is worth a thousand tables though (see `?matplot` in R). The reader is kindly asked to draw conclusions themself.







```{r whichK5,echo=FALSE,cache=TRUE,fig.cap="Performance of $K$-nn classifiers as a function of $K$ "}
Ks <- seq(1, 51, by=2)
Ps <- as.data.frame(t(sapply(Ks, # on each element in this vector
  knn_metrics,     # apply this function
  X_train, X_test, y_train, y_test # also passing these as args
)))
matplot(Ks, Ps[,1:4], xlab="K", ylab="Metric", type="l",
    ylim=c(0,1))
# legend("top", legend=names(Ps[,1:4]),
#     col=1:4, lty=1:4, ncol=4, bg="white")
```

<!--

(\*) **ROC** (Receiver Operating Characteristic) curve:

```{r }
TPR <- Ps$TP/(Ps$TP+Ps$FN) # True Positive Rate (recall)
FPR <- Ps$FP/(Ps$FP+Ps$TN) # False Positive Rate
plot(FPR, TPR, asp=1, xlim=c(0,1), ylim=c(0,1))
abline(a=0, b=1, lty=3)
```

-->





### Training, Validation and Test sets





In the $K$-NN classification task, there are many hyperparameters to tune up:

- Which $K$ should we choose?

- Should we standardise the dataset?

- Which variables should be taken into account when computing the Euclidean distance?

<!--
- Which metric should be used?
-->




{ BEGIN remark }
**If we select the best hyperparameter set based on test
sample error, we will run into the trap of overfitting again**.
This time we'll be overfitting to the test set --- the model that is optimal
for a given test sample doesn't have to generalise well to other test samples (!).
{ END remark }




In order to overcome this problem,
we can perform a random **train-validation-test split** of the original dataset:

- *training sample*  (e.g., 60%) -- used to construct the models
- *validation sample* (e.g., 20%) -- used to tune the hyperparameters of the classifier
- *test sample* (e.g., 20%) -- used to assess the goodness of fit

<!--
By the way, this is how most data mining competitions are assessed --
you will never have access to the final test sample used
to determine the winner. The best you can do is to "guess".
-->





An example way to perform a 60/20/20% train-validation-test split:

```{r train_val_test_split}
# set.seed(123) # reproducibility matters
# random_indices <- sample(n)
# n1 <- floor(n*0.6)
# n2 <- floor(n*0.8)
# X2_train <- X[random_indices[1     :n1], ]
# Y2_train <- Y[random_indices[1     :n1]  ]
# X2_valid <- X[random_indices[(n1+1):n2], ]
# Y2_valid <- Y[random_indices[(n1+1):n2]  ]
# X2_test  <- X[random_indices[(n2+1):n ], ]
# Y2_test  <- Y[random_indices[(n2+1):n ]  ]
# stopifnot(nrow(X2_train)+nrow(X2_valid)+nrow(X2_test)
#     == nrow(X))
```


{ BEGIN exercise }
Find the best $K$ on the validation set and compute the error metrics
on the test set.
{ END exercise }




{ BEGIN remark }
(\*) If our dataset is too small,
we can use various *cross-validation* techniques
instead of a train-validate-test split.
<!-- TODO: see exercise.... -->
{ END remark }




## Implementing a K-NN Classifier (\*)




### Main Routine (\*)




Let's implement a K-NN classifier ourselves
by using a top-bottom approach.

We will start with a general description of the admissible inputs
and the expected output.

Then we will arrange the processing of data into
conveniently manageable chunks.

The function's declaration will look like:

```{r our_knn1,eval=FALSE}
our_knn <- function(X_train, X_test, y_train, k=1) {
    # k=1 denotes a parameter with a default value
    # ...
}
```




Load an example dataset on which we will test our algorithm:

```{r our_knn2}
wine_quality <- read.csv("datasets/wine_quality_all.csv",
    comment.char="#")
wine_train <- wine_quality[wine_quality$color == "white",]
X <- as.matrix(wine_train[,1:10])
Y <- factor(as.character(as.numeric(wine_train$alcohol >= 12)))
```


Note that `Y` is now a factor object.


Train-test split:

```{r our_knn3}
set.seed(123)
n <- nrow(X)
random_indices <- sample(n)
train_indices <- random_indices[1:floor(n*0.6)]
X_train <- X[train_indices,]
y_train <- Y[train_indices]
X_test  <- X[-train_indices,]
y_test  <- Y[-train_indices]
```









First, we should specify the type and form of the arguments
we're expecting:

```{r our_knn_paramchecks,eval=FALSE}
# this is the body of our_knn() - part 1
stopifnot(is.numeric(X_train), is.matrix(X_train))
stopifnot(is.numeric(X_test), is.matrix(X_test))
stopifnot(is.factor(y_train))
stopifnot(ncol(X_train) == ncol(X_test))
stopifnot(nrow(X_train) == length(y_train))
stopifnot(k >= 1)
n_train <- nrow(X_train)
n_test  <- nrow(X_test)
p <- ncol(X_train)
M <- length(levels(y_train))
```

Therefore,

$\mathtt{X\_train}\in\mathbb{R}^{\mathtt{n\_train}\times \mathtt{p}}$,
$\mathtt{X\_test}\in\mathbb{R}^{\mathtt{n\_test}\times \mathtt{p}}$ and
$\mathtt{Y\_train}\in\{1,\dots,M\}^{\mathtt{n\_train}}$



{ BEGIN remark }
Recall that R `factor` objects are internally encoded as integer vectors.
{ END remark }



Next, we will call the (to-be-done) function `our_get_knnx()`,
which seeks nearest neighbours of all the points:

```{r our_knn_flow1,eval=FALSE}
# our_get_knnx returns a matrix nn_indices of size n_test*k,
# where nn_indices[i,j] denotes the index of
# X_test[i,]'s j-th nearest neighbour in X_train.
# (It is the point X_train[nn_indices[i,j],]).
nn_indices <- our_get_knnx(X_train, X_test, k)
```





Then, for each point in `X_test`,
we fetch the labels corresponding to its nearest neighbours
and compute their mode:

```{r our_knn_flow2,eval=FALSE}
y_pred <- numeric(n_test) # vector of length n_test
# For now we will operate on the integer labels in {1,...,M}
y_train_int <- as.numeric(y_train)
for (i in 1:n_test) {
    # Get the labels of the NNs of the i-th point:
    nn_labels_i <- y_train_int[nn_indices[i,]]
    # Compute the mode (majority vote):
    y_pred[i] <- our_mode(nn_labels_i) # in {1,...,M}
}
```

Finally, we should convert the resulting integer vector
to an object of type `factor`:

```{r our_knn_flow3,eval=FALSE}
# Convert y_pred to factor:
return(factor(y_pred, labels=levels(y_train)))
```



```{r our_knn_final,echo=FALSE}
our_knn <- function(X_train, X_test, y_train, k=1) {

    <<our_knn_paramchecks>>
    <<our_knn_flow1>>
    <<our_knn_flow2>>
    <<our_knn_flow3>>
}
```



<!--
**Test-driven development** -- before writing

```{r}
test_our_knn <- function() {
    # ...
}
```


```{r}
test_our_mode <- function() {
    stopifnot(our_mode(c(1, 1, 1, 1)) == 1)
    stopifnot(our_mode(c(2, 2, 2, 2)) == 2)
    stopifnot(our_mode(c(3, 1, 3, 3)) == 3)
    stopifnot(our_mode(c(1, 1, 3, 3, 2)) %in% c(1, 3))
}
```


-->

### Mode




To implement the mode, we can use the `tabulate()` function.


{ BEGIN exercise }
Read the function's man page, see `?tabulate`.
{ END exercise }

For example:

```{r tabulate}
tabulate(c(1, 2, 1, 1, 1, 5, 2))
```




There might be multiple modes -- in such a case, we should pick one at random.

For that, we can use the `sample()` function.


{ BEGIN exercise }
Read the function's man page, see `?sample`.
Note that its behaviour is different when it's first argument is a vector of length 1.
{ END exercise }




An example implementation:

```{r our_mode}
our_mode <- function(Y) {
    # tabulate() will take care of
    # checking the correctness of Y
    t <- tabulate(Y)
    mode_candidates <- which(t == max(t))
    if (length(mode_candidates) == 1) return(mode_candidates)
    else return(sample(mode_candidates, 1))
}
```




```{r our_mode2,echo=-1}
set.seed(7)
our_mode(c(1, 1, 1, 1))
our_mode(c(2, 2, 2, 2))
our_mode(c(3, 1, 3, 3))
our_mode(c(1, 1, 3, 3, 2))
our_mode(c(1, 1, 3, 3, 2))
```




###  NN Search Routines (\*)




Last but not least, we should implement the `our_get_knnx()` function.

It is the function responsible for seeking the indices of nearest neighbours.

It turns out this function will  actually constitute the K-NN classifier's performance
bottleneck in case of big data samples.

```{r our_get_knnx}
# our_get_knnx returns a matrix nn_indices of size n_test*k,
# where nn_indices[i,j] denotes the index of
# X_test[i,]'s j-th nearest neighbour in X_train.
# (It is the point X_train[nn_indices[i,j],]).
our_get_knnx <- function(X_train, X_test, k) {
    # ...
}
```






A naive approach to `our_get_knnx()` relies on computing all pairwise distances,
and sorting them.

```{r our_get_knnx2}
our_get_knnx <- function(X_train, X_test, k) {
    n_test <- nrow(X_test)
    nn_indices <- matrix(NA_real_, nrow=n_test, ncol=k)
    for (i in 1:n_test) {
        d <- apply(X_train, 1, function(x)
            sqrt(sum((x-X_test[i,])^2)))
        # now d[j] is the distance
        # between X_train[j,] and X_test[i,]
        nn_indices[i,] <- order(d)[1:k]
    }
    nn_indices
}
```





A comparison with `FNN:knn()`:

```{r fnn_knn,cache=TRUE}
system.time(Ya <- knn(X_train, X_test, y_train, k=5))
system.time(Yb <- our_knn(X_train, X_test, y_train, k=5))
mean(Ya == Yb) # 1.0 on perfect match
```

Both functions return identical results but our implementation is "slightly" slower.






`FNN:knn()` is efficiently written in C++, which is a compiled programming language.

R, on the other hand (just like Python and Matlab) is interpreted, therefore
as a rule of thumb we should consider it an order of magnitude slower (see, however, the Julia language).

Let's substitute our naive implementation with the equivalent one,
but written in C++ (available in the `FNN` package).


{ BEGIN remark }
(\*) Note that we can write a C++ implementation ourselves,
see the Rcpp package for seamless R and C++ integration.
{ END remark }



```{r our_get_knnx_time,cache=TRUE}
our_get_knnx <- function(X_train, X_test, k) {
    # this is used by our_knn()
    FNN::get.knnx(X_train, X_test, k, algorithm="brute")$nn.index
}
system.time(Ya <- knn(X_train, X_test, y_train, k=5))
system.time(Yb <- our_knn(X_train, X_test, y_train, k=5))
mean(Ya == Yb) # 1.0 on perfect match
```




Note that our solution requires $c\cdot n_\text{test}\cdot n_\text{train}\cdot p$
arithmetic operations for some $c>1$.
The overall cost of sorting is at least $d\cdot n_\text{test}\cdot n_\text{train}\cdot\log n_\text{train}$
for some $d>1$.

This does not scale well with both $n_\text{test}$ and $n_\text{train}$
(think -- big data).

. . .

It turns out that there are special **spatial data structures**
-- such as *metric trees* -- that aim to speed up searching for nearest
neighbours in *low-dimensional spaces* (for small $p$).


{ BEGIN remark }
(\*) Searching in high-dimensional spaces is hard due to the so-called
curse of dimensionality (see, e.g., [@foundds]).
{ END remark }

For example, `FNN::get.knnx()` also implements the so-called
kd-trees.


<!--

 curse of dimensionality [@foundds]
 https://minds.wisconsin.edu/bitstream/handle/1793/60174/TR1377.pdf?sequence=1
 etc.

 vp-trees, gnats, kd-trees, FANN
-->







```{r test_speed,cache=TRUE}
library("microbenchmark")
test_speed <- function(n, p, k) {
    A <- matrix(runif(n*p), nrow=n, ncol=p)
    s <- summary(microbenchmark::microbenchmark(
        brute=FNN::get.knnx(A, A, k, algorithm="brute"),
        kd_tree=FNN::get.knnx(A, A, k, algorithm="kd_tree"),
        times=3
    ), unit="s")
    # minima of 3 time measurements:
    structure(s$min, names=as.character(s$expr))
}
```





```{r test_speed2cache=TRUE,dependson='test_speed'}
test_speed(10000, 2, 5)
test_speed(10000, 5, 5)
test_speed(10000, 10, 5)
test_speed(10000, 20, 5)
```




## Exercises

TODO


<!--

TODO: exercise -- density, not NN-based classification
regression - consider the epsilon-neighbourhood.

-->



## Outro



Note that K-NN is suitable for any kind of multiclass classification.

However, in practice it's pretty slow for larger datasets -- to
classify a single point we have to query the whole training set (which
should be available at all times).


### Side Note: K-NN Regression


TODO: later



The K-Nearest Neighbour scheme is intuitively pleasing.

No wonder it has inspired a similar approach for solving a regression task.

In order to make a prediction for a new point $\mathbf{x}'$:

1. find the K-nearest neighbours of  $\mathbf{x}'$ amongst the points in the train set,
denoted $\mathbf{x}_{i_1,\cdot}, \dots, \mathbf{x}_{i_K,\cdot}$,
2. fetch the corresponding reference outputs $y_{i_1}, \dots, y_{i_K}$,
3. return their arithmetic mean as a result,
\[\hat{y}=\frac{1}{K} \sum_{j=1}^K y_{i_j}.\]








Recall our modelling of the Credit Rating ($Y$)
as a function of the average Credit Card Balance ($X$)
based on the `ISLR::Credit` dataset.



```{r knnreg1}
library("ISLR") # Credit dataset
Xc <- as.matrix(as.numeric(Credit$Balance[Credit$Balance>0]))
Yc <- as.matrix(as.numeric(Credit$Rating[Credit$Balance>0]))
```



```{r knnreg2,message=FALSE}
library("FNN") # knn.reg function
x <- as.matrix(seq(min(Xc), max(Xc), length.out=101))
y1  <- knn.reg(Xc, x, Yc, k=1)$pred
y5  <- knn.reg(Xc, x, Yc, k=5)$pred
y25 <- knn.reg(Xc, x, Yc, k=25)$pred
```


The three models are depicted in Figure \@ref(fig:knnreg3).
Again, the higher the $K$, the smoother the curve. On the other hand, for
small $K$ we adapt better to what's in a point's neighbourhood.

```{r knnreg3,message=FALSE,fig.cap="K-NN regression example"}
plot(Xc, Yc, col="#666666c0",
    xlab="Balance", ylab="Rating")
lines(x, y1,  col=2, lwd=3)
lines(x, y5,  col=3, lwd=3)
lines(x, y25, col=4, lwd=3)
legend("topleft", legend=c("K=1", "K=5", "K=25"),
    col=c(2, 3, 4), lwd=3, bg="white")
```




{ LATEX \color{gray} }

. . .

**TODO** .....


. . .


(\*) 1-NN classification is essentially based
on a dataset's so-called Voronoi diagram.

{ LATEX \normalcolor }




Recommended further reading: [@esl: Section 13.3]

Next Chapter....


Further we will discuss some other well-known classifiers:

- *Decision trees*
- *Logistic regression*




