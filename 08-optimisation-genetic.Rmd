# Optimisation with Genetic Algorithms

<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->



## Introduction

### Recap


---

Recall that an **optimisation task** deals with finding an element ${x}$
in a **search space** $D$,
that minimises or maximises an **objective function** $f:D\to\mathbb{R}$:

\[
\min_{{x}\in D} f({x}) \quad\text{or}\quad\max_{{x}\in D} f({x}),
\]


In one of the previous chapters, we were dealing with
**unconstrained continuous optimisation**,
i.e., we assumed the search space is $D=\mathbb{R}^p$ for some $p$.

\ \

Example problems of this kind: minimising mean squared error
in linear regression
or cross-entropy in logistic regression.


---

The class of general-purpose iterative algorithms we've previously studied
fit into the following scheme:


1. $\mathbf{x}^{(0)}$ -- initial guess (e.g., generated at random)

2. for $i=1,...,M$:
    a. $\mathbf{x}^{(i)} = \mathbf{x}^{(i-1)}+\text{[guessed direction, e.g.,}-\eta\nabla f(\mathbf{x})\text{]}$
    b. if $|f(\mathbf{x}^{(i)})-f(\mathbf{x}^{(i-1)})| < \varepsilon$ break

3. return $\mathbf{x}^{(i)}$ as result


where:

* $M$ = maximum number of iterations
* $\varepsilon$ = tolerance, e.g, $10^{-8}$
* $\eta>0$ = learning rate

---

The algorithms such as gradient descent and BFGS (see `optim()`)
give satisfactory results in the case of **smooth and well-behaving objective functions**.

However, if an objective has, e.g., many plateaus (regions where it is almost constant),
those methods might easily get stuck in local minima.

The K-means clustering's objective function is a not particularly pleasant
one -- it involves a nested search for the closest cluster, with the use of the $\min$ operator.



### K-means Revisited

---


In **k-means clustering** we are minimising the squared Euclidean distance
to each point's cluster centre:
\[
\min_{\boldsymbol\mu^{(1)}, \dots, \boldsymbol\mu^{(k)} \in \mathbb{R}^p}
\sum_{i=1}^n \left(
\min_{j=1,\dots,k}
\sum_{\ell=1}^p \left(x_\ell^{(i)}-\mu_\ell^{(j)}\right)^2
\right)
\]


This is an (NP-)hard problem!
There is no efficient exact algorithm.

We need approximations. In the last chapter, we have
discussed the iterative Lloyd's algorithm (1957),
which is amongst a few procedures implemented in the `kmeans()` function.



---


To recall, Lloyd's algorithm (1957) is sometimes referred to as "the" k-means algorithm:

1. Start with random cluster centres $\boldsymbol\mu^{(1)}, \dots, \boldsymbol\mu^{(k)}$.

2. For each point $\mathbf{x}^{(i)}$, determine its closest centre $C(\mathbf{x}^{(i)})\in\{1,\dots,k\}$.

3. For each cluster $j\in\{1,\dots,k\}$, compute the new cluster centre $\boldsymbol\mu^{(j)}$ as the componentwise arithmetic mean
of the coordinates of all the points such that $C(\mathbf{x}^{(i)})=j$.

4. If the cluster centres changed since last iteration, go to step 2, otherwise stop and return the result.

\ \

As the procedure might get stuck in a local minimum,
a few restarts are recommended (as usual).

Hence, we are used to calling:

```{r,eval=FALSE}
kmeans(X, centers=k, nstart=10)
```


### optim() vs. kmeans()

---

Let us compare how a general-purpose optimiser such as the BFGS algorithm
implemented in `optim()` compares with a customised, problem-specific solver.

We will need some benchmark data.

```{r,gendata,cache=TRUE}
gen_cluster <- function(n, p, m, s) {
    vectors <- matrix(rnorm(n*p), nrow=n, ncol=p)
    unit_vectors <- vectors/sqrt(rowSums(vectors^2))
    unit_vectors*rnorm(n, 0, s)+rep(m, each=n)
}
```

The above function generates $n$ points in $\mathbb{R}^p$
from a distribution centred at $\mathbf{m}\in\mathbb{R}^p$,
spread randomly in every possible direction with scale factor $s$.

---

Two example clusters in $\mathbb{R}^2$:

```{r,dependson='gendata',gendata_example,cache=TRUE,echo=-(1:2)}
set.seed(1243)
par(mar=c(2,2,0.5,0.5))
# plot the "black" cluster
plot(gen_cluster(500, 2, c(0, 0), 1), col="#00000022", pch=16,
    xlim=c(-3, 4), ylim=c(-3, 4), asp=1, ann=FALSE, las=1)
# plot the "red" cluster
points(gen_cluster(250, 2, c(1.5, 1), 0.5), col="#ff000022", pch=16)
```

---

Let's generate the benchmark dataset $\mathbf{X}$
that consists of three clusters in a high-dimensional space.

```{r,dependson='gendata',gendata2,cache=TRUE}
set.seed(123)
p  <- 32
Ns <- c(50, 100, 20)
Ms <- c(0, 1, 2)
s  <- 1.5*p
k  <- length(Ns)

X <- lapply(1:k, function(i)
    gen_cluster(Ns[i], p, rep(Ms[i], p), s))
X <- do.call(rbind, X) # rbind(X[[1]], X[[2]], X[[3]])
```

---

The objective function for the k-means clustering problem:

```{r,dependson='gendata2',gendata3,cache=TRUE}
library("FNN")
get_fitness <- function(mu, X) {
    # For each point in X,
    # get the index of the closest point in mu:
    memb <- FNN::get.knnx(mu, X, 1)$nn.index

    # compute the sum of squared distances
    # between each point and its closes cluster centre:
    sum((X-mu[memb,])**2)
}
```

---

Setting up the solvers:

```{r,dependson='gendata3',gendata3b,cache=TRUE}
min_HartiganWong <- function(mu0, X)
    get_fitness(
        # algorithm="Hartigan-Wong"
        kmeans(X, mu0, iter.max=100)$centers,
    X)

min_Lloyd <- function(mu0, X)
    get_fitness(
        kmeans(X, mu0, iter.max=100, algorithm="Lloyd")$centers,
    X)

min_optim <- function(mu0, X)
    optim(mu0,
        function(mu, X) {
            get_fitness(matrix(mu, nrow=nrow(mu0)), X)
        }, X=X, method="BFGS", control=list(reltol=1e-16)
    )$val
```

---

Running the simulation:

```{r,dependson='gendata3b',gendata4,cache=TRUE}
nstart <- 100
set.seed(123)
res <- replicate(nstart, {
  mu0 <- X[sample(nrow(X), k),]
    c(
        HartiganWong=min_HartiganWong(mu0, X),
        Lloyd=min_Lloyd(mu0, X),
        optim=min_optim(mu0, X)
    )
})
```



---

Notice a considerable variability of the
objective function at the local minima found:

```{r,dependson='gendata4',gendata5,cache=TRUE}
par(mar=c(2, 6.5, 0.5, 0.5)) # figure margins
boxplot(as.data.frame(t(res)), horizontal=TRUE, las=1)
```


---


```{r,dependson='gendata5',gendata6,cache=TRUE}
print(apply(res, 1, function(x)
    c(summary(x), sd=sd(x))
))
```

Of course, we are interested in the smallest value of the objective,
because we're trying to pinpoint the global minimum.


```{r,dependson='gendata5',gendata6c,cache=TRUE}
print(apply(res, 1, min))
```


---

The Hartigan-Wong algorithm (the default one in `kmeans()`)
is the most reliable one of the three:

* it gives the best solution (low bias)
* the solutions have the lowest degree of variability (low variance)
* it is the fastest:


---

```{r,dependson='gendata3b',gendata4b,cache=TRUE}
library("microbenchmark")
set.seed(123)
mu0 <- X[sample(nrow(X), k),]
summary(microbenchmark(
    HartiganWong=min_HartiganWong(mu0, X),
    Lloyd=min_Lloyd(mu0, X),
    optim=min_optim(mu0, X),
    times=10
), unit="relative")
```


---

```{r,dependson='gendata5',gendata6b,cache=TRUE}
print(min(res))
```


Is it the global minimum?

> We don't know, we just didn't happen to find anything better (yet).

Did we put enough effort to find it?

> Well, maybe. We can try more random restarts:

```{r,dependson='gendata6',gendata7,cache=TRUE}
res_tried_very_hard <- kmeans(X, k, nstart=100000, iter.max=10000)$centers
print(get_fitness(res_tried_very_hard, X))
```

Is it good enough?

> It depends what we'd like to do with this. Does it make your boss happy?
Does it generate revenue? Does it help solve any other problem? Is it useful anyhow?
Are you really looking for the global minimum?








## A Note on Convex Optimisation (\*)

### Introduction

---

Are there cases where we are sure that a local minimum is the global minimum?

Yes. For example when we minimise *convex objective functions*.

> Here is just a very brief overview of the topic for the interested,
see also [@boyd_vandenberghe] for more.

For example, the linear regression or the logistic regression
have convex objectives -- they are very well-behaving.

Note that this doesn't mean that we know an **analytic solution**.



### Convex Combinations (\*)

---

A **convex combination** of a set of points $x_1,\dots,x_n\in D$
is a *linear combination*
\[
\theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n
\]
for some $\theta_1,\theta_2,\dots,\theta_n$
that fulfils $\theta_1,\theta_2,\dots,\theta_n\ge 0$
and $\theta_1+\theta_2+\dots+\theta_n=1$.

> Think of this as a weighted arithmetic mean of these points.

---

![](figures/combination1){width=50%}

The set of all convex combinations of two points $x_1,x_2\in D$:
\[\{\theta x_{1}+(1-\theta)x_{2}: \theta\in [0,1]\}\]
is just the line segment between these two points.

---



![](figures/combination2){width=50%}

The set of all convex combinations
of 3 points yields a triangle
(unless $D$ is one-dimensional).

---


![](figures/combination3){width=50%}

More generally, we get the *convex hull* of a set of points --
the smallest set $C$ enclosing all the points that is convex,
i.e., a line segment between any two points in $C$ is fully
included in $C$.


> In two dimensions, think of a rubber band stretched around all the points
like a fence.

---


![](figures/convex_set){width=75%}



### Convex Functions (\*)


---

We call a function $f:D\to\mathbb{R}$ **convex**, whenever:
\[
(forall x_{1},x_{2}\in D) (\forall \theta\in [0,1])\quad
f(\theta x_{1}+(1-\theta)x_{2})\leq \theta f(x_{1})+(1-\theta)f(x_{2})
\]


![](figures/convex_function){width=75%}

(function value at any convex combination of two points
is not greater than that combination of the function values at these two points)

---

![](figures/convex_concave){width=75%}


Convex: $f(\theta x_{1}+(1-\theta)x_{2})\leq \theta f(x_{1})+(1-\theta)f(x_{2})$

Concave: $f(\theta x_{1}+(1-\theta)x_{2})\geq \theta f(x_{1})+(1-\theta)f(x_{2})$


---

Theorem

: For any convex function $f$, if $f$ has a local minimum at $x$
then $x$ is also its global minimum.


Optimising convex functions is *relatively* easy, especially if they are differentiable.

Methods such as gradient descent or BFGS should work very well
(unless there are large regions where a function is constant...).

\ \


> (\*\*) There is a special class of constrained optimisation problems called linear
and quadratic programming that involves convex functions, see [@nocedal_wright;@fletcher]

> (\*\*\*) See also the Karush–Kuhn–Tucker (KKT) conditions
for a more general problem of minimisation with constraints.




### Examples

---

* $x^2, |x|, e^x$ are all convex.
* $|x|^p$ is convex for app $p\ge 1$.
* if $f$ is convex, then $-f$ is concave.
* if $f_1$ and $f_2$ are convex, then $w_1 f_1 + w_2 f_2$ are convex for any
$w_1,w_2\ge 0$.
* if $f_1$ and $f_2$ are convex, then $\max\{f_1, f_2\}$ are convex.
* if $f$ and $g$ are convex and $g$ is non-decreasing, then $g(f(x))$ is convex.
* Sum of squared residuals in linear regression is a convex function of the underlying parameters.
* Cross-entropy in logistic regression  is a convex function of the underlying parameters.







## Genetic Algorithms

### Introduction

---


What if our optimisation problem cannot be solved reliably with
gradient-based methods like those in `optim()` and we don't have
any custom solver for the task at hand?

There are a couple of useful *metaheuristics* in the literature
that can serve this purpose.

Most of them rely on clever randomised search.

---

There is a wide class of **nature-inspired** algorithms (that traditionally
belong to the subfield of AI called *computational intelligence* or *soft computing*);
see, e.g, [@evolution]:

* evolutionary algorithms -- inspired by the principle of natural selection

    > maintain a population of candidate solutions, let the "fittest" combine with each
    other to generate new "offspring" solutions.

---

* swarm algorithms

    > maintain a herd of candidate solutions, allow them to "explore" the environment,
    "communicate" with each other in order to seek the best spot to "go to".

    For example:

    * ant colony
    * bees
    * cuckoo search
    * particle sward
    * krill herd

* other metaheuristics:

    * harmony search
    * memetic algorithm
    * firefly algorithm



### Overview of the Method

---

Genetic algorithms (GAs) are amongst the simplest evolutionary approaches.

They are based on Charles Darwin's work on evolution by natural selection.

See (Goldberg, 1989) for a comprehensive overview
and (Simon, 2013) for extensions.


Here is the general idea of  a GA (there might be many)
to minimise a given objective/fitness function $f$
over a given domain $D$.


---

1. Generate a random initial population of individuals -- $n_\text{pop}$ points in $D$,
e.g., $n_\text{pop}=128$

2. Repeat until converged:

    a. evaluate the fitness of each individual
    b. select the pairs of the individuals for reproduction, the fitter
    should be selected more eagerly
    c. apply crossover operations to create offspring
    d. mutate randomly selected individuals slightly
    e. replace the old population with the new one



### Example Implementation - GA for k-means

---

Initial setup:

```{r,dependson='gendata3b',genetic1,cache=TRUE}
set.seed(123)

# simulation parameters:
npop <- 32
niter <- 100

# randomly generate an initial population of size `npop`:
pop <- lapply(1:npop, function(i) X[sample(nrow(X), k),])

# evaluate fitness of each individual:
cur_fitness <- sapply(pop, get_fitness, X)
cur_best_fitness <- min(cur_fitness)
best_fitness <- cur_best_fitness
```

Each individual in the population is just the set of $k$
candidate cluster centres represented as a matrix in $\mathbb{R}^{k\times p}$.

---


Let's assume that the fitness of each individual should a function of
the rank of the objective function's value
(smallest objective == highest rank == best fit).

For the crossover, we will sample pairs of individuals with
probabilities proportional to their fitness.

```{r,dependson='genetic1',genetic2,cache=TRUE}
selection <- function(cur_fitness) {
    npop <- length(cur_fitness)
    probs <- rank(-cur_fitness)
    probs <- probs/sum(probs)
    left  <- sample(npop, npop, replace=TRUE, prob=probs)
    right <- sample(npop, npop, replace=TRUE, prob=probs)
    cbind(left, right)
}
```


---


An example crossover combines each cluster centre
in such a way that we take a few coordinates of the "left" parent
and the remaining ones from the "right" parent (see below for an illustration):


```{r,dependson='genetic2',genetic3,cache=TRUE}
crossover <- function(pop, pairs, k, p) {
    old_pop <- pop
    pop <- pop[pairs[,2]]
    for (j in 1:length(pop)) {
        wh <- sample(p-1, k, replace=TRUE)
        for (l in 1:k)
            pop[[j]][l,1:wh[l]] <- old_pop[[pairs[j,1]]][l,1:wh[l]]
    }
    pop
}
```



---

![](figures/crossover){width=75%}

---

Mutation (occurring with a very small probability)
substitutes some cluster centre with a random vector from the input dataset.


```{r,dependson='genetic3',genetic4,cache=TRUE}
mutate <- function(pop, X, k) {
    for (j in 1:length(pop)) {
        if (runif(1) < 0.025) {
            szw <- sample(1:k, 1)
            pop[[j]][szw,] <- X[sample(nrow(X), length(szw)),]
        }
    }
    pop
}
```

---

We also need a function that checks if
the new cluster centres aren't too far away from the input points.

If it will happen that we have empty clusters, our solution is degenerate and
we must correct it.

All "bad" cluster centres will be substituted with randomly chosen points from $\mathbf{X}$.

Moreover, we will recompute the cluster centres as the componentwise arithmetic mean
of the closest points, just like in Lloyd's algorithm, to speed up convergence.

---


```{r,dependson='genetic4',genetic4b,cache=TRUE}
recompute_mus <- function(pop, X, k) {
    for (j in 1:length(pop)) {
        # get nearest cluster centres for each point:
        memb <- get.knnx(pop[[j]], X, 1)$nn.index
        sz <- tabulate(memb, k) # number of points in each cluster
        # if there are empty clusters, fix them:
        szw <- which(sz==0)
        if (length(szw)>0) { # random points in X will be new cluster centres
            pop[[j]][szw,] <- X[sample(nrow(X), length(szw)),]
            memb <- FNN::get.knnx(pop[[j]], X, 1)$nn.index
            sz <- tabulate(memb, k)
        }
        # recompute cluster centres - componentwise average:
        pop[[j]][,] <- 0
        for (l in 1:nrow(X))
            pop[[j]][memb[l],] <- pop[[j]][memb[l],]+X[l,]
        pop[[j]] <- pop[[j]]/sz
    }
    pop
}
```

---


We are ready to build our genetic algorithm to solve a k-means clustering  problem:

```{r,dependson='genetic4b',genetic5,cache=TRUE,eval=FALSE}
for (i in 1:niter) {
    pairs <- selection(cur_fitness)
    pop <- crossover(pop, pairs, k, p)
    pop <- mutate(pop, X, k)
    pop <- recompute_mus(pop, X, k)
    # re-evaluate fitness:
    cur_fitness <- sapply(pop, get_fitness, X)
    cur_best_fitness <- min(cur_fitness)
    # give feedback on what's going on:
    if (cur_best_fitness < best_fitness) {
        best_fitness <- cur_best_fitness
        best_mu <- pop[[which.min(cur_fitness)]]
        cat(sprintf("%5d: f_best=%10.5f\n", i, best_fitness))
    }
}
```

---

```{r,dependson='genetic5',genetic6,cache=TRUE,echo=FALSE}
<<genetic5>>
```

```{r,dependson='genetic6',genetic7,cache=TRUE}
print(get_fitness(best_mu, X))
print(get_fitness(res_tried_very_hard, X))
```

It works! :)

<!--

(\*) Interestingly, the above can be rewritten as:
\[
\min_{ (c_1,\dots,c_n)\in\{1,\dots,k\}^n }
\sum_{i=1}^n \left(
\sum_{\ell=1}^p \left(x_\ell^{(i)}-\mu_\ell^{(c_i)}\right)^2
\right)
\]
with $c_i$ denoting the cluster number (between $1$ and $k$) of the $i$-th input point
and the centre of the $j$-th cluster:
\[
\mu_\ell^{(j)} =
\frac{1}{|\{i: c_i=j\}|}
 \sum_{i: c_i=j} x_\ell^{(i)}
\]
being defined as the arithmetic mean of the coordinates
of all the input points belonging to that cluster.

> Here $|\{\cdots\}|$ denotes the number of elements in a given set.

-->






## Outro

### Remarks

---


For any $p\ge 1$, the search space type determines the problem class:

- $D\subseteq\mathbb{R}^p$ -- **continuous optimisation**

    In particular:
    - $D=\mathbb{R}^p$ -- continuous unconstrained
    - $D=[a_1,b_1]\times\dots\times[a_n,b_n]$ -- continuous with box constraints
    - constrained with $k$ linear inequality constraints

        $a_{1,1} x_1 + \dots + a_{1,p} x_p \le b_1$, ...,
        $a_{k,1} x_1 + \dots + a_{k,p} x_p \le b_k$

---

However, there are other possibilities as well:

- $D\subseteq\mathbb{Z}^p$ ($\mathbb{Z}$ -- the set of integers) -- **discrete optimisation**

    In particular:
    - $D=\{0,1\}^p$ -- 0--1 optimisation (hard!)


- $D$ is finite (but perhaps large, its objects can be enumerated) -- **combination optimisation**

    For example:
    - $D=$ all possible routes between two points on a map.

> These  optimisation tasks tend to be much harder than the continuous ones.

Genetic algorithms might come in handy in such cases.


---


Specialised methods, customised to solve a specific problem (like Lloyd's algorithm)
will often outperform generic ones (like SGD, genetic algorithms)
in terms of speed and reliability.


All in all, we prefer a suboptimal solution obtained by means of heuristics
to no solution at all.

---

Other interesting algorithms:

* Hill Climbing (a simple variation of GD with no gradient use)
* Simulated annealing
* CMA-ES
* Tabu search
* Particle swarm optimisation
* Artificial bee/ant colony optimisation
* Cuckoo Search


### Further Reading

#### {.allowframebreaks .unnumbered}

Recommended further reading:

- [@genetic]

Other:

- [@evolution]
- [@boyd_vandenberghe]
