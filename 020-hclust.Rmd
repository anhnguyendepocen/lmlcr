# Agglomerative Hierarchical Clustering {#chap:hclust}

<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->




{ LATEX \color{gray} }

**TODO** In this chapter, we will:

* concepts: clustering, data partition,

* Euclidean distance as a means to measure the (dis)similarity
between two objects (rows in a matrix)

* algorithms: agglomerative hierarchical clustering with single, complete and
average linkage

this is not our last encounter with clustering, more in chapter TODO


{ LATEX \normalcolor }





<!--

Exercise: dimensionality reduction with auto-encoders?

Exercise: apply genieclust

Exercise: implement MDS with optim?

-->



The aim of clustering (a.k.a. data segmentation or quantisation)
is to split the input dataset into *interesting*
subgroups in an unsupervised manner. Example applications of clustering include:

* taxonomisation, e.g.,
partition consumers to more "uniform"
groups to better understand who they are and what do they need,

* aggregation, e.g., to reduce the number of observations
by substituting them by "group representatives" or "prototypes",

* object detection in images, e.g.,
tumour tissues on PET/CT scans,

* complex networks analysis,
e.g., detecting communities in friendship,
retweets, and other networks,

* spatial data analysis, e.g., identifying densely populated areas
or traffic jams on maps,

* text analysis, e.g., for grouping documents into automatically
detected topics,

* fine-tuning of supervised learning algorithms,
e.g., recommender systems indicating content
that was rated highly by users from the same group
or learning multiple manifolds in a dimension reduction task.





## Dataset Partitions

### Label Vectors

Let's assume that the input data set $\mathbf{X}\in\mathbb{R}^{n\times p}$
consists of $n$ observations and that we wish to identify
$K$ clusters, for some $K\ge 2$ which gives the number of subgroups.
Such a clustering of $\mathbf{X}$ will be represented
by a vector $\boldsymbol{y}\in\{1, 2, \dots, K\}^n$
such that $y_i$ gives the *cluster ID* (cluster identifier or number,
an integer between
1 and K) of the $i$-th observation, $i=1,2,\dots,n$.
Thus, we can think of clustering as of the process of labelling
or assigning colours to each data point (for example, 1 = black, 2 = red,
3 = green, 4 = blue etc.).
The only restriction we impose on each such *label* vector is that
*each of the $K$ labels must occur at least once*.
This will guarantee that we have indeed a division of $\mathbf{X}$
into $K$ subgroups, and not less.


{ BEGIN remark }
Given an integer $K$
and a vector $\boldsymbol{y}$ with elements $\{1, 2, \dots, K\}$,
for instance:

```{r label-vector-check1}
y <- c(3, 1, 1, 2, 2, 1, 2, 3, 1, 1, 1)
K <- 3 # by the way, it's max(y)
```

we can check whether it represents a proper label vector by
computing the number of occurrences of each label:

```{r label-vector-check2}
tabulate(y, K) # how many 1s, ..., Ks are there in y, respectively
all(tabulate(y, K) > 0) # do we have at least 1 occurrence of each label?
```

or, for example, determining the set of all unique
values in $\boldsymbol{y}$:

```{r label-vector-check3}
unique(y)
length(unique(y)) == K # do we have K unique values in y?
```
{ END remark }


Each such label vector  yields a $K$-partition
of the input data set, that is, its grouping into
$K$ disjoint subsets -- every element in $\mathbf{X}$
belongs to one (and only one) cluster.




### K-Partitions

As it is beneficial for us to be *gently* exposed to as many easy
mathematical formalisms as possible, let us use some set-theoretic notation
to come up with a rigorous definition of a dataset grouping.
Formally, *clustering* aims to find a *special kind*
of a *$K$-partition* of the input dataset,
which -- without loss of generality -- we can identify
with the set $\{1, 2, \dots, n\}$ of indexes of observations (rows)
in $\mathbf{X}$.




{ BEGIN definition }
We say that $\mathcal{C}=\{C_1,C_2,\dots,C_K\}$ forms
a *$K$-partition* of the set $\{1, 2, \dots, n\}$,
whenever:

* $\bigcup_{k=1}^K C_k=C_1\cup C_2\cup\dots\cup C_K=\{1, 2, \dots, n\}$
(the union of all clusters
is the whole set; every point is assigned to some cluster;
no point is neglected),

* $C_k\cap C_l=\emptyset$ for all $k\neq l$ (clusters are pairwise disjoint;
their intersection is empty; they share no common elements),

* $C_k\neq\emptyset$ for all $k$ (each cluster is nonempty).

<!-- * $C_k\subseteq\{1, 2, \dots, n\}$ for all $k$, -->
{ END definition }


<!--(\*) There is a one-to-one correspondence between partitions
and equivalence relations-->

In the label vector representation,
we assume that $y_i = k$ iff $i\in C_k$, i.e.,
the $i$-th point is assigned the $k$-th label if and only if
it belongs to the $k$-th cluster.
It is easily seen that each label vector yields
a grouping of $\{1, 2, \dots, n\}$ that obviously fulfil
the first two conditions in the above definition,
and that the third property is fulfilled by assuming
that $(\forall k\in\{1,2,\dots,K\})$ $(\exists i\in\{1,2,\dots,n\})$
$y_i = k$ (read: *for every $k$ there exists $i$ such that...*),
i.e., each label occurs at least once in $\boldsymbol{y}$.





{ BEGIN remark }
The benefit of using mathematical notation is that we are super-precise,
efficient, and universal. Natural tongue is vague and leaves much room
for interpretation. We used so many *words* to explain the above concepts.
As soon as we further develop our skills of *speaking* the language
of mathematics
(note that we've started with basic phrases such as *Good Afternoon*,
*I am Sorry*, and *Thank You*), it'll be more natural for
us to just write:

"Let $\mathcal{C}=\{ C_1,\dots, C_K \}$ s.t.
$\textstyle\bigcup_{k=1}^K C_k = \{1,\dots, n\}$,
$(\forall k\neq l)$ $C_k\cap C_l = \emptyset$, $C_k\neq \emptyset$"

without all the talking. This is what more advanced books related
to machine learning and statistics do. We'll get to that.
By the way, thanks to this it's relatively easy to read maths
papers (especially those from the 1950s) written in other languages
(such as Russian, German or French).
{ END remark }



In theory, the number of possible $K$-partitions of a set
with $n$ elements is given by
*the Stirling number of the second kind*:
\[
\left\{{n \atop K}\right\}=\sum_{{j=0}}^{{K}} \frac{(-1)^{{j}}
(K-j)^{n}}{j! (K-j)!} = \frac{K^n}{K!}
- \frac{(K-1)^n}{(K-1)!}
+ \frac{(K-2)^n}{2 (K-2)!}
- \frac{(K-3)^n}{6 (K-3)!}
+ \dots,
\]
where $n!=1\cdot 2 \cdot 3\cdot \dots\cdot n$ with $0!=1$ ($n$-factorial).
In particular, already $\left\{{n \atop 2}\right\}=2^{n-1}-1$
and $\left\{{n \atop K}\right\}\simeq K^n/K!$ for large $n$ -- that is a lot.




Table: (\#tab:stirling) The number of possible partitions of a dataset with $n$ elements to $K$ clusters, denoted $\left\{{n \atop K}\right\}$, grows rapidly as $n$ increases

$n$ | $\left\{{n \atop 2}\right\}$ | $\left\{{n \atop 3}\right\}$ | $\left\{{n \atop 4}\right\}$ | $\left\{{n \atop 5}\right\}$ | $\left\{{n \atop 6}\right\}$ | $\left\{{n \atop 7}\right\}$ | $\left\{{n \atop 8}\right\}$ | $\left\{{n \atop 9}\right\}$ | $\left\{{n \atop 10}\right\}$
---:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|-----------------------------:|
2   | 1                            |                              |                              |                              |                              |                              |                              |                              |                              |
3   | 3                            |                              | 1                            |                              |                              |                              |                              |                              |                              |
4   | 7                            |                              | 6                            | 1                            |                              |                              |                              |                              |                              |
5   | 15                           | 25                           | 10                           | 1                            |                              |                              |                              |                              |                              |
6   | 31                           | 90                           | 65                           | 15                           | 1                            |                              |                              |                              |                              |
7   | 63                           | 301                          | 350                          | 140                          | 21                           | 1                            |                              |                              |                              |
8   | 127                          | 966                          | 1701                         | 1050                         | 266                          | 28                           | 1                            |                              |                              |
9   | 255                          | 3025                         | 7770                         | 6951                         | 2646                         | 462                          | 36                           | 1                            |                              |
10  | 511                          | 9330                         | 34105                        | 42525                        | 22827                        | 5880                         | 750                          | 45                           | 1                            |





### "Interesting" Partitions

We are not just interested in "any" partition, because
there are simply way too plentiful (compare Table \@ref(tab:stirling);
some of them will certainly be more meaningful or valuable than others.
However, even one of the most famous ML textbooks provides us with only
a vague hint of what we might be looking for:



{ BEGIN definition }
(or, rather, a "definition").
Clustering concerns "segmenting a collection of objects into subsets
so that those within each cluster are more **closely related**
to one another than objects assigned to different clusters" [@esl].
{ END definition }


It is not uncommon (see, e.g., [@many_clustering_algorithms])
to equate the general definition of data clustering problems with... the
particular outputs generated by specific clustering algorithms. It some sense,
that sounds fair -- clustering is both art and science
(compare [@clustering_science_art]).
From this perspective, we might be interested in
identifying the two main types of clustering algorithms:


* *parametric* (model-based) --
find clusters of specific shapes or following specific multidimensional
probability distributions,
e.g., K-means,
expectation-maximisation for Gaussian mixtures (EM),
average linkage agglomerative clustering;

* *nonparametric* (model-free) -- identify high-density or
well-separable regions, perhaps in the presence of noise points,
e.g., single linkage agglomerative clustering, Genie, (H)DBSCAN, BIRCH.




In this chapter we'll take a look at
*(agglomerative) hierarchical clustering* algorithms which build
a cluster structure
*from the bottom*, i.e., by merging small points groups to form larger ones.
In Chapter \@ref(chap:kmeans), we'll discuss the K-means algorithm,
which seeks "good" cluster prototypes.





## Euclidean Distance {#sec:euclidean}


In order to build our first clustering algorithm,
we need to refer to the notion of a *distance*,
which
quantifies the extent to which two observations
in a dataset (two rows in $\mathbf{X}$) are different from
or similar to each other.

In this chapter we will be dealing with the most natural,
"school" straight-line distance, called the Euclidean metric.
It works as if we were measuring how two points
are far away from each other with a ruler.

Given two $p$-tuples $\boldsymbol{u}=(u_1,\dots,u_p)$
and $\boldsymbol{v}=(p_1,\dots,v_p)$, the *Euclidean metric*
is a function $d$ such that:
\[
d(\boldsymbol{u}, \boldsymbol{v}) = \sqrt{
(u_1-v_1)^2 + (u_2-v_2)^2 + \dots + (u_p-v_p)^2
}
=
\sqrt{
\sum_{i=1}^p (u_i-v_i)^2
}.
\]
The Euclidean metric will often also be denoted with
$\|\boldsymbol{u}-\boldsymbol{v}\|$.



{ BEGIN remark }
$\sum$ (Greek capital letter Sigma) denotes summation.
Read "$\sum_{i=1}^p z_i$" as "the sum of $z_i$ for $i$ from $1$ to $p$";
this is just a convenient shorthand for $z_1+z_2+\dots+z_p$.
{ END remark }



{ BEGIN exercise }
Consider the following $\mathbf{X}\in\mathbb{R}^{3\times 2}$:
\[
\mathbf{X} = \left[\begin{array}{cc}
0 & 0 \\
1 & 0 \\
\frac{1}{4} & 1 \\
\end{array}\right]
\]
Calculate (by hand)
$d(\mathbf{x}_{1,\cdot}, \mathbf{x}_{2,\cdot})$,
$d(\mathbf{x}_{1,\cdot}, \mathbf{x}_{3,\cdot})$,
$d(\mathbf{x}_{2,\cdot}, \mathbf{x}_{3,\cdot})$,
$d(\mathbf{x}_{1,\cdot}, \mathbf{x}_{1,\cdot})$, and
$d(\mathbf{x}_{2,\cdot}, \mathbf{x}_{1,\cdot})$.
{ END exercise }



In order to compute all pairwise distances between the rows in a
matrix with R, we can call the `dist()` function
(see also Figure \@ref(fig:euclidean-distance-fig) for a graphical illustration):

```{r euclidean-distance}
X <- rbind(
    c(0,    0),
    c(1,    0),
    c(0.25, 1)
)
print(X)
dist(X)
````




```{r euclidean-distance-fig,echo=FALSE,fig.cap="Example points and Euclidean distances between them"}
plot(X[,1], X[,2], asp=1)
text(X[1,1], X[1,2], sprintf("(%g,%g)", X[1,1], X[1,2]), pos=2)
text(X[2,1], X[2,2], sprintf("(%g,%g)", X[2,1], X[2,2]), pos=4)
text(X[3,1], X[3,2], sprintf("(%g,%g)", X[3,1], X[3,2]), pos=4)
polygon(X[,1], X[,2])
text(mean(X[c(1,2),1]),mean(X[c(1,2),2]), round(dist(X[c(1,2),]),4), pos=3)
text(mean(X[c(1,3),1]),mean(X[c(1,3),2]), round(dist(X[c(1,3),]),4), pos=4)
text(mean(X[c(3,2),1]),mean(X[c(3,2),2]), round(dist(X[c(3,2),]),4), pos=2)
```


Note that only 3 values were reported. This is due to the fact
that:

* the distance between a point and itself is always 0,
i.e., $d(\boldsymbol{u},\boldsymbol{u})=0$;

* the order of the points (from-to or to-from) doesn't matter,
we have $d(\boldsymbol{u},\boldsymbol{v})=d(\boldsymbol{v},\boldsymbol{u})$
(this is called *symmetry*).


If we want the full distance matrix, we can call:

```{r euclidean-distance-matrix}
as.matrix(dist(X))
```

Note the zeros on the main diagonal and that the matrix is symmetric
around it.


{ BEGIN remark }
Overall, there are $n(n-1)/2$ "interesting" pairwise distances
for a dataset of $n$ points. In R, these are stored using `numeric` type,
each being an 8-byte floating point number. Therefore, by calling `dist()`
on matrices with too many rows we can easily run out of memory;
already for $n=100{,}000$ we need ca. 40 GB of RAM.
{ END remark }

{ BEGIN remark }
Later <!-- TODO -->
we will mention that there are many possible distances,
allowing to measure the similarity of points not only in $\mathbb{R}^p$,
but also character strings, protein sequences, film ratings, etc.;
there is even an encyclopedia of distances (see [@encyclopedia_distances])!
{ END remark }






## Hierarchical Clustering at a Glance


Hierarchical methods generate a whole hierarchy
of *nested* partitions. A $K$-partition for any $K$
can be extracted later at any time.
In the case of *agglomerative* hierarchical algorithms:

* at the lowest level of the hierarchy, each point belongs to its own
cluster (there are $n$ singletons);

* at the highest level of the hierarchy,
there is one cluster that embraces all the points;

* moving from the $i$-th to the $(i+1)$-th level,
we select (somehow; see below) a pair of clusters to be merged.


Let's consider the  Sustainable Society Indices Dataset
(see Section \@ref(sec:ssi))
that measures the Human,
Environmental,  and Economic Wellbeing
in each country on the scale $[0,10]$.

```{r ssi-load}
ssi <- read.csv("datasets/ssi_2016_dimensions.csv",
    comment.char="#")
head(ssi)
```

In other to better understand how the algorithms of interest work,
we'll restrict ourselves a smaller sample of countries, say,
to 37 members of the OECD:

```{r ssi-oecd}
oecd <- c("Australia", "Austria", "Belgium", "Canada", "Chile", "Colombia",
"Czech Republic", "Denmark", "Estonia", "Finland", "France", "Germany",
"Greece", "Hungary", "Iceland", "Ireland", "Israel", "Italy", "Japan",
"Korea, South", "Latvia", "Lithuania", "Luxembourg", "Mexico",
"Netherlands", "New Zealand", "Norway", "Poland", "Portugal",
"Slovak Republic", "Slovenia", "Spain", "Sweden", "Switzerland",
"Turkey", "United Kingdom", "United States")
ssi <- ssi[ssi[,"Country"] %in% oecd, ]
X <- as.matrix(ssi[,-1]) # everything except the Country column
dimnames(X)[[1]] <- ssi[,1] # set row names
head(X)
dim(X) # n and p
```

Hence, we have $\mathbf{X}\in\mathbb{R}^{`r nrow(X)` \times `r ncol(X)`}$.
Note that the matrix in R has row and column names set
for greater readability.



. . .

The most basic implementation of a few
agglomerative hierarchical clustering algorithms
is provided by the `stats::hclust()` function, which works on a pairwise
distance matrix.
By default, the method computes the so-called *complete* linkage.
We'll explain what that means later, let's just simply use it in
a "black-box" fashion now, i.e., without going into details:

```{r dist-hclust-complete}
# Euclidean distances between all pairs of points:
D <- dist(X)
h <- hclust(D) # method="complete"
print(h)
```


{ BEGIN remark }
`stats::hclust()` refers to the `hclust()` function
in the `stats` package. This is a package that comes with R, therefore
there should be no need to load it explicitly. Otherwise, we would
have to use the complete descriptor (with the `stats::` prefix)
or call `library("stats")` to attach the package's namespace.
{ END remark }

The obtained hierarchy can be *cut* at an arbitrary level
by applying the `cutree()` function. Let's use $K=3$:

```{r cutree}
K <- 3
y <- cutree(h, K) # extract the 3-partition
```

A picture is worth a thousand words, therefore
let's generate a map with the `rworldmap` package (see Figure \@ref(fig:ssi-map)):

```{r ssi-map,echo=-c(1,2,7),results="hide",cache=TRUE,message=FALSE,fig.cap="OECD countries grouped w.r.t. the SSI dimensions"}
par(ann=FALSE) # no axes
par(mar=c(0, 0, 0, 0)) # no figure margins
library("rworldmap") # see the package's manual for details
mapdata <- data.frame(Country=dimnames(X)[[1]], Cluster=y)
mapdata <- joinCountryData2Map(mapdata, joinCode="NAME",
  nameJoinColumn="Country")
mapCountryData(mapdata, nameColumnToPlot="Cluster",
    catMethod="categorical", missingCountryCol="white",
    colourPalette=palette.colors(K, "Okabe-Ito"),
    mapTitle="", nameColumnToHatch="Cluster")
legend("bottomleft", bg="white", fill=palette.colors(K, "Okabe-Ito"),
    sprintf("Cluster %d", 1:K), density=c(NA, 50, 25),
    title="Complete linkage")
```

We have so many questions now! First of all, why the countries were grouped
in such a way? Well, this is because the algorithm is programmed
to perform certain computational steps that lead to this particular solution.
We'll discuss them in very detail soon.

A better question is: what characterises the countries in each cluster?
To explore possible answers to this question, we can compute, e.g.,
the average indicators in each group:

```{r ssi-colmeans}
aggregate(as.data.frame(X), list(Cluster=y), mean)
```

Therefore, on average, the countries in the 1st cluster
have higher Economic Wellbeing scores than the countries in the 2nd cluster,
and these outperform those from the 3rd cluster. On the other hand,
the 3rd cluster countries score much more highly on the Environmental
Wellbeing dimension. This summary, however, is valid only if
the countries in each cluster are score similarly on each scale
(are homogeneous).

Luckily, we have only 3 dimensions, therefore we can visualise
different two-dimensional projections of this dataset.
Figure \@ref(fig:ssi-complete-pairs)
depicts a slightly "tuned up" (we've added the ISO 3-letter country codes)
version of a scatter plot matrix
that we would normally obtain by calling `pairs(X, col=y, pch=y)`.



```{r ssi-complete-pairs,echo=FALSE,fig.height=6,fig.cap="Scatterplot matrix for the SSI dimensions with the 3-partition generated by complete linkage"}
#pairs(X, col=y, pch=y)
par(mfcol=c(2,2))

country_iso3 <- as.character(mapdata@data$ISO3[match(dimnames(X)[[1]], mapdata@data$Country)])

plot(X[,3], X[,2], col=y, pch=y,
    xlab=dimnames(X)[[2]][3], ylab=dimnames(X)[[2]][2])
text(X[,3], X[,2], col=y, country_iso3, pos=1, xpd=TRUE)

plot.new()

plot(X[,1], X[,2], col=y, pch=y,
    xlab=dimnames(X)[[2]][1], ylab=dimnames(X)[[2]][2])
text(X[,1], X[,2], col=y, country_iso3, pos=1, xpd=TRUE)

plot(X[,1], X[,3], col=y, pch=y,
    xlab=dimnames(X)[[2]][1], ylab=dimnames(X)[[2]][3])
text(X[,1], X[,3], col=y, country_iso3, pos=1, xpd=TRUE)


```

In seems that Economic Wellbeing is the deciding factor for distinguishing
between these countries. We see on the Economic vs. Environmental Wellbeing plot
that the clusters are nicely separated. Human Wellbeing, on the other hand,
is pretty "mixed up" across the clusters.





## Cluster Dendrograms


A *dendrogram* can provide us with some insight into the underlying
data structure as well as with some hints about how many clusters (K)
should extract.


```{r ssi-dendrogram-complete,echo=-1,fig.cap="Cluster dendrogram generated by complete linkage",echo=-1}
par(mar=c(0.5, 2, 0.5, 0.5))
# Plotting of dendrogram: essentially plot(hclust(dist(X)))
plot(h, xlab=NA, main=NA); box()
```

Figure \@ref(fig:ssi-dendrogram-complete) depicts
the dendrogram corresponding to the performed
agglomerative hierarchical clustering.

A dendrogram is a tree-like diagram whose nodes represent
clusters at different stages of the algorithm's run:

* The *leaves* of the tree, i.e., the nodes at the very bottom,
stand for the individual data points (recall that in agglomerative clustering
we start with $n$ singletons). In our case, these are the 37  OECD countries.

* Each two-teeth "fork" represents two clusters being merged at some
iteration. For example, at some stages we have clusters like:
{Italy} and {Portugal, Spain} which are merged so that we obtain
{Italy, Portugal, Spain}.
Moreover, the union of {Chile, Israel} and {Mexico, Turkey}
gives {Chile, Israel, Mexico, Turkey}.

* At the tree top ("root" -- the tree is plotted upside down)
we have a state where all the clusters
have been merged with each other (there is one cluster consisting
of all the points).

* The tree can be "cut" at any level so as to obtain
a clustering to any number of subgroups.
In particular, if we cut the tree at height $\simeq 4$, we will
get a 3-partition from Figure \@ref(fig:ssi-complete-pairs).




## Linkage Functions

Let's formalise the agglomerative hierarchical clustering process.
Initially ("iteration 0"),
each of the $n$ points is a member of its own cluster.
Let $\mathcal{C}^{(0)}$ denote this $n$-partition:
\[
\mathcal{C}^{(0)}=\left\{
    C_1^{(0)}, C_2^{(0)}, \dots, C_n^{(0)}
\right\},
\]
where $C_i^{(0)}$ is the cluster that consists of the $i$-th point only,
i.e., $C_i^{(0)}=\{i\}$.

While an agglomerative  hierarchical clustering algorithm is being computed,
there are $n-k$ clusters at the $k$-th step of the procedure,
\[
\mathcal{C}^{(k)}=\left\{
C_1^{(k)},C_2^{(k)},\dots,C_{u-1}^{(k)},
C_u^{(k)},
C_{u+1}^{(k)},\dots,C_{v-1}^{(k)},
C_v^{(k)},
C_{v+1}^{(k)},\dots,C_{n-k}^{(k)}
\right\},
\]
where $k=0,1,\dots,n-1$.

When proceeding from step $k$ to $k+1$,
we determine two clusters $C_u^{(k)}$ and $C_v^{(k)}$, $u<v$,
to be *merged* so that the partition at the higher level
is of the form:
\[
\mathcal{C}^{(k+1)} = \left\{
C_1^{(k)},C_2^{(k)},\dots,C_{u-1}^{(k)},
C_u^{(k)}{\cup C_v^{(k)}},
C_{u+1}^{(k)},\dots,C_{v-1}^{(k)},
C_{v+1}^{(k)},\dots,C_{n-k}^{(k)}
\right\}.
\]

<!--
Thus, $(\mathcal{C}^{(0)}, \mathcal{C}^{(1)}, \dots, \mathcal{C}^{(n-1)})$
form a sequence of *nested* partitions of
the input dataset
with the last level being just one big cluster,
$\mathcal{C}^{(n-1)}=\left\{ \{\mathbf{x}_{1,\cdot},\mathbf{x}_{2,\cdot},\dots,\mathbf{x}_{n,\cdot}\} \right\}$.
-->

It is evident that this way we will arrive at a 1-partition (a single
boring cluster consisting of all the points),
$\mathcal{C}^{(n-1)}=\left\{ C_1^{(n-1)} \right\}$
with $C_1^{(n-1)}=\{1,2,\dots,n\}$.




{ BEGIN example }
Restricting ourselves to the first 5 countries
on the left in Figure \@ref(fig:ssi-dendrogram-complete), we have:

\[
\begin{array}{llll}
\mathcal{C}^{(0)} & = & \left\{ \{\text{Latvia}\}, \{\text{Lithuania}\}, \{\text{Poland}\}, \{\text{Denmark}\}, \{\text{Switzerland}\} \right\},& {\footnotesize (C_{2}^{(0)}\cup C_{3}^{(0)})}\\
\mathcal{C}^{(1)} & = & \left\{ \{\text{Latvia}\}, \{\text{Lithuania}, \text{Poland}\}, \{\text{Denmark}\}, \{\text{Switzerland}\} \right\},    & {\footnotesize (C_{1}^{(1)}\cup C_{2}^{(1)})}\\
\mathcal{C}^{(2)} & = & \left\{ \{\text{Latvia}, \text{Lithuania}, \text{Poland}\}, \{\text{Denmark}\}, \{\text{Switzerland}\} \right\},        & {\footnotesize (C_{2}^{(2)}\cup C_{3}^{(2)})}\\
\mathcal{C}^{(3)} & = & \left\{ \{\text{Latvia}, \text{Lithuania}, \text{Poland}\}, \{\text{Denmark}, \text{Switzerland}\} \right\},            & {\footnotesize (C_{1}^{(3)}\cup C_{2}^{(3)})}\\
\mathcal{C}^{(4)} & = & \left\{ \{\text{Latvia}, \text{Lithuania}, \text{Poland}, \text{Denmark}, \text{Switzerland}\} \right\}.                & \\
\end{array}
\]
{ END example }




There is one component missing -- how to determine the pair
of clusters $C_u^{(k)}$ and $C_v^{(k)}$ to be merged
at the $k$-th iteration?
For classic agglomerative clustering algorithms,
we choose two clusters that are *closest to each other* -- they minimise
the inter-cluster distance.

Formally, we write the condition upon our  decision is based as:
\[
\min_{u,v: u < v} d^*(C_u^{(k)}, C_v^{(k)}),
\]
(read: "find $u$ and $v$ such that $u<v$ and for which
$d^*(C_u^{(k)}, C_v^{(k)})$ is the smallest"),
where $d^*(C_u^{(k)}, C_v^{(k)})$ is the *distance* between two clusters
$C_u^{(k)}$ and $C_v^{(k)}$.

Note that usually we consider only the distances between *individual points*,
not sets of points, at least this is what we've done in
Section \@ref(sec:euclidean).
Therefore, what we need to do is to introduce $d^*$ as a suitable extension
of the Euclidean metric $d$.

First, we will assume that $d^*(\{i\}, \{j\})=
d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot})$, i.e., the distance between
singleton clusters is the same as the distance between the corresponding
points (rows in $\mathbf{X}$) themselves.

As far as more populous point groups are concerned, there are many popular
choices of $d^*$ (which in the context of hierarchical clustering we call
*linkage functions*):

- single linkage -- the distance between two clusters is equal to
the distance between the closest pair of points:

    \[
    d_\text{S}^*(C_u^{(k)}, C_v^{(k)}) =
    \min_{i\in C_u^{(k)}, j\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}),
    \]

- complete linkage -- we choose the pair of points farthest away:

    \[
    d_\text{C}^*(C_u^{(k)}, C_v^{(k)}) =
    \max_{i\in C_u^{(k)}, j\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot}),
    \]

- average linkage -- we consider the average distance:

    \[
    d_\text{A}^*(C_u^{(k)}, C_v^{(k)}) =
    \frac{1}{|C_u^{(k)}| |C_v^{(k)}|} \sum_{i\in C_u^{(k)}}\sum_{j\in C_v^{(k)}} d(\mathbf{x}_{i,\cdot},\mathbf{x}_{j,\cdot})
    \]
    where $|C_u^{(k)}|$ and $|C_v^{(k)}|$ denote the total number of
    points in these two clusters.

An illustration of the way different linkages are computed
is given in Figure \@ref(fig:linkages).



```{r linkages,echo=FALSE,fig.cap="In single linkage, we find the closest pair of points; in complete linkage, we seek the pair furthest away from each other; in average linkage, we determine the arithmetic mean of all inter-cluster pairwise distances"}
set.seed(123)
par(mar=rep(0.5,4))
XXX <- jitter(t(as.matrix(iris[,2:3])))
y <- as.numeric(iris[,5])
XXX[,y==3] <- c(2, 1)*XXX[,y==3]+c(-0.5,-0.5)
cy <- c(1,1,1)[y]
py <- c(16,17,19)[y]

d <- as.matrix(dist(t(XXX)))
d12 <- d[1:50, 51:100]
d23 <- d[51:100, 101:150]
d13 <- d[1:50, 101:150]

kol1 <- rgb(0.8,0.3,0.0,0.02)
kol2 <- rgb(0.0,0.3,0.8,0.02)
kol3 <- rgb(0.3,0.8,0.3,0.02)
kol1b <- rgb(0.8,0.3,0.0,1.0)
kol2b <- rgb(0.0,0.3,0.8,1.0)
kol3b <- rgb(0.3,0.8,0.3,1.0)

par(mfrow=c(1,3))
# single
plot(XXX[1,], XXX[2,], col=y, pch='.', asp=1, axes=FALSE, xlab=NA, ylab=NA)
box()
# stopifnot(min(d23) < min(d13) && min(d23) < min(d12))
IDXXX_single <- which(min(d23) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol3b, lwd=2, lty=1)
IDXXX_single <- which(min(d13) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol1b, lwd=2, lty=1)
IDXXX_single <- which(min(d12) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol2b, lwd=2, lty=1)
legend("bottomright", legend="single linkage", bg="white")
points(XXX[1,], XXX[2,], col=cy, pch=py)

# complete
plot(XXX[1,], XXX[2,], col=y, pch='.', asp=1, axes=FALSE, xlab=NA, ylab=NA)
box()
IDXXX_single <- which(max(d23) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol2b, lwd=2, lty=1)
IDXXX_single <- which(max(d13) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol1b, lwd=2, lty=1)
IDXXX_single <- which(max(d12) == d, arr.ind=TRUE)[1,]
lines(XXX[1,IDXXX_single], XXX[2,IDXXX_single], col=kol3b, lwd=2, lty=1)
legend("bottomright", legend="complete linkage", bg="white")
points(XXX[1,], XXX[2,], col=cy, pch=py)

# average
plot(XXX[1,], XXX[2,], col=y, pch='.', asp=1, axes=FALSE, xlab=NA, ylab=NA)
box()
#c(mean(d12), mean(d13), mean(d23))
for (i in sample(1:50)) {
    segments(XXX[1,0+i], XXX[2,0+i], XXX[1,51:100], XXX[2, 51:100], col=kol1, lty=1)
    segments(XXX[1,0+i], XXX[2,0+i], XXX[1,101:150], XXX[2, 101:150], col=kol2, lty=1)
    segments(XXX[1,50+i], XXX[2, 50+i], XXX[1,101:150], XXX[2, 101:150], col=kol3, lty=1)
}
legend("bottomright", legend="average linkage", bg="white")
points(XXX[1,], XXX[2,], col=cy, pch=py)
```








{ BEGIN exercise }
Here are the pairwise distances between the first 5 countries
(w.r.t. the SSI dimensions)
on the left in Figure \@ref(fig:ssi-dendrogram-complete):

```{r ssi_dist_5}
as.matrix(
    dist(X[c("Latvia", "Lithuania", "Poland", "Denmark", "Switzerland"),])
)
```

Calculate (by hand):

* $d_\text{S}\left(\{\text{Latvia}\}, \{\text{Lithuania}, \text{Poland}\}\right)$,

* $d_\text{C}\left(\{\text{Latvia}\}, \{\text{Lithuania}, \text{Poland}\}\right)$,

* $d_\text{A}\left(\{\text{Latvia}\}, \{\text{Lithuania}, \text{Poland}\}\right)$,

* $d_\text{S}\left(\{\text{Latvia}, \text{Lithuania}, \text{Poland}\}, \{\text{Denmark}, \text{Switzerland}\}\right)$,

* $d_\text{C}\left(\{\text{Latvia}, \text{Lithuania}, \text{Poland}\}, \{\text{Denmark}, \text{Switzerland}\}\right)$,

* $d_\text{A}\left(\{\text{Latvia}, \text{Lithuania}, \text{Poland}\}, \{\text{Denmark}, \text{Switzerland}\}\right)$.
{ END exercise }





. . .

<!--Assuming $d_\text{S}^*$, $d_\text{C}^*$ or $d_\text{A}^*$
in the aforementioned procedure leads to single, complete or average
linkage-based agglomerative hierarchical clustering algorithms,
respectively
(referred to as single linkage etc. for brevity).-->



We can perform agglomerative clustering of our dataset with respect
to different linkages by setting the `method` argument in the `hclust()`
function:

```{r ssi-linkages}
D <- dist(X)
hs <- hclust(D, method="single")
hc <- hclust(D, method="complete") # the default
ha <- hclust(D, method="average")
```



Figure \@ref(fig:ssi-dendrograms) depicts the dendrograms
corresponding to single and average linkages,
which we can plot by calling `plot(hs)` and `plot(ha)`.



```{r ssi-dendrograms,fig.height=6,fig.cap="Cluster dendrograms generated by single and average linkages",echo=FALSE}
par(mfrow=c(2,1))
par(mar=c(0.5,2,0.5,0.5))
h <- hclust(D, method="single")
plot(h, xlab=NA, main=NA, sub=NA); box()
legend("topright", legend="single", bg="white")
# h <- hclust(D, method="complete")
# plot(h, xlab=NA, main=NA, sub=NA); box()
# legend("topright", legend="complete", bg="white")
h <- hclust(D, method="average")
plot(h, xlab=NA, main=NA, sub=NA); box()
legend("topright", legend="average", bg="white")
```

{ BEGIN remark }
The `height` on the Y axis in each dendrogram's plot
represents the distance between the merged clusters (as measured
by the assumed linkage function).
{ END remark }


Both complete and average linkages give us nicely "balanced" cluster hierarchy
(average linkage identified Colombia as an "outlier" though).
The obtained groupings are similar to some extent, yet, they are not
identical -- both algorithms are based on different heuristics.
However, single linkage tends to produce somehow "ugly" trees -- it's
often be the case that it'll produce few large clusters and a lot of small ones.

The reader is encouraged to perform a similar "debugging" of the obtained
clusterings (say, for the 3- and 4-partitions) as we performed above
(compute the average features in each cluster, draw pairwise scatterplots).


<!--
Figure \@ref(fig:linkages-hier52) compares the 5-, 4- and 3-partitions
obtained by applying the 3 above linkages. Note that it's in very nature of
the single linkage algorithm that it's highly sensitive to outliers.

```{r linkages-hier52,fig.height=6,fig.cap="3 cuts of 3 different hierarchies",echo=FALSE}
# #par(c(1,1,0.5,0.5))
# par(mfrow=c(3,3))
# for (k in 5:3) {
#     ccc <- cutree(hs, k=k)
#     plot(X, col=ccc, pch=ccc, ann=FALSE)
#     legend("top", legend=sprintf("k=%d, single", k), bg="white")
#     ccc <- cutree(hc, k=k)
#     plot(X, col=ccc, pch=ccc, ann=FALSE)
#     legend("top", legend=sprintf("k=%d, complete", k), bg="white")
#     ccc <- cutree(ha, k=k)
#     plot(X, col=ccc, pch=ccc, ann=FALSE)
#     legend("top", legend=sprintf("k=%d, average", k), bg="white")
# }
```
-->




<!--
```{r ssi-linkages-genieclust}
#library("genieclust")
#plot(genieclust::gclust(D))
```
-->


## Exercises


{ BEGIN exercise }
Calculate the complete linkage agglomerative clustering algorithm
by hand on the space of the 3 SSI dimensions for
France, Ireland, Spain, Colombia, Austria, and Germany.
For each iteration of the algorithm, print the members of the
current partition and the distances between each pair of clusters.
{ END exercise }

{ BEGIN solution }
The pairwise distance matrix is:

```{r ex-hclust-byhand-1}
X2 <- X[c("France", "Ireland", "Spain", "Colombia", "Austria", "Germany"),]
D2 <- dist(X2)
as.matrix(D2)
```

```{r ex-hclust-byhand-2, results='asis', echo=FALSE}
hclust_byhand <- function(X2, method, agfun, istart=1) {
    n <- nrow(X2)

    country_iso3 <- as.character(mapdata@data$ISO3[match(dimnames(X2)[[1]], mapdata@data$Country)])
    dimnames(X2)[[1]] <- country_iso3

    D2 <- dist(X2)
    h <- hclust(D2, method=method)
    DM <- as.matrix(D2)
    for (i in istart:n) {
        K <- n-i+1
        cc <- cutree(h, K)
        part <- sapply(split(dimnames(X2)[[1]], cc), function(memb) {
            sprintf("\\{%s\\}", paste(memb, collapse=","))
        })
        cat(sprintf("$\\mathcal{C}^{(%d)} = \\left\\{ %s \\right\\}$\n\n",
            i-1, paste(part, collapse=",")))


        if (i == n) break

        cat(sprintf("Inter-cluster distances (%s linkage):\n\n", method))
        DT <- matrix(0, nrow=K, ncol=K, dimnames=list(part,part))

        for (i in 1:(K-1)) {
            for (j in (i+1):K) {
                DT[i,j] <- DT[j,i] <-
                    agfun(DM[as.matrix(expand.grid(which(cc==i), which(cc==j)))])
            }
        }

        #cat("\\[")
        #print(xtable::xtable(DT), floating=FALSE,
        #    tabular.environment="array",
        #    sanitize.rownames.function=NULL,
        #    sanitize.colnames.function=NULL)
        #cat("\\]")

        print(knitr::kable(DT, digits=2))

        w <- which(DT==min(DT[upper.tri(DT)]), arr.ind=TRUE)
        cat(sprintf("\nMerging %s and %s (distance=%.2f)\n\n",
            dimnames(DT)[[1]][w[1,1]], dimnames(DT)[[2]][w[1,2]],
            DT[w[1,,drop=FALSE]]))
    }
}


hclust_byhand(X2, "complete", max)
```

{ END solution }




{ BEGIN exercise }
As in the previous exercise, but apply the average linkage.
{ END exercise }

{ BEGIN solution }
$\mathcal{C}^{(0)}$ is identical to the one in the previous exercise
and the first merge decision is the same.

```{r ex-hclust-byhand-3, results='asis', echo=FALSE}
hclust_byhand(X2, "average", mean, 2)
```
{ END solution }




{ BEGIN exercise }
As in the previous exercise, but apply the single linkage.
{ END exercise }


{ BEGIN solution }

The first is identical to the above one.

```{r ex-hclust-byhand-4, results='asis', echo=FALSE}
hclust_byhand(X2, "single", min, 2)
```
{ END solution }



{ BEGIN exercise }
Perform cluster analysis of the OECD countries
based on the 7 SSI categories `ssi_2016_categories.csv`.
Apply single, complete, and average linkage. Draw the
corresponding dengrograms. Compute the averages of all indicators
in each point group for clusterings of different sizes.
Draw scatter plots for different pairs of variables (e.g., by calling
`pairs()`), where clusters are represented by points of different
shapes and colours.
{ END exercise }

{ BEGIN exercise }
Perform cluster analysis of the OECD countries
based on the 21 SSI indicators `ssi_2016_indicators.csv`
{ END exercise }


{ BEGIN exercise }
Perform cluster analysis for all the countries covered by the
Sustainable Society Indices.
{ END exercise }





## Remarks

Complete linkage dates back to Denmark in the year 1948;
a similar idea (for different dissimilarity measures)
was used in [@completelinkage] to "establish groups of equal
amplitude in plant sociology based on similarity of species content".
Single linkage has been proposed by Polish mathematicians
as "Wrocław taxonomy" (or the dendrite method) in 1951
[@singlelinkage].
Average linkage, also known as UPGMA (unweighted pair group method with arithmetic mean),
has been proposed in [@averagelinkage].

There are many other linkages, e.g., the Ward linkage minimises the variance
of within-cluster distances. Together with the linkages presented in this
chapter it is generalised by the Lance--Williams formula [@lancewilliams],
see also [@muellner].

. . .

Hierarchical clustering algorithms are fantastic, because they
output a whole hierarchy of *nested* partitions -- it's not that
when we cut it at the $k$-th level we'll obtain something totally unrelated
to what can we find at the $l$-th one ($k\neq l$).
However, this comes at a price -- these algorithms are quite slow
for large datasets.
In particular, the implementations included in the `stats::hclust()` function
(which we have used above) have $O(n^3)$ time complexity.
Therefore, in practice it's better to use
function `fastcluster::hclust()` [@fastcluster],
which provides a drop-in replacement
for the original routine.

Single linkage is the fastest -- even though it has $O(n^2)$
worst-case time complexity in generic metric spaces,
however, it can run much faster in real spaces
of small dimensionality (e.g., when analysing spatial data), as it
can be computed based on a minimum spanning tree of the pairwise
distance graph.

```{r fastcluster_showoff,eval=FALSE,echo=FALSE}
library("fastcluster")
```

Moreover, as we've said, there are $n(n-1)/2$ unique pairwise distances
between $n$ points.
Don't try calling `dist(hclust(X))` on large data matrices.
`fastcluster::hclust.vector()` implements single and Ward
(but not complete and average) linkages without the need of precomputing
the whole distance
matrix.

For a comprehensive overview of the efficient algorithms to compute
the aforementioned linkages, see [@muellner].



. . .


{ LATEX \color{gray} }

**TODO** .....

other distances/metrics


Note that all the indicators were on the same scale.
We'll deal with different datasets in the next chapter

otherwise, we need to perform some feature weighting or calibration;
e.g., standardisation

TODO: chapter 3 ????



. . .

Agglomerative clustering algorithms utilise the "bottom-up" approach

there are also divisive algorithms (top-down, start with the $1$-partition,
split clusters into smaller chunks), but they tend to be much more computationally
intensive, see, Mueller's ITM, however

what we did above, we call a greedy strategy btw

There are modification to this scheme, e.g., the Genie algorithm
(see package `genieclust`) [@genie] merges clusters based
on the closest pairs of points (just as single linkage), however
under the constraint that the inequality of the cluster size distribution
cannot go beyond some threshold -- this prevents leaving out very small
clusters at higher levels of the hierarchy.



. . .



is the obtained clustering a good one?
well, if it leads to interesting "discoveries" or something useful,
it is;
more discussion in Chapter \@ref(chap:kmeans)

we mention genieclust and centroid-based linkages later

will we play with HDBSCAN*?

[@jainsurvey]
[@wierzchon_klopotek]
[@foundds]

{ LATEX \normalcolor }



