<!--
kate: indent-width 4; word-wrap-column 74; default-dictionary en_AU
Copyright (C) 2020, Marek Gagolewski, https://www.gagolewski.com
This material is licensed under the Creative Commons BY-NC-ND 4.0 License.
-->

# Classification with Decision Trees {#chap:trees}



{ LATEX \color{gray} }

**TODO** In this chapter, we will:

* ...

* ...

data generators -- why 100% accuracy might not be possible,
why we need statistics or information theory (amongst others)
[@probtheorypatrecog], [@foundds] -- research in machine learning
vs. research with machine learning (citations! - asymptotics etc.)
-- 2 normal distributions, non-separable illustration

in [@probtheorypatrecog] presented are several interesting results
of the asymptotic behaviour of the K-NN classifier when $K\to\infty$
and $K/n\to\infty$ as $n\to\infty$ (e.g., both training sample size
$n$ and $K$ grow
but $n$ grows at least an order of magnitude faster than $K$)

maths can be used to answer questions about the general behaviour of the method:
"it is always true that", "with probability XX it holds", "if XXX, then the expected is YYY" etc.




prediction vs. description

decision trees -- towards models (explanation)

{ LATEX \normalcolor }






<!-- TODO: citations

*Random Forests* and *XGBoost* (see also: *AdaBoost*)
SVMs
-->





## Introduction

### Classification Task


Let $\mathbf{X}\in\mathbb{R}^{n\times p}$ be an input matrix
that consists of $n$ points in a $p$-dimensional space (each of the $n$ objects
is described by means of $p$ numerical features)

Recall that in supervised learning, with each
$\mathbf{x}_{i,\cdot}$ we associate the desired output $y_i$.

Hence, our dataset is $[\mathbf{X}\ \mathbf{y}]$ --
where each object is represented as a row vector
$[\mathbf{x}_{i,\cdot}\ y_i]$, $i=1,\dots,n$:

\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} & y_1\\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} & y_2\\
\vdots & \vdots & \ddots & \vdots    & \vdots\\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} & y_n\\
\end{array}
\right].
\]


. . .



In this chapter we are still  interested in  **classification** tasks;
we assume that each $y_i$ is a descriptive label.





Let's assume that we are faced with **binary classification** tasks.

Hence, there are only two possible labels that we traditionally denote with $0$s and $1$s.

For example:

0       | 1
--------|------------
no      | yes
false   | true
failure | success
healthy | ill







Let's recall the synthetic 2D dataset from the previous chapter
(true decision boundary is at $X_1=0$), see Figure \@ref(fig:synthetic).


```{r synthetic,echo=FALSE,fig.cap="A synthetic 2D dataset with the true decision boundary at $X_1=0$"}
set.seed(123)
n0 <- 50 # n0 points in class 0
n1 <- 50 # n0 points in class 0
Xs <- rbind(
    cbind(rnorm(n0, -1, 1), rnorm(n0, 0, 1)), # N( (-1, 0), (1, 1) )
    cbind(rnorm(n1, +1, 1), rnorm(n1, 0, 1))  # N( (+1, 0), (1, 1) )
)
Ys <- factor(rep(c("0", "1"), c(n0, n1)))

plot(Xs[,1], Xs[,2], col=Ys, pch=c(1,2)[Ys], xlab="X1", ylab="X2", asp=1)
legend("topleft", col=c(1,2), pch=c(1,2), legend=c("Class 0", "Class 1"), bg="white")
abline(v=0, lty=3, col="blue")
```








### Data




For illustration, we'll be considering the Wine Quality dataset
(white wines only):


```{r load1,cache=TRUE}
wine_quality <- read.csv("datasets/wine_quality_all.csv",
    comment.char="#")
white_wines <- wine_quality[wine_quality$color == "white",]
(n <- nrow(white_wines)) # number of samples
```




The input matrix $\mathbf{X}\in\mathbb{R}^{n\times p}$
consists of the first 10 numeric variables:

```{r load2,dependson='load1',cache=TRUE}
X <- as.matrix(white_wines[,1:10])
dim(X)
head(X, 2) # first two rows
```







The 11th variable measures the amount of alcohol (in %).

We will convert this dependent variable to a binary one:

- 0 == (`alcohol  < 12`) == lower-alcohol wines,
- 1 == (`alcohol >= 12`) == higher-alcohol wines

```{r load3,dependson='load2',cache=TRUE}
# recall that TRUE == 1
Y <- factor(as.character(as.numeric(white_wines$alcohol >= 12)))
table(Y)
```






60/40% train-test split:

```{r load4,dependson='load3',cache=TRUE}
set.seed(123) # reproducibility matters
random_indices <- sample(n)
head(random_indices) # preview
# first 60% of the indices (they are arranged randomly)
# will constitute the train sample:
train_indices <- random_indices[1:floor(n*0.6)]
X_train <- X[train_indices,]
Y_train <- Y[train_indices]
# the remaining indices (40%) go to the test sample:
X_test  <- X[-train_indices,]
Y_test  <- Y[-train_indices]
```






Let's also compute `Z_train` and `Z_test`, being the standardised versions of `X_train`
and `X_test`, respectively.

```{r load5,dependson='load4',cache=TRUE}
means <- apply(X_train, 2, mean) # column means
sds   <- apply(X_train, 2, sd)   # column standard deviations
Z_train <- t(apply(X_train, 1, function(r) (r-means)/sds))
Z_test  <- t(apply(X_test,  1, function(r) (r-means)/sds))
```







```{r loaded,dependson='load5',cache=TRUE}
get_metrics <- function(Y_pred, Y_test)
{
    C <- table(Y_pred, Y_test) # confusion matrix
    stopifnot(dim(C) == c(2, 2))
    c(Acc=(C[1,1]+C[2,2])/sum(C),  # accuracy
      Prec=C[2,2]/(C[2,2]+C[2,1]), # precision
      Rec=C[2,2]/(C[2,2]+C[1,2]),  # recall
      F=C[2,2]/(C[2,2]+0.5*C[1,2]+0.5*C[2,1]), # F-measure
      # Confusion matrix items:
      TN=C[1,1], FN=C[1,2],
      FP=C[2,1], TP=C[2,2]
    ) # return a named vector
}
```








## Decision Trees

### Introduction


Note that a K-NN classifier discussed in the previous chapter
is **model-free**.
The whole training set must be stored and referred to at all times.

Therefore, it doesn't *explain* the data we have -- we may use it solely
for the purpose of *prediction*.


Perhaps one of the most interpretable (and hence human-friendly) models
consist of decision rules of the form:

**IF $x_{i,j_1}\le v_1$ AND ... AND $x_{i,j_r}\le v_r$ THEN $\hat{y}_i=1$.**

These can be organised into a **hierarchy** for greater readability.

This idea inspired the notion of **decision trees** [@cart].






```{r plot_rpart,echo=FALSE,fig.cap="The simplest decision tree for the synthetic 2D dataset and the corresponding decision boundaries"}
library("rpart")
library("rpart.plot")
set.seed(123)



plot_rpart <- function(ts, XYs) {
  xx1 <- seq(-4, 4, length.out=250)
  xx2 <- seq(-4, 4, length.out=250)
  xx <- expand.grid(xx1, xx2)

  dimnames(xx)[[2]] <- names(XYs)[1:2]

  yy <- predict(ts, data.frame(xx, names=c("V1", "V2")), type="class")
  image(xx1, xx2, matrix(as.numeric(yy)-1, nrow=length(xx1), ncol=length(xx2)),
        col=c("#00000044", "#ff000044"), xlab="X1", ylab="X2", asp=1)


  points(Xs[,1], Xs[,2], col=Ys, pch=c(1,2)[Ys])
  legend("topleft", col=c(1,2), pch=c(1,2), legend=c("Class 0", "Class 1"), bg="white")
  abline(v=0, lty=3, col="blue")
}


XYs <- as.data.frame(cbind(Xs, Y=as.numeric(as.character(Ys))))
names(XYs) <- c("X1", "X2", "Y")
cp <- 0.5
ts <- rpart(Y~., data=XYs, method="class", cp=cp)
par(mfrow=c(1,2))
plot_rpart(ts, XYs)
par(mar=c(0,0,0,0))
par(ann=FALSE)
rpart.plot(ts)
```


Figure \@ref(fig:plot_rpart2) depicts a very simple decision tree
for the aforementioned synthetic dataset.
There is only one decision boundary (based on $X_1$) that splits
data into the "left" and "right" sides.
Each tree node reports 3 pieces of information:

- dominating class (0 or 1)
- (relative) proportion of 1s  represented in a node
- (absolute) proportion of all observations in a node




Figures \@ref(fig:plot_rpart2) and \@ref(fig:plot_rpart3) depict
trees with more decision rules.
Take a moment to contemplate how the corresponding decision boundaries
changed with the introduction of new decision rules.


```{r plot_rpart2,echo=FALSE,fig.cap="A more complicated decision tree for the synthetic 2D dataset and the corresponding decision boundaries"}
cp <- 0.01
ts <- rpart(Y~., data=XYs, method="class", cp=cp)
par(mfrow=c(1,2))
plot_rpart(ts, XYs)
par(mar=c(0,0,0,0))
par(ann=FALSE)
rpart.plot(ts)
```




```{r plot_rpart3,echo=FALSE,fig.cap="An even more complicated decision tree for the synthetic 2D dataset and the corresponding decision boundaries"}
cp <- 0.001
control <- list(minsplit=1, minbucket=5)
ts <- rpart(Y~., data=XYs, method="class", cp=cp, control=control)
par(mfrow=c(1,2))
plot_rpart(ts, XYs)
par(mar=c(0,0,0,0))
par(ann=FALSE)
rpart.plot(ts, tweak=1.05)
```






### Example in R






We will use the `rpart()` function from the `rpart` package
to build a classification tree.

```{r rpart_load}
library("rpart")
library("rpart.plot")
set.seed(123)
```

`rpart()` uses a formula (`~`) interface, hence it will be easier
to feed it with data in a data.frame form.

```{r xy_data}
XY_train <- cbind(as.data.frame(X_train), Y=Y_train)
XY_test <- cbind(as.data.frame(X_test), Y=Y_test)
```





Fit and plot a decision tree, see Figure \@ref(fig:plot_rpart1).

```{r plot_rpart1,fig.height=5,echo=-(1:2),fig.cap="A decision tree for the `white_wines` dataset"}
par(mar=c(0,0,0,0))
par(ann=FALSE)
t1 <- rpart(Y~., data=XY_train, method="class")
rpart.plot(t1, tweak=1.1, fallen.leaves=FALSE, digits=3)
```


We can build  less or more complex trees by playing
with the `cp` parameter, see Figures \@ref(fig:plot_rpart222)
and \@ref(fig:tree333).

```{r plot_rpart222,echo=-(1:2),fig.cap="A (simpler) decision tree for the `white_wines` dataset"}
par(mar=c(0,0,0,0))
par(ann=FALSE)
# cp = complexity parameter, smaller → more complex tree
t2 <- rpart(Y~., data=XY_train, method="class", cp=0.1)
rpart.plot(t2, tweak=1.1, fallen.leaves=FALSE, digits=3)
```


```{r tree333,fig.height=8,echo=-(1:2),fig.cap="A (more complex) decision tree for the `white_wines` dataset"}
par(mar=c(0,0,0,0))
par(ann=FALSE)
# cp = complexity parameter, smaller → more complex tree
t3 <- rpart(Y~., data=XY_train, method="class", cp=0.00001)
rpart.plot(t3, tweak=1.1, fallen.leaves=FALSE, digits=3)
```

Trees with few decision rules actually are very nicely interpretable.
On the other hand, plotting of the complex ones is just hopeless;
we should treat them as "black boxes" instead.



<!--
The fitted model is rather... simple.

Only the `alcohol` variable is taken into account.


Well note how these two distributions are shifted:

```{r,echo=-1}
#vioplot::vioplot(alcohol~Y, data=XY_train,
#    horizontal=TRUE)
```





-->

Let's make some predictions:

```{r predict_rpart1}
Y_pred <- predict(t1, XY_test, type="class")
get_metrics(Y_pred, Y_test)
```



```{r predict_rpart222}
Y_pred <- predict(t2, XY_test, type="class")
get_metrics(Y_pred, Y_test)
```


```{r tree333pred}
Y_pred <- predict(t3, XY_test, type="class")
get_metrics(Y_pred, Y_test)
```


Remark.

: (\*) Interestingly, `rpart()` also provides us with information
about the importance degrees of each independent variable.

```{r importances}
t1$variable.importance/sum(t1$variable.importance)
```

















### A Note on Decision Tree Learning




Learning an optimal decision tree is a computationally hard problem
-- we need some heuristics.

Examples:

* ID3 (Iterative Dichotomiser 3) [@id3]
* C4.5 algorithm [@c45]
* CART by Leo Breiman et al., [@cart]

(\*\*) Decision trees are most often constructed by a *greedy*, *top-down*
*recursive partitioning*, see., e.g., [@rpart].







## Exercises




### EdStats -- Where Girls Are Better at Maths Than Boys?


In this task we will consider the "wide" version of the EdStats
dataset:

```{r girlsboys1}
edstats <- read.csv("datasets/edstats_2019_wide.csv",
    comment.char="#")
edstats[1, 1:6]
meta <- read.csv("datasets/edstats_meta.csv",
    comment.char="#")
```


This dataset is small, moreover, we'll be more interested  in the description
(understanding) of data, not prediction of the response variable
to unobserved samples. Note that we have the *population* of the World
countries at hand here (new countries do not arise on a daily basis).
Therefore, a train-test split won't be performed.

<!--
```{r}
#pairs(edstats[c("LO.PISA.MAT", "LO.PISA.SCI", "LO.PISA.REA")])
```
-->




{ BEGIN exercise }
Add a 0/1 factor-type variable `girls_rule_maths` that is equal to 1
if and only if a country's average score of 15-year-old female students on the PISA
mathematics scale is greater than the corresponding indicator for the male ones.
{ END exercise }

{ BEGIN solution }

Recall that a conversion of a logical value to a number
yields 1 for `TRUE` and `0` for `FALSE`. Hence:

```{r girlsboys2}
edstats$girls_rule_maths <-
    factor(as.numeric(
        edstats$LO.PISA.MAT.FE>edstats$LO.PISA.MAT.MA
    ))
head(edstats$girls_rule_maths, 10)
```

Unfortunately, there are many missing values in the dataset.
More precisely:

```{r girlsboys3}
sum(is.na(edstats$girls_rule_maths)) # count
mean(is.na(edstats$girls_rule_maths)) # proportion
```

Countries such as Egypt, India, Iran or Venezuela
are not amongst the 79 members of the Programme for International
Student Assessment. Thus, we'll have to deal with the data we have.

The percentage of counties where "girls rule" is equal to:

```{r girlsboys4}
mean(edstats$girls_rule_maths==1, na.rm=TRUE)
```

Here is the list of those counties:

```{r girlsboys5}
as.character(na.omit(
    edstats[edstats$girls_rule_maths==1, "CountryName"]
))
```

{ END solution }




{ BEGIN exercise }
Learn a decision tree that distinguishes between the countries where
girls are better at maths than boys and assess the quality of this classifier.
{ END exercise }

{ BEGIN solution }

Let's first create a subset of `edstats` that doesn't include
the country names as well as the boys' and girls' math scores.

```{r girlsboys6}
edstats_subset <- edstats[!(names(edstats) %in%
    c("CountryName", "LO.PISA.MAT.FE", "LO.PISA.MAT.MA"))]
```

Fitting and plotting (see Figure \@ref(fig:girlsboys7tree))
of the tree can be performed as follows:

```{r girlsboys7tree,echo=-(1:2),fig.cap="A decision tree explaining the `girls_rule_maths` variable"}
set.seed(123)
par(ann=FALSE)
library("rpart")
library("rpart.plot")
tree <- rpart(girls_rule_maths~., data=edstats_subset,
    method="class", model=TRUE)
rpart.plot(tree)
```

The variables included in the model are:

```{r girlsboys8,results='asis',echo=FALSE}
for (var in levels(tree$frame$var)[-1]) {
    cat(sprintf("* %s: *%s*\n", var, meta$Series[meta$Code==var]))
}
```

Note that the decision rules are well-interpretable, we can make a whole
story around it. Whether or not it is actually true -- is a different... story.

To compute the basic classifier performance scores,
let's recall the `get_metrics()` function:

```{r girlsboys9}
get_metrics <- function(Y_pred, Y_test)
{
    C <- table(Y_pred, Y_test) # confusion matrix
    stopifnot(dim(C) == c(2, 2))
    c(Acc=(C[1,1]+C[2,2])/sum(C), # accuracy
      Prec=C[2,2]/(C[2,2]+C[2,1]), # precision
      Rec=C[2,2]/(C[2,2]+C[1,2]), # recall
      F=C[2,2]/(C[2,2]+0.5*C[1,2]+0.5*C[2,1]), # F-measure
      # Confusion matrix items:
      TN=C[1,1], FN=C[1,2],
      FP=C[2,1], TP=C[2,2]
    ) # return a named vector
}
```

Now we can judge the tree's character:

```{r girlsboys10}
Y_pred <- predict(tree, edstats_subset, type="class")
get_metrics(Y_pred, edstats_subset$girls_rule_maths)
```



{ END solution }


{ BEGIN exercise }
Learn a decision tree that this time doesn't rely on any of the PISA indicators.
{ END exercise }

{ BEGIN solution }

Let's remove the unwanted variables:

```{r girlsboys11,echo=-1,fig.cap="Another decision tree explaining the `girls_rule_maths` variable"}
set.seed(123)
edstats_subset <- edstats[!(names(edstats) %in%
    c("LO.PISA.MAT", "LO.PISA.MAT.FE", "LO.PISA.MAT.MA",
      "LO.PISA.REA", "LO.PISA.REA.FE", "LO.PISA.REA.MA",
      "LO.PISA.SCI", "LO.PISA.SCI.FE", "LO.PISA.SCI.MA",
      "CountryName"))]
```

On a side note, this could be done more easily by calling, e.g.,
`stri_startswith_fixed(names(edstats), "LO.PISA")` from the `stringi` package.

Fitting and plotting (see Figure \@ref(fig:girlsboys11tree)) of the tree:

```{r girlsboys11tree,echo=-1,fig.cap="Another decision tree explaining the `girls_rule_maths` variable"}
par(ann=FALSE)
tree <- rpart(girls_rule_maths~., data=edstats_subset,
    method="class", model=TRUE)
rpart.plot(tree)
```

Performance metrics:

```{r girlsboys12}
Y_pred <- predict(tree, edstats, type="class")
get_metrics(Y_pred, edstats_subset$girls_rule_maths)
```

It's interesting to note that some of the
goodness-of-fit measures are actually higher now.

The variables included in the model are:

```{r girlsboys13,results='asis',echo=FALSE}
for (var in levels(tree$frame$var)[-1]) {
    cat(sprintf("* %s: *%s*\n", var, meta$Series[meta$Code==var]))
}
```

{ END solution }



### EdStats and World Factbook -- Joining Forces

In the course of our data science journey, we have considered two datasets
dealing with country-level indicators: the World Factbook and
World Bank's EdStats.


```{r joinedstats1}
factbook <- read.csv("datasets/world_factbook_2020.csv",
    comment.char="#")
edstats <- read.csv("datasets/edstats_2019_wide.csv",
    comment.char="#")
```

Let's combine the information they provide
and see if we come up with a better model of where
girls' math scores are higher.

<!--
#factbook$country[!(factbook$country %in% edstats$CountryName)]
#edstats$CountryName[!(edstats$CountryName %in% factbook$country)]
-->

{ BEGIN exercise }
Some country names in one dataset don't match those in the other
one, for instance: Czech Republic vs. Czechia,
Myanmar vs. Burma, etc. Resolve these conflicts as best you can.
{ END exercise }

{ BEGIN solution }
To get a list of the mismatched country names, we can call either:

```{r joinedstats2,eval=FALSE}
factbook$country[!(factbook$country %in% edstats$CountryName)]
```

or:

```{r joinedstats3,eval=FALSE}
edstats$CountryName[!(edstats$CountryName %in% factbook$country)]
```

Unfortunately, the data need to be cleaned manually -- it's a tedious task.
The following consists of what we hope are the best matches
between the two datasets (yet, the list is not perfect;
in particular, the Republic of North Macedonia
is completely missing in one of the datasets):

```{r joinedstats4}
from_to <- matrix(ncol=2, byrow=TRUE, c(
# FROM (edstats)                  # TO (factbook)
"Brunei Darussalam"             , "Brunei"                            ,
"Congo, Dem. Rep."              , "Congo, Democratic Republic of the" ,
"Congo, Rep."                   , "Congo, Republic of the"            ,
"Czech Republic"                , "Czechia"                           ,
"Egypt, Arab Rep."              , "Egypt"                             ,
"Hong Kong SAR, China"          , "Hong Kong"                         ,
"Iran, Islamic Rep."            , "Iran"                              ,
"Korea, Dem. People’s Rep."     , "Korea, North"                      ,
"Korea, Rep."                   , "Korea, South"                      ,
"Kyrgyz Republic"               , "Kyrgyzstan"                        ,
"Lao PDR"                       , "Laos"                              ,
"Macao SAR, China"              , "Macau"                             ,
"Micronesia, Fed. Sts."         , "Micronesia, Federated States of"   ,
"Myanmar"                       , "Burma"                             ,
"Russian Federation"            , "Russia"                            ,
"Slovak Republic"               , "Slovakia"                          ,
"St. Kitts and Nevis"           , "Saint Kitts and Nevis"             ,
"St. Lucia"                     , "Saint Lucia"                       ,
"St. Martin (French part)"      , "Saint Martin"                      ,
"St. Vincent and the Grenadines", "Saint Vincent and the Grenadines"  ,
"Syrian Arab Republic"          , "Syria"                             ,
"Venezuela, RB"                 , "Venezuela"                         ,
"Virgin Islands (U.S.)"         , "Virgin Islands"                    ,
"Yemen, Rep."                   , "Yemen"
))
```

Conversion of the names:

```{r joinedstats5}
for (i in 1:nrow(from_to)) {
    edstats$CountryName[edstats$CountryName==from_to[i,1]] <- from_to[i,2]
}
```


On a side note (\*), this could be done with a single call to
a function in the `stringi` package:

```{r joinedstats6}
library("stringi")
edstats$CountryName <- stri_replace_all_fixed(edstats$CountryName,
    from_to[,1], from_to[,2], vectorize_all=FALSE)
```

{ END solution }


{ BEGIN exercise }
Merge (join) the two datasets based on the country names.
{ END exercise }

{ BEGIN solution }
This can be done by means of the `merge()` function.

```{r joinedstats7}
edbook <- merge(edstats, factbook, by.x="CountryName", by.y="country")
ncol(edbook) # how many columns we have now
```

{ END solution }



{ BEGIN exercise }
Learn a decision tree that distinguishes between the countries where
girls are better at maths than boys and assess the quality of this classifier.
{ END exercise }

{ BEGIN solution }

We proceed as in one of the previous exercises:

```{r joinedstats8,echo=-1}
set.seed(123)
edbook$girls_rule_maths <-
    factor(as.numeric(
        edbook$LO.PISA.MAT.FE>edbook$LO.PISA.MAT.MA
    ))
edbook_subset <- edbook[!(names(edbook) %in%
    c("CountryName", "LO.PISA.MAT.FE", "LO.PISA.MAT.MA"))]
```

Fitting and plotting (see Figure \@ref(fig:joinedstats9)):

```{r joinedstats9,echo=-1,fig.cap="Yet another decision tree explaining the `girls_rule_maths` variable"}
par(ann=FALSE)
library("rpart")
library("rpart.plot")
tree <- rpart(girls_rule_maths~., data=edbook_subset,
    method="class", model=TRUE)
rpart.plot(tree)
```

Performance metrics:

```{r joinedstats10}
Y_pred <- predict(tree, edbook_subset, type="class")
get_metrics(Y_pred, edbook_subset$girls_rule_maths)
```


The variables included in the model are:

```{r joinedstats11,results='asis',echo=FALSE}
for (var in levels(tree$frame$var)[-1]) {
    if (length(meta$Series[meta$Code==var])==1)
        cat(sprintf("* %s: *%s*\n", var, meta$Series[meta$Code==var]))
    else
        cat(sprintf("* %s\n", var))
}
```

This is... not at all enlightening.
Rest assured that experts in education
or econometrics for whom we work in this (imaginary) project
would raise many questions at this very point. Merely applying
some computational procedure on a dataset doesn't cut it;
it's too early to ask for a paycheque.
Classifiers are just blind *tools* in our gentle yet firm hands;
new questions are risen, new answers must be sought. Further
explorations are of course left as an exercise to the kind reader.

{ END solution }


### Wine Quality -- Random Forest and XGBoost (\*)

Let's consider the Wine Quality dataset:

```{r task_classification_3_1}
wine_quality <- read.csv("datasets/wine_quality_all.csv",
    comment.char="#")
head(wine_quality, 3)
```

Recall that there are 11 physicochemical features of wines reported
(columns 1-11).
Moreover, there is a wine rating (variable `response`)
on the scale 0 (bad) to 10 (excellent) given by wine experts.

{ BEGIN exercise }
Add a new column named `quality`. A wine should get a `quality` of `1`
if its rating is greater than or equal to 7 (a good wine)
and a quality of `0` otherwise.
{ END exercise }


```{r task_classification_3_2,echo=FALSE}
wine_quality$quality <- 0
wine_quality$quality[wine_quality$response>=7] <- 1
# table(wine_quality$quality)
```

{ BEGIN exercise }
Perform a random train-test split of size 60-40%:
create the matrices `X_train` and `X_test` containing
the 11 physicochemical wine features and
the corresponding label vectors `Y_train` and `Y_test`
that inform on the wines' `quality`.
{ END exercise }

{ BEGIN exercise }
Construct (of course, on the train set)
a decision tree that models wine quality  as a function
the 11 physicochemical features.
Assess the quality of the obtained model (of course, based on the test set)
by computing the basic
performance metrics for this classifier (accuracy, precision, recall,
F-measure). Also, print the confusion matrix (see `table()`).
Discuss the obtained results. Is this  a good model?
{ END exercise }


```{r task_classification_3_3,eval=FALSE,echo=FALSE}
X <- cbind(wine_quality[1:11], wine_quality["quality"])
set.seed(123)
library("rpart")
library("rpart.plot")
tree <- rpart(quality~., data=X,
    method="class", model=TRUE)
get_metrics <- function(Y_pred, Y_test)
{
    C <- table(Y_pred, Y_test) # confusion matrix
    stopifnot(dim(C) == c(2, 2))
    c(Acc=(C[1,1]+C[2,2])/sum(C), # accuracy
      Prec=C[2,2]/(C[2,2]+C[2,1]), # precision
      Rec=C[2,2]/(C[2,2]+C[1,2]), # recall
      F=C[2,2]/(C[2,2]+0.5*C[1,2]+0.5*C[2,1]), # F-measure
      # Confusion matrix items:
      TN=C[1,1], FN=C[1,2],
      FP=C[2,1], TP=C[2,2]
    ) # return a named vector
}
Y_pred <- predict(tree, X, type="class")
get_metrics(Y_pred, X$quality)
```


Ensemble learning, bagging and random forest.

:   *Ensemble learning*  is a rather simple yet appealing
    idea that emphasises on the fact
    that no single model will likely ever be omniscient. Instead,
    we can construct numerous diverse models explaining a given dependent variable
    and then properly aggregate the obtained classifier committee
    to obtain a single answer.

    *Bagging* (bootstrap aggregating)
    is one of the most popular ensemble types:
    here, we fit numerous models, each time to a different
    randomly selected subset of observations in the input dataset.
    In order to classify a new point, we ask each classifier
    what it "thinks" the corresponding label should be and
    report the mode (majority vote) of the candidate outputs.

    The famous *random forest* algorithm is an example application
    of the bagging technique that is based on decision trees
    (with an extension that not only random subsets of observations,
    but also random columns in the dataset are used to learn the
    underlying models).

{ BEGIN exercise }
Ronstruct a random forest that models `quality` as a function
of the 11 physicochemical features -- see R package `randomForest`.
Assess and discuss the classifier's performance.
{ END exercise }


```{r task_classification_3_4,eval=FALSE,echo=FALSE}
library("randomForest")
```


Boosting.

: Another powerful ensemble technique is called *boosting*.
Here, we construct a supervised learner in an iterative manner;
in each step a simple ("weak") classifier is added to the ensemble
by learning "something new" about the dataset, e.g., by uplifting
its performance in the cases where frequent misclassifications occur.
When determining the final outcome, each underlying model
might be assigned a different weight.



{ BEGIN exercise }
The XGBoost algorithm is a modern implementation of
the gradient boosting approach     based on decision trees.
Fit a model with XGBoost (see package `xgboost`) that describes
`quality` as a function of the 11 physicochemical features.
Assess and discuss the classifier's performance.
{ END exercise }

```{r task_classification_3_5,eval=FALSE,echo=FALSE}
library("xgboost")
```







## Outro



The state-of-the art classifiers called
*Random Forests* and *XGBoost* (see also: *AdaBoost*) are based on decision trees.
They tend to be more accurate but -- at the same time -- they fail to
exhibit the decision trees' important feature: interpretability.

Trees can also be used for regression tasks, see R package `rpart`.




{ LATEX \color{gray} }

. . .

**TODO** .....


. . .

{ LATEX \normalcolor }




Recommended further reading: [@islr: Chapters 4 and 8]  <!-- TODO: update -->

Other: [@esl: Chapters 4 and 7 as well as (\*) Chapters 9,  10,  13, 15]  <!-- TODO: update -->


Next Chapter


