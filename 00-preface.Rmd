# Preface {.unnumbered}

<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->


> **This is a draft version (distributed in the hope that it will be useful)
of the book *Machine Learning Classics with R*
by Marek Gagolewski.**

> **Please submit any feature requests, remarks and bug fixes
via the project site at [github](https://github.com/gagolews/lmlcr/issues). Thanks!**


Copyright (C) 2020, [Marek Gagolewski](https://www.gagolewski.com).
This material is licensed under the Creative Commons
[Attribution-NonCommercial-NoDerivatives 4.0 International](https://creativecommons.org/licenses/by-nc-nd/4.0/)
License (CC BY-NC-ND 4.0).

You can access this book at:

* https://lmlcr.gagolewski.com/ (a browser-friendly version)
* https://lmlcr.gagolewski.com/lmlcr.pdf (PDF)
* https://github.com/gagolews/lmlcr (source code)



<!-- ![](figures/cover) -->


#### Aims and Scope  {.unnumbered}


Machine learning has numerous exciting real-world applications,
including stock market prediction, speech recognition,
computer-aided medical diagnosis, content, and product recommendation,
anomaly detection in security camera footages, game playing,
autonomous vehicle operation, and many others.

In this book we will take an unpretentious glance at the most fundamental
algorithms that have stood the test of time and which  form
the basis for state-of-the-art solutions of modern AI,
which is principally (big) data-driven.
We will learn how to use the R language [@rpoject]
for implementing various stages
of data processing and modelling activities.
For a more in-depth treatment of R, refer to this book's Appendices
and, for instance, [@Rintro; @rprogdatascience; @r4ds].

These pages contain solid underpinnings for further studies
related to statistical learning, machine learning,
data science, data analytics,
data mining, and artificial intelligence,
including [@esl; @bishop; @casi].
We will also appreciate the vital role of mathematics as a universal
language for formalising data-intense problems and communicating their
solutions. The book is aimed at readers who are yet to be fluent with
university-level linear algebra, calculus as well as probability theory,
such as 1st year undergrads or those who have forgotten
all the maths they have learned and need a gentle, non-invasive,
yet rigorous introduction to the topic.
For a nice, machine learning-focused introduction to mathematics alone,
see, e.g., [@mml].


<!-- TODO: This book is structured as follows. -->

<!-- TODO: This book is non-standard in the way that:
* it doesn't assume that the reader is familiar with the concept
of probability, unlike many other books on statistical learning (especially with the
use of the R language)
* it only assumes high school-level mathematics; basic linear algebra concepts
(of which matrix multiplication is the most difficult one) are explained;
there is some reference to function differentiation, however the material is optional
and marked with an asterisk;
I am aware of the fact that some readers might struggle with the $\sum$ notation
for summation, but this is explained in the book;
I hope this book is a nice appreciation of the higher maths courses such as
linear algebra, calculus, discrete maths as well as probability and statistics
* then, once the math groundwork is laid, the reader will be ready for other
excellent books, like ESL, Bishop etc.
* it takes an algorithmic approach to the description of machine learning models
(yet, the models are not treated as black-boxes; we don't just apply functions
from different packages, but try to implement them on our own)
* it doesn't  assume any prior exposure to programming languages;
essential R basics are introduced in a rigorous manner and are self-contained;
it's not going to be easy, but all the building blocks (together with exercises)
are provided
* it demystifies and deconstructs the "coolness" of AI -- putting
all the hype aside, super-duper-tensor-GPU-cloud-self-driving-1-billion-revenue-
you-must-buy-our-products, it demonstrates that the basic concepts behind most methods
have been known for decades (basically, our computers are now much faster
and we have much more data, that's it); you don't need to be excited
about each new product, new toolbox, new algorithms (and don't FOMO!);
stay calm, it's just an old dog that learned new tricks.

instead of finding examples of datasets where ml methods work
("good datasets are hard to find")
the author took a few realistic databases and.... well, showed
how to solve the problems with the methods that don't work ;)


you might want to become a ML engineer or data analyst/scientist
or use the methods in your research

greedy companies store a lot of data about their customers --
you might want to understand the principles behind the methods they use
to recommend you that ad or product


We would like to appreciate the role of open source software.
Most started as non-for-profit and it's beautiful that some of them
continue this way.
Not everything is for sale. Not everything has its price.
Let's not forget it.




We need more than just ML:

Mathematical modelling, differential equations, discrete maths

Statistics for small samples but not needed for big data




It's not all in the data, real life processes are not static

Machine learning models are inherently static, they describe today. Tomorrow may be different, especially if people start to trick the system. Think spam detection , spammers will change their behaviour as new detection algorithms arise

People are smarter than your think, they will challenge the system, by choosing a quality metric you change the direction, but can you really predict what other factors gonna be like when we get there?


Examples : covid prediction not taking into account country policies and other things going on

Garmin vo2max estimation and the risk of going mad/overtraining



 *AI is a tool*.
Yes, AI is really useful, but it's not a remedy for all our problems.


Behind every success story related to  AI, there are always people involved. Sometimes large
teams of:

* those who deal with hardware, data storage, high performance computing
* those who wrangle data, fit models to data and develop prototypes
* those who are responsible for making the algorithms
work well in production environments
* those who are responsible for providing you
with software/algorithms/models that enable everyone to perform the above,
e.g., languages such as R and Python, libraries, packages

You can call them data engineers, data analysts, machine learning engineers,
data scientists, whatever.

Oftentimes, especially in introductory courses such as this one,
a "static" view on the problem domain must be assumed (for the so much
required simplicity) -- that there is something to learn about a dataset
and the reality it comes from never changes.
However, the truth is that "all things are flowing" (Heraclitus' *panta rhei*),
and some systems are already outdated at the time when they go into production.
Also, people themselves might change their behaviours when interacting
with an ML system.
Notably, some systems have an inherent ability to adapt based on forthcoming
observations as well
(learn to modify its behaviour *on-line*)


Good introduction to probability and statistics:

{R. BartoszyÅ„ski and M. Niewiadomska-Bugaj},
{Probability and Statistical Inference},
{Wiley},
{2008}

{J.E. Gentle},
{Theory of Statistics}


Information theory:

{D.J.C. MacKay}
{Information Theory, Inference, and Learning Algorithms}
{Cambridge University Press}
{2003}


-->




#### About the Author  {.unnumbered}


I, Marek Gagolewski, am currently a Senior Lecturer (equivalent to Associate Professor in the US)
in Applied AI at Deakin University in Melbourne, VIC, Australia
and an Associate Professor in Data Science at Warsaw University of Technology,
Poland. My main passion is in research -- my primary interests currently include
machine learning and optimisation algorithms, data aggregation and clustering,
statistical modelling, and scientific computing.
*Explaining* of things matters to me more than merely tuning the knobs
so as to increase a chosen performance metric (with uncontrollable consequences
to other ones); the latter rather belongs to technology and wizardry,
not science.

I'm an author of more than 70 publications.
I've developed several open source R and Python packages, including
[stringi](http://www.gagolewski.com/software/stringi/),
which is among the most often downloaded R extensions.

On top of that, I teach various courses related to
R and Python programming, algorithms,
data science, and machine learning -- and I'm good at it.
This book was also influenced by my teaching experience at
[Data Science Retreat](https://datascienceretreat.com) in Berlin, Germany.





#### Acknowledgements  {.unnumbered}

This book has been prepared with `r R.Version()$version.string`
as well as pandoc, Markdown, and GitBook.
R code chunks have been processed with knitr.
A little help of bookdown, good ol' Makefiles, and shell scripts
did the trick.


```{r,echo=FALSE}
library("knitr")
library("bookdown")
library("stringi")
fs <- list.files(".", "\\.Rmd$", full=TRUE)
pkgs <- stri_unique(stri_sort(unlist(lapply(fs, function(f)
    na.omit(stri_match_first_regex(
        readLines(f), "^library\\(['\"]?(.*?)['\"]?\\)")[,2])))))
```

The following R packages are used or referred to in the text:
`r stri_flatten(pkgs, collapse=", ")`.

During the writing of this book, I've been listening to the music
featuring John Coltrane, Krzysztof Komeda,
Henry Threadgill, Albert Ayler, Paco de Lucia, and Tomatito.



{ LATEX \mainmatter }
