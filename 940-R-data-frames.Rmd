# Data Frame Wrangling in R   {#appendix:rdf}

<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->


{ LATEX \color{gray} }

**TODO** In this chapter, we will:

* ...

* ...


{ LATEX \normalcolor }






R `data.frame`s are similar to matrices in the sense that
we use them to store tabular data.
However, in data frames each column can be of different type:

```{r df1}
head(iris)
head(rpart::car90, 2)
```


## Creating Data Frames

Most frequently, we will be creating data frames
based on a series of numeric, logical, characters vectors of identical lengths.

```{r df2,echo=-(1:2)}
set.seed(1236)
x <- data.frame(
    u=runif(5),
    v=sample(c(TRUE, FALSE), 5, replace=TRUE),
    w=LETTERS[1:5]
)
print(x)
```

<!--

Not the case anymore in R 4.0!



Note that when we create objects of type data frame,
strings are automatically converted to factors.

```{r df3}
# class(x$w)
```

Throughout the history of computing with R, this has caused way too many bugs
(recall, for instance, what's the result of calling `as.numeric()` on a factor).
In order to change this behaviour, either pass `stringsAsFactors=FALSE`
argument to `data.frame()` or switch this feature off globally (recommended):

```{r df4}
# options(stringsAsFactors=FALSE)
```

-->





Some objects, such as matrices, can easily be coerced to data frames:

```{r df5}
(A <- matrix(1:12, byrow=TRUE, nrow=3,
    dimnames=list(
        NULL,     # row labels
        c("x", "y", "z", "w") # column labels
)))
as.data.frame(A)
```

Named lists are amongst other candidates for a meaningful conversion:

```{r df6}
(l <- lapply(split(iris$Sepal.Length, iris$Species),
    function(x) {
        c(min=min(x), median=median(x), mean=mean(x), max=max(x))
    }))
as.data.frame(l)
```



## Importing Data Frames

Many interesting data frames come from external sources, such as csv files,
web APIs, SQL databases and so on.

In particular, `read.csv()` (see `?read.table` for a long list
of tunable parameters) imports data from plain text files
organised in a tabular manner (such as *c*omma-*s*eparated lists of *v*alues):

```{r csv}
f <- tempfile() # temporary file name
write.csv(x, f, row.names=FALSE) # save data frame to file
cat(readLines(f), sep="\n") # print file contents
read.csv(f)
```

Note that CSV is by far the most portable format for exchanging matrix-like
objects between different programs (statistical or numeric computing
environments, spreadsheets etc.).




## Data Frame Subsetting

### Each Data Frame is a List

First of all, we should note that
each data frame is in fact represented as an ordinary named list:

```{r dfclass}
class(x)
typeof(x)
```

Each column is stored as a separate list item.
Having said that, we shouldn't be surprised that we already know
how to perform quite a few operations on data frames:

```{r listdf}
length(x) # number of columns
names(x)  # column labels
x$u # accessing column `u` (synonym: x[["u"]])
x[[2]] # 2nd column
x[c(1,3)] # a sub-data.frame
sapply(x, class) # apply class() on each column
```





### Each Data Frame is Matrix-like

Data frames can be considered as "generalised" matrices.
Therefore, operations such as subsetting will work in the same manner.

```{r matsubs1}
dim(x) # number of rows and columns
x[1:2,] # first two rows
x[,c(1,3)] # 1st and 3rd column, synonym: x[c(1,3)]
x[,1] # synonym: x[[1]]
x[,1,drop=FALSE] # synonym: x[1]
```

Take a special note of selecting rows based on logical vectors.
For instance, let's extract all the rows from `x` where the values
in the column named `u` are between 0.3 and 0.6:

```{r matsubs2}
x[x$u>=0.3 & x$u<=0.6, ]
x[!(x[,"u"]<0.3 | x[,"u"]>0.6), ] # equivalent
```

Moreover, subsetting based on integer vectors can be used to
change the order of rows. Here is how we can sort the rows in `x`
with respect to the values in column `u`:

```{r matsubs3}
(x_sorted <- x[order(x$u),])
```


Let's stress that the programming style we emphasise on here is very
transparent. If we don't understand how a complex operation is being executed,
we can always decompose it into smaller chunks that can be studied separately.
For instance, as far as the last example is concerned, we can
take a look at the manual of `?order` and then inspect the result
of calling `order(x$u)`.

On a side note, we can re-set the row names by referring to:

```{r rownamesreset}
row.names(x_sorted) <- NULL
x_sorted
```






## Common Operations


We already know how to filter rows based on logical conditions, e.g.:

```{r iris}
iris[iris$Petal.Width >= 1.2 & iris$Petal.Width <= 1.3,
    c("Petal.Width", "Species")]
iris[iris$Sepal.Length > 6.5 & iris$Species == "versicolor", ]
```

and aggregate information in individual columns:

```{r summary}
sapply(iris[1:4], summary)
```

Quite frequently, we will be interested
in summarising data within subgroups generated by a list
of factor-like variables.

```{r}
aggregate(iris[1:4], iris[5], mean)
ToothGrowth[sample(nrow(ToothGrowth), 5), ] # 5 random rows
aggregate(ToothGrowth["len"], ToothGrowth[c("supp", "dose")], median)
```

Taking into account that `split()` accepts a data frame input as well,
we can perform what follows:

```{r sapply}
sapply(
    # split iris into 3 sub-data.frames:
    split(iris, iris[5]),
    # on each sub-data.frame, apply the following function
    function(df) {
        # compute the mean of first four columns:
        sapply(df[1:4], mean)
    })
sapply(split(iris, iris[5]), function(df) {
    c(Sepal.Length=summary(iris$Sepal.Length),
      Petal.Length=summary(iris$Petal.Length)
     )
})
```

The above syntax is not super-convenient,
but it only uses the building blocks that we have already mastered!
That should be very appealing to the minimalists.
Note that R packages such as data.table and dplyr offer more convenient
substitutes -- you can always learn them on your own
(which takes time, but it's worth the hassle).
They simplify the most common data wrangling tasks. Moreover,
they have been optimised for speed -- they can handle much larger data sets
efficiently.



## Metaprogramming and Formulas (\*)

R (together with a few other programming languages such as Lisp and Scheme,
that heavily inspired R's semantics) allows its programmers to apply
some *metaprogramming* techniques, that is,
to write programs that manipulate unevaluated R expressions.

For instance, take a close look at the following plot:

```{r plotz-metaprogramming,fig.cap="Metaprogramming in action: Take a look at the Y axis label"}
z <- seq(-2*pi, 2*pi, length.out=101)
plot(z, sin(z), type="l")
```

How did the `plot()` function know that we are plotting `sin` of `z`
(see Figure \@ref(fig:plotz-metaprogramming))?
It turns out that, at any time, we not only have access to the value
of an object (such as the result of evaluating `sin(z)`, which is
a vector of 101 reals) but also to the expression that was passed as
a function's argument itself.

```{r test_meta}
test_meta <- function(x) {
    cat("x equals to ", x, "\n") # \n == newline
    cat("x stemmed from ", deparse(substitute(x)), "\n")
}
test_meta(2+7)
```

This is very powerful and yet potentially very confusing to the users, because
we can write functions that don't compute the arguments provided
in a way we expect them to (i.e., following the R language specification).
Each function can constitute a new micro-verse, where with its own rules --
we should always refer to the documentation.

For instance, consider the `subset()` function:

```{r subset}
head(iris)
subset(iris, Sepal.Length>7.5, select=-(Sepal.Width:Petal.Width))
```

Neither `Sepal.Length>6` nor `-(Sepal.Width:Petal.Width)` make sense
as standalone R expressions! However, according to the `subset()` function's
own rules, the former expression is considered as a row selector
(here, `Sepal.Length` refers to a particular column *within* the `iris` data frame).
The latter plays the role of a column filter (select everything but all the columns
between...).

The data.table and dplyr packages (which are very popular)
rely on this language feature
all the time, so we shouldn't be surprised when we see them.

. . .


There is one more interesting language feature that is possible
thanks to metaprogramming.
*Formulas* are special R objects that consist of two unevaluated
R expressions separated by a tilde (`~`).
For example:

```{r formula}
len ~ supp+dose
```

A formula on its own has no meaning. However, many R functions
accept formulas as arguments and can interpret them in various different ways.

For example, the `lm()` function that fits a linear regression model,
uses formulas to specify the output and input variables:

```{r lm}
lm(Sepal.Length~Petal.Length+Sepal.Width, data=iris)
```

On the other hand, `boxplot()` (see Figure \@ref(fig:boxplot-metaprogramming))
allows for creating
separate box-and-whisker plots for each subgroup given by a combination
of factors.

```{r boxplot-metaprogramming,fig.cap="Example box plot created via the formula interface"}
boxplot(len~supp+dose, data=ToothGrowth,
    horizontal=TRUE, col="white")
```

The `aggregate()` function supports formulas too:

```{r aggregate}
aggregate(cbind(Sepal.Length, Sepal.Width)~Species, data=iris, mean)
```

We should therefore make sure that we know how every function interacts
with a formula -- information on that can be found in `?lm`, `?boxplot`,
`?aggregate` and so forth.



## Exercises


### Urban Forest

In this task we will be working with the data on more than 70,000 trees
in Melbourne, VIC, Australia. Before proceeding any further, read the
dataset's description in Appendix F. <!-- TODO: -->


```{r df_ex1a}
urban_forest <- read.csv("datasets/urban_forest.csv.gz",
    comment.char="#")
head(urban_forest, 3)
```

{ BEGIN exercise }
Write R code that answers the following questions:

* How many trees  have been planted between 2010 and 2015
    (inclusive)?
* Are more trees located within a public park or along a street?
* What is the average age (in years, the current one is 2020)
    of the London Planes? (Give an approximate answer based
    on the `Year.Planted` column).
{ END exercise }



{ BEGIN exercise }
Fetch the IDs (`CoM.ID`), common names (`Common.Name`)
and trunk diameters (`Diameter.Breast.Height`)
of the 5 trees with the largest diameters at breast height.
The output data frame must be sorted with respect to
`Diameter.Breast.Height`, decreasingly.

```{r df_ex1b,echo=-c(1,3)}
.. <- FALSE ############ HIDDEN
widest_trees <- ..
widest_trees <- head(urban_forest[order(urban_forest$Diameter.Breast.Height, decreasing=TRUE),
c("CoM.ID", "Common.Name", "Diameter.Breast.Height")], 5) ############ HIDDEN
print(widest_trees)
```
{ END exercise }

{ BEGIN exercise }
Create a new data frame that gives the number of trees planted in each year.
Recall (from the dataset's description) that pre-2003 data might be inaccurate.


```{r df_ex1c,echo=-c(2,3)}
yearly_trees <- ..
yearly_trees <- as.data.frame(table(urban_forest$Year.Planted)) ############ HIDDEN
names(yearly_trees) <- c("Year.Planted", "Count") ############ HIDDEN
head(yearly_trees)
```
{ END exercise }



{ BEGIN exercise }
Depict the post-2003 tree count data on a plot
similar to the one in Figure \@ref(fig:df-ex1d).
Is there any tendency in the data?

```{r df-ex1d,echo=FALSE,fig.cap="The number of trees planted in the City of Melbourne each year"}
yearly_trees[[1]] <- as.numeric(as.character(yearly_trees[[1]]))
plot(yearly_trees[[1]][yearly_trees[[1]]>=2003],
     yearly_trees[[2]][yearly_trees[[1]]>=2003],
     type="b", lty=3, xlab="Year.Planted", ylab="Count")
```
{ END exercise }



{ BEGIN exercise }
The City of Melbourne released the aforementioned dataset as part of
the Open Data Platform, see https://data.melbourne.vic.gov.au.
In your own words answer the following questions.

* What are the benefits of sharing such data
with the general public?
* Are there any concerns?
* What can a machine learning engineer do with them?
{ END exercise }




## Air Quality


In this task, we will be working with the 2018 air quality data in the
state of Victoria, Australia.
Read the dataset's description in Appendix F. <!-- TODO: -->

```{r df_ex2a}
air_quality <- read.csv("datasets/air_quality.csv.gz",
    comment.char="#")
head(air_quality, 3)
```


{ BEGIN exercise }
Create a new dataframe name `gee` that consists
of air quality data for `Geelong South` only.
Omit all columns but `sample_datetime`, `param_id` and `value`.

```{r df_ex2b,echo=-c(1,3)}
.. <- FALSE ###### HIDDEN
gee <- ..
gee <- air_quality[air_quality$sp_name=="Geelong South", c("sample_datetime", "param_id", "value")]  ###### HIDDEN
head(gee, 14)
```

```{r df_ex2c,echo=FALSE,results='hide'}
params <- table(gee$param_id)
colz <- c("BPM2.5", "NO2", "O3", "PM10")
```
{ END exercise }



{ BEGIN exercise }
There are `r length(params)` air quality parameters:
`r paste(names(params), collapse=", ")`. However, currently the data frame
is in the "long" format -- each measurement is represented in a separate row
and there are many rows corresponding to the same dates.
Convert the data frame to the "wide" format so as to obtain:

```{r df_ex2d,echo=-(2:3)}
gee_wide <- ..
library("tidyr") ###### HIDDEN
gee_wide <- as.data.frame(pivot_wider(gee, names_from="param_id", values_from="value")) ###### HIDDEN
head(gee_wide, 3)
```

Missing measurements should be represented as `NA`s.
By the way, conversion from "long" to "wide" format is also called
"spreading" or "casting".
{ END exercise }



{ BEGIN exercise }
Note that the `sample_datetime` column consists of ordinary strings.
Using the `strptime()` function (see `?strptime`), convert it
to proper date-time data:

```{r df_ex2e,echo=-c(2,4)}
class(gee_wide$sample_datetime) # before
old <- gee_wide$sample_datetime ###### HIDDEN
gee_wide$sample_datetime <- ..
gee_wide$sample_datetime <- strptime(old, "%d/%m/%Y %H:%M:%S") ###### HIDDEN
class(gee_wide$sample_datetime) # after
head(gee_wide, 3)
```

{ END exercise }


Technical note.

: `POSIXlt` means "local time", a special format returned
by `strptime()` to represent date-time information, see `?POSIXlt`.


{ BEGIN exercise }
Below we extract month numbers from each date and add them as a new column
in `gee_wide` (see `?strftime`):

```{r df-ex2f}
gee_wide$month <- strftime(gee_wide$sample_datetime, "%m")
```


Compute the monthly averages
of the `r length(params)` parameters.


```{r df-ex2g,echo=-c(2,3)}
gee_ave <- ..
options(digits=3) ###### HIDDEN
gee_ave <- aggregate(gee_wide[2:8], gee_wide[9], mean, na.rm=TRUE) ###### HIDDEN
gee_ave
```
{ END exercise }




{ BEGIN exercise }
Draw a plot of the monthly averages of
`r paste(colz, collapse=', ')`. Add a legend by calling `legend()`,
compare Figure \@ref(fig:df-ex2h).

```{r df-ex2h,echo=FALSE,fig.cap="Monthly averages of the air quality parameters in Geelong, VIC, Australia"}
matplot(gee_ave$month, as.matrix(gee_ave[c(colz)]), type="l",
xlab="Month", ylab="Measurement", ylim=c(0,25))
legend("bottom", ncol=4, lty=seq_along(colz), bg="white",
    col=seq_along(colz), legend=colz)
```
{ END exercise }



{ BEGIN exercise }
In your own words answer the following questions.

* What can be deduced from the above plot?
* How machine learning algorithms could help different government
bodies predict the air quality?
* What can they do with this information?
{ END exercise }






## Further Reading



Recommended further reading: [@Rintro]

Other: [@rprogdatascience], [@r4ds]

R packages dplyr and data.table implement the most common
data frame wrangling procedures. You may find them very useful.
Moreover, they are very fast even for large data sets.
Additionally, the magrittr package provides a pipe operator, `%>%`,
that simplifies the writing of complex, nested function calls.
Do note that not everyone is a big fan of these, however.



{ LATEX \color{gray} }

. . .

**TODO** .....


. . .

{ LATEX \normalcolor }


Next chapter...

