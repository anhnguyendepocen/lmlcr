# Multiple Regression

<!-- (C) 2020 Marek Gagolewski, https://www.gagolewski.com -->


## Introduction

### Formalism

---

Let $\mathbf{X}\in\mathbb{R}^{n\times p}$ be an input matrix
that consists of $n$ points in a $p$-dimensional space.

In other words, we have a database on $n$ objects, each of which
being described by means of $p$ numerical features.

\[
\mathbf{X}=
\left[
\begin{array}{cccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} \\
\end{array}
\right]
\]

---


Recall that in supervised learning,
apart from $\mathbf{X}$, we are also given the corresponding $\mathbf{y}$;
with each input point $\mathbf{x}_{i,\cdot}$ we associate the desired output $y_i$.

In this chapter we are still interested in  **regression** tasks;
hence, we assume that each $y_i$
it is a real number, i.e., $y_i\in\mathbb{R}$.

Hence, our dataset is $[\mathbf{X}\ \mathbf{y}]$ --
where each object is represented as a row vector
$[\mathbf{x}_{i,\cdot}\ y_i]$, $i=1,\dots,n$:

\[
[\mathbf{X}\ \mathbf{y}]=
\left[
\begin{array}{ccccc}
x_{1,1} & x_{1,2} & \cdots & x_{1,p} & y_1\\
x_{2,1} & x_{2,2} & \cdots & x_{2,p} & y_2\\
\vdots & \vdots & \ddots & \vdots    & \vdots\\
x_{n,1} & x_{n,2} & \cdots & x_{n,p} & y_n\\
\end{array}
\right].
\]


### Simple Linear Regression - Recap

---

In a simple regression task, we have assumed that $p=1$ -- there is only
one independent variable,
denoted $x_i=x_{i,1}$.

We restricted ourselves to linear models of the form $Y=f(X)=aX+b$ that minimised the  sum of squared residuals (SSR), i.e.,


\[
\min_{a,b\in\mathbb{R}} \sum_{i=1}^n \left(
ax_i+b-y_i
\right)^2.
\]

The solution is:

\[
\left\{
\begin{array}{rl}
a^*  = & \dfrac{
n \displaystyle\sum_{i=1}^n x_i y_i - \displaystyle\sum_{i=1}^n  y_i \displaystyle\sum_{i=1}^n x_i
}{
n \displaystyle\sum_{i=1}^n x_i x_i -   \displaystyle\sum_{i=1}^n x_i\displaystyle\sum_{i=1}^n x_i
}\\
b^* = & \dfrac{1}{n}\displaystyle\sum_{i=1}^n  y_i - a^*  \dfrac{1}{n} \displaystyle\sum_{i=1}^n x_i  \\
\end{array}
\right.
\]


---

Fitting in R:

```{r}
library("ISLR") # Credit dataset
X <- as.numeric(Credit$Balance[Credit$Balance>0])
Y <- as.numeric(Credit$Rating[Credit$Balance>0])
f <- lm(Y~X) # Y~X is a formula, read: Y is a function of X
print(f)
```


---

```{r,echo=-1}
par(mar=c(4,4,0.5,0.5))
plot(X, Y, col="#000000aa", las=1)
abline(f, col=2, lwd=3)
```



## Multiple Linear Regression

### Problem Formulation

---

Let's now generalise the above to the case of many variables
$X_1, \dots, X_p$.

We wish to model the dependent variable as a function of $p$ independent variables.
\[
Y = f(X_1,\dots,X_p)   \qquad (+\varepsilon)
\]

Restricting ourselves to the class of **linear models**, we have
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p.
\]

Above we studied the case where $p=1$ with $\beta_1=a$ and $\beta_0=b$.


---


The above equation defines:

- $p=1$ --- a line
- $p=2$ --- a plane
- $p\ge 3$ --- a hyperplane

Most people find it difficult to imagine objects in high dimensions,
but we are lucky to have this thing called maths.



```{r,echo=FALSE,scatterplot3dexamplefit}
X1 <- as.numeric(Credit$Balance[Credit$Balance>0])
X2 <- as.numeric(Credit$Income[Credit$Balance>0])
Y  <- as.numeric(Credit$Rating[Credit$Balance>0])
f <- lm(Y~X1+X2)
```

```{r,echo=FALSE,scatterplot3dexample}
par(mar=c(4, 4, 0.5, 0.5))
library("scatterplot3d")
s3d <- scatterplot3d(X1, X2, Y,
  angle=60, # change angle to reveal more
  highlight.3d=TRUE, xlab="Balance", ylab="Income",
  zlab="Credit Rating", las=1)
s3d$plane3d(f, lty.box="solid")
```



### Fitting a Linear Model in R

---

`lm()` accepts a formula of the form `Y~X1+X2+...+Xp`.

It finds the least squares fit, i.e., solves
\[
\min_{\beta_0, \beta_1,\dots, \beta_p\in\mathbb{R}}
\sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_i \right) ^2
\]

```{r}
<<scatterplot3dexamplefit>>
f$coefficients # ß0, ß1, ß2
```

---

By the way, the above 3D scatter plot was generated by calling:

```{r,fig.keep='none',eval=FALSE}
<<scatterplot3dexample>>
```

(`s3d` is an R list, one of its elements named `plane3d` is a function object -- this is legal)




## Finding the Best Model

### Model Diagnostics

---


Consider the three following models.


Formula                   |  Equation
--------------------------|----------------------------------------------------
Rating ~ Balance + Income |  $Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2$
Rating ~ Balance          |  $Y=a X_1 + b$ ($\beta_0=b, \beta_1=a, \beta_2=0$)
Rating ~ Income           |  $Y=a X_2 + b$ ($\beta_0=b, \beta_1=0, \beta_2=a$)

---

```{R}
f12 <- lm(Y~X1+X2) # Rating ~ Balance + Income
f12$coefficients
f1  <- lm(Y~X1)    # Rating ~ Balance
f1$coefficients
f2  <- lm(Y~X2)    # Rating ~ Income
f2$coefficients
```

Which of the three models is the best?

---


"Best" --- with respect to what kind of measure?


So far we were fitting w.r.t. SSR,
as the multiple regression model generalise the two simple ones,
the former must yield a not-worse SSR.

```{r}
sum(f12$residuals^2)
sum(f1$residuals^2)
sum(f2$residuals^2)
```

We get that $f_{12} \succeq f_{2} \succeq f_{1}$ but these error values
are meaningless.

> Interpretability in ML has always been an important issue, think the EU GDPR, amongst others.

---

The quality of fit can be assessed by performing some descriptive
statistical analysis of the residuals, $\hat{y}_i-y_i$.

Interestingly, the mean of residuals (this can be shown analytically)
in the least squared fit is always equal to $0$:
\[
 \frac{1}{n} \sum_{i=1}^n (\hat{y}_i-y_i)=0.
\]
Therefore, we need a different metric.

> (\*) A proof of this fact is left as an exercise to the curious;
assume $p=1$ just as in the previous chapter and note that $\hat{y}_i=a x_i+b$.

```{r}
mean(f12$residuals) # almost zero numerically
all.equal(mean(f12$residuals), 0)
```


---

Sum of squared residuals (SSR) is not interpretable, but the mean squared residuals
(MSR) -- also called mean squared error (MSE) regression loss -- is a little better.

Recall that mean is defined as the sum divided by number of samples.

\[
 \mathrm{MSE}(f) = \frac{1}{n} \sum_{i=1}^n (f(\mathbf{x}_{i,\cdot})-y_i)^2.
\]


```{r}
mean(f12$residuals^2)
mean(f1$residuals^2)
mean(f2$residuals^2)
```




---

However, if the original $Y$s are, say, in metres $[\mathrm{m}]$,
MSE is expressed in metres squared $[\mathrm{m}^2]$.

Root mean squared error (RMSE):
\[
 \mathrm{RMSE}(f) = \sqrt{\frac{1}{n} \sum_{i=1}^n (f(\mathbf{x}_{i,\cdot})-y_i)^2}.
\]

```{r}
sqrt(mean(f12$residuals^2))
sqrt(mean(f1$residuals^2))
sqrt(mean(f2$residuals^2))
```


This is just like with the sample variance compare vs. standard deviation --
the latter is defined as the square root of the former.

Interpretation: average deviance from the true rating
(which is on the scale 0--1000, hence we see that the first model is not
that bad).


---

Still there's a problem with interpreting these values.

Mean absolute error (MAE) might be a better idea then:
\[
 \mathrm{MAE}(f) = \frac{1}{n} \sum_{i=1}^n |f(\mathbf{x}_{i,\cdot})-y_i|.
\]

```{r}
mean(abs(f12$residuals))
mean(abs(f1$residuals))
mean(abs(f2$residuals))
```

"On average, the predicted rating differs from the observed one by\dots"



---

Descriptive statistics for residuals:

```{r}
summary(f12$residuals)
summary(f1$residuals)
summary(f2$residuals)
```

---

The outputs include:

- `Min.` -- sample minimum
- `1st Qu.` -- 1st quartile == 25th percentile == quantile of order 0.25
- `Median` -- median == 50th percentile == quantile of order 0.5
- `3rd Qu.` -- 3rd quartile = 75th percentile == quantile of order 0.75
- `Max.` --  sample maximum

See `?quantile` in R.

For example, 1st quartile is the observation $q$ such that
25\% values are $\le q$ and 75\% values are $\ge q$.

---

**Box and whisker plot**:

```{r,echo=FALSE}
par(mar=c(4,0.5,0.5,0.5))
x <- f1$residuals
boxplot(x, las=1, horizontal=TRUE, xlim=c(-0.1, 2.1), ylim=c(-300, 300))


q1 <- quantile(x, 0.25)
q2 <- quantile(x, 0.5)
q3 <- quantile(x, 0.75)
iqr <- q3-q1

segments(q2, 0, q2, 1, lty=3)
text(q2, 0, "Median")

segments(q1, 2, q1, 1, lty=3)
text(q1, 2, "1st Qu. (Q1)")

segments(q3, 1.8, q3, 1, lty=3)
text(q3, 1.8, "3rd Qu. (Q3)")


segments(min(x), 2, min(x), 1, lty=3)
text(min(x), 2, "Min.")


segments(max(x), 1.8, max(x), 1, lty=3)
text(max(x), 1.8, "Max.")

segments(q3+1.5*iqr, 2, sort(x[x>q3+1.5*iqr])[c(1, 5, 10)], 1, lty=3)
text(q3+1.5*iqr, 2, "(potential) outliers")

segments(q3+1.5*iqr, 0, q3+1.5*iqr, 1, lty=3)
text(q3+1.5*iqr, 0, "Q3 + 1.5 IQR")

segments(q1-1.5*iqr, 0, q1-1.5*iqr, 1, lty=3)
text(q1-1.5*iqr, 0, "Q1 - 1.5 IQR")
```

* IQR == Interquartile range == Q3$-$Q1 (box width)
* The box contains 50\% of the "most typical" observations
* Box and whiskers altogether have width $\le$ 4 IQR
* Outliers == observations potentially worth inspecting (is it a bug or a feature?)

---



A picture is worth a thousand words:

```{r,echo=-1}
par(mar=c(4,2.5,0.5,0.5))
boxplot(las=1, horizontal=TRUE, xlab="residuals",
  list(f12=f12$residuals, f1=f1$residuals, f2=f2$residuals))
abline(v=0, lty=3)
```



---

*Violin plot* -- a blend of a box plot and a (kernel) density estimator (histogram-like):

```{r,echo=-1,message=FALSE}
par(mar=c(4,2,0.5,0.5))
library("vioplot")
vioplot(las=1, horizontal=TRUE, xlab="residuals",
  list(f12=f12$residuals, f1=f1$residuals, f2=f2$residuals))
abline(v=0, lty=3)
```


---

By the way, this is Rating ($Y$) as function of Balance ($X_1$, top subfigure)
and Income ($X_2$, bottom subfigure).

```{r,echo=FALSE,fig.height=6}
par(mfrow=c(2,1))
par(mar=c(4,4,0.5, 0.5))
plot(X1, Y, las=1, col="#00000044")
abline(f1, col=2,lwd=3)
plot(X2, Y, las=1, col="#00000044")
abline(f2, col=2,lwd=3)
```

---

Descriptive statistics for absolute values of residuals:

```{r}
summary(abs(f12$residuals))
summary(abs(f1$residuals))
summary(abs(f2$residuals))
```




---

This picture is worth \$1000:

```{r,echo=-1}
par(mar=c(4,2,0.5,0.5))
boxplot(las=1, horizontal=TRUE, xlab="abs(residuals)",
  list(f12=abs(f12$residuals), f1=abs(f1$residuals),
       f2=abs(f2$residuals)))
abline(v=0, lty=3)
```

---


The (unadjusted) **$R^2$ score** (the coefficient of determination):

\[
R^2(f) = 1 - \frac{\sum_{i=1}^{n} \left(y_i-f(\mathbf{x}_{i,\cdot})\right)^2}{\sum_{i=1}^{n} \left(y_i-\bar{y}\right)^2},
\]
where $\bar{y}$ is the arithmetic mean $\frac{1}{n}\sum_{i=1}^n y_i$.

```{R}
(r12 <- 1 - sum(f12$residuals^2)/sum((Y-mean(Y))^2) )
(r1  <- 1 - sum(f1$residuals^2)/sum((Y-mean(Y))^2)  )
(r2  <- 1 - sum(f2$residuals^2)/sum((Y-mean(Y))^2)  )
```

$R^2(f)\simeq 1$ indicates a perfect fit -- it is the
proportion of variance of the dependent variable explained by independent variables in the model.

---

Unfortunately, $R^2$ tends to automatically increase as the number of independent variables
increase.

To correct for this phenomenon, we can consider the **adjusted $R^2$**:

\[
\bar{R}^2(f) = 1 - (1-{R}^2(f))\frac{n-1}{n-p-1}
\]

```{R}
n <- length(x)
1 - (1 - r12)*(n-1)/(n-3)
1 - (1 - r1 )*(n-1)/(n-2)
1 - (1 - r2 )*(n-1)/(n-2)
```

The adjusted $R^2$ penalises for more complex models.

---



> (\*) Side note -- results of some statistical tests (e.g., significance of coefficients)
are reported by calling `summary(f12)` etc. --- refer to a more advanced source to obtain more information.
These, however, require the verification of some assumptions regarding the input data
and the residuals.

---

#### {.allowframebreaks .unnumbered}

```{r}
summary(f12)
```




### Variable Selection


---


Consider all quantitative (numeric-continuous) variables in the `Credit` data set.

```{r}
C <- Credit[Credit$Balance>0,
    c("Rating", "Limit", "Income", "Age",
      "Education", "Balance")]
head(C)
```


Let's draw a *pair plot* -- a matrix of scatter plots for every pair of variables:

```{r, fig.keep='none',eval=FALSE}
pairs(C)
```

---

```{r, fig.height=5, echo=FALSE}
par(mar=c(2,2,2,2))
pairs(C)
```

---

Seems like `Rating` depends on `Limit` almost linearly...

Pearson's $r$ -- linear correlation coefficient:

\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y})
}{
    \sqrt{\sum_{i=1}^n (x_i-\bar{x})^2} \sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}
}.
\]

It holds $r\in[-1,1]$, where:

* $r=1$ -- positive linear dependence ($y$ increases as $x$ increases)
* $r=-1$ -- negative linear dependence ($y$ decreases as $x$ increases)
* $r\simeq 0$ -- uncorrelated or non-linearly dependent

<!-- TODO anscombe -->



---

Interpretation:

```{r,echo=FALSE,fig.height=6}
par(mfrow=c(2,2))
par(mar=rep(0.3,4))
set.seed(123)
x <- runif(25)

y <- 7*x+3+rnorm(25,0,0.5)
plot(x, y, las=1, axes=FALSE)
legend("topleft", bg="white", legend=c(sprintf("r=%.2f", cor(x,y)), "positive correlation"))
box()

y <- -7*x+3+rnorm(25,0,0.5)
plot(x, y, las=1, axes=FALSE)
legend("topright", bg="white", legend=c(sprintf("r=%.2f", cor(x,y)), "negative correlation"))
box()

y <- runif(25)
plot(x, y, las=1, axes=FALSE)
legend("topleft", bg="white", legend=c(sprintf("r=%.2f", cor(x,y)), "no correlation"))
box()

x <- runif(25)
y <- abs((x-0.5)*100)+rnorm(length(x))
plot(x, y, las=1, axes=FALSE)
legend("top", bg="white", legend=c(sprintf("r=%.2f", cor(x,y)), "non-linear correlation"))
box()

```


---

Compute Pearson's $r$ between all pairs of variables:

```{r}
round(cor(C), 3)
```


`Rating` and `Limit` are almost perfectly linearly correlated,
and both seem to describe the same thing.

For practical purposes, we'd rather model `Rating` as a function of the other variables.

For simple linear regression models, we'd choose either `Income` or `Balance`.

> How about multiple regression?

---

The best model:

* has high predictive power,
* is simple.

These are often mutually exclusive.


Which variables should be included in the optimal model?

---

Again, the definition of the "best" object needs a *fitness* function.

For fitting a single model to data, we use the SSR.

We need a metric that takes the number of dependent variables into account.

It turns out that the adjusted $R^2$, despite its interpretability,
is not suitable for this task.

---

Instead, here we'll be using **the Akaike Information Criterion** (AIC).

For a model $f$ with $p$ independent variables:
\[
\mathrm{AIC}(f) = 2(p+1)+n\log(\mathrm{SSR}(f)/n)
\]

Our task is to find the combination of independent variables
that minimises the AIC.

For $p$ variables, the number of possible combinations is $2^p$
(grows exponentially with $p$).

For large $p$, an extensive search is impractical.

---

Therefore, to find the variable combination minimising the AIC,
we often rely on one of the two following greedy heuristics:

- forward selection:

    1. start with an empty model
    2. find an independent variable
whose addition to the current model would yield the highest decrease in the AIC and add it to the model
    3. go to step 2 until AIC decreases

- backward elimination:

    1. start with the full model
    2. find an independent variable
whose removal from the current model would  decrease the AIC the most and eliminate it from the model
    3. go to step 2 until AIC decreases


> (\*) There are of course many other methods, e.g., lasso regression
whose side effect is variable selection.

---

#### {.allowframebreaks .unnumbered}

Forward selection example:

```{r}
C <- Credit[Credit$Balance>0,
    c("Rating", "Income", "Age",
      "Education", "Balance")]
step(lm(Rating~1, data=C), # empty model
    scope=formula(lm(Rating~., data=C)), # full model
    direction="forward")
```


#### {.allowframebreaks .unnumbered}


Backward elimination example:

```{r}
step(lm(Rating~., data=C), # full model
     scope=formula(lm(Rating~1, data=C)), # empty model
     direction="backward")
```



#### {.allowframebreaks .unnumbered}


Forward selection example -- full dataset:

```{r}
C <- Credit[,  # do not restrict to Credit$Balance>0
    c("Rating", "Income", "Age",
      "Education", "Balance")]
step(lm(Rating~1, data=C), # empty model
    scope=formula(lm(Rating~., data=C)), # full model
    direction="forward")
```


#### {.allowframebreaks .unnumbered}


Backward elimination example -- full dataset:


```{r}
step(lm(Rating~., data=C), # full model
     scope=formula(lm(Rating~1, data=C)), # empty model
     direction="backward")
```


### Variable Transformation

---

So far we have been fitting linear models of the form:
\[
Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p.
\]


What about some non-linear models such as polynomials etc.? For example:
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_1^3 + \beta_4 X_2.
\]

. . .


Solution: pre-process inputs by setting
$X_1' := X_1$, $X_2' := X_1^2$, $X_3' := X_1^3$, $X_4' := X_2$
and fit a linear model:

\[
Y = \beta_0 + \beta_1 X_1' + \beta_2 X_2' + \beta_3 X_3' + \beta_4 X_4'.
\]


This trick works for every model of the form
$Y=\sum_{i=1}^k \sum_{j=1}^p \varphi_{i,j}(X_j)$ for any $k$
and any univariate functions $\varphi_{i,j}$.


---

Also, with a little creativity (and maths), we might be able to transform
a few other models to a linear one, e.g.,

\[
Y = b e^{aX} \qquad \to \qquad \log Y = \log b + aX \qquad\to\qquad Y'=aX+b'
\]

This is an example of a model's **linearisation**.

---




For example, here's a series of simple polynomial regression models
of the form `Rating~poly(Balance)`:

```{r}
f1_1 <- lm(Y~X1)
f1_3 <- lm(Y~X1+I(X1^2)+I(X1^3)) # also: Y~poly(X1, 3)
f1_14 <- lm(Y~poly(X1, 14))
```

---

```{r,echo=-1}
par(mar=c(4,4,0.5,0.5))
plot(X1, Y, las=1, col="#000000aa")
x <- seq(min(X1), max(X1), length.out=101)
lines(x, predict(f1_1, data.frame(X1=x)), col="red", lwd=3)
lines(x, predict(f1_3, data.frame(X1=x)), col="blue", lwd=3)
lines(x, predict(f1_14, data.frame(X1=x)), col="darkgreen", lwd=3)
```




### Predictive vs. Descriptive Power

---

The above high-degree polynomial model (`f1_14`) is a clear example of an **overfit**.

Clearly (based on our expert knowledge), the `Rating` shouldn't decrease as Balance increases.


In other words, `f1_14` gives a better fit to data actually observed,
but fails to produce good results for the points that are yet to come.

We say that it **generalises** poorly to unseen data.


---


Assume our true model is of the form:

```{r}
true_model <- function(x) 3*x^3+5
```

And we generate the following random sample from this model (with $Y$ subject
to error):

```{r}
n <- 25
X <- runif(n, min=0, max=1)
Y <- true_model(X)+rnorm(n, sd=0.1)
```

```{r,figBIASVARIANCE1,fig.keep='none'}
plot(X, Y, las=1)
x <- seq(0, 1, length.out=101)
lines(x, true_model(x), col=2, lwd=3, lty=2)
```

---

```{r,echo=FALSE}
par(mar=c(4,4,0.5,0.5))
<<figBIASVARIANCE1>>
```


---

Let's fit polynomials of different degrees:

```{r,figBIASVARIANCE2,fig.keep='none'}
plot(X, Y, las=1)
lines(x, true_model(x), col=2, lwd=3, lty=2)

dmax <- 15 # maximal polynomial degree
MSE_train <- numeric(dmax)
MSE_test  <- numeric(dmax)
for (d in 1:dmax) { # for every polynomial degree
    f <- lm(Y~poly(X, d)) # fit a d-degree polynomial
    y <- predict(f, data.frame(X=x))
    lines(x, y, col=d)
    # MSE on given random X,Y:
    MSE_train[d] <- mean(f$residuals^2)
    # MSE on many more points:
    MSE_test[d]  <- mean((y-true_model(x))^2)
}
```

---

```{r,echo=FALSE}
par(mar=c(4,4,0.5,0.5))
<<figBIASVARIANCE2>>
```



---

Compare the mean squared error (MSE) for the observed vs. future data points:

```{r,figBIASVARIANCE3,warning=FALSE,fig.keep='none'}
matplot(1:dmax, cbind(MSE_train, MSE_test), type='b',
    las=1, ylim=c(1e-3, 1e3), log="y", pch=1:2,
    xlab='Model complexity (degree of polynomial)',
    ylab="MSE")
legend("topleft", legend=c("MSE train", "MSE test"),
    lty=1:2, col=1:2, pch=1:2)
```


Note the logarithmic scale on the $y$ axis.

---

```{r,echo=FALSE}
par(mar=c(4,4,0.5,0.5))
<<figBIASVARIANCE3>>
```

---

This is a very typical behaviour!

- A model's fit to observed data improves as the model's complexity increases.

- A model's generalisation to unseen data initially improves, but then becomes worse.

- In the above example, the sweet spot is at a polynomial of degree 3, which is exactly
our true underlying model.


---


Hence, most often **we should be interested in the accuracy of the predictions
made in the case of unobserved data.**



If we have a data set of a considerable size,
we can divide it (randomly) into two parts:

- *training sample* (say, 60\% or 80\%) -- used to fit a model
- *test sample* (the remaining 40\% or 20\%) -- used to assess its quality
(e.g., using MSE)

More on this issue in the chapter on Classification.

> (\*) We shall see that sometimes a train-test-validate split will be necessary,
e.g., 60-20-20\%.






## Outro

### Remarks

---

Multiple regression is simple, fast to apply and interpretable.


Linear models go beyond fitting of straight lines and other hyperplanes!


A complex model may overfit and hence generalise poorly to unobserved inputs.


Note that the SSR criterion makes the models sensitive to outliers.



---

**Remember:**

good models
\[=\]
better understanding of the modelled reality $+$ better predictions
\[=\]
more revenue, your boss' happiness, your startup's growth etc.





### Other Methods for Regression

---


Other example approaches to regression:

- ridge regression,
- lasso regression,
- least absolute deviations (LAD) regression,
- multiadaptive regression splines (MARS),
- K-nearest neighbour (K-NN) regression, see `FNN::knn.reg()` in R,
- regression trees,
- support-vector regression (SVR),
- neural networks (also deep) for regression.




###  Derivation of the Solution  (\*\*)

---

We would like to find an analytical solution
to the problem of minimising of the sum of squared residuals:

\[
\min_{\beta_0, \beta_1,\dots, \beta_p\in\mathbb{R}} E(\beta_0, \beta_1, \dots, \beta_p)=
\sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)^2
\]

This requires computing the $p+1$ partial derivatives
${\partial E}/{\partial \beta_j}$ for $j=0,\dots,p$.


---

The partial derivatives are very similar to each other;
$\frac{\partial E}{\partial \beta_0}$ is given by:
\[
\frac{\partial E}{\partial \beta_0}(\beta_0,\beta_1,\dots,\beta_p)=
2 \sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)
\]
and $\frac{\partial E}{\partial \beta_j}$ for $j>0$ is equal to:
\[
\frac{\partial E}{\partial \beta_j}(\beta_0,\beta_1,\dots,\beta_p)=
2 \sum_{i=1}^n x_{i,j} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)
\]

---

Then all we need to do is to solve the system of linear equations:


\[
\left\{
\begin{array}{rcl}
\frac{\partial E}{\partial \beta_0}(\beta_0,\beta_1,\dots,\beta_p)&=&0 \\
\frac{\partial E}{\partial \beta_1}(\beta_0,\beta_1,\dots,\beta_p)&=&0 \\
\vdots\\
\frac{\partial E}{\partial \beta_p}(\beta_0,\beta_1,\dots,\beta_p)&=&0 \\
\end{array}
\right.
\]


---

The above system of $p+1$ linear equations, which we are supposed to solve
for $\beta_0,\beta_1,\dots,\beta_p$:
\[
\left\{
\begin{array}{rcl}
2 \sum_{i=1}^n \phantom{x_{i,0}}\left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)&=&0 \\
2 \sum_{i=1}^n x_{i,1} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)&=&0 \\
\vdots\\
2 \sum_{i=1}^n x_{i,p} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)&=&0 \\
\end{array}
\right.
\]
can be rewritten as:
\[
\left\{
\begin{array}{rcl}
\sum_{i=1}^n \phantom{x_{i,0}}\left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}\right)&=& \sum_{i=1}^n \phantom{x_{i,0}} y_i \\
\sum_{i=1}^n x_{i,1} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}\right)&=&\sum_{i=1}^n x_{i,1} y_i \\
\vdots\\
\sum_{i=1}^n x_{i,p} \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p}\right)&=&\sum_{i=1}^n x_{i,p} y_i \\
\end{array}
\right.
\]

---

and further as:
\[
\left\{
\begin{array}{rcl}
\beta_0\ n\phantom{\sum_{i=1}^n x} + \beta_1\sum_{i=1}^n \phantom{x_{i,0}}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n \phantom{x_{i,0}}  x_{i,p} &=&\sum_{i=1}^n\phantom{x_{i,0}} y_i \\
\beta_0 \sum_{i=1}^n x_{i,1} + \beta_1\sum_{i=1}^n x_{i,1}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n x_{i,1}  x_{i,p} &=&\sum_{i=1}^n x_{i,1} y_i \\
\vdots\\
\beta_0 \sum_{i=1}^n x_{i,p} + \beta_1\sum_{i=1}^n x_{i,p}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n x_{i,p}  x_{i,p} &=&\sum_{i=1}^n x_{i,p} y_i \\
\end{array}
\right.
\]
Note that the terms involving $x_{i,j}$ and $y_i$ (the sums) are all constant
-- these are some fixed real numbers. We have learned how to solve such
problems in high school.

> Try deriving the analytical solution and implementing it for $p=2$.
Recall that in the previous chapter we solved the special case of $p=1$.




###  Solution in Matrix Form (\*\*\*)

---


Assume that $\mathbf{X}\in\mathbb{R}^{n\times p}$ (a matrix with inputs),
$\mathbf{y}\in\mathbb{R}^{n\times 1}$ (a column vector of reference outputs)
and
$\boldsymbol{\beta}\in\mathbb{R}^{(p+1)\times 1}$ (a column vector of parameters).

Firstly, note that a linear model of the form:
\[
f_{\boldsymbol\beta}(\mathbf{x})=\beta_0+\beta_1 x_1+\dots+\beta_p x_p
\]
can be rewritten as:
\[
f_{\boldsymbol\beta}(\mathbf{x})=\beta_0 1+\beta_1 x_1+\dots+\beta_p x_p
=\mathbf{\dot{x}}\boldsymbol\beta,
\]
where $\mathbf{\dot{x}}=[1\ x_1\ x_2\ \cdots\ x_p]$.

. . .

Similarly, if we assume that $\mathbf{\dot{X}}=[\boldsymbol{1}\ \mathbf{X}]\in\mathbb{R}^{n\times (p+1)}$
is the input matrix with a prepended column of $1$s, i.e.,
$\boldsymbol{1}=[1\ 1\ \cdots\ 1]^T$ and $\dot{x}_{i,0}=1$ (for brevity of notation
the columns added will have index $0$),
$\dot{x}_{i,j}=x_{i,j}$ for all $j\ge 1$ and all $i$,
then:
\[
\mathbf{\hat{y}} = \mathbf{\dot{X}} \boldsymbol\beta
\]
gives the vector of predicted outputs for every input point.

---

This way, the sum of squared residuals
\[
E(\beta_0, \beta_1, \dots, \beta_p)=
\sum_{i=1}^n \left( \beta_0 + \beta_1 x_{i,1}+\dots+\beta_p x_{i,p} - y_{i} \right)^2
\]
can be rewritten as:
\[
E(\boldsymbol\beta)=\| \mathbf{\dot{X}} \boldsymbol\beta - \mathbf{y} \|^2,
\]
where as usual $\|\cdot\|^2$ denotes the squared Euclidean norm.

Recall that this can be re-expressed as:
\[
E(\boldsymbol\beta)= (\mathbf{\dot{X}} \boldsymbol\beta - \mathbf{y})^T (\mathbf{\dot{X}} \boldsymbol\beta - \mathbf{y}).
\]

---

In order to find the minimum of $E$ w.r.t. $\boldsymbol\beta$,
we need to find the parameters that make the partial derivatives vanish, i.e.:

\[
\left\{
\begin{array}{rcl}
\frac{\partial E}{\partial \beta_0}(\boldsymbol\beta)&=&0 \\
\frac{\partial E}{\partial \beta_1}(\boldsymbol\beta)&=&0 \\
\vdots\\
\frac{\partial E}{\partial \beta_p}(\boldsymbol\beta)&=&0 \\
\end{array}
\right.
\]

> (\*\*\*) Interestingly, the above can also be expressed in matrix form,
using the special notation:
\[
\nabla E(\boldsymbol\beta) = \boldsymbol{0}
\]
Here, $\nabla E$ (nabla symbol = differential operator)
denotes the function gradient, i.e., the vector of all partial derivatives.
This is nothing more than syntactic sugar for this quite commonly applied operator.

---

Anyway,  the system of linear equations we have derived above:
\[
\left\{
\begin{array}{rcl}
\beta_0\ n\phantom{\sum_{i=1}^n x} + \beta_1\sum_{i=1}^n \phantom{x_{i,0}}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n \phantom{x_{i,0}}  x_{i,p} &=&\sum_{i=1}^n\phantom{x_{i,0}} y_i \\
\beta_0 \sum_{i=1}^n x_{i,1} + \beta_1\sum_{i=1}^n x_{i,1}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n x_{i,1}  x_{i,p} &=&\sum_{i=1}^n x_{i,1} y_i \\
\vdots\\
\beta_0 \sum_{i=1}^n x_{i,p} + \beta_1\sum_{i=1}^n x_{i,p}  x_{i,1}+\dots+\beta_p \sum_{i=1}^n x_{i,p}  x_{i,p} &=&\sum_{i=1}^n x_{i,p} y_i \\
\end{array}
\right.
\]
can be  rewritten in matrix terms as:
\[
\left\{
\begin{array}{rcl}
\beta_0 \mathbf{\dot{x}}_{\cdot,0}^T \mathbf{\dot{x}}_{\cdot,0} + \beta_1 \mathbf{\dot{x}}_{\cdot,0}^T \mathbf{\dot{x}}_{\cdot,1}+\dots+\beta_p \mathbf{\dot{x}}_{\cdot,0}^T \mathbf{\dot{x}}_{\cdot,p} &=& \mathbf{\dot{x}}_{\cdot,0}^T \mathbf{y} \\
\beta_0 \mathbf{\dot{x}}_{\cdot,1}^T \mathbf{\dot{x}}_{\cdot,0} + \beta_1 \mathbf{\dot{x}}_{\cdot,1}^T \mathbf{\dot{x}}_{\cdot,1}+\dots+\beta_p \mathbf{\dot{x}}_{\cdot,1}^T \mathbf{\dot{x}}_{\cdot,p} &=& \mathbf{\dot{x}}_{\cdot,1}^T \mathbf{y} \\
\vdots\\
\beta_0 \mathbf{\dot{x}}_{\cdot,p}^T \mathbf{\dot{x}}_{\cdot,0} + \beta_1 \mathbf{\dot{x}}_{\cdot,p}^T \mathbf{\dot{x}}_{\cdot,1}+\dots+\beta_p \mathbf{\dot{x}}_{\cdot,p}^T \mathbf{\dot{x}}_{\cdot,p} &=& \mathbf{\dot{x}}_{\cdot,p}^T \mathbf{y}\\
\end{array}
\right.
\]

---

This can be restated as:
\[
\left\{
\begin{array}{rcl}
\left(\mathbf{\dot{x}}_{\cdot,0}^T \mathbf{\dot{X}}\right)\, \boldsymbol\beta &=& \mathbf{\dot{x}}_{\cdot,0}^T \mathbf{y} \\
\left(\mathbf{\dot{x}}_{\cdot,1}^T \mathbf{\dot{X}}\right)\, \boldsymbol\beta  &=& \mathbf{\dot{x}}_{\cdot,1}^T \mathbf{y} \\
\vdots\\
\left(\mathbf{\dot{x}}_{\cdot,p}^T \mathbf{\dot{X}}\right)\, \boldsymbol\beta  &=& \mathbf{\dot{x}}_{\cdot,p}^T \mathbf{y}\\
\end{array}
\right.
\]
which in turn is equivalent to:
\[
\left(\mathbf{\dot{X}}^T\mathbf{X}\right)\,\boldsymbol\beta = \mathbf{\dot{X}}^T\mathbf{y}.
\]

Such a system of linear equations in matrix form can be solved numerically using,
amongst others, the `solve()` function.

> (\*\*\*) In practice, we'd rather rely on QR or SVD decompositions
of matrices for efficiency and numerical accuracy reasons.


---

Numeric example -- solution via `lm()`:

```{r}
X1 <- as.numeric(Credit$Balance[Credit$Balance>0])
X2 <- as.numeric(Credit$Income[Credit$Balance>0])
Y  <- as.numeric(Credit$Rating[Credit$Balance>0])
lm(Y~X1+X2)$coefficients
```

Recalling that $\mathbf{A}^T \mathbf{B}$ can be computed
by calling `t(A) %*% B` or -- even faster -- by calling `crossprod(A, B)`,
we can also use `solve()` to obtain the same result:

```{r}
X_dot <- cbind(1, X1, X2)
solve( crossprod(X_dot, X_dot), crossprod(X_dot, Y) )
```






### Pearson's r in Matrix Form (\*\*)

---

Recall the Pearson linear correlation coefficient:
\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y})
}{
    \sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}\ \sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}
}
\]

Denote with $\boldsymbol{x}^\circ$ and $\boldsymbol{y}^\circ$ the centred versions
of $\boldsymbol{x}$ and $\boldsymbol{y}$, respectively,
i.e., $x_i^\circ=x_i-\bar{x}$ and $y_i^\circ=y_i-\bar{y}$.

Rewriting the above yields:
\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \sum_{i=1}^n x_i^\circ y_i^\circ
}{
    \sqrt{\sum_{i=1}^n ({x_i^\circ})^2}\  \sqrt{\sum_{i=1}^n ({y_i^\circ})^2}
}
\]
which is exactly:
\[
r(\boldsymbol{x},\boldsymbol{y}) = \frac{
    \boldsymbol{x}^\circ\cdot \boldsymbol{y}^\circ
}{
    \| \boldsymbol{x}^\circ \|\    \| \boldsymbol{y}^\circ \|
}
\]
i.e., the normalised dot product of the centred versions of the two vectors.

This is the cosine of the angle between the two vectors
(in $n$-dimensional spaces)!

---

(\*\*) Recalling from the previous chapter that $\mathbf{A}^T \mathbf{A}$
gives the dot product between all the pairs of columns in a matrix $\mathbf{A}$,
we can implement an equivalent version of `cor(C)` as follows:

```{r}
C <- Credit[Credit$Balance>0,
    c("Rating", "Limit", "Income", "Age",
    "Education", "Balance")]
C_centred <- apply(C, 2, function(c) c-mean(c))
C_normalised <- apply(C_centred, 2, function(c)
    c/sqrt(sum(c^2)))
round(t(C_normalised) %*% C_normalised, 3)
```


### Further Reading

#### {.allowframebreaks .unnumbered}

Recommended further reading:

- [@islr: Chapters 1, 2 and 3]

Other:

- [@esl: Chapter 1, Sections 3.2 and 3.3]
